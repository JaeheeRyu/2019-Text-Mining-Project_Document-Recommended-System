9
1
0
2

v
o

N

1
2

]

V

C

.

s

c

[

3
v
9
5
5
2
0

.

1
1
9
1

:

v

i

X

r

a

SCL: Towards Accurate Domain Adaptive Object Detection via
Gradient Detach Based Stacked Complementary Losses

Zhiqiang Shen1 ∗, Harsh Maheshwari2∗ †, Weichen Yao1∗ , Marios Savvides1
1Carnegie Mellon University 2 Indian Institute of Technology, Kharagpur

{zhiqians,wyao2,marioss}@andrew.cmu.edu
harshmaheshwari135@gmail.com

Abstract

Unsupervised domain adaptive object detection aims
to learn a robust detector in the domain shift circum-
stance, where the training (source) domain is label-rich
with bounding box annotations, while the testing (target)
domain is label-agnostic and the feature distributions be-
tween training and testing domains are dissimilar or even
totally different. In this paper, we propose a gradient detach
based stacked complementary losses (SCL) method that
uses detection losses as the primary objective, and cuts in
several auxiliary losses in different network stages accom-
panying with gradient detach training to learn more dis-
criminative representations. We argue that the prior meth-
ods [3, 11] mainly leverage more loss functions for train-
ing but ignore the interaction of different losses and also
the compatible training strategy (gradient detach updating
in our work). Thus, our proposed method is a more syn-
cretic adaptation learning process. We conduct comprehen-
sive experiments on seven datasets, the results demonstrate
that our method performs favorably better than the state-of-
the-art methods by a signiﬁcant margin. For instance, from
Cityscapes to FoggyCityscapes, we achieve 37.9% mAP,
outperforming the previous art Strong-Weak [25] by 3.6%.

1. Introduction

In real world scenarios, generic object detection always
faces severe challenges from variations in viewpoint, back-
ground, object appearance, illumination, occlusion condi-
tions, scene change, etc. These unavoidable factors make
object detection in domain-shift circumstance a challeng-
ing and new rising research topic in the recent years. Also,
domain change is a widely-recognized, intractable problem
that urgently needs to break through in reality of detection

∗ Equal contribution.
†Work done while visiting CMU.

1

tasks, like video surveillance, autonomous driving, etc.

Revisiting Domain-Shift Object Detection. Common

approaches for tackling domain-shift object detection are
mainly in two directions: (i) training supervised model and
then ﬁne-tuning on the target domain; or (ii) unsupervised
cross-domain representation learning. The former requires
additional instance-level annotations on target data, which
is fairly laborious, expensive and time-consuming. So most
approaches focus on the latter one but still have some chal-
lenges. The ﬁrst challenge is that the representations of
source and target domain data should be embedded into a
common space for matching the object, such as the hid-
den feature space [25, 3], input space [29, 1] or both of
them [16]. The second is that a feature alignment/matching
operation or mechanism for source/target domains should
be further deﬁned, such as subspace alignment [23], H-
divergence and adversarial learning [3], MRL [16], Strong-
Weak alignment [25], universal alignment [31], etc. In gen-
eral, our SCL targets at these two challenges, and it is also
a learning-based alignment method across domains with an
end-to-end framework.
Our Key Ideas. The goal of this paper is to introduce a
simple design that is speciﬁc to convolutional neural net-
work optimization and improves its training on tasks that
adapt on discrepant domains. Unsupervised domain adap-
tation for recognition has been widely studied by a large
body of previous literature [8, 19, 30, 22, 12, 21, 34, 32],
our method more or less draws merits from them, like align-
ing source and target distributions with adversarial learning
(domain-invariant alignment). However, object detection is
a technically different problem from classiﬁcation, since we
would like to focus more on the object of interests (regions).
Some recent work [35] has proposed to conduct align-
ment only on local regions so that to improve the efﬁ-
ciency of model learning. While this operation may cause
a deﬁciency of critical information from context. Inspired
by strong-weak/multi-feature alignment [25, 33, 11] which
proposed to align corresponding local-region on shallow
layers with a small respective ﬁeld (RF) and align image-

 
 
 
 
 
 
(a) Non-adapted

(b) CVPR’18 [3]

(c) CVPR’19 [25]

(d) SCL (Ours)

(e) Non-adapted

(f) CVPR’18 [3]

(g) CVPR’19 [25]

(h) SCL (Ours)

Figure 1: Visualization of features from PASCAL to Clipart (ﬁrst row) and from Cityscapes to FoggyCityscapes (second
row) by t-SNE [20]. Red indicates the source examples and blue is the target one. If source and target features locate in the
same position, it is shown as light blue. All models are re-trained with a uniﬁed setting to ensure fair comparisons. It can
be observed that our feature embedding results are consistently much better than previous approaches on either dissimilar
domains (PASCAL and Clipart) or similar domains (Cityscapes and FoggyCityscapes). Best viewed in color and zoom in.

level features on deep layers with large RF, we extend
this idea by studying diverse complementary objectives and
their potential assembly for domain adaptive circumstance.
We observe that domain adaptive object detection is sup-
ported dramatically by the deep supervision, however, the
diverse supervisions should be applied in a controlled man-
ner, including the cut-in locations, loss types, orders, up-
dating strategy, etc., which is one of the contributions of
this paper. Furthermore, our experiments show that even
with the existing objectives, after elaborating the different
combinations and training strategy, our method can obtain
competitive results. By plugging-in a new sub-network that
learns the context features independently with gradient de-
tach updating strategy in a hierarchical manner, we obtain
the best results on several domain adaptive object detection
benchmarks.

The Relation to Complement Objective Training [2] and

Deep Supervision [17]. COL [2] proposed to involve addi-
tional loss function that complements the primary objective
which is moderately similar to our goal in spirit. The cross
entropy in COL is used as the primary objective Hp :

N(cid:88)

i=1

Hp (y, ˆy) = − 1
N

· log ( ˆyi )

yT

i

(1)

where yi ∈ {0, 1}D is the label of the i-th sample in one-hot
representation and ˆyi ∈ [0, 1]D is the predicted probabili-
ties.
Th complement entropy Hc is deﬁned in COT [2] as the

2

N(cid:88)

i=1

average of sample-wise entropies over complement classes
in a mini-batch:

Hc ( ˆyc ) =

1
N

H ( ˆyic )

(2)

where H is the entropy function. ˆyc is the predicted prob-
abilities of complement classes c. The training process is
that: for each iteration of training, 1) update parameters by
Hp ﬁrst; then 2) update parameters by Hc . Different from
COL, we don’t use the alternate strategy but update the pa-
rameters simultaneously using gradient detach strategy with
primary and complement objectives. Since we aim to let the
network enable to adapt on both source and target domain
data and meanwhile still being able to distinguish objects
from them. Thus our complement objective design is quite
different from COT. We will describe with details in Sec-
tion 2.
In essence, our method is more likely to be the deeply
supervised formulation [17] that backpropagation of error
now proceeds not only from the ﬁnal layer but also simul-
taneously from our intermediate complementary outputs.
While DSN is basically proposed to alleviate “vanishing”
gradient problem, here we focus on how to adopt these
auxiliary losses to promote to mix two different domains
through domain classiﬁers for detection. Interestingly, we
observe that diverse objectives can lead to better general-
ization for network adaptation. Motivated by this, we pro-
pose Stacked Complementary Losses (SCL), a simple yet

effective approach for domain-shift object detection. Our
SCL is fairly easy and straightforward to implement, but can
achieve remarkable performance. We conjecture that previ-
ous approaches that focus on conducting domain alignment
on high-level layers only [3] cannot fully adapt shallow
layer parameters to both source and target domains (even
local alignment is applied [25]) which restricts the ability
of model learning. Also, gradient detach is a critical part of
learning with our complementary losses. We further visu-
alize the features obtained by non-adapted model, DA [3],
Strong-Weak [25] and ours, features are from the last layer
of backbone before feeding into the Region Proposal Net-
work (RPN). As shown in Figure 1, it is obvious that the
target features obtained by our model are more compactly
matched with the source domain than any other models.
Contributions. Our contributions are three-fold.

• We study the interaction of multi-loss (deep super-
vision with complement objective) and gradient de-
tach (training strategy for maximizing context infor-

mation) in an end-to-end learnable framework for the
challenging domain adaptive object detection task.
• We provide a step-by-step ablation study to empiri-
cally verify the effectiveness of each component and
design in our framework. Thus, this work gives intu-
itive and practical guidance for building a high perfor-
mance framework with multi-objective learning on do-
main adaptive object detection.
• To the best of our knowledge, this is a pioneer work
to investigate the inﬂuence of diverse loss functions
and gradient detach for domain adaptive object detec-
tion. Our method achieves the highest accuracy on sev-
eral domain adaptive or cross-domain object detection
benchmarks1 .

2. Methodology

Following the common formulation of domain adaptive
object detection, we deﬁne a source domain S where anno-
tated bound-box is available, and a target domain T where
only the image can be used in training process without any
labels. Our purpose is to train a robust detector that can
adapt well to both source and target domain data, i.e., we
aim to learn a domain-invariant feature representation that
works well for detection across two different domains.

2.1. Multi-Complement Objective Learning

As shown in Figure 2, we focus on the complement ob-
jective learning and let S = {(x(s)
)} where x(s)
Rn denotes an image, y(s)
is the corresponding bounding

i ∈

, y(s)
i

i

i

1Our code and models are available at: https://github.com/

harsh- 99/SCL.

(cid:17)

(cid:16) ˆΘ(s)
log

= E (cid:104)

Lk

i

c , y (s)

Each label y(s) = (y (s)

box and category labels for sample x(s)
, and i is an index.
b ) denotes a class label y (s)
where c is the category, and a 4-dimension bounding-box
coordinate y (s)
b ∈ R4 . For the target domain we only use
image data for training, so T = {x(t)
i }. We deﬁne a recur-
sive function for layers k = 1, 2, . . . , K where we cut in
complementary losses:

c

ˆΘk = F (Zk ) , and Z0 ≡ x

(3)
where ˆΘk is the feature map produced at layer k, F is the
function to generate features at layer k and Zk is input at
layer k. We formulate the complement loss of domain clas-
siﬁer k as follows:

k , ˆΘ(t)

(cid:16)

(cid:16) ˆΘ(s)
k ; Dk

Dk

k

(cid:17)(cid:17)(cid:105)

= L(s)
k ( ˆΘ(s)

+ E (cid:104)

k ; Dk ) + L(t)
(cid:16) ˆΘ(t)
k ; Dk )
log
1 − Dk

(cid:17)(cid:17)(cid:105)

k ( ˆΘ(t)

(cid:16)

k

ˆΘ(s)

k and ˆΘ(t)

(4)
where Dk is the k-th domain classiﬁer or discriminator.
k denote feature maps from source and target
domains respectively. Following [3, 25], we also adopt gra-
dient reverse layer (GRL) [7] to enable adversarial training
where a GRL layer is placed between the domain classiﬁer
and the detection backbone network. During backpropaga-
tion, GRL will reverse the gradient that passes through from
domain classiﬁer to detection network.
For our instance-context alignment loss LILoss , we take
the instance-level representation and context vector as in-
puts. The instance-level vectors are from RoI layer that
each vector focuses on the representation of local object
only. The context vector is from our proposed sub-network
that combine hierarchical global features. We concatenate
instance features with same context vector. Since context
information is fairly different from objects, joint training
detection and context networks will mix the critical infor-
mation from each part, here we proposed a better solution
that uses detach strategy to update the gradients. We will in-
troduce it with details in the next section. Aligning instance
and context representation simultaneously can help to alle-
viate the variances of object appearance, part deformation,
object size, etc. in instance vector and illumination, scene,
etc. in context vector. We deﬁne di as the domain label of
i-th training image where di = 1 for the source and di = 0
for the target, so the instance-context alignment loss can be
further formulated as:

Ns(cid:88)
Nt(cid:88)
i=1
i=1

(cid:88)
(cid:88)

i,j

i,j

LILoss = − 1

Ns

− 1

Nt

(1 − di ) log P(i,j )
di log (cid:0)1 − P(i,j )

(cid:1)

(5)

where Ns and Nt denote the numbers of source and target

3

Figure 2: Overview of our SCL framework. More details please refer to Section 2.

examples. P(i,j ) is the output probabilities of the instance-
context domain classiﬁer for the j -th region proposal in the
i-th image. So our total SCL objective LSCL can be written
as:

K(cid:88)

LSCL =

Lk + LILoss

(6)

k=1

2.2. Gradients Detach Updating

In this section, we introduce a simple yet effective de-
tach strategy which prevents the ﬂow of gradients from con-
text sub-network through the detection backbone path. We
ﬁnd this can help to obtain more discriminative context and
we show empirical evidence (see Figure 5) that this path
carries information with diversity and hence gradients from
this path getting suppressed is superior for such task.
As aforementioned, we deﬁne a sub-network to gener-
ate the context information from early layers of detection
backbone.
Intuitively, instance and context will focus on
perceptually different parts of an image, so the representa-
tions from either of them should also be discrepant. How-
ever, if we train with the conventional process, the com-
panion sub-network will be updated jointly with the detec-
tion backbone, which may lead to an indistinguishable be-
havior from these two parts. To this end, in this paper we
propose to suppress gradients during backpropagation and
force the representation of context sub-network to be dis-
similar to the detection network, as shown in Algorithm 1.
To our best knowledge, this may be the ﬁrst work to show
the effectiveness of gradient detach that can help to learn
better context representation for domain adaptive object de-
tection. The details of our context sub-network architecture
are illustrated in Appendix D.

Algorithm 1: Backward Pass of Our Detach Algorithm
1 INPUT: Gc is gradient of context network, Gd is the
gradient of detection network, Ldet is the detection
objective, LSCL is the complementary objective;

2 for t ← 1 to ntrain steps do

3

4
5

Lrpn )+LILoss

1. Update context net by detection and
instance-context objectives: Ldet (w/o
2. Gd ← stop-gradient(Gc ;Ldet )
3. Update detection net by detection and
complementary objectives: Ldet+LSCL

2.3. Framework Overall

Our detection part is based on the Faster RCNN [24],
including the Region Proposal Network (RPN) and other
modules. This is a conventional practice in many adaptive
detection works. The objective of the detection loss is sum-
marized as:

Ldet = Lrpn + Lcls + Lreg

(7)
where Lcls is the classiﬁcation loss and Lreg is the
bounding-box regression loss. To train the whole model us-
ing SGD, the overall objective function in the model is:

minF ,R

max

D

Ldet (F (Z), R) − λLSCL (F (Z), D)

(8)

where λ is the trade-off coefﬁcient between detection loss
and our complementary loss. R denotes the RPN and other
modules in Faster RCNN.

4

GRLL1GRLL2GRLLKForward1Forward2Forward3InputConvB1xxxConvB2ConvB3ConvB4GRLILossRPN+Classification+RegressionInstanceHierarchical-ContextInstance-Context AlignmentComplementary LossesDetachDomainLabelMapDomainClassifierGradientReversalLayer…1283843. Empirical Results

Datasets. We evaluate our approach in three different do-
main shift scenarios: (1) Similar Domains; (2) Discrepant
Domains; and (3) From Synthetic to Real Images. All ex-
periments are conducted on seven domain shift datasets:
Cityscapes [4] to FoggyCityscapes [26], Cityscapes to
KITTI [9], KITTI to Cityscapes, INIT Dataset [27], PAS-
CAL [6] to Clipart [13], PASCAL to Watercolor [13], GTA
(Sim 10K) [14] to Cityscapes.
Implementation Details. In all experiments, we resize the
shorter side of the image to 600 following [24, 25] with
ROI-align [10]. We train the model with SGD optimizer
and the initial learning rate is set to 10−3 , then divided by
10 after every 50,000 iterations. Unless otherwise stated,
we set λ as 1.0 and γ as 5.0, and we use K = 3 in our
experiments (the analysis of hyper-parameter K is shown in
Table 7). We report mean average precision (mAP) with an
IoU threshold of 0.5 for evaluation. Following [3, 25], we
feed one labeled source image and one unlabeled target one
in each mini-batch during training. SCL is implemented on
PyTorch platform, we will release our code and models.

3.1. How to Choose Complementary Losses in Our
Framework

There are few pioneer works for exploring the combi-
nation of different losses for domain adaptive object de-
tection, hence we conduct extensive ablation study for this
part to explore the intuition and best collocation of our SCL
method. We mainly adopt three losses which have been in-
troduced in many literature. Since they are not our contri-
bution so here we just give some brief formulations below.
Cross-entropy (CE) Loss. CE loss measures the perfor-
mance of a classiﬁcation model whose output is a probabil-
ity value. It increases as the predicted probability diverges
from the actual label:

LCE (pc ) = − C(cid:88)

c=1

yc log pc

(9)

where pc ∈ [0, 1] is the predicted probability observation of
c class. yc is the c class label.

Weighted Least-squares (LS) Loss. Following [25], we

adopt LS loss to stabilize the training of the domain classi-
ﬁer for aligning low-level features. The loss is designed to
align each receptive ﬁeld of features with the other domain.
The least-squares loss is formulated as:

LLS = αL(s)
loc + βL(t)
loc =

H(cid:88)

h=1

W(cid:88)
H(cid:88)
(cid:16)

w=1

α
HW

W(cid:88)

1 − D

(cid:16) ˆΘ(s)(cid:17)2
D
(cid:16) ˆΘ(t)(cid:17)

wh

(cid:17)2

wh

+

β
HW

w=1

h=1

(10)

5

(cid:16) ˆΘ(s)(cid:17)

wh

where D
denotes the output of the domain classi-
ﬁer in each location (w, h). α and β are balance coefﬁcients
and we set them to 1 following [25].
Focal Loss (FL). Focal loss LFL [18] is adopted to ig-
nore easy-to-classify examples and focus on those hard-to-
classify ones during training:

LFL (pt ) = −f (pt ) log (pt ) , f (pt ) = (1 − pt )γ

(11)

where pt = p if di = 1, otherwise, pt = 1 − p.

3.2. Ablation Studies from Cityscapes to FoggyCi-
tyscapes

We ﬁrst investigate each component and design of our
SCL framework from Cityscapes to FoggyCityscapes. Both
source and target datasets have 2,975 images in the training
set and 500 images in the validation set. We design several
controlled experiments for this ablation study. A consistent
setting is imposed on all the experiments, unless when some
components or structures are examined. In this study, we
train models with the ImageNet [5] pre-trained ResNet-101
as a backbone, we also provide the results with pre-trained
VGG16 model.
The results are summarized in Table 1. We present
several combinations of four complementary objectives
with their loss names and performance. We observe that
“LS—CE—F L—F L” obtains the best accuracy with
Context and Detach.
It indicates that LS can only be
placed on the low-level features (rich spatial information
and poor semantic information) and F L should be in the
high-level locations (weak spatial information and strong
semantic information). For the middle location, CE will
be a good choice. If you use LS for the middle/high-level
features or use F L on the low-level features, it will con-
fuse the network to learn hierarchical semantic outputs, so
that ILoss+detach will lose effectiveness under that circum-
stance. This veriﬁes that domain adaptive object detection
relies heavily on the deep supervision, however, the diverse
supervisions should be adopted in a controlled and correct
manner. Furthermore, our proposed method performs much
better than baseline Strong-Weak [25] (37.9% vs.34.3%)
and other state-of-the-arts.

3.3. Similar Domains

Between Cityspaces and KITTI. In this part, we focus on

studying adaptation between two real and similar domains,
as we take KITTI and Cityscapes as our training and test-
ing data. Following [3], we use KITTI training set which
contains 7,481 images. We conduct experiments on both
adaptation directions K → C and C → K and evaluate our
method using AP of car as in DA.
As shown in Table 2, our proposed method performed
much better than the baseline and other state-of-the-art
methods. Since Strong-Weak [25] didn’t provide the results

Table 1: Ablation study (%) on Cityscapes to FoggyCityscapes (we use 150m visibility, the densest one) adaptation. Please
refer to Section 3.2 for more details.

Method
Faster RCNN (Non-adapted)
DA (CVPR’18)
MAF [11] (ICCV’19)
Strong-Weak (CVPR’19)
Diversify&match [16] (CVPR’19)
Strong-Weak (Our impl. w/ VGG16) (cid:88)
Strong-Weak (Our impl. w/ Res101)

(cid:88)

(cid:88)

(cid:88)

AP on a target domain
Context L1 L2 L3 ILoss Detach person rider car truck bus train mcycle bicycle mAP
24.1 33.1 34.3 4.1 22.3 3.0
15.3
26.5
20.3
25.0 31.0 40.5 22.1 35.3 20.2
20.0
27.1
27.6
28.2 39.5 43.9 23.8 39.9 33.3
29.2
33.9
34.0
29.9 42.3 43.5 24.5 36.2 32.6
30.0
35.3
34.3
30.8 40.5 44.3 27.2 38.4 34.5
28.4
32.2
34.6
30.0 40.0 43.4 23.2 40.1 34.6
27.8
33.4
34.1
29.1 41.2 43.8 26.0 43.2 27.0
26.2
30.6
33.4
29.6 42.2 43.4 23.1 36.4 31.5
25.1
30.5
32.7
29.0 41.4 43.9 24.6 46.5 28.5
27.0
32.8
34.2
28.6 44.0 44.2 25.2 42.9 31.1
27.4
33.0
34.5
28.5 42.6 43.8 23.2 41.6 24.9
28.3
30.3
32.9
28.6 41.8 43.8 27.9 43.3 24.0
28.7
31.3
33.7
28.8 45.5 44.3 28.6 44.6 29.1
27.8
31.4
35.0
29.6 42.6 42.6 28.4 46.3 31.0
28.4
33.0
35.3
30.0 42.7 44.2 30.0 50.2 34.1
27.1
32.2
36.3
26.3 42.8 44.2 26.7 41.6 36.4
29.2
30.9
34.8
29.5 43.2 44.2 27.0 42.1 33.3
29.4
30.6
34.9
29.7 43.6 43.7 26.6 43.8 33.1
30.7
31.5
35.3
28.3 41.9 43.1 25.4 45.1 35.5
26.7
31.6
34.7
29.8 43.9 44.0 29.4 46.3 30.0
31.8
31.8
35.8
29.0 42.5 43.9 28.9 45.7 42.4
26.4
30.5
36.2
30.7 44.1 44.3 30.0 47.9 42.9
29.6
33.7


LS F L 


LS CE F L 

LS CE F L F L
(cid:88) LS F L 

(cid:88) LS F L F L 
(cid:88) LS LS F L 
(cid:88) LS CE F L 
(cid:88) LS CE F L 
(cid:88) LS F L F L F L
(cid:88) LS LS F L F L
(cid:88) LS F L F L F L
(cid:88) LS CE F L CE
(cid:88) LS CE F L F L
(cid:88) LS CE F L CE
(cid:88) LS CE F L F L
(cid:88) LS CE F L F L

31.6 44.0 44.8 30.4 41.8 40.7









37.9
37.9

(cid:88)
(cid:88)
(cid:88)

33.6

36.2

(cid:88)
(cid:88)




(cid:88)



Our full model w/ VGG16
Upper Bound [25]

–

–

–

–

–

–

33.2 45.9 49.7 35.6 50.0 37.4

34.7

36.2

40.3

LS: Least-squares Loss; CE: Cross-entropy Loss; FL: Focal Loss; ILoss: Instance-Context Alignment Loss.

Table 2: Adaptation results between KITTI and Cityscapes.
We report AP of Car on both directions: K→C and C→K.
We re-implemented DA [3] and Weak-Strong [25] based on
the same Faster RCNN framework [24].

K→C C→K
Method
Faster RCNN
30.2
53.5
DA [3]
38.5
64.1
DA (Our impl.) [3]
35.6
70.8
WS (Our impl.) [25] 37.9
71.0
Ours

41.9

72.7

on this dataset, we re-implement it and obtain 37.9% AP on
K→C and 71.0% AP on C→K. Our method is 4% higher
than the former and 1.7% higher than latter. If comparing
to the non-adapted results (source only), our method out-
performs it with a huge margin (about 10% and 20% higher,
respectively).

INIT Dataset.

INIT Dataset [27] contains 132,201 im-
ages for training and 23,328 images for testing. There are
four domains: sunny, night, rainy and cloudy, and three
instance categories, including: car, person, speed limited
sign. This dataset is ﬁrst proposed for the instance-level
image-to-image translation task, here we use it for the do-
main adaptive object detection purpose.

Table 3: Adaptation results on INIT dataset.

s2n

s2r

s2c

67.92 65.89 32.52 55.44

Car Sign Person mAP
Faster
63.33 63.96 32.00 53.10
Strong-Weak 67.43 64.33 32.53 54.76
Ours
Oracle
80.12 84.68 44.57 69.79
Faster
70.20 72.71 36.22 59.71
Strong-Weak 71.56 78.07 39.27 62.97
ours
Oracle
71.83 79.42 45.21 65.49
Faster
–
–
–
–
Strong-Weak 71.32 72.71 43.18 62.40
Ours
Oracle

71.41 78.93 39.79 63.37

71.28 72.91 43.79 62.66

76.60 76.72 47.28 66.87

Our results are shown in Table 3. Following [27], we
conduct experiments on three domain pairs: sunny→night
(s2n), sunny→rainy (s2r) and sunny→cloudy (s2c). Since
the training images in rainy domain are much fewer than
sunny, for s2r experiment we randomly sample the train-
ing data in sunny set with the same number of rainy set
and then train the detector.
It can be observed that our
method is consistently better than the baseline method. We
don’t provide the results of s2c (faster) because we found
that cloudy images are too similar to sunny in this dataset

6

Table 4: Results on adaptation from PASCAL VOC to Clipart Dataset. Average precision (%) is evaluated on target images.

Method
aero bcycle bird boat bottle bus car cat chair cow table dog hrs bike prsn plnt sheep sofa train tv mAP
Faster (Non-adapted) 35.6 52.5 24.3 23.0 20.0 43.9 32.8 10.7 30.6 11.7 13.8 6.0 36.8 45.9 48.7 41.9 16.5 7.3 22.9 32.0 27.8
BDC-Faster
20.2 46.4 20.4 19.3 18.7 41.3 26.5 6.4 33.2 11.7 26.0 1.7 36.6 41.5 37.7 44.5 10.6 20.4 33.3 15.5 25.6
DA
15.0 34.6 12.4 11.9 19.8 21.1 23.2 3.1 22.1 26.3 10.6 10.0 19.6 39.4 34.6 29.3 1.0 17.1 19.7 24.8 19.8
WST-BSR [15]
28.0 64.5 23.9 19.0 21.9 64.3 43.5 16.4 42.2 25.9 30.5 7.9 25.5 67.6 54.5 36.4 10.3 31.2 57.4 43.5 35.7
26.2 48.5 32.6 33.7 38.5 54.3 37.1 18.6 34.8 58.3 17.0 12.5 33.8 65.5 61.6 52.0 9.3 24.9 54.1 49.1 38.1
Strong-Weak [25]
Ours w/LILoss = F L 33.4 49.2 36.0 27.1 38.4 55.7 38.7 15.9 39.0 59.2 18.8 23.7 36.9 70.0 60.6 49.7 25.8 34.8 47.2 51.2 40.6
Ours w/LILoss = CE 44.7 50.0 33.6 27.4 42.2 55.6 38.3 19.2 37.9 69.0 30.1 26.3 34.4 67.3 61.0 47.9 21.4 26.3 50.1 47.3 41.5

Table 5: Adaptation results from PASCAL to WaterColor.

AP on a target domain
Method
bike bird car
cat dog prsn mAP
Source Only
68.8 46.8 37.2 32.7 21.3 60.7 44.6
BDC-Faster
68.6 48.3 47.2 26.5 21.7 60.5 45.5
DA [3]
75.2 40.6 48.0 31.5 20.6 60.0 46.0
WST-BSR [15]
75.6 45.8 49.3 34.1 30.3 64.1 49.9
Strong-Weak [25] 82.3 55.9 46.5 32.7 35.5 66.7 53.3
Ours

82.2 55.1 51.8 39.6 38.4 64.0 55.2

(nearly the same), thus the non-adapted result is very close
to the adapted methods.

3.4. Discrepant Domains

In this section, we focus on the dissimilar domains, i.e.,
adaptation from real images to cartoon/artistic. Follow-
ing [25], we use PASCAL VOC dataset (2007+2012 train-
ing and validation combination for training) as the source
data and the Clipart or Watercolor [13] as the target data.
The backbone network is ImageNet pre-trained ResNet-
101.
PASCAL to Clipart. Clipart dataset contains 1,000 images
in total, with the same 20 categories as in PASCAL VOC.
As shown in Table 4, our proposed SCL outperforms all
baselines. In addition, we observe that replacing F L with
CE loss on instance-context classiﬁer can further improve
the performance from 40.6% to 41.5%. More ablation re-
sults are shown in our Appendix A.2 (Table 12).
PASCAL to WaterColor. Watercolor dataset contains 6
categories in common with PASCAL VOC and has totally
2,000 images (1,000 images are used for training and 1,000
test images for evaluation). Results are summarized in Ta-
ble 5, SCL consistently outperforms other state-of-the-arts.

3.5. From Synthetic to Real Images

Sim10K to Cityscapes. Sim 10k dataset [14] contains
10,000 images for training which are generated by the gam-
ing engine Grand Theft Auto (GTA). Following [3, 25], we
use Cityscapes as target domain and evaluate our models on
Car class. Our result is shown in Table 6, which consis-
tently outperforms the baselines.

Table 6: Adaptation results on Car from Sim10k to
Cityscapes Dataset (%). Source Only indicates the non-
adapted results (λ = 0.1 and γ = 2.0 are used).

Method
Faster
DA [3]
Strong-Weak [25]
MAF [11]
Ours

AP on Car
34.6
38.9
40.1
41.1

42.6

Figure 3: Parameter sensitivity for the value of λ (left) and
γ (right) in adaptation from Cityscapes to FoggyCityscapes
and from Sim10k to Cityscapes.

4. Analysis

Hyper-parameter K. Table 7 shows the results for sen-
sitivity of hyper-parameter K in Figure 2. This parameter
controls the number of SCL losses and context branches.
It can be observed that the proposed method performs best
when K = 3 on all three datasets.
Table 7: Analysis of hype-parameter K.

Method
K=2 K=3 K=4
from Cityscapes to Foggycityscapes 32.7 37.9 34.5
from PASCAL VOC to Clipart
39.0 41.5 39.3
from PASCAL VOC to Watercolor
54.7 55.2 53.4

Parameter Sensitivity on λ and γ . Figure 3 shows the

results for parameter sensitivity of λ and γ in Eq. 8 and
Eq. 11. λ is the trade-off parameter between SCL and de-
tection objectives and γ controls the strength of hard sam-
ples in Focal Loss. We conduct experiments on two adapta-
tions: Cityscapes → FoggyCityscapes (blue) and Sim10K
→ Cityscapes (red). On Cityscapes → FoggyCityscapes,
we achieve the best performance when λ = 1.0 and γ = 5.0
and the best accuracy is 37.9%. On Sim10K → Cityscapes,

7

(a) from Cityscapes and FoggyCityscapes

(b) from PASCAL VOC to Clipart

(c) from PASCAL VOC to Watercolor

Figure 4: AP (%) with different IoU thresholds. We show comparisons on three datasets and all results are calculated with
different IoU thresholds and illustrated in different colors.

Figure 5: Visualization of Attention Maps on source and target domains. We use feature maps after Conv B3 in Figure 2
for visualizing. Top: Input images; Middle: Heatmaps from models w/o gradient detach; Bottom: Heatmaps from models
w/ gradient detach. The colors (red→blue) indicate values from high to low. It can be observed that w/ detach training, our
models can learn more discriminative representation between object areas and background (context).

the best result is obtained when λ = 0.1, γ = 2.0.

Analysis of IoU Threshold. The IoU threshold is an im-

portant indicator to reﬂect the quality of detection, and a
higher threshold means better coverage with ground-truth.
In our previous experiments, we use 0.5 as a threshold sug-
gested by many literature [24, 3]. In order to explore the
inﬂuence of IoU threshold with performance, we plot the
performance vs. IoU on three datasets. As shown in Fig-
ure 4, our method is consistently better than the baselines
on different thresholds by a large margin (in most cases).

Why Gradient Detach Can Help Our Model? To further

explore why gradient detach can help to improve perfor-
mance vastly and what our model really learned, we visual-
ize the heatmaps on both source and target images from our
models w/o and w/ detach training. As shown in Figure 5,
the visualization is plotted with feature maps after Conv B3
in Figure 2. We can observe that the object areas and con-
text from detach-trained models have stronger contrast than
w/o detach model (red and blue areas). This indicates that
detach-based model can learn more discriminative features
from the target object and context. To be more precise, w/o
detach model is attentive more on the background (green
color), in contrast, with detach model is mainly activated on
the object areas with less attention on the background (blue

color, i.e., less attention). More visualizations are shown in
Appendix F (Figure 8).
Detection Visualization. Figure 9 shows several qualita-
tive comparisons of detection examples on three test sets
with DA [3], Strong-Weak [25] and our SCL models. Our
method detects more small and blurry objects in dense
scene (FoggyCityscapes) and suppresses more false posi-
tives (Clipart and Watercolor) than the other two baselines.

5. Conclusion

We have addressed unsupervised domain adaptive object
detection through stacked complementary losses. One of
our key contributions is gradient detach training, enabled by
suppressing gradients ﬂowing back to the detection back-
bone. In addition, we proposed to use multiple complemen-
tary losses for better optimization. We conduct extensive
experiments and ablation studies to verify the effectiveness
of each component that we proposed. Our experimental re-
sults outperform the state-of-the-art approaches by a large
margin on a variety of benchmarks. Our future work will
focus on exploring the domain-shift detection from scratch,
i.e., without the pre-trained models like DSOD [28], to
avoid involving bias from the pre-trained dataset.

8

SourceDomainTargetDomainInputW/ODetachW/Detach(a) FoggyCityscapes

(b) Clipart

Figure 6: Detection examples with DA [3], Strong-Weak [25] and our proposed SCL on three datasets. For each group, the
ﬁrst row is the result of DA, the second row is from Strong-Weak and the last row is ours. We show detections with the scores
higher than a threshold (0.3 for FoggyCityscapes and 0.5 for other two).

(c) Watercolor

9

References

[1] Qi Cai, Yingwei Pan, Chong-Wah Ngo, Xinmei Tian, Lingyu
Duan, and Ting Yao. Exploring object relation in mean
teacher for cross-domain detection. In CVPR, 2019. 1
[2] Hao-Yun Chen, Pei-Hsin Wang, Chun-Hao Liu, Shih-Chieh
Chang, Jia-Yu Pan, Yu-Ting Chen, Wei Wei, and Da-Cheng
Juan. Complement objective training. In ICLR, 2019. 2
[3] Yuhua Chen, Wen Li, Christos Sakaridis, Dengxin Dai, and
Luc Van Gool. Domain adaptive faster r-cnn for object de-
tection in the wild. In CVPR, 2018. 1, 2, 3, 5, 6, 7, 8, 9,
12
[4] Marius Cordts, Mohamed Omran, Sebastian Ramos, Timo
Rehfeld, Markus Enzweiler, Rodrigo Benenson, Uwe
Franke, Stefan Roth, and Bernt Schiele. The cityscapes
dataset for semantic urban scene understanding. In CVPR,
2016. 5
[5] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li,
and Li Fei-Fei. Imagenet: A large-scale hierarchical image
database. In 2009 IEEE conference on computer vision and
pattern recognition, pages 248–255. Ieee, 2009. 5
[6] Mark Everingham, Luc Van Gool, Christopher KI Williams,
John Winn, and Andrew Zisserman. The pascal visual object
classes (voc) challenge.
International journal of computer
vision, 88(2):303–338, 2010. 5
[7] Yaroslav Ganin and Victor Lempitsky. Unsupervised domain
adaptation by backpropagation. In ICML, 2015. 3
[8] Yaroslav Ganin, Evgeniya Ustinova, Hana Ajakan, Pas-
cal Germain, Hugo Larochelle, Franc¸ ois Laviolette, Mario
Marchand, and Victor Lempitsky. Domain-adversarial train-
ing of neural networks. The Journal of Machine Learning
Research, 2016. 1
[9] Andreas Geiger, Philip Lenz, and Raquel Urtasun. Are we
ready for autonomous driving? the kitti vision benchmark
suite. In Conference on Computer Vision and Pattern Recog-
nition (CVPR), 2012. 5
[10] Kaiming He, Georgia Gkioxari, Piotr Doll ´ar, and Ross Gir-
shick. Mask r-cnn. In ICCV, 2017. 5
[11] Zhenwei He and Lei Zhang. Multi-adversarial faster-rcnn for
unrestricted object detection. In ICCV, 2019. 1, 6, 7
[12] Judy Hoffman, Eric Tzeng, Taesung Park, Jun-Yan Zhu,
Phillip Isola, Kate Saenko, Alexei Efros, and Trevor Darrell.
Cycada: Cycle-consistent adversarial domain adaptation. In
ICML, 2018. 1
[13] Naoto Inoue, Ryosuke Furuta, Toshihiko Yamasaki, and Kiy-
oharu Aizawa. Cross-domain weakly-supervised object de-
tection through progressive domain adaptation.
In CVPR,
2018. 5, 7
[14] Matthew Johnson-Roberson, Charles Barto, Rounak Mehta,
Sharath Nittur Sridhar, Karl Rosaen, and Ram Vasudevan.
Driving in the matrix: Can virtual worlds replace human-
generated annotations for real world tasks? arXiv preprint
arXiv:1610.01983, 2016. 5, 7
[15] Seunghyeon Kim, Jaehoon Choi, Taekyung Kim, and Chang-
ick Kim. Self-training and adversarial background regular-
ization for unsupervised domain adaptive one-stage object
detection. In ICCV, 2019. 7

[16] Taekyung Kim, Minki Jeong, Seunghyeon Kim, Seokeon
Choi, and Changick Kim. Diversify and match: A domain
adaptive representation learning paradigm for object detec-
tion. In CVPR, 2019. 1, 6
[17] Chen-Yu Lee, Saining Xie, Patrick Gallagher, Zhengyou
Zhang, and Zhuowen Tu. Deeply-supervised nets.
In Ar-
tiﬁcial intelligence and statistics, 2015. 2
[18] Tsung-Yi Lin, Priya Goyal, Ross Girshick, Kaiming He, and
Piotr Doll ´ar. Focal loss for dense object detection. In ICCV,
2017. 5
[19] Mingsheng Long, Han Zhu, Jianmin Wang, and Michael I
Jordan. Unsupervised domain adaptation with residual trans-
fer networks. In Advances in Neural Information Processing
Systems, 2016. 1
[20] Laurens van der Maaten and Geoffrey Hinton. Visualiz-
ing data using t-sne. Journal of machine learning research,
2008. 2, 14
[21] Zak Murez, Soheil Kolouri, David Kriegman, Ravi Ra-
mamoorthi, and Kyungnam Kim. Image to image translation
for domain adaptation. In CVPR, 2018. 1
[22] Pau Panareda Busto and Juergen Gall. Open set domain
adaptation. In ICCV, 2017. 1
[23] Anant Raj, Vinay P Namboodiri, and Tinne Tuytelaars. Sub-
space alignment based domain adaptation for rcnn detector.
arXiv preprint arXiv:1507.05578, 2015. 1
[24] Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun.
Faster r-cnn: Towards real-time object detection with region
proposal networks. In Advances in neural information pro-
cessing systems, 2015. 4, 5, 6, 8
[25] Kuniaki Saito, Yoshitaka Ushiku, Tatsuya Harada, and Kate
Saenko. Strong-weak distribution alignment for adaptive ob-
ject detection. In CVPR, 2019. 1, 2, 3, 5, 6, 7, 8, 9
[26] Christos Sakaridis, Dengxin Dai, and Luc Van Gool. Seman-
tic foggy scene understanding with synthetic data. Interna-
tional Journal of Computer Vision, 126(9):973–992, 2018.
5
[27] Zhiqiang Shen, Mingyang Huang, Jianping Shi, Xiangyang
Xue, and Thomas Huang. Towards instance-level image-to-
image translation. In CVPR, 2019. 5, 6
[28] Zhiqiang Shen, Zhuang Liu, Jianguo Li, Yu-Gang Jiang,
Yurong Chen, and Xiangyang Xue. Dsod: Learning deeply
supervised object detectors from scratch. In ICCV, 2017. 8
[29] Eric Tzeng, Kaylee Burns, Kate Saenko, and Trevor Darrell.
Splat: Semantic pixel-level adaptation transforms for detec-
tion. arXiv preprint arXiv:1812.00929, 2018. 1
[30] Eric Tzeng, Judy Hoffman, Kate Saenko, and Trevor Darrell.
Adversarial discriminative domain adaptation.
In CVPR,
2017. 1
[31] Xudong Wang, Zhaowei Cai, Dashan Gao, and Nuno Vas-
concelos. Towards universal object detection by domain at-
tention. In CVPR, 2019. 1
[32] Yifan Wu, Ezra Winston, Divyansh Kaushik, and Zachary
Lipton. Domain adaptation with asymmetrically-relaxed dis-
tribution alignment. In ICML, 2019. 1
[33] Weichen Zhang, Wanli Ouyang, Wen Li, and Dong Xu. Col-
laborative and adversarial network for unsupervised domain
adaptation. In CVPR, 2018. 1

10

[34] Han Zhao, Remi Tachet Des Combes, Kun Zhang, and Ge-
offrey Gordon. On learning invariant representations for do-
main adaptation. In ICML, 2019. 1
[35] Xinge Zhu, Jiangmiao Pang, Ceyuan Yang, Jianping Shi, and
Dahua Lin. Adapting object detectors via selective cross-
domain alignment. In CVPR, 2019. 1

11

Appendix

A. More Ablation Studies

Table 8 and 12 show the detailed results on target do-
mains when conducting adaptation from PASCAL VOC to
WaterColor and from PASCAL VOC to Clipart dataset. We
present results with different combinations of SCL and di-
verse ablation experiments.

A.1. From Pascal VOC to Watercolor Dataset

Table 8: AP (%) on adaptation from PASCAL VOC to Wa-
terColor. “W/O CLoss (L2 )” means we remove the L2 in
our complementary losses.

AP on a target domain
Method
bike bird car cat dog prsn mAP
LS—CE—CE—F L 76.1 48.8 48.1 29.9 41.2 56.5 50.1
LS—LS—F L—F L 72.4 51.8 49.7 41.9 36.6 65.5 53.0
LS—F L—F L—F L 77.8 50.6 48.9 40.1 38.7 63.7 53.3
LS—CE—F L—F L 82.2 55.1 51.8 39.6 38.4 64.0 55.2
LS—CE—F L—CE 64.2 54.8 47.3 38.7 41.7 67.9 52.4
W/O Detach
76.2 54.0 49.2 36.7 35.0 68.6 53.3
W/O ILoss
76.1 51.7 48.0 31.6 40.4 64.3 52.0
W/O Context
83.1 54.5 48.4 34.4 38.8 65.5 54.1
W/O Context&ILoss 69.3 52.8 43.2 42.7 36.7 66.0 51.8
W/O CLoss (L2 )
77.1 53.1 49.6 41.0 39.3 67.9 54.7

A.2. From Pascal VOC to Clipart Dataset

The results are shown in Table 12.

B. Results on Source Domains

In this section, we show the adaptation results on source
domains in Table 9, 10, 13 and 11. Surprisingly, we observe
that the best-trained models (on target domains) are not per-
forming best on the source data, e.g., from PASCAL VOC
to WaterColor, DA [3] obtained the highest results on source
domain images (although the gaps with Strong-Weak and
ours are marginal). We conjecture that the adaptation pro-
cess for target domains will affect the learning and perform-
ing on source domains, even we have used the bounding box
ground-truth on source data for training. We will investi-
gate it more thoroughly in our future work and we think the
community may also need to rethink whether evaluating on
source domain should be a metric for domain adaptive ob-
ject detection, since it can help to understand the behavior
of models on both source and target images.

C. Detailed Results of Parameter Sensitivity on
λ and γ

We provide the detailed results of parameter sensitiv-
ity on λ and γ in Table 15 and 16 with the adaptation of

12

Table 9: AP (%) of adaptation from Cityscapes to Fog-
gyCityscapes. Results are evaluated on source images
(Cityscapes) with the same classes as in the target dataset.

AP on a source domain
Method
person rider car
truck bus train mcycle bicycle mAP
DA (CVPR’18)
33.5 48.1 51.1 37.0 61.3 50.0
33.6
36.9
43.9
Strong-Weak (CVPR’19) 33.7 47.9 52.3 33.5 57.1 39.1
35.1
42.0
Ours
33.0 46.7 51.3 39.8 59.2 51.6

37.4

36.8

36.5

44.4

Table 10: AP (%) on adaptation from PASCAL VOC to Wa-
terColor. Results are evaluated on source images (PASCAL
VOC) with the same classes as in the WaterColor.

AP on a source domain
bike bird car cat dog prsn mAP

Method
DA (CVPR’18)
Strong-Weak (CVPR’19) 81.0 77.4 85.3 89.0 82.9 81.4 82.8
Ours
80.3 78.1 86.5 87.9 83.5 82.0 83.1

82.0 78.0 86.3 89.4 83.5 82.6 83.6

Table 11:
Adaptation results between KITTI and
Cityscapes. We report AP of Car on both directions, in-
cluding: K→C and C→K of source domain.

Method
DA (CVPR’18)
Strong-Weak (CVPR’19) 78.6
Ours
78.5

87.9

K→C C→K
52.6

52.9

51.3

from Cityscapes to FoggyCityscapes and from Sim10K to
Cityscapes.

D. Context Network

Our context networks are shown in Table 14. We use
three branches (forward networks) to deliver the context in-
formation and each branch generates a 128-dimension fea-
ture vector from the corresponding backbone layers of SCL.
Then we naively concatenate them and obtain the ﬁnal con-
text feature with a 384-dimension vector.

E. Visualization of Intermediate Feature Em-
bedding

In this section, we visualize the intermediate feature em-
bedding on three adaptation datasets. As shown in Figure 7,
the gradient detach-based models can adapt source and tar-
get images to a similar distribution better than w/o detach
models.

F. More Visualizations of Heatmaps in Fig. 8
G. More Detection Visualization in Fig. 9

Table 12: AP (%) on adpatation from PASCAL VOC to Clipart Dataset. Results are evaluated on target images. “W/O CLoss
(L2 )” means we remove the L2 in our complementary losses.

Method
aero bcycle bird boat bottle bus car cat chair cow table dog hrs bike prsn plnt sheep sofa train tv mAP
LS—CE—CE—F L 24.2 48.3 32.6 26.0 31.2 55.3 37.6 12.1 33.0 47.1 23.1 17.0 23.4 57.4 57.3 43.8 19.9 31.7 48.2 42.7 35.4
LS—CE—F L—CE 44.7 50.0 33.6 27.4 42.2 55.6 38.3 19.2 37.9 69.0 30.1 26.3 34.4 67.3 61.0 47.9 21.4 26.3 50.1 47.3 41.5
LS—F L—F L—F L 31.4 52.4 31.5 27.5 39.5 56.9 38.4 13.6 38.3 45.5 23.9 15.8 33.7 73.1 64.6 49.5 19.3 26.8 55.0 49.9 39.3
LS—LS—F L—F L 32.3 56.8 33.2 23.8 39.6 46.0 39.6 17.6 38.7 52.4 14.7 21.2 33.0 72.0 59.6 46.7 21.9 26.9 49.2 51.8 38.9
LS—CE—F L—F L 33.4 49.2 36.0 27.1 38.4 55.7 38.7 15.9 39.0 59.2 18.8 23.7 36.9 70.0 60.6 49.7 25.8 34.8 47.2 51.2 40.6
W/O Detach
33.1 54.5 33.9 28.2 45.3 59.4 31.4 17.4 34.7 39.9 9.8 20.8 33.5 63.0 60.3 40.8 18.7 20.6 51.8 45.6 37.1
W/O ILoss
27.2 54.0 31.9 24.7 38.6 53.7 36.9 15.1 40.2 52.4 12.4 29.6 36.5 69.3 63.6 43.3 20.2 26.9 50.6 44.3 38.6
W/O Context&ILoss 38.3 65.4 25.4 24.6 35.2 47.7 40.9 20.9 32.6 29.6 4.6 14.7 26.5 85.2 60.9 46.6 17.4 22.5 43.9 50.2 36.7
W/O Context
22.5 50.8 33.8 23.5 37.6 48.3 39.4 16.4 38.5 55.7 16.0 23.8 33.0 62.8 59.8 48.4 17.3 28.6 47.6 46.5 37.5
W/O CLoss (L2 )
33.1 57.0 32.5 24.6 39.0 55.9 37.3 15.7 39.5 50.7 20.5 19.8 37.7 75.3 60.8 43.9 21.1 26.2 42.9 45.6 39.0

Table 13: AP (%) on adaptation from PASCAL VOC to Clipart Dataset. Results are evaluated on source images (PASCAL
VOC) with the same classes as in the Clipart.

aero bcycle bird boat bottle bus car cat chair cow table dog hrs bike prsn plnt sheep sofa train tv mAP

Method
DA
Strong-Weak 74.3 78.6 66.4 52.7 54.5 80.1 81.4 77.6 43.1 72.9 65.1 74.6 76.5 77.0 75.2 46.3 71.6 64.1 77.0 70.1 69.0
Ours
78.4 81.7 78.4 69.4 60.8 86.4 86.0 87.7 57.9 84.8 68.2 86.4 84.6 82.2 79.3 50.5 79.9 73.8 84.2 75.2 76.8

79.7 83.2 81.3 70.0 66.6 86.0 87.3 87.1 57.3 85.3 68.5 87.0 86.6 82.3 80.7 49.7 80.5 75.5 82.9 81.6 78.0

Table 14: Architectures of the forward networks.

Forward Net1
Conv 3 × 3 × 256, stride 1, pad 1
ReLU
Conv 3 × 3 × 128, stride 1, pad 1
ReLU
Conv 3 × 3 × 128, stride 1, pad 1
ReLU

Forward Net2
Conv 3 × 3 × 256, stride 1, pad 1
ReLU
Conv 3 × 3 × 128, stride 1, pad 1
ReLU
Conv 3 × 3 × 128, stride 1, pad 1
ReLU

Forward Net3
Conv 3 × 3 × 512, stride 1, pad 1
ReLU
Conv 3 × 3 × 128, stride 1, pad 1
ReLU
Conv 3 × 3 × 128, stride 1, pad 1
ReLU

Table 15: AP (%) of adaptation from Cityscapes to Fog-
gyCityscapes with different λ and γ .

AP on a target domain
λ person rider car truck bus train mcycle bicycle mAP
0.1 25.8 37.2 24.6 24.2 42.0 33.6
17.5
29.9
29.4
0.5 29.5 42.2 44.4 24.4 45.3 34.1
27.2
32.8
35.0
1.0 30.7 44.1 44.3 30.0 47.9 42.9
29.6
33.7
1.5 26.3 42.2 43.6 25.5 43.8 36.4
26.7
32.0
2.0 29.5 39.4 43.7 28.7 46.0 39.7
28.7
32.0
2.5 25.9 40.3 43.3 26.1 40.8 35.2
26.2
30.2

34.6
36.0
33.5

37.9

γ

1
2
3
4
5
6

27.1 41.6 41.3 25.5 41.6 20.3
27.8 41.3 36.4 24.2 38.8 12.8
29.8 40.7 43.9 29.0 45.0 41.5
30.3 42.6 44.2 25.4 45.7 33.9
30.7 44.1 44.3 30.0 47.9 42.9
26.4 42.0 43.8 23.6 45.2 35.2

20.5
22.9
30.8
28.6
29.6
26.7

30.0
30.9
32.0
30.3
33.7
30.3

31.0
29.4
36.6
35.1

37.9

34.2

Table 16: AP (%) of adaptation from Sim10K to Cityscapes
with different λ and γ .

AP on a target domain

λ

0.1 0.5 1.0 1.5 2.0 2.5

41.4 40.9 41.6 41.9 39.7 34.5

γ

1

2

3

4

5

6

41.5 42.6 41.7 40.9 41.4 41.1

13

(a) from Cityscapes to FoggyCityscapes

(b) from PASCAL to Watercolor

Figure 7: Visualization of feature embedding on three adaptation datasets by t-SNE [20]. Red indicates the source examples
and blue indicates the target one. In each group, the ﬁrst row is the result of w/o detach model, the second row is from with
detach model. In each row, from left to right are results from features after B1 , B2 , B3 and the 384-dim context features.

(c) from PASCAL to Clipart

14

cityscapesFigure 8: More visualizations of Attention Maps on source and target domains. Top: Input images; Middle: Heatmaps from
models w/o gradient detach; Bottom: Heatmaps from models w/ gradient detach. The colors (red→blue) indicate values
from high to low.

15

SourceTarget(a) Clipart

Figure 9: More detection examples with our proposed SCL on Clipart and Watercolor. We show detections with the scores
higher than 0.5.

(b) Watercolor

16

clipart