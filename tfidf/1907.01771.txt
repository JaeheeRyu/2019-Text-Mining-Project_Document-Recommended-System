9
1
0
2

v
o

N

1
2

]

C

O

.

h

t

a

m

[

2
v
1
7
7
1
0

.

7
0
9
1

:

v

i

X

r

a

Globally Convergent Newton Methods for Ill-conditioned
Generalized Self-concordant Losses

Ulysse Marteau-Ferey, Francis Bach, Alessandro Rudi

INRIA - Département d’Informatique de l’École Normale Supérieure
PSL Research University
Paris, France

Abstract

In this paper, we study large-scale convex optimization algorithms based on the Newton
method applied to regularized generalized self-concordant losses, which include logistic regres-
sion and softmax regression. We ﬁrst prove that our new simple scheme based on a sequence
of problems with decreasing regularization parameters is provably globally convergent, that
this convergence is linear with a constant factor which scales only logarithmically with the
condition number. In the parametric setting, we obtain an algorithm with the same scaling
than regular ﬁrst-order methods but with an improved behavior, in particular in ill-conditioned
problems. Second, in the non-parametric machine learning setting, we provide an explicit
algorithm combining the previous scheme with Nyström projection techniques, and prove that
it achieves optimal generalization bounds with a time complexity of order O(ndf λ ), a memory
complexity of order O(df 2
λ ) and no dependence on the condition number, generalizing the
results known for least-squares regression. Here n is the number of observations and df λ is
the associated degrees of freedom. In particular, this is the ﬁrst large-scale algorithm to solve
logistic and softmax regressions in the non-parametric setting with large condition numbers
and theoretical guarantees.

1 Introduction

Minimization algorithms constitute a crucial algorithmic part of many machine learning methods,
with algorithms available for a variety of situations [10]. In this paper, we focus on ﬁnite sum
problems of the form

min
x∈H fλ (x) = f (x) +

(cid:107)x(cid:107)2 , with f (x) =

λ
2

1
n

fi (x),

n(cid:88)

i=1

where H is a Euclidean or a Hilbert space, and each function is convex and smooth. The running-
time of minimization algorithms classically depends on the number of functions n, the explicit
(for Euclidean spaces) or implicit (for Hilbert spaces) dimension d of the search space, and the
condition number of the problem, which is upper bounded by κ = L/λ, where L characterizes the
smoothness of the functions fi , and λ the regularization parameter.
In the last few years, there has been a strong focus on problems with large n and d, leading
to ﬁrst-order (i.e., gradient-based) stochastic algorithms, culminating in a sequence of linearly
convergent algorithms whose running time is favorable in n and d, but scale at best in
κ [15, 22,
14, 4]. However, modern problems lead to objective functions with very large condition numbers,
i.e., in many learning problems, the regularization parameter that is optimal for test predictive

√

1

 
 
 
 
 
 
√

κ is not practical anymore (see examples

performance may be so small that the scaling above in
in Sect. 5).
These ill-conditioned problems are good candidates for second-order methods (i.e., that use
the Hessians of the objective functions) such as Newton method. These methods are traditionally
discarded within machine learning for several reasons: (1) they are usually adapted to high precision
results which are not necessary for generalization to unseen data for machine learning problems [9],
(2) computing the Newton step ∆λ (x) = ∇2fλ (x)−1∇fλ (x) requires to form the Hessian and
solve the associated linear system, leading to complexity which is at least quadratic in d, and
thus prohibitive for large d, and (3) the global convergence properties are not applicable, unless
the function is very special, i.e., self-concordant [24] (which includes only few classical learning
problems), so they often are only shown to converge in a small area around the optimal x.
In this paper, we argue that the three reasons above for not using Newton method can be
circumvented to obtain competitive algorithms: (1) high absolute precisions are indeed not needed
for machine learning, but faced with strongly ill-conditioned problems, even a low-precision
solution requires second-order schemes; (2) many approximate Newton steps have been designed
for approximating the solution of the associated large linear system [1, 27, 25, 8]; (3) we propose
a novel second-order method which is globally convergent and which is based on performing
approximate Newton methods for a certain class of so-called generalized self-concordant functions
which includes logistic regression [6]. For these functions, the conditioning of the problem is also
characterized by a more local quantity: κ(cid:96) = R2/λ, where R characterizes the local evolution of
Hessians. This leads to second-order algorithms which are competitive with ﬁrst-order algorithms
for well-conditioned problems, while being superior for ill-conditioned problems which are common
in practice.

Contributions. We make the following contributions:

(a) We build a global second-order method for the minimization of fλ , which relies only on
computing approximate Newton steps of the functions fµ , µ (cid:62) λ. The number of such steps
will be of order O(c log κ(cid:96) + log 1
 ) where  is the desired precision, and c is an explicit
constant. In the parametric setting (H = Rd ), c can be as bad as
κ(cid:96) in the worst-case
but much smaller in theory and practice. Moreover in the non-parametric/kernel machine
learning setting (H inﬁnite dimensional), c does not depend on the local condition number κ(cid:96) .
(b) Together with the appropriate quadratic solver to compute approximate Newton steps, we
obtain an algorithm with the same scaling as regular ﬁrst-order methods but with an improved
behavior, in particular in ill-conditioned problems.
Indeed, this algorithm matches the
performance of the best quadratic solvers but covers any generalized self-concordant function,
up to logarithmic terms.

√

(c) In the non-parametric/kernel machine learning setting we provide an explicit algorithm com-
bining the previous scheme with Nyström projections techniques. We prove that it achieves
optimal generalization bounds with O(ndf λ ) in time and O(df 2
λ ) in memory, where n is the
number of observations and df λ is the associated degrees of freedom. In particular, this is
the ﬁrst large-scale algorithm to solve logistic and softmax regression in the non-parametric
setting with large condition numbers and theoretical guarantees.

1.1 Comparison to related work

We consider two cases for H and the functions fi that are common in machine learning: H = Rd
with linear (in the parameter) models with explicit feature maps, and H inﬁnite-dimensional,
corresponding in machine learning to learning with kernels [32]. Moreover in this section we

2

1

ﬁrst consider the quadratic case, for example the squared loss in machine learning (i.e., fi (x) =
2 (x(cid:62)zi − yi )2 for some zi ∈ H, yi ∈ R). We ﬁrst need to introduce the Hessian of the problem,
for any λ > 0, deﬁne

H(x) := ∇2f (x),

Hλ (x) := ∇2fλ (x) = H(x) + λI,

in particular we denote by H (and analogously Hλ ) the Hessian at optimum (which in case of
squared loss corresponds to the covariance matrix of the inputs).

Quadratic problems and H = Rd (ridge regression). The problem then consists in solving

a (ill-conditioned) positive semi-deﬁnite symmetric linear system of dimension d × d. Methods
based on randomized linear algebra, sketching and suitable subsampling [17, 18, 11] are able to
ﬁnd the solution with precision  in time that is O((nd + min(n, d)3 ) log(L/λ)), so essentially
independently of the condition number, because of the logarithmic complexity in λ.

Quadratic problems and H inﬁnite-dimensional (kernel ridge regression). Here the problem

corresponds to solving a (ill-conditioned) inﬁnite-dimensional linear system in a reproducing kernel
Hilbert space [32]. Since however the sum deﬁning f is ﬁnite, the problem can be projected on a
subspace of dimension at most n [5], leading to a linear system of dimension n × n. Solving it
with the techniques above would lead to a complexity of the order O(n2 ), which is not feasible
on massive learning problems (e.g., n ≈ 107 ). Interestingly these problems are usually approxi-
mately low-rank, with the rank represented by the so called effective-dimension df λ [13], counting
essentially the eigenvalues of the problem larger than λ,

df λ = Tr(HH−1
λ ).

(1)
Note that df λ is bounded by min{n, L/λ} and in many cases df λ (cid:28) min(n, L/λ). Using suitable
projection techniques, like Nyström [34] or random features [26] it is possible to further reduce
the problem to dimension df λ , for a total cost to ﬁnd the solution of O(ndf 2
λ ). Finally recent
methods [29], combining suitable projection methods with reﬁned preconditioning techniques, are
able to ﬁnd the solution with precision compatible with the optimal statistical learning error [13] in
time that is O(ndf λ log(L/λ)), so being essentially independent of the condition number of the
problem.

Convex problems and explicit features (logistic regression). When the loss function is self-

concordant it is possible to leverage the fast techniques for linear systems in approximate Newton
algorithms [25] (see more in Sec. 2), to achieve the solution in essentially O(nd + min(n, d)3 )
time, modulo logarithmic terms. However only few loss functions of interest are self-concordant,
in particular the widely used logistic and soft-max losses are not self-concordant, but generalized-
solution in O(dn + d(cid:112)nL/λ + min(n, d)3 ) time, which does not present any improvement on a
self-concordant [6]. In such cases we need to use (accelerated/stochastic) ﬁrst order optimization
methods to enter in the quadratic convergence region of Newton methods [2], which leads to a
simple accelerated ﬁrst-order method. Globally convergent second-order methods have also been
proposed to solve such problems [21], but the number of Newton steps needed being bounded only
by L/λ, they lead to a solution in O(L/λ (nd + min(n, d)3 )). With λ that could be as small as
10−12 in modern machine learning problems, this makes both these kind of approaches expensive
from a computational viewpoint for ill-conditioned problems. For such problems, with our new
global second-order scheme, the algorithm we propose achieves instead a complexity of essentially

O((nd + min(n, d)3 ) log(R2/λ)) (see Thm. 1).

3

Convex problems and H inﬁnite-dimensional (kernel logistic regression). Analogously to the

case above, it is not possible to use Newton methods proﬁtably as global optimizers on losses that
(cid:112)nL/λ) time. This can still be prohibitive in the very small regularization
are not self-concordant as we see in Sec. 3. In such cases by combining projecting techniques
developped in Sec. 4 and accelerated ﬁrst-order optimization methods, it is possible to ﬁnd a
solution in O(ndf λ + df λ
scenario, since it strongly depends on the condition number L/λ. In Sec. 4 we suitably combine our
optimization algorithm with projection techniques achieving optimal statistical learning error [23]

in essentially O(ndf λ log(R2/λ)).

First-order algorithms for ﬁnite sums.

time proportional O((n + (cid:112)nL/λ)d). This can be improved with preconditioning to O((n +
In dimension d, accelerated algorithms for strongly-
(cid:112)dL/λ)d) for large n [2]. Quasi-Newton methods can also be used [20], but typically without the
convex smooth (not necessarily self-concordant) ﬁnite sums, such as K-SVRG [4], have a running
guarantees that we provide in this paper (which are logarithmic in the condition number in natural
scenarios).

2 Background: Newton methods and generalized self concordance

In this section we start by recalling the deﬁnition of generalized self concordant functions and
motivate it with examples. We then recall basic facts about Newton and approximate Newton
methods, and present existing techniques to efﬁciently compute approximate Newton steps. We
start by introducing the deﬁnition of generalized self-concordance, that here is an extension of the
one in [6].
Deﬁnition 1 (generalized self-concordant (GSC) function). Let H be a Hilbert space. We say that
f is a generalized self-concordant function on G ⊂ H, when G is a bounded subset of H and f is a
convex and three times differentiable mapping on H such that

∀x ∈ H, ∀h, k ∈ H, ∇(3)f (x)[h, k , k ] (cid:54) supg∈G |g · h| ∇2f (x)[k , k ].

We will usually denote by R the quantity supg∈G (cid:107)g(cid:107) < ∞ and often omit G when it is clear
from the context (for simplicity think of G as the ball in H centered in zero and with radius R > 0,
then supg∈G |g · h| = R(cid:107)h(cid:107)). The globally convergent second-order scheme we present in Sec. 3
is speciﬁc to losses which satisfy this generalized self-concordance property. The following loss
functions, which are widely used in machine learning, are generalized-self-concordant, and motivate
this work.
Example 1 (Application to ﬁnite-sum minimization). The following loss functions are generalized
(b) Softmax regression: fi (x) = log (cid:0) (cid:80)k
self-concordant functions, but not self-concordant:
(a) Logistic regression: fi (x) = log(1 + exp(−yiw(cid:62)
i x)), where x, wi ∈ Rd and yi ∈ {−1, 1}.
yi wi , where now x ∈ Rd×k and
yi ∈ {1, . . . , k} and xj denotes the j -th column of x.
(c) Generalized linear models with bounded features (see details in [7, Sec. 2.1]), which include
conditional random ﬁelds [33].
(d) Robust regression: fi (x) = ϕ(yi − w(cid:62)
Note that these losses are not self-concordant in the sense of [25]. Moreover, even if the losses
fi are self-concordant, the objective function f is not necessarily self-concordant, making any
attempt to prove the self-concordance of the objective function f almost impossible.

j wi )(cid:1) − x(cid:62)
j=1 exp(x(cid:62)

i x) with ϕ(u) = log(eu + e−u ).

4

Newton method (NM). Given x0 ∈ H, the Newton method consists in doing the following
update:

xt+1 = xt − ∆λ (xt ),

∆λ (xt ) := H−1
λ (xt )∇fλ (xt ).

(2)
The quantity ∆λ (x) := H−1
λ (x)∇fλ (x) is called the Newton step at point x, and x − ∆λ (x) is the
minimizer of the second order approximation of fλ around x. Newton methods enjoy the following
key property: if x0 is close enough to the optimum, the convergence to the optimum is quadratic
and the number of iterations required to a given precision is independent of the condition number
of the problem [12].
However Newton methods have two main limitations: (a) the region of quadratic convergence
can be quite small and reaching the region can be computationally expensive, since it is usually
done via ﬁrst order methods [2] that converge linearly depending on the condition number of the
problem, (b) the cost of computing the Hessian can be really expensive when n, d are large, and
also (c) the cost of computing ∆λ (xt ) can be really prohibitive. In the rest of the section we recall
some ways to deal with (b) and (c). Our main result of Sec. 3 is to provide globalization scheme for
the Newton method to tackle problem (a), which is easily integrable with approximate techniques
to deal with (b) ans (c), to make second-order technique competitive.

Approximate Newton methods (ANM) and approximate solutions to linear systems. Com-

(cid:101)∆λ ≈ ∆λ (xt ).

xt+1 = xt − (cid:101)∆λ (xt ),

puting exactly the Newton increment ∆λ (xt ), which corresponds essentially to the solution of
a linear system, can be too expensive when n, d are large. A natural idea is to approximate the
Newton iteration, leading to approximate Newton methods,
In this paper, more generally we consider any technique to compute (cid:101)∆λ (xt ) that provides a relative
(3)
approximation [16] of ∆λ (xt ) deﬁned as follows.
Deﬁnition 2 (relative approximation). Let ρ < 1, let A be an invertible positive deﬁnite Her-
mitian operator on H and b in H. We denote by LinApprox(A, b, ρ) the set of all ρ-relative
approximations of z ∗ = A−1 b, i.e., LinApprox(A, b, ρ) = {z ∈ H | (cid:107)z − z ∗(cid:107)A (cid:54) ρ(cid:107)z ∗(cid:107)A}.
mating linear systems have been used to compute (cid:101)∆λ , in particular sketching of the Hessian matrix
via fast transforms and subsampling (see [25, 8, 2] and references therein). Assuming for simplicity
i x), with (cid:96)i : R → R and wi ∈ H, it holds:

Sketching and subsampling for approximate Newton methods. Many techniques for approxi-

that fi = (cid:96)i (w(cid:62)

n(cid:88)

i=1

5

H(x) =

1
n

i (w(cid:62)
i x)wiw(cid:62)
i = V (cid:62)
x Vx ,

(cid:96)(2)

(4)

((cid:96)(2)

min(n, d), (pj )n

i (w(cid:62)
i x))1/2 and W ∈ Rn×d deﬁned as W = (w1 , . . . , wn )(cid:62) .

with Vx ∈ Rn×d = DxW , where Dx ∈ Rn×n is a diagonal matrix deﬁned as (Dx )ii =
(cid:101)Hλ (x)−1∇fλ (x), in particular, in the case of subsampling (cid:101)H(x) = (cid:80)Q
Both sketching and subsampling methods approximate z ∗ = Hλ (x)−1∇fλ (x) with ˜z =
where Q (cid:28)
with suitable probabilities. Sketching methods instead use (cid:101)H(x) = (cid:101)V (cid:62)
j=1 are suitable weights and (ij )Q
j=1 are indices selected at random from {1, . . . , n}
x (cid:101)Vx , with (cid:101)Vx = ΩVx with
Ω ∈ RQ×n a structured matrix such that computing (cid:101)Vx has a cost in the order of O(nd log n); to
this end usually Ω is based on fast Fourier or Hadamard transforms [25]. Note that essentially all
the techniques used in approximate Newton methods guarantee relative approximation. In particular
the following results can be found in the literature (see Lemmas 28 and 29 in Appendix I and [25],
Lemma 2 for more details).

j=1 pj wij w(cid:62)
ij

i

Lemma 1. Let x, b ∈ H and assume that (cid:96)(2)

(cid:54) a for a > 0. With probability 1 − δ the following

methods output an element in LinApprox(Hλ (x), b, ρ), in O(Q2d + Q3 + c) time, O(Q2 + d)

c = O(1).

space:
(a) Subsampling with uniform sampling (see [27, 28]), where Q = O(ρ−2a/λ log 1
λδ ) and
(b) Subsampling with approximate leverage scores [27, 3, 28]), where Q = O(ρ−2 ¯df λ log 1/λδ), c =
(c) Sketching with fast Hadamard transform [25], where Q = O(ρ−2 ¯df λ log a/λδ), c = O(nd log n).

2 ) and ¯df λ = Tr(W (cid:62)W (W (cid:62)W + λ/aI )−1 ) [30]. Note that ¯df λ (cid:54) min(n, d).

O(min(n, a/λ) ¯df λ

3 Globally convergent scheme for ANM algorithms on GSC func-
tions

The algorithm is based on the observation that when fλ is generalized self concordant, there exists
a region where t steps of ANM converge as fast as 2−t . Our idea is to start from a very large
regularization parameter λ0 , such that we are sure that x0 is in the convergence region and perform
some steps of ANM such that the solution enters in the convergence region of fλ1 , with λ1 = qλ0
with q < 1, and to iterate this procedure until we enter the convergence region of fλ . First we
deﬁne the region of interest and characterize the behavior of NM and ANM in the region, then we
analyze the globalization scheme.

Preliminary results: the Dikin ellipsoid. We consider the following region that we prove to
be contained in the region of quadratic convergence for the Newton method and that will be
useful to build the globalization scheme. Let c, R > 0 and fλ be generalized self-concordant with
coefﬁcient R, we call Dikin ellipsoid and denote by Dλ (c) the region

Dλ (c) := (cid:8)x | νλ (x) (cid:54) c

√

λ/R(cid:9), with νλ (x) := (cid:107)∇fλ (x)(cid:107)H−1

λ (x) ,

where νλ (x) is usually called the Newton decrement and (cid:107)x(cid:107)A stands for (cid:107)A1/2x(cid:107).
Lemma 2. Let λ > 0, c (cid:54) 1/7, let fλ be generalized self-concordant and x ∈ Dλ (c). Then it
λ ) (cid:54) νλ (x)2 . Moreover Newton method starting from x0 has
holds: 1
quadratic convergence, i.e., let xt be obtained via t ∈ N steps of Newton method in Eq. (2), then
convergence rate, i.e., let xt given by Eq. (3), with (cid:101)∆t ∈ LinApprox(Hλ (xt ), ∇fλ (xt ), ρ) and
νλ (xt ) (cid:54) 2−(2t−1)νλ (x0 ). Finally, approximate Newton methods starting from x0 have a linear

4 νλ (x)2 (cid:54) fλ (x) − fλ (x(cid:63)

ρ (cid:54) 1/7, then νλ (xt ) (cid:54) 2−tνλ (x0 ).

This result is proved in Lemma 11 in Appendix B.3. The crucial aspect of the result above is
that when x0 ∈ Dλ (c), the convergence of the approximate Newton method is linear and does not
depend on the condition number of the problem. However Dλ (c) itself can be very small depending
on
λ/R. In the next subsection we see how to enter in Dλ (c) in an efﬁcient way.

√

Entering the Dikin ellipsoid using a second-order scheme. The lemma above shows that Dλ (c)

is a good region where to use the approximate Newton algorithm on GSC functions. However
such approaches require a number of steps that is usually proportional to (cid:112)L/λ making them
the region itself is quite small, since it depends on
λ/R. Some other globalization schemes
arrive to regions of interest by ﬁrst-order methods or back-tracking schemes [2, 1]. However
non-beneﬁcial in machine learning contexts. Here instead we consider the following simple scheme
where ANMρ (fλ , x, t) is the result of a ρ-relative approximate Newton method performing t steps
of optimization starting from x.

√

6

The main ingredient to guarantee the scheme to work is the following lemma (see Lemma 13 in
Appendix C.1 for a proof).
Lemma 3. Let µ > 0, c < 1 and x ∈ H. Let s = 1 + R(cid:107)x(cid:107)/c, then for q ∈ [1 − 2/(3s), 1)

Dµ (c/3) ⊆ Dqµ (c).

Now we are ready to show that we can guarantee the loop invariant xk ∈ Dµk (c). Indeed

assume that xk−1 ∈ Dµk−1 (c). Then νµk−1 (xk−1 ) (cid:54) c
µk−1/R. By taking t = 2, ρ = 1/7,
and performing xk = ANMρ (fµk−1 , xk−1 , t), by Lemma 2, νµk−1 (xk ) (cid:54) 1/4νµk−1 (xk−1 ) (cid:54)
c/4

µk−1/R, i.e., xk ∈ Dµk−1 (c/4). If qk is large enough, this implies that xk ∈ Dqk µk−1 (c) =
Dµk (c), by Lemma 3. Now we are ready to state our main theorem of this section.

√

√

Proposed Globalization Scheme

Phase I: Getting in the Dikin ellispoid of fλ

For k ∈ N

Start with x0 ∈ H, µ0 > 0, t, T ∈ N and (qk )k∈N ∈ (0, 1].
xk+1 ← ANMρ (fµk , xk , t)

µk+1 ← qk+1µk

Stop when µk+1 < λ and set xlast ← xk .
Phase II: reach a certain precision starting from inside the Dikin ellipsoid

Return (cid:98)x ← ANMρ (fλ , xlast , T )

(cid:16)

(cid:16)

Fully adaptive method. The scheme presented above converges with the following parameters.
(cid:112)1 ∨ (λ−1/R2 )(cid:101). Then denoting
Theorem 1. Let  > 0. Set µ0 = 7R(cid:107)∇f (0)(cid:107), x0 = 0, and perform the globalization scheme

above for ρ (cid:54) 1/7, t = 2, and qk = 1/3+7R(cid:107)xk (cid:107)
1+7R(cid:107)xk (cid:107) , T = (cid:100)log2
fλ ((cid:98)x) − fλ (x(cid:63)
λ ) (cid:54) ,
K (cid:54) (cid:98)(3 + 11R(cid:107)x(cid:63)
λ(cid:107)) log(7R(cid:107)∇f (0)(cid:107)/λ)(cid:99) .

by K the number of steps performed in the Phase I, it holds:

Note that the theorem above (proven in Appendix C.3) guarantees a solution with error 
with K steps of ANM each performing 2 iterations of approximate linear system solving, plus a
ﬁnal step of ANM which performs T iterations of approximate linear system solving. In case of
i x), with (cid:96)i : R → R, wi ∈ H with (cid:96)(2)
(cid:54) a, for a > 0, the ﬁnal runtime cost of
the proposed scheme to achieve precision , when combined with of the methods for approximate
linear system solving from Lemma 1 (i.e. sketching), is O(Q2 + d) in memory and

fi (x) = (cid:96)i (w(cid:62)

i

R(cid:107)x(cid:63)
λ(cid:107) log

R
λ

O

+ log

(nd log n + dQ2 + Q3 )

in time, Q = O
where ¯df λ , deﬁned in Lemma 1, measures the effective dimension of the correlation matrix W (cid:62)W
with W = (w1 , . . . , wn )(cid:62) ∈ Rn×d , corresponding essentially to the number of eigenvalues of
W (cid:62)W larger than λ/a. In particular note that ¯df λ (cid:54) min(n, d, rank(W ), ab2/λ), with b :=
maxi (cid:107)wi(cid:107), and usually way smaller than such quantities.
Remark 1. The proposed method does not depend on the condition number of the problem L/λ,
but on the term R(cid:107)x(cid:63)
λ(cid:107) which can be in the order of R/
λ in the worst case, but usually way
smaller. For example, it is possible to prove that this term is bounded by an absolute constant not
depending on λ, if at least one minimum for f exists. In the appendix (see Proposition 7), we show
a variant of this adaptive method which can leverage the regularity of the solution with respect to
λ ) instead of R(cid:107)x(cid:63)
λ(cid:107).
the Hessian, i.e., depending on the smaller quantity R

λ(cid:107)H−1
λ (x(cid:63)

λ(cid:107)x(cid:63)

√

√

(cid:16) ¯df λ log

(cid:17)

,

1
λδ

(cid:17)(cid:17)

λ
R

7

Finally note that it is possible to use qk = q ﬁxed for all the iterations and way smaller than the
one in Thm. 1, depending on some regularity properties of H (see Proposition 8 in Appendix C.2).

4 Application to the non-parametric setting: Kernel methods

In supervised learning the goal is to predict well on future data, given the observed training dataset.
Let X be the input space and Y ⊆ Rp be the output space. We consider a probability distribution P
over X × Y generating the data and the goal is to estimate g∗ : X → Y solving the problem

g∗ = arg min

g :X →Y

L(g), L(g) = E[(cid:96)(g(x), y)],

(5)

1
n

w∈H

n

(cid:80)n

(cid:80)n

dataset (xi , yi )n

i=1 fi (w) + λ(cid:107)w(cid:107)2 ,

(cid:98)wλ = arg min

(cid:98)gλ (x) = (cid:98)w(cid:62)
λ φ(x),

for a given loss function (cid:96) : Y × Y → R. Note that P is not known, and accessible only via the
the regularized minimizer of the empirical risk (cid:98)L(g) = 1
i=1 , with n ∈ N, independently sampled from P . A prototypical estimator for g∗ is
of φ(x), that is, G = {w(cid:62)φ(·) | w ∈ H}. Then the regularized minimizer of (cid:98)L, denoted by (cid:98)gλ ,
i=1 (cid:96)(g(xi ), yi ) over a suitable space
of functions G . Given φ : X → H a common choice is to select G as the set of linear functions
corresponds to
Learning theory guarantees how fast (cid:98)gλ converges to the best possible estimator g∗ with respect
(6)
to the number of observed examples, in terms of the so called excess risk L((cid:98)gλ ) − L(g∗ ). The
following theorem recovers the minimax optimal learning rates for squared loss and extend them to
any generalized self-concordant loss function.
Note on df λ . In this section, we always denote with df λ the effective dimension of the problem
in Eq. (5). When the loss belongs to the family of generalized linear models (see Example 1) and
if the model is well-speciﬁed, then df λ is deﬁned exactly as in Eq. (1) otherwise we need a more
reﬁned deﬁnition (see [23] or Eq. (30) in Appendix D).
Then there exists c0 not depending on n, λ, δ, df λ , C, g∗ , such that if (cid:112)df λ/n, bλ (cid:54) λ1/2/R, and
Theorem 2 (from [23], Thm. 4). Let λ > 0, δ ∈ (0, 1]. Let (cid:96) be generalized self-concordant
with parameter R > 0 and supx∈X (cid:107)φ(x)(cid:107) (cid:54) C < ∞. Assume that there exists g∗ minimizing L.
n (cid:62) C/λ log(δ−1C/λ) the following holds with probability 1 − δ :

fi (w) = (cid:96)(w(cid:62)φ(xi ), yi ).

L((cid:98)gλ ) − L(g∗ ) (cid:54) c0

(cid:16) df λ
n

(cid:17)

+ b2

λ

log(1/δ),

bλ := λ(cid:107)g∗(cid:107)H−1

λ (g∗ ) .

(7)

b2

Under standard regularity assumptions of the learning problems [23], i.e., (a) the capacity
condition σj (H(g∗ )) (cid:54) C j−α , for α (cid:62) 1, C > 0 (i.e., a decay of eigenvalues σj (H(g∗ )) of the
Hessian at the optimum), and (b) the source condition g∗ = H(g∗ )r v , with v ∈ H and r > 0
(i.e., the control of the optimal g∗ for a speciﬁc Hessian-dependent norm), df λ (cid:54) C (cid:48)λ−1/α and
(cid:54) C (cid:48)(cid:48)λ1+2r , leading to the following optimal learning rate,
Now we propose an algorithmic scheme to compute efﬁciently an approximation of (cid:98)gλ that achieves
(8)
the same optimal learning rates. First we need to introduce the technique we are going to use.

L((cid:98)gλ ) − L(g∗ ) (cid:54) c1n

1+α+2rα log(1/δ), when λ = n

α
1+α+2rα .

− 1+2rα

−

λ

8

Nyström projection.

It consists in suitably selecting { ¯x1 , . . . , ¯xM } ⊂ {x1 , . . . , xn}, with M (cid:28)
n and computing ¯gM ,λ , i.e., the solution of Eq. (6) over HM = span{φ( ¯x1 ), . . . , φ( ¯xM )} instead
of H. In this case the problem can be reformulated as a problem in RM as

¯fλ (α),

(9)

n(cid:88)

i=1

¯f (α) =

¯fi (α) + λ(cid:107)α(cid:107)2 ,

¯gM ,λ = ¯α(cid:62)
M ,λT−1v(x),

1
¯αM ,λ = arg min
α∈RM
n
where ¯fi (α) = (cid:96)(v(xi )(cid:62)T−1α, yi ) and v(x) ∈ RM , v(x) = (k(x, ¯x1 ), . . . , k(x, ¯xM )) with

k(x, x(cid:48) ) = φ(x)(cid:62)φ(x(cid:48) ) the associated positive-deﬁnite kernel [32], while T is the upper triangular
matrix such that K = T(cid:62)T, with K ∈ RM ×M with Kij = k( ¯xi , ¯xj ). In the next theorem we
choosing the Nyström points { ¯x1 , . . . , ¯xM }.
characterize the sufﬁcient M to achieve minimax optimal rates, for two standard techniques of
Theorem 3 (Optimal rates for learning with Nyström). Let λ > 0, δ ∈ (0, 1]. Assume the conditions
of Thm. 2. Then the excess risk of ¯gM ,λ is bounded with prob. 1 − 2δ as in Eq. (7) (with c(cid:48)
1 ∝ c1 ),
when
(1) Uniform Nyström method [28, 29] is used and M (cid:62) C1/λ log(C2/λδ).
(2) Approximate leverage score method [3, 28, 29] is used and M (cid:62) C3 df λ log(C4/λδ).

Here C, C1 , C2 , C4 do not depend on λ, n, M , df λ , δ .

Thm. 3 generalizes results for learning with Nyström and squared loss [28], to GSC losses. It is
proved in Thm. 6, in Appendix D.4. As in [28], Thm. 3 shows that Nyström is a valid technique
for dimensionality reduction. Indeed it is essentially possible to project the learning problem on a
subspace HM of dimension M = O(c/λ) or even as small as M = O(df λ ) and still achieve the
optimal rates of Thm. 2. Now we are ready to introduce our algorithm.

Proposed algorithm. The algorithm conceptually consists in (a) performing a projection step
with Nyström, and (b) solving the resulting optimization problem with the globalization scheme
proposed in Sec. 3 based on ANM in Eq. (3). In particular, we want to avoid to apply explicitly
T−1 to each v(xi ) in Eq. (9), which would require O(nM 2 ) time. Then we will use the following
approximation technique based only on matrix vector products, so we can just apply T−1 to α at
each iteration, with a total cost proportional only to O(nM + M 2 ) per iteration. Given α, ∇ ¯fλ (α),
we approximate z ∗ = ¯Hλ (α)−1∇ ¯fλ (α), where ¯Hλ is the Hessian of ¯fλ (α), with ˜z deﬁned as

˜z = prec-conj-gradt ( ¯Hλ (α), ∇ ¯fλ (α)),

where prec-conj-gradt corresponds to performing t steps of preconditioned conjugate gradi-
ent [19] with preconditioner computed using a subsampling approach for the Hessian among the
ones presented in Sec. 2, in the paragraph starting with Eq. (4). The pseudocode for the whole
procedure is presented in Alg. 1, Appendix E. This technique of approximate linear system solving
has been studied in [29] in the context of empirical risk minimization for squared loss.
Lemma 4 ([29]). Let λ > 0, α, b ∈ RM . The previous method, applied with t = O(log 1/ρ),
outputs an element of LinApprox( ¯Hλ (α), b, ρ), with probability 1 − δ with complexity O((nM +

M 2Q + M 3 + c)t) in time and O(M 2 + n) in space, with Q = O(C1/λ log(C1/λδ)), c = O(1)
λ min(n, 1

if uniform sub-sampling is used or Q = O(C2df λ log(C1/λδ)), c = O(df 2
sampling with leverage scores is used [30].

λ )) if sub-

A more complete version of this lemma is shown in Proposition 12 in Appendix D.5.1. We
conclude this section with a result proving the learning properties of the proposed algorithm.

9

Theorem 4 (Optimal rates for the proposed algorithms). Let λ > 0 and  < λ/R2 . Under the
hypotheses of Thm. 3, if we set M as in Thm. 3, Q as in Lemma 4 and setting the globalization
scheme as in Thm. 1, then the proposed algorithm (Alg. 1, Appendix E) ﬁnishes in a ﬁnite number of
newton steps Nns = O(R(cid:107)g∗(cid:107) log(C/λ) + log(C/)) and returns a predictor gQ,M ,λ of the form
gQ,M ,λ = α(cid:62)T−1v(x). With probability at least 1 − δ , this predictor satisﬁes:

(cid:16) df λ
n

(cid:17)

L(gQ,M ,λ ) − L(g∗ ) (cid:54) c0

+ b2
λ + 

log(1/δ),

bλ := λ(cid:107)g∗(cid:107)H−1

λ (g∗ ) .

(10)

The theorem above (see Proposition 14, Appendix D.6 for exacts quantiﬁcations) shows that the
proposed algorithm is able to achieve the same learning rates of plain empirical risk minimization as
in Thm. 2. The total complexity of the procedure, including the cost of computing the preconditioner,
the selection of the Nyström points via approximate leverage scores and also the computation of
the leverage scores [30] is then

O (cid:0)R(cid:107)g∗(cid:107) log(R2/λ) (cid:0)n df λ log(C λ−1 δ−1 ) cX + + df 3
λ log3 (C λ−1 δ−1 ) + min(n, C/λ) df 2

(cid:1)(cid:1)

λ

λ

O (cid:0)R log(R2/λ) log3 (C λ−1 δ−1 ) (cid:107)g∗(cid:107) · n · df λ · cX
df 2

in time and O(df 2
λ log2 (C λ−1 δ−1 )) in space, where cX is the cost of computing the inner product
k(x, x(cid:48) ) (in the kernel setting assumed when the input space X is X = Rp it is c = O(p)). As
noted in [30], under the standard regularity assumptions on the learning problem seen above,
(cid:54) df λ/λ (cid:54) n when the optimal λ is chosen. So the total computational complexity is
(cid:1) in time, O(df 2
even implicitly on (cid:112)C/λ, but only on log(C/λ), so the algorithm runs in essentially O(ndf λ ),
First note, the fact that due to the statistical properties of the problem the complexity does not depend
(cid:112)nC/λ) of the accelerated ﬁrst-order methods we develop in Appendix F and
(cid:112)C/λ) of other Newton schemes (see Sec. 1.1). To our knowledge, this is the ﬁrst
compared to O(df λ
with complexity only (cid:101)O(ndf λ ). This generalizes similar results for squared loss [29, 30].
algorithm to achieve optimal statistical learning rates for generalized self-concordant losses and

λ ·log2 (C λ−1 δ−1 )) in space.

the O(ndf λ

5 Experiments

The code necessary to reproduce the following experiments is available on GitHub at https:

//github.com/umarteau/Newton-Method-for-GSC-losses- .

We compared the performances of our algorithm for kernel logistic regression on two large
scale classiﬁcation datasets (n ≈ 107 ), Higgs and Susy, pre-processed as in [29]. We implemented
the algorithm in pytorch and performed the computations on 1 Tesla P100-PCIE-16GB GPU. For
Susy (n = 5 × 106 , p = 18): we used Gaussian kernel with k(x, x(cid:48) ) = e−(cid:107)x−x(cid:48) (cid:107)2 /(2σ2 ) , with
σ = 5, which we obtained through a grid search (in [29], σ = 4 is taken for the ridge regression);
M = 104 Nyström centers and a subsampling Q = M for the preconditioner, both obtained with
uniform sampling. Analogously for Higgs (n = 1.1 × 107 , p = 28): , we used a Gaussian kernel
with σ = 5 and M = 2.5 × 104 and Q = M , using again uniform sampling. To ﬁnd reasonable
λ for supervised learning applications, we cross-validated λ ﬁnding the minimum test error at
λ = 10−10 for Susy and λ = 10−9 for Higgs (see Figs. 2 and 3 in Appendix F) for such values our
algorithm and the competitor achieve an error of 19.5% on the test set for Susy, comparable to the
state of the art (19.6% [29]) and analogously for Higgs (see Appendix F). We then used such λ’s as
regularization parameters and compared our algorithm with a well known accelerated stochastic
gradient technique Katyusha SVRG (K-SVRG) [4], tailored to our problem using mini batches.
In Fig. 1 we show the convergence of the training loss and classiﬁcation error with respect to the
number of passes on the data, of our algorithm compared to K-SVRG. It is possible to note our

10

Figure 1: Training loss and test error as as function of the number of passes on the data for our
algorithm vs. K-SVRG. on the (left) Susy and (right) Higgs data sets.
methods go as O((n + (cid:112)nL/λ)df λ ). Moreover, as mentioned in the introduction, this highlights
algorithm is order of magnitude faster in achieving convergence, validating empirically the fact
that the proposed algorithm scales as O(ndf λ ) in learning settings, while accelerated ﬁrst order
the fact that precise optimization is necessary to achieve a good performance in terms of test error.
Finally, note that since a pass on the data is much more expensive for K-SVRG than for our second
order method (see Appendix F for details), the difference in computing time between the second
order scheme and K-SVRG is even more in favour of our second order method (see Figs. 4 and 5 in
Appendix F).

Acknowledgments

We acknowledge support from the European Research Council (grant SEQUOIA 724063).

References

[1] Murat A. Erdogdu and Andrea Montanari. Convergence rates of sub-sampled Newton methods.
Technical Report 1508.02810, ArXiv, 2015.

[2] Naman Agarwal, Brian Bullins, and Elad Hazan. Second-order stochastic optimization for
machine learning in linear time. J. Mach. Learn. Res., 18(1):4148–4187, January 2017.

[3] Ahmed Alaoui and Michael W Mahoney. Fast randomized kernel ridge regression with
statistical guarantees. In Advances in Neural Information Processing Systems, pages 775–783,
2015.

[4] Zeyuan Allen-Zhu. Katyusha: The ﬁrst direct acceleration of stochastic gradient methods. In
Proceedings of the Symposium on Theory of Computing, pages 1200–1205, 2017.

[5] Nachman Aronszajn. Theory of reproducing kernels. Transactions of the American Mathe-
matical Society, 68(3):337–404, 1950.

[6] Francis Bach. Self-concordant analysis for logistic regression. Electronic Journal of Statistics,
4:384–414, 2010.

[7] Francis Bach. Adaptivity of averaged stochastic gradient descent to local strong convexity for
logistic regression. Journal of Machine Learning Research, 15(1):595–627, 2014.

[8] Raghu Bollapragada, Richard H. Byrd, and Jorge Nocedal. Exact and inexact subsampled
newton methods for optimization. IMA Journal of Numerical Analysis, 39(2):545–578, 2018.

11

020406080100120passes over data19.419.619.820.020.220.420.620.821.0classification error104103102101distance to optimumsecond orderK-SVRG20406080100120passes over data27.828.028.228.428.628.829.0classification error106105104103102101distance to optimumsecond orderK-SVRG[9] Léon Bottou and Olivier Bousquet. The tradeoffs of large scale learning. In Advances in
Neural Information Processing Systems, pages 161–168, 2008.

[10] Léon Bottou, Frank E. Curtis, and Jorge Nocedal. Optimization methods for large-scale
machine learning. Siam Review, 60(2):223–311, 2018.

[11] Christos Boutsidis and Alex Gittens. Improved matrix algorithms via the subsampled ran-
domized hadamard transform. SIAM Journal on Matrix Analysis and Applications, 34(3):
1301–1340, 2013.

[12] Stephen Boyd and Lieven Vandenberghe. Convex optimization. Cambridge university press,
2004.

[13] A. Caponnetto and E. De Vito. Optimal rates for the regularized least-squares algorithm.
Found. Comput. Math., 7(3):331–368, July 2007.

[14] Aaron Defazio. A simple practical accelerated method for ﬁnite sums. In Advances in Neural
Information Processing Systems, pages 676–684, 2016.

[15] Aaron Defazio, Francis Bach, and Simon Lacoste-Julien. SAGA: A fast incremental gradient
method with support for non-strongly convex composite objectives. In Advances in Neural
Information Processing systems, pages 1646–1654, 2014.

[16] Peter Deuﬂhard. Newton Methods for Nonlinear Problems: Afﬁne Invariance and Adaptive
Algorithms. Springer, 2011.

[17] Petros Drineas, Michael W Mahoney, Shan Muthukrishnan, and Tamás Sarlós. Faster least
squares approximation. Numerische mathematik, 117(2):219–249, 2011.

[18] Petros Drineas, Malik Magdon-Ismail, Michael W Mahoney, and David P Woodruff. Fast
approximation of matrix coherence and statistical leverage. Journal of Machine Learning
Research, 13(Dec):3475–3506, 2012.

[19] Gene H. Golub and Charles F. Van Loan. Matrix Computations, volume 3. JHU Press, 2012.

[20] Robert Gower, Filip Hanzely, Peter Richtárik, and Sebastian U. Stich. Accelerated stochas-
tic matrix inversion: general theory and speeding up BFGS rules for faster second-order
optimization. In Advances in Neural Information Processing Systems, pages 1619–1629,
2018.

[21] Sai Praneeth Karimireddy, Sebastian U. Stich, and Martin Jaggi. Global linear convergence
of newton’s method without strong-convexity or lipschitz gradients. CoRR, abs/1806.00413,

2018. URL http://arxiv.org/abs/1806.00413.

[22] Hongzhou Lin, Julien Mairal, and Zaid Harchaoui. A universal catalyst for ﬁrst-order
optimization. In Advances in Neural Information Processing Systems, pages 3384–3392,
2015.

[23] Ulysse Marteau-Ferey, Dmitrii Ostrovskii, Francis Bach, and Alessandro Rudi. Beyond
least-squares: Fast rates for regularized empirical risk minimization through self-concordance.
In Proceedings of the Conference on Computational Learning Theory, 2019.

[24] Arkadii Nemirovskii and Yurii Nesterov. Interior-point polynomial algorithms in convex
programming. Society for Industrial and Applied Mathematics, 1994.

12

[25] Mert Pilanci and Martin J Wainwright. Newton sketch: A near linear-time optimization
algorithm with linear-quadratic convergence. SIAM Journal on Optimization, 27(1):205–245,
2017.

[26] Ali Rahimi and Benjamin Recht. Random features for large-scale kernel machines.
Advances in Neural Information Processing Systems, pages 1177–1184, 2008.

In

[27] Farbod Roosta-Khorasani and Michael W. Mahoney. Sub-sampled Newton methods. Math.
Program., 174(1-2):293–326, 2019.

[28] Alessandro Rudi, Raffaello Camoriano, and Lorenzo Rosasco. Less is more: Nyström
computational regularization. In Advances in Neural Information Processing Systems 28,
pages 1657–1665. 2015.

[29] Alessandro Rudi, Luigi Carratino, and Lorenzo Rosasco. FALKON: An optimal large scale
kernel method. In Advances in Neural Information Processing Systems 30, pages 3888–3898.
2017.

[30] Alessandro Rudi, Daniele Calandriello, Luigi Carratino, and Lorenzo Rosasco. On fast
leverage score sampling and optimal learning. In Advances in Neural Information Processing
Systems, pages 5672–5682, 2018.

[31] Y. Saad. Iterative Methods for Sparse Linear Systems. Society for Industrial and Applied
Mathematics, Philadelphia, PA, USA, 2nd edition, 2003.

[32] John Shawe-Taylor and Nello Cristianini. Kernel Methods for Pattern Analysis. Cambridge
University Press, 2004.

[33] Charles Sutton and Andrew McCallum. An introduction to conditional random ﬁelds. Foun-
dations and Trends R(cid:13) in Machine Learning, 4(4):267–373, 2012.
[34] Christopher K. I. Williams and Matthias Seeger. Using the Nyström method to speed up
kernel machines. In Advances in Neural Information Processing Systems, pages 682–688,
2001.

13

Organization of the Appendix

A. Main results on generalized self-concordant functions

Notations, deﬁnitions and basic results concerning generalized self-concordant functions.

B. Results on approximate Newton methods

In this section, the interaction between the notion of Dikin ellipsoid, approximate Newton
methods and generalized self-concordant functions is studied. The results needed in the main
paper are all concentrated in Appendix B.3. In particular the results in Lemma 2 are proven
in a more general form in Lemma 11.

C. Proof of bounds for the globalization scheme

In this section, we leverage the results of the previous two sections to analyze the globalization
scheme.

C.1. Main technical lemmas

We start by proving the result on the inclusion of Dikin ellipsoids (Lemma 3).

C.2. Proof of main theorems

In particular, a general version of Thm. 1 is proven. Moreover Remark 1 is proven in
Proposition 7, while the ﬁxed scheme to choose (qk )k∈N is proven in Proposition 8.

C.3. Proof of Thm. 1

Finally, we prove the properties of the globalization schemes presented in Thm. 1.

D. Non-parametric learning with generalized self-concordant functions

In this section, some basic results about non-parametric learning with generalized self-
concordant functions are recalled and the main results of Sec. 4 are proven.

D.1. General setting and assumptions, statistical result for regularized ERM.

More details about the generalization properties of empirical risk minimization as well
as the optimal rates in Thm. 2 are recalled.

D.2. Reducing the dimension: projecting on a subspace using Nyström sub-sampling.
D.3. Sub-sampling techniques.

The basics of uniform sub-sampling and sub-sampling with approximate leverage scores
are recalled.

D.4. Selecting the M Nyström points

Thm. 3 is proven in a more general version in Thm. 6.

D.5 Performing the globalization scheme to approximate βM ,λ

A general scheme is proposed to solve the projected problem approximately using the
globalization scheme.

D.5.1. Performing approximate Newton steps
D.5.2. Applying the globalization scheme to control (cid:98)νM ,λ (β )

We start by describing the way of computing approximate Newton steps. A
generalized version of Lemma 4 is proven in Proposition 12.

We then completely analyse the approximating of βM ,λ from an optimization point
of view (see Proposition 13).

14

D.6. Final algorithm and results

Finally, the proof of Thm. 4 is provided, using the results of the previous subsections.

E. Algorithm

In this section, the pseudocode for the algorithm presented in Sec. 4 and analyzed in Thm. 7
is provided.

F. Experiments

In this section, more details about the experiments are provided.

G. Solving a projected problem to reduce dimension

In this section, more details about the problem of randomized projections are provided.

G.2. Relating the projected to the original problem

In particular, results to relate the ERM with the projected ERM in terms of excess risk
are provided for generalized self-concordant functions.

H. Relations between statistical problems and empirical problem.

In this section, we provide results to relate excess expected risk with excess empirical risk
for generalized self-concordant functions.

I. Multiplicative approximations for Hermitian operators

In this section, some general analytic results on multiplicative approximations for Hermitian
operators are derived. Moreover they are used to provide a simpliﬁed proof for the results in
Lemma 1. See in particular Lemmas 28 and 29 and [25], Lemma 2.

A Main results on generalized self-concordant functions

In this section, we start by introducing a few notations. We deﬁne the key notion of generalized self-
concordance in Appendix A.1, and present the main results concerning generalized self-concordant
functions. In Appendix A.2, we describe how generalized self-concordance behaves with respect to
an expectation or to certain relaxations.

Notations Let λ (cid:62) 0 and A be a bounded positive semideﬁnite Hermitian operator on H. We
denote with I the identity operator, and

(11)
(12)
Let f be a twice differentiable convex function on a Hilbert space H. We adopt the following
notation for the Hessian of f :

(cid:107)x(cid:107)A := (cid:107)A1/2x(cid:107),
Aλ := A + λI.

∀x ∈ H, Hf (x) := ∇2f (x) ∈ L(H).

15

For any λ > 0, we deﬁne the λ-regularization of f :

fλ := f +

(cid:107) · (cid:107)2 .

λ
2

fλ is λ-strongly convex and has a unique minimizer which we denote with xf ,λ
(cid:63) . Moreover, deﬁne

∀x ∈ H, Hf ,λ (x) := ∇2fλ (x) = Hf (x) + λI,

νf ,λ (x) := (cid:107)∇fλ (x)(cid:107)H−1

f ,λ (x) .

The quantity νf ,λ (x) is called the Newton decrement at point x and will play a signiﬁcant role.
When the function f is clear from the context, we will omit the subscripts with f and use

H, Hλ , νλ ....

A.1 Deﬁnitions and results on generalized self-concordant functions

In this section, we introduce the main deﬁnitions and results for self-concordant functions. These
results are mainly the same as in appendix B of [23].
Deﬁnition 3 (generalized self-concordant function). Let H be a Hilbert space. Formally, a
generalized self-concordant function on H is a couple (f , G ) where:
i G is a bounded subset of H; we will usually denote (cid:107)G (cid:107) or R the quantity supg∈G (cid:107)g(cid:107) < ∞;
ii f is a convex and three times differentiable mapping on H such that

∀x ∈ H, ∀h, k ∈ H, ∇(3)f (x)[h, k , k ] (cid:54) sup

g∈G

|g · h| ∇2f (x)[k , k ].

To make notations lighter, we will often omit G from the notations and simply say that f stands
both for the mapping and the couple (f , G ).
Deﬁnition 4 (Deﬁnitions). Let f be a generalized self-concordant function. We deﬁne the following
quantities.

• ∀h ∈ H, tf (h) := supg∈G |h · g |;
• ∀x ∈ H, ∀λ > 0, rf ,λ (x) :=

supg∈G (cid:107)g(cid:107)
1

;

(x)

H−1
f ,λ

• ∀c (cid:62) 0, ∀λ > 0, Df ,λ (c) := {x : νf ,λ (x) (cid:54) crf ,λ (x)}.

We also deﬁne the following functions:

ψ(t) =

et − t − 1
t2

, φ(t) =

1 − e−t
t

, φ(t) =

et − 1
t

.

(13)

Note that ψ , φ are increasing functions and that φ is a decreasing function. Moreover, φ(t)
Once again, if f is clear, we will often omit the reference to f in the quantities above, keeping only

φ(t) = et .

t, rλ , Dλ ...

We condense results obtained in [23] under a slightly different form. The proofs, however, are
exactly the same.
While in [23], only the regularized case is dealt with, the proof techniques are exactly the same
to obtain Proposition 1. Proposition 2 is proved explicitly in Proposition 4 of [23] and Lemma 5 is
proved in Proposition 5.
Omitting the subscript f , we get the following results.

16

Proposition 1 (Bounds for the non-regularized function f ). Let f be a generalized self-concordant
function. Then the following bounds hold (we omit f in the subscripts):

∀x ∈ H, ∀h ∈ H, e−t(h)H(x) (cid:22) H(x + h) (cid:22) et(h)H(x),

∀x, h ∈ H, ∀λ > 0, (cid:107)∇f (x + h) − ∇f (x)(cid:107)H−1

λ (x)

(cid:54) φ(t(h))(cid:107)h(cid:107)Hλ (x) ,

∀x, h ∈ H, ψ(−t(h))(cid:107)h(cid:107)2

H(x)

(cid:54) f (x + h) − f (x) − ∇f (x).h (cid:54) ψ(t(h))(cid:107)h(cid:107)2

H(x) .

We get the analoguous bounds in the regularized case.

(14)

(15)

(16)

Proposition 2 (Bounds for the regularized function fλ ). Let f be a generalized self-concordant
function and λ > 0 be a regularizer. Then the following bounds hold:

∀x, h ∈ H, e−t(h)Hλ (x) (cid:22) Hλ (x + h) (cid:22) et(h)Hλ (x),

∀x, h ∈ H, φ(t(h))(cid:107)h(cid:107)Hλ (x) (cid:54) (cid:107)∇fλ (x + h) − ∇fλ (x)(cid:107)H−1

λ (x)

(cid:54) φ(t(h))(cid:107)h(cid:107)Hλ (x) ,

(17)

(18)

∀x, h ∈ H, ψ(−t(h))(cid:107)h(cid:107)2

(19)
Corollary 1. Let f be a G generalized self-concordant function and λ > 0 be a regularizer, and
λ the unique minimizer of fλ . Then the following bounds hold for any x ∈ H:

(cid:54) fλ (x + h) − fλ (x) − ∇fλ (x).h (cid:54) ψ(t(h))(cid:107)h(cid:107)2

Hλ (x) .

Hλ (x)

x(cid:63)

φ(t(x − x(cid:63)
λ ))(cid:107)x − x(cid:63)

λ(cid:107)Hλ (x) (cid:54) (cid:107)∇fλ (x)(cid:107)H−1

(cid:54) φ(t(x − x(cid:63)
λ ))(cid:107)x − x(cid:63)

λ(cid:107)Hλ (x) ,

(20)

(cid:124)

(cid:123)(cid:122)

νλ (x)

(cid:125)

λ (x)

ψ(−t(x − x(cid:63)
λ ))(cid:107)x − x(cid:63)

λ(cid:107)2
Hλ (x(cid:63)
λ )

(cid:54) fλ (x) − fλ (x(cid:63)
λ ) (cid:54) ψ(t(x − x(cid:63)
λ ))(cid:107)x − x(cid:63)

λ(cid:107)2
λ ) .
Hλ (x(cid:63)

(21)

Moreover, the following localization lemma holds.

Lemma 5 (localization). Let λ > 0 be ﬁxed. If νλ (x)

rλ (x) < 1, then
t(x − x(cid:63)
λ ) (cid:54) − log

1 − νλ (x)
rλ (x)

(cid:18)

(cid:19)

.

(22)

In particular, this shows:

∀c < 1, ∀λ > 0, x ∈ Dλ (c) =⇒ t(x − x(cid:63)
λ ) (cid:54) − log(1 − c).

We now state a Lemma which shows that the difference to the optimum in function values is
equivalent to the squared newton decrement in a small Dikin ellipsoid. We will use this result in the
main paper.
Lemma 6 (Equivalence of norms). Let λ > 0 and x ∈ Dλ ( 1
7 ). Then the following holds:

νλ (x)2 (cid:54) fλ (x) − fλ (x(cid:63)
λ ) (cid:54) νλ (x)2 .

1
4

17

Proof.Apply Lemma 5 knowing x ∈ Dλ ( 1
and Eq. (18) to get:

7 ) to get t(x − x(cid:63)

λ ) (cid:54) log(7/6). Then apply Eq. (19)

fλ (x) − fλ (x(cid:63)
λ ) (cid:54) ψ(t(x − x(cid:63)
λ ))(cid:107)x − x(cid:63)
λ )ψ(t(x − x(cid:63)
λ ))(cid:107)x − x(cid:63)
λ )ψ(t(x − x(cid:63)
λ ))
φ(t(x − x(cid:63)
λ ))2

(cid:54) et(x−x(cid:63)
(cid:54) et(x−x(cid:63)

λ(cid:107)2
Hλ (x(cid:63)
λ )
λ(cid:107)2
Hλ (x)

νλ (x)2 .

Replacing with the bound above, we get

∀λ > 0, ∀x ∈ Dλ (

1
7

), fλ (x) − fλ (x(cid:63)
λ ) (cid:54) νλ (x)2 .

For the lower bound, proceed in exactly the same way.

A.2 Comparison between generalized self-concordant functions

The following result is straightforward.
Lemma 7 (Comparison between generalized self-concordant functions). Let G1 ⊂ G2 ⊂ H be
two bounded subsets. If (f , G1 ) is generalized self-concordant, then (f , G2 ) is also generalized
self-concordant. Moreover,

In particular, we will often use the following fact. If (f , G ) is generalized self-concordant, and
G is bounded by R, then (f , BH (R)) is also generalized self-concordant. Moreover,

∀x ∈ H, ∀λ > 0, r(f ,G1 ),λ (x) (cid:62) r(f ,G2 ),λ (x).
(cid:112)λ + λmin (Hf (x))
R

r(f ,BH (R)),λ (x) =

λ
R

√

(cid:62)

.

We now state a result which shows that, given a family of generalized self-concordant functions,
the expectancy of that family is also generalized self-concordant. This can be seen as a reformulation
of Proposition 2 of [23].
Proposition 3 (Expectation). Let Z be a polish space equipped with its Borel sigma-algebra, and
H be a Hilbert space. Let ((fz , Gz ))z∈Z be a family of generalized self-concordant functions such
that the mapping (z , x) (cid:55)→ fz (x) is measurable.
Assume we are given a random variable Z on Z , whose support we denote with supp(Z ), such
that
• the random variables (cid:107)fZ (0)(cid:107), (cid:107)∇fZ (0)(cid:107), Tr(∇2fZ (0)) are are bounded;
z∈supp(Z ) Gz is a bounded subset of H.
Then the mapping f : x ∈ H (cid:55)→ E [fZ (x)] is well deﬁned, (f , G ) is generalized self-concordant,
and we can differentiate under the expectation.
Corollary 2. Let n ∈ N and (fi , Gi )1(cid:54)i(cid:54)n be a family of generalized self-concordant functions.
Deﬁne

• G := (cid:83)

n(cid:88)

n(cid:91)

f (x) =

1
n

fi (x), G =

Gi .

Then (f , G ) is generalized self-concordant.

i=1

i=1

18

B Results on approximate Newton methods

In this section, we assume we are given a generalized self-concordant function f in the sense of
Appendix A. As f will be ﬁxed throughout this part, we will omit it from the notations. Recall the
deﬁnitions from Deﬁnition 4:

(cid:26)

(cid:27)

.

x :

νλ (x)
rλ (x)

(cid:54) c

νλ (x) := (cid:107)∇fλ (x)(cid:107)H−1

λ (x) ,

1
rλ (x)

:= sup

g∈G

(cid:107)g(cid:107)H−1
λ (x) , Dλ (c) :=

Deﬁne the following quantities:
• the true Newton step at point x for the λ-regularized problem:
• the renormalized Newton decrement (cid:101)νλ (x):

∆λ (x) := H−1
λ (x)∇fλ (x).
(cid:101)νλ (x) :=

.

νλ (x)
rλ (x)

Moreover, note that a direct application of Eq. (17) yields the following equation which relates
the radii at different points:

∀λ > 0, ∀x ∈ H, ∀h ∈ H, e−t(h) rλ (x) (cid:54) rλ (x + h) (cid:54) et(h) rλ (x).

(23)

In this appendix, we develop a complete analysis of so-called approximate Newton methods in
xt+1 = xt − (cid:101)∆t where (cid:101)∆t is an approximation of the real Newton step. We will characterize this
the case of generalized self-concordant losses. By "approximate Newton method", we mean that
instead of performing the classical update xt+1 = xt − ∆λ (xt ), we perform an update of the form
approximation by measuring its distance to the real Newton step using two parameters ρ and 0 :

(cid:107) (cid:101)∆t − ∆λ (xt )(cid:107) (cid:54) ρνλ (xt ) + 0 .

We start by presenting a few technical results in Appendix B.1. We continue by proving that an
approximate Newton method has linear convergence guarantees in the right Dikin ellipsoid in
Appendix B.2. In Appendix B.3, we adapt these results to a certain way of computing approximate
Newton steps, which will be the one we use in the core of the paper. In Appendix B.4, we mention
ways to reduce the computational burden of these methods by showing that since all Hessians
are equivalent in Dikin ellipsoids, one can actually sketch the Hessian at one given point in that
ellipsoid instead of re-sketching it at each Newton step. For the sake of simplicity, this is not
mentioned in the core paper, but works very well in practice.
We start with a technical decomposition of the Newton decrement at point x − (cid:101)∆ for a given
form x − (cid:101)∆ for a certain (cid:101)∆ ∈ H. Deﬁne
Lemma 8 (Technical decomposition). Let λ > 0, x ∈ H be ﬁxed. Assume we perform a step of the

B.1 Main technical results

(cid:101)∆ ∈ H.

δ := (cid:107) (cid:101)∆ − ∆λ (x)(cid:107)Hλ (x) ,

(cid:101)δ :=

δ
rλ (x)

.

19

The following holds:(cid:101)νλ (x − (cid:101)∆) (cid:54) e(cid:101)νλ (x)+(cid:101)δ (cid:104)

νλ (x − (cid:101)∆λ (x)) (cid:54) e(cid:101)νλ (x)+(cid:101)δ (cid:104)

ψ((cid:101)νλ (x) + (cid:101)δ)((cid:101)νλ (x) + (cid:101)δ)2 + (cid:101)δ
ψ((cid:101)νλ (x) + (cid:101)δ)((cid:101)νλ (x) + (cid:101)δ)(νλ (x) + δ) + δ
;

(cid:105)

(cid:105)

.

(24)

(25)

Proof.

Note that by deﬁnition, ∇fλ (x) = Hλ (x)∆λ (x). Hence

λ (x)

−1/2
λ

λ (x)

λ (x)

= (cid:107)

(cid:54)

0

λ (x) + δ

(cid:107)H

−1/2
λ

(cid:90) 1
(cid:90) 1

0

(x) − I(cid:107) (cid:54) est( (cid:101)∆) − 1, whose
integral on s is ψ(t( (cid:101)∆))t( (cid:101)∆) where ψ is deﬁned in Deﬁnition 4. Morever, bounding
Now using Eq. (17), one has (cid:107)H

−1/2
λ

−1/2
λ

(cid:107)∇f λ (x − (cid:101)∆)(cid:107)H−1
λ (x) = (cid:107)∇f λ (x − (cid:101)∆) − ∇f λ (x) + Hλ (x)∆λ (x)(cid:107)H−1
(cid:54) (cid:107)∇f λ (x − (cid:101)∆) − ∇f λ (x) + Hλ (x) (cid:101)∆(cid:107)H−1
+ (cid:107)Hλ (x)(∆λ (x) − (cid:101)∆)(cid:107)H−1
[Hλ (x − s (cid:101)∆) − Hλ (x)] (cid:101)∆ds(cid:107)H−1
(x)Hλ (x − s (cid:101)∆)H
(x) − I(cid:107)ds (cid:107) (cid:101)∆(cid:107)Hλ (x) + δ.
(x)Hλ (x − s (cid:101)∆)H
(cid:107) (cid:101)∆(cid:107)Hλ (x) (cid:54) (cid:107) (cid:101)∆ − ∆λ (x)(cid:107)Hλ (x) + (cid:107)∆λ (x)(cid:107)Hλ (x) = δ + νλ (x),
(cid:107)∇f λ (x − (cid:101)∆)(cid:107)H−1
(cid:54) ψ(t( (cid:101)∆))t( (cid:101)∆) (νλ (x) + δ) + δ.
νλ (x − (cid:101)∆) (cid:54) et( (cid:101)∆)/2 (cid:16)
(cid:101)νλ (x − (cid:101)∆) (cid:54) et( (cid:101)∆) (cid:16)
ψ(t( (cid:101)∆))t( (cid:101)∆) ((cid:101)νλ (x) + (cid:101)δ) + (cid:101)δ
t( (cid:101)∆) (cid:54) (cid:107) (cid:101)∆(cid:107)Hλ (x)
(cid:54) (cid:101)νλ (x) + (cid:101)δ ,
rλ (x)

it holds
1. Now note that using Eq. (17), it holds: νλ (x − (cid:101)∆) (cid:54) et( (cid:101)∆)/2(cid:107)∇f λ (x − (cid:101)∆)(cid:107)H−1
λ (x) and hence:

ψ(t( (cid:101)∆))t( (cid:101)∆) (νλ (x) + δ) + δ

and bounding Eq. (26) simply by taking et( (cid:101)∆)/2 (cid:54) et( (cid:101)∆) , we get the two bounds in the lemma.
the following form. Assume λ and x are ﬁxed, and that we approximate ∆λ (x) with (cid:101)∆ such that
We now place ourselves in the case where we are given an approximation of the Newton step of
there exists ρ (cid:62) 0 and 0 (cid:62) 0 such that it holds:

2. Moreover, using Eq. (23),

(26)

(27)

(cid:17)

.

(cid:17)

.

Noting that

λ (x)

(cid:107) (cid:101)∆ − ∆λ (x)(cid:107)Hλ (x) (cid:54) ρνλ (x) + 0 .

We deﬁne/prove the three different following regimes.

20

7

(cid:0) 1

(cid:1) and λ > 0 be ﬁxed. Let
Lemma 9 (3 regimes). Let x ∈ Dλ
Let (cid:101)∆ be an approximation of the Newton steps satisfying (cid:107) (cid:101)∆ − ∆λ (x)(cid:107)Hλ (x) (cid:54) ρνλ (x) + 0 . The
three following regimes appear.
• If (cid:101)νλ (x) (cid:62) ρ and (cid:101)νλ (x)2 (cid:62) ˜ε0 , then we are in the quadratic regime, i.e.

, 0 (cid:62) 0 s.t. ˜ε0 :=

0 (cid:54) ρ (cid:54) 1
7

0
rλ (x)

(cid:54) 1

21

.

(cid:19)

(cid:19)2

(cid:19) (cid:18) 10

Proof.Using the previous lemma,

• If ρ (cid:62) (cid:101)νλ (x) and ρ(cid:101)νλ (x) (cid:62) (cid:101)0 , then we are in the linear regime, i.e.

• If (cid:101)0 (cid:62) (cid:101)νλ (x)2 , ρ (cid:101)νλ (x), then the maximal precision of the approximation is reached, and it
holds:

10(cid:101)νλ (x − (cid:101)∆λ (x))
(cid:18) 10(cid:101)νλ (x)
, νλ (x − (cid:101)∆λ (x)) (cid:54) 10
3 (cid:101)νλ (x)νλ (x).
3
(cid:18) 10ρ
3
3 (cid:101)νλ (x − (cid:101)∆λ (x)) (cid:54)
3 (cid:101)νλ (x)
, νλ (x − (cid:101)∆λ (x)) (cid:54) 10
3 (cid:101)νλ (x)νλ (x).
10
3
(cid:101)νλ (x − (cid:101)∆λ (x)) (cid:54) 3(cid:101)0 (cid:54) 1
, νλ (x − (cid:101)∆λ (x)) (cid:54) 30 .
(cid:101)νλ (x − (cid:101)∆λ (x)) (cid:54) e(1+ρ)(cid:101)νλ (x)+(cid:101)0 (cid:2)ψ((1 + ρ)(cid:101)νλ (x) + (cid:101)0 )((1 + ρ)(cid:101)νλ (x) + (cid:101)0 )2 + ρ(cid:101)νλ (x) + (cid:101)0
7
(cid:54) (cid:3)1 ((cid:101)νλ (x), ρ, (cid:101)0 ) (cid:101)νλ (x)2 + (cid:3)2 ((cid:101)νλ (x), ρ, (cid:101)0 ) ρ(cid:101)νλ (x) + (cid:3)3 ((cid:101)νλ (x), ρ, (cid:101)0 ) (cid:101)0 ,
νλ (x − (cid:101)∆λ (x)) (cid:54) (cid:3)1 ((cid:101)νλ (x), ρ, (cid:101)0 ) (cid:101)νλ (x)νλ (x) + (cid:3)2 ((cid:101)νλ (x), ρ, (cid:101)0 ) ρνλ (x) + (cid:3)3 ((cid:101)νλ (x), ρ, (cid:101)0 ) 0 ,
(cid:3)1 ((cid:101)ν , ρ, (cid:101)0 ) := e(1+ρ)(cid:101)ν+(cid:101)0 ψ((1 + ρ)(cid:101)ν + (cid:101)0 )(1 + ρ)2 ,
(cid:3)2 ((cid:101)ν , ρ, (cid:101)0 ) := e(1+ρ)(cid:101)ν+(cid:101)0 ,
(cid:3)3 ((cid:101)ν , ρ, (cid:101)0 ) := e(1+ρ)(cid:101)ν+(cid:101)0 [2ψ((1 + ρ)(cid:101)ν + (cid:101)0 )(1 + ρ)(cid:101)ν + 1] .
21 , (cid:101)νλ (x), ρ (cid:54) 1

Now assume (cid:101)0 (cid:54) 1
7 . Replacing these values in the functions above bounds (cid:3)1 , (cid:3)2
and (cid:3)3 , and using the case distinction, we get the result.

where the following deﬁntions are used:

and

(cid:54)

(cid:3)

B.2 General analysis of an approximate Newton method

The following proposition describes the behavior of an approximate newton method where ρ and 0
are ﬁxed a priori.
7 be ﬁxed and x0 ∈ Dλ (c)
Proposition 4 (General approximate Newton scheme results). Let c (cid:54) 1
be a given starting point.
Let ρ (cid:54) 1
7 and 0 such that 0 (cid:54) c
Deﬁne the following approximate Newton scheme:

4 rλ (x0 ).
∀t (cid:62) 0, xt+1 = xt − (cid:101)∆t ,

(cid:107) (cid:101)∆t − ∆λ (xt )(cid:107)Hλ (xt ) (cid:54) ρνλ (xt ) + 0 .

The following guarantees hold.

21

• ∀t (cid:62) 0, xt ∈ Dλ (c).

• Let tc =

log2 log2

3
10ρ

(cid:106)

(cid:107)

+ 1.

10(cid:101)νλ (xt )
∀t (cid:54) tc ,
10(cid:101)νλ (xt )
3
3

(cid:54) max

(cid:18) 120
, 2−2t (cid:19)
(cid:18) 10ρ
rλ (x0 )
3

,

,

(cid:19)t−tc+1(cid:33)

.

120
rλ (x0 )

(cid:54) max

(cid:32)

∀t (cid:62) tc ,

• We can bound the relative decrease for both the Newton decrement and the renormalized
Newton decrement:

(cid:18) 1

(cid:19)2t−1

(cid:33)

∀t (cid:54) tc ,

∀t (cid:62) tc ,

νλ (xt ) (cid:54) max
(cid:101)νλ (xt ) (cid:54) max
νλ (xt ) (cid:54) max
(cid:101)νλ (xt ) (cid:54) max

30 ,

(cid:33)
(cid:33)

(cid:18) 1

(cid:19)t−tc+1

(cid:19)2t−1 (cid:101)νλ (x0 )
νλ (x0 )
,
2
180
(cid:18) 10ρ
,
.
5rλ (x0 )
2
(cid:18) 10ρ
(cid:19)t−tc+1 (cid:101)νλ (x0 )
νλ (x0 )
,
3
3

180
5rλ (x0 )

30 ,

,

(cid:33)

.

(cid:32)
(cid:32)
(cid:32)
(cid:32)

Proof.Start by noting, using Eq. (23),

(cid:18) 1

(cid:19)

7

∀x ∈ Dλ

, ε (cid:54) rλ (x)
21

,

6
7

rλ (x0 ) (cid:54) rλ (x) (cid:54) 7
6

rλ (x0 ).

(28)

In particular, this holds for any x ∈ Dλ (c), c (cid:54) 1
7 . Thus,

∀c (cid:54) 1

, ∀x0 ∈ Dλ (c),

7

0
rλ (x0 )

(cid:54) c

4

=⇒ ∀x ∈ Dλ (c),

0
rλ (x)

(cid:54) c

.

3

(cid:101)νλ (xt+1 ) (cid:54) 10
21 (cid:101)νλ (xt ) (cid:54) c.

1. Proving the ﬁrst point is simple by induction. Indeed, assume (cid:101)νλ (xt ) (cid:54) c. We can apply
Lemma 9 since the conditions on ε and ρ guarantee that the conditions of this lemma are satisﬁed.
If we are in either the linear or quadratic regime, the fact that 10ρ
If we are in the last case, (cid:101)νλ (xt+1 ) (cid:54) 30
21 show that
(cid:54) c.
2. Let us prove the second bullet point by induction. Start by assuming the property holds at t.
By the previous point, the hypothesis of Lemma 9 are satisﬁed at xt with ρ and ε. Assume we are
in the limiting case; we easily show that in this case,

3 , 10(cid:101)νλ (xt )
3

(cid:54) 10

rλ (xt )

10(cid:101)νλ (xt+1 )
3

(cid:54) 10
3

3

0
rλ (xt )

(cid:54) 350
3rλ (x0 )

.

Here, the last inequality comes from Eq. (28). If we are not in the limiting case, let us distinguish
between the two following cases.

22

If t (cid:54) tc − 1,

10(cid:101)νλ (xt+1 )
3

(cid:54) 10(cid:101)νλ (xt )
3
(cid:54) max
350
3rλ (x0 )

max

(cid:18) 10(cid:101)νλ (xt )
10(cid:101)νλ (xt )
3
3

,

10ρ
3

(cid:19)
(cid:32)(cid:18) 1

(cid:32)

,

max

2

(cid:19)2t

,

10ρ
3

(cid:33)(cid:33)
(cid:107)

,

where the last inequality comes from using the induction hypothesis and the fact that 10(cid:101)νλ (xt )
Using once again the induction hypotheses and the fact that t (cid:54) (cid:106)
(cid:54) 1.
which implies
, we ﬁnally get

3

log2 log2

3
10ρ

10ρ
3

(cid:54) (cid:0) 1

2

(cid:1)2t

10(cid:101)νλ (xt+1 )
3

(cid:54) max

(cid:32)

350
3rλ (x0 )

,

(cid:18) 1

2

(cid:19)2t+1 (cid:33)

.

The fact that the second property holds for t = tc is trivial Now consider the case where t (cid:62) tc .
Using the same technique as before but noting that in this case

10(cid:101)νλ (xt )
3

(cid:54) max

(cid:32)

350
3rλ (x0 )

,

(cid:18) 10ρ
3

(cid:19)t−tc+1(cid:33)

(cid:54) max

(cid:18) 350
3rλ (x0 )

,

10ρ
3

(cid:19)

,

We easily use Lemma 9 to reach the desired conclusion.

3. Let t < tc . Then using Lemma 9:

∀s (cid:54) t, νλ (xs+1 ) (cid:54) max

(cid:18)

30 , max(

10ρ
3

,

10(cid:101)νλ (xs )
3

)νλ (xs )

(cid:19)

.

Using the fact that for any s (cid:54) t, 10(cid:101)νλ (xs )

3

(cid:54) max( 350

3rλ (x0 ) , (cid:0) 1
2

(cid:1)2s

):

∀s (cid:54) t, νλ (xs+1 ) (cid:54) max

(cid:32)

30 ,

350
3

νλ (xs )
, max(
rλ (x0 )
3 , (cid:0) 1
30 ,

10ρ
,
3
) = (cid:0) 1

(cid:18) 1

2

(cid:19)2s
(cid:1)2s

)νλ (xs )
.
6 (cid:101)νλ (xs ) (cid:54) 1

(cid:33)

Now using the fact that for any s (cid:54) t, (cid:101)νλ (xs ) (cid:54) 1
7 , we see that νλ (xs )
(cid:54) 30 . Moreover, since s (cid:54) t < tc , max( 10ρ

rλ (x0 )

(cid:54) 7

6 and hence

350
3

νλ (xs )
rλ (x0 )

2

(cid:1)2s
(cid:19)2s

2

. Thus:

∀s (cid:54) t, νλ (xs+1 ) (cid:54) max

(cid:32)

(cid:18) 1

2

νλ (xs )

(cid:33)

.

Combining these results yields:

νλ (xt+1 ) (cid:54) max

(cid:32)

30 ,

(cid:18) 1

2

(cid:19)2t+1−1

νλ (x0 )

(cid:33)

.

This shows the ﬁrst equation, that is:

∀t (cid:54) tc , νλ (xt ) (cid:54) max

(cid:32)

30 ,

(cid:18) 1

2

(cid:19)2t−1

νλ (x0 )

(cid:33)

.

The case for t (cid:62) tc is completely analogous. We can also reproduce the same proof to get the same
bounds for (cid:101)ν , since the bounds in Lemma 9 are the same for both.
23

B.3 Main results in the paper

In the main paper, we mention two types of Newton method. First, we present a result of conver-
gence on the full Newton method:
Lemma 10 (Quadratic convergence of the full Newton method). Let c (cid:54) 1
7 and x0 ∈ Dλ (c). Deﬁne

Then this scheme converges quadratically, i.e.:

∀t ∈ N,

xt+1 = xt − ∆λ (xt ).
, (cid:101)νλ (xt )
(cid:101)νλ (x0 )
νλ (xt )
(cid:54) 2−(2t−1) .
νλ (x0 )
(cid:0)1 + log2
(cid:1)(cid:7) , xt ∈ Dλ (˜c).
, νλ (xt ) (cid:54) √
1 + log2

c
˜c
νλ (x0 )√
ε

log2

• ∀t ∈ N, xt ∈ Dλ (c).

Thus :
• For any ˜c (cid:54) c then ∀t (cid:62) (cid:6)log2
• For any ε > 0, ∀t (cid:62) (cid:108)
• If we perform the Newton method and return the ﬁrst xt such that νλ (xt ) (cid:54) √
number of Newton steps computations is at most 1 +
.

(cid:17)(cid:109)

(cid:17)(cid:109)

(cid:16)

log2

1 + log2

νλ (x0 )√
ε

(cid:108)

(cid:16)

ε, fλ (x) − fλ (x(cid:63)
λ ) (cid:54) ε.

ε, then the

Proof.

A full Newton method is an approximate Newton method where ρ, 0 = 0. Thus apply
Proposition 4; note that in this case tc = +∞. The last point shows that if c (cid:54) 1
7 , and if we perform
the Newton method with a full Newton step, then

∀t (cid:62) 0, (cid:101)νλ (xt ) (cid:54) 2−(2t−1)νλ (x0 ), (cid:101)νλ (xt ) (cid:54) 2−(2t−1)νλ (x0 ).

This shows the quadratic convergence, and the ﬁrst two points directly follow. For the third point,
the result for νλ (xt ) directly follows from the previous equation, and the one on function values is
a direct consequence of Lemma 6 and the fact that xt ∈ Dλ (1/7).
For the last point, note that νt (xt ) = ∇fλ (xt ) · ∆λ (xt ) is accessible. Moreover, the bound on t
is given in the point before, and since one has to compute ∆λ (xs ) for 0 (cid:54) s (cid:54) t, there are at most
t + 1 computations.
In the main paper, we compute approximate Newton steps by considering methods which
naturally yield only a relative error ρ and no absolute error 0 . Indeed, we take the following
notation.

Approximate solutions to linear problems. Let A be a positive deﬁnite Hermitian operator on
H, b in H, and a wanted relative precision ρ.
We say that x is a ρ-relative approximation to the linear problem Ax = b and write x ∈
LinApprox(A, b, ρ) if the following holds:

(cid:107)A−1 b − x(cid:107)A (cid:54) ρ(cid:107)b(cid:107)A−1 = ρ(cid:107)A−1 b(cid:107)A .
Note that if x ∈ LinApprox(A, b, ρ) for ρ < 1, then
(1 − ρ)(cid:107)b(cid:107)A−1 (cid:54) x · b (cid:54) (1 + ρ)(cid:107)b(cid:107)A−1 .

The following lemma shows that if, instead of computing the exact Newton step, we compute a
relative approximation of the Newton step belonging to LinApprox(Hλ (x), ∇fλ (x), ρ) for a given
ρ < 1, then one has linear convergence. Moreover, we show that we can still perform a method
which automatically stops.

24

Proposition 5 (relative approximate Newton method). Let λ > 0, ρ (cid:54) 1
point x0 ∈ Dλ (c). Assume we perform the following Newton scheme:

7 and a starting

∀t (cid:62) 0, xt+1 = xt − (cid:101)∆t ,

Then the scheme converges linearly, i.e.

∀t ∈ N,

7 , c (cid:54) 1
(cid:101)∆t ∈ LinApprox(Hλ (xt ), ∇fλ (xt ), ρ).
, (cid:101)νλ (xt )
(cid:101)νλ (x0 )
νλ (xt )
νλ (x0 )
(cid:7) , xt ∈ Dλ (˜c).
, νλ (xt ) (cid:54) √

(cid:54) 2−t .

(cid:109)

• ∀t ∈ N, xt ∈ Dλ (c).

Thus,
• For any ˜c (cid:54) c then ∀t (cid:62) (cid:6)log2
• For any ε > 0, ∀t (cid:62) (cid:108)
• If the method is performed and returns the ﬁrst xt such that xt · (cid:101)∆t (cid:54) 6
7 ε, then at most
approximate Newton steps computations have been performed, and

ε, fλ (x) − fλ (x(cid:63)
λ ) (cid:54) ε

(cid:16)(cid:113) 4

(cid:17)(cid:107)

νλ (x0 )√
ε

log2

(cid:106)

c
˜c

2 +
νλ (xt ) (cid:54) √
log2

νλ (x0 )√
ε

3

ε, fλ (x) − fλ (x(cid:63)
λ ) (cid:54) ε.

(cid:54)

∀t ∈ N,

(cid:18) 10
21

(cid:19)t (cid:54) 2−t .

Proof.Apply Proposition 4 with 0 = 0 and ρ = 1
7 , since if ρ (cid:54) 1
7 , then a fortiori the
approximation satisﬁes the condition for ρ = 1
7 . The last point clearly states that

For the last point, note that since (cid:101)∆t ∈ LinApprox(Hλ (xt ), ∇fλ (xt ), ρ), the following holds:
From this, using Lemma 6 for the third point, the ﬁrst three points are easily proven.
. Now bound

, (cid:101)νλ (xt )
(cid:101)νλ (x0 )
νλ (x0 )
νλ (xt )
∇fλ (xt ) · (cid:101)∆t = νλ (xt )2 + ∇fλ (xt ) · (cid:16) (cid:101)∆t − H−1
|∇fλ (xt ) · (cid:16) (cid:101)∆t − H−1
(cid:17) | (cid:54) νλ (xt ) (cid:107) (cid:101)∆t − H−1
λ (xt )∇fλ (xt )
λ (xt )∇fλ (xt )
λ (xt )∇fλ (xt )(cid:107)Hλ (xt ) (cid:54) ρνλ (xt )2 .
(1 − ρ)νλ (xt )2 (cid:54) ∇fλ (xt ) · (cid:101)∆t (cid:54) (1 + ρ)νλ (xt )2 .

Thus:
7 , we see that if ∇fλ (xt ) · (cid:101)∆t (cid:54) 6
ﬁrst t where ∇fλ (xt ) · (cid:101)∆t (cid:54) 6
7 ε, then νλ (xt )2 (cid:54) ε. Moreover, since we stop at the
Since ρ = 1
7 ε, then if t denotes the time at which we stop,

(cid:17)

ε < ∇fλ (xt−1 ) · (cid:101)∆t−1 (cid:54) 8
7

6
7

Since νλ (xt−1 )2 (cid:54) 2−2(t−1)νλ (x0 )2 , this implies in turn that t − 1 (cid:54) log2
necessarily, t (cid:54) 1 +
, and since we compute approximate Newton steps for
s = 0, ..., t, we ﬁnally have that the number of approximate Newton steps is bounded by

νλ (x0 )√
ε

νλ (x0 )√
ε

. Thus,

log2

3

3

(cid:106)

(cid:16)(cid:113) 4

νλ (xt−1 )2 =⇒ νλ (xt−1 )2 (cid:62) 3
4

ε.

(cid:16)(cid:113) 4

(cid:17)

(cid:17)(cid:107)
(cid:36)

(cid:32)(cid:114) 4

(cid:33)(cid:37)

.

2 +

log2

νλ (x0 )√
ε

3

Last but not least, we summarize all these theorem in the following simple result.

25

Lemma 11. Let λ > 0, c (cid:54) 1/7, let fλ be generalized self-concordant and x ∈ Dλ (c).
It
holds: 1
λ ) (cid:54) νλ (x)2 . Moreover, the full Newton method starting
if xt is obtained via t ∈ N steps of the Newton
from x0 has quadratic convergence, i.e.
method Eq. (2), then νλ (xt ) (cid:54) 2−(2t−1)νλ (x0 ). Finally, the approximate Newton method start-
if xt is obtained via t ∈ N steps of Eq. (3), with
ing from x0 has linear convergence, i.e.

4 νλ (x)2 (cid:54) fλ (x) − fλ (x(cid:63)
(cid:101)∆t ∈ LinApprox(Hλ (xt ), ∇fλ (xt ), ρ) and ρ (cid:54) 1/7, then νλ (xt ) (cid:54) 2−tνλ (x0 ).

4 νλ (x)2 (cid:54) fλ (x) − fλ (x(cid:63)

Proof.The three points are obtained in the following lemmas, assuming x ∈ Dλ (1/7).
• For 1
λ ) (cid:54) νλ (x)2 , see Lemma 6 in Appendix A.1.
• The convergence rate of the full Newton method starting in Dλ (1/7) is obtained in Lemma 10.
• The convergence rate of the approximate Newton method starting in Dλ (1/7) is obtained in
Proposition 5.

B.4 Sketching the Hessian only once in each Dikin ellispoid

(cid:17)

(cid:16)

Assume

H−1

−1/2
λ

In this section, we provide a lemma which shows in essence that if we are in a small Dikin
ellipsoid, then we can keep the Hessian of the starting point and compute approximations of
λ (x0 )∇fλ (xt ); they will be good approximations to H−1
Let (cid:101)H be an approximation of the Hessian at x0 , approximation wich we quantify with
Lemma 12. Let c < 1 and x0 ∈ Dλ (c) be ﬁxed.

λ (xt )∇fλ (xt ) as well.

Hλ (x0 ) − (cid:101)H
t := (cid:107)H
(x0 )
Let b ∈ H. If (cid:101)∆ ∈ LinApprox( (cid:101)Hλ , b, ˜ρ), then
1 + t < 2(1 − c)2 .
∀x ∈ Dλ (c), (cid:101)∆ ∈ LinApprox(Hλ (x), b, ρ), ρ =
( ˜ρ − 1)(1 − c)2 + (1 + t)
2(1 − c)2 − (1 + t)
30 , x0 ∈ Dλ (c),
∀x ∈ Dλ (c), ∀b ∈ H, (cid:101)∆ ∈ LinApprox(Hλ (x0 ), b,
) =⇒ (cid:101)∆ ∈ LinApprox(Hλ (x), b,

In particular, if c (cid:54) 1

(x0 )(cid:107).

−1/2
λ

H

.

1
7

).

1
20

Proof.First, start with a general theoretical result.

Let A and B be two positive semi-deﬁnite hermitian operators. Let λ > 0, b ∈ H and

1.

(cid:107)A−1

(cid:101)∆ ∈ LinApprox(Bλ , b, ˜ρ). Decompose
λ b − (cid:101)∆(cid:107)Aλ
λ b − (cid:101)∆(cid:107)Aλ
λ b − B−1
λ b(cid:107)Aλ + (cid:107)B−1
(cid:54) (cid:107)A1/2
λ (A−1
λ − B−1
λ (cid:107) (cid:107)b(cid:107)A−1
+ (cid:107)A1/2
λ B
λ − B−1
λ = B−1
λ (B − A)A−1

Now using the fact that A−1

(cid:54) (cid:107)A−1

λ )A1/2

λ ,

λ

λ b − (cid:101)∆(cid:107)Bλ .

(cid:107) (cid:107)B−1

−1/2
λ

(cid:107)A1/2

λ (A−1
λ − B−1

λ )A1/2

λ (cid:107) (cid:54) (cid:107)A

= (cid:107)A

(cid:107) (cid:107)A1/2
λ B−1
(cid:107) (cid:107)A1/2
λ B

λ A1/2
−1/2
λ

(cid:107)2 .

λ (cid:107)

−1/2
λ
−1/2
λ

(B − A)A
(B − A)A

−1/2
λ
−1/2
λ

26

Moreover,

λ b − (cid:101)∆(cid:107)Bλ

(cid:107)B−1

(cid:54) ˜ρ(cid:107)b(cid:107)B−1

(cid:54) (cid:107)A1/2B−1/2(cid:107) (cid:107)b(cid:107)A−1

.

λ

λ

Putting things together, and noting that from Lemma 21, (cid:107)A1/2B−1/2(cid:107)2 (cid:54)
as soon as (cid:107)A
(cid:107) < 1, it holds:

1−(cid:107)A

−1/2
λ

1
(B−A)A

−1/2
λ

(cid:107)

−1/2
λ

−1/2
λ

(B − A)A
(cid:101)∆ ∈ LinApprox(Aλ , b, ρ), ρ =

˜ρ + (cid:107)A

1 − (cid:107)A

−1/2
−1/2
λ
λ

(B − A)A
(B − A)A

−1/2
−1/2
λ
λ

(cid:107)
(cid:107) .

The aim is now to apply this lemma to A = H(x) and B = (cid:101)H.
Let x, x0 ∈ Dλ (c). Using Lemma 22, we see that

2.

1 + (cid:107)H

−1/2
λ

(x)( (cid:101)H − H(x))H

−1/2
λ

(x)(cid:107) (cid:54) (1 + t)(1 + (cid:107)H

−1/2
λ

(x)(H(x0 ) − H(x))H

−1/2
λ

(x)(cid:107)).

Using Eq. (17), it holds:

(e−t(x−x0 ) − 1)I (cid:22) H

−1/2
λ

(x)(H(x0 ) − H(x))H

−1/2
λ

(x) (cid:22) (et(x0−x) − 1)I.

Thus,

(cid:107)H

−1/2
λ

(x)(H(x0 ) − H(x))H

−1/2
λ

(x)(cid:107) (cid:54) max(1 − e−t(x−x0 ) , et(x−x0 ) − 1) = et(x−x0 ) − 1.

Finally, using the fact that x0 , x ∈ Dλ (c) for c < 1 yields t(x − x0 ) (cid:54) 2 log 1
1−c . Hence

1 + (cid:107)H

−1/2
λ

(x)(H(x0 ) − H(x))H
(x)( (cid:101)H − H(x))H

−1/2
λ

−1/2
λ

(x)(cid:107) (cid:54)

1
(1 − c)2 .

(x)(cid:107) (cid:54) 1 + t
(1 − c)2 − 1.

Thus,

(cid:107)H

−1/2
λ

The result then follows.

27

C Proof of bounds for the globalization scheme

In this section, we prove that the scheme of decreasing µ towards λ converges.

C.1 Main technical lemmas

Lemma 13 (Next µ). Let µ > 0, c < 1.

νµ (x) (cid:54) c
3

√

µ
R

=⇒ ν(cid:101)µ (x) (cid:54) c

(cid:112)(cid:101)µ

R

,

(cid:16) c

(cid:17)

3

x ∈ Dµ

=⇒ x ∈ D(cid:101)µ (c) ,

Proof.For any (cid:101)µ < µ, note that

∀x ∈ H, (cid:107)H

µ (x)(cid:107) =
(x)H1/2

−1/2

(cid:101)µ

(cid:54) (cid:113) µ(cid:101)µ (cid:107) · (cid:107)H−1

This shows that (cid:107) · (cid:107)H−1(cid:101)µ (x)
Using this fact, it holds:

(cid:101)µ := q µ,
(cid:101)µ := q µ,

(cid:115)

λmin (H(x)) + (cid:101)µ
λmin (H(x)) + µ

q (cid:62) 1
3 +
1 +

√

R

µ(cid:107)x(cid:107)

H−1
µ (x)

√

R

c

µ(cid:107)x(cid:107)

H−1
µ (x)

.

c

1

3 +

q (cid:62)

µ(cid:107)x(cid:107)

H−1
µ (x)

c rµ (x)

µ(cid:107)x(cid:107)

H−1
µ (x)

.

c rµ (x)

1 +
(cid:54) (cid:112)µ/(cid:101)µ.
(cid:54) (cid:112)µ/(cid:101)µ 1

r (cid:101)µ (x)
1

µ (x) , and in particular that

rµ (x) .

(cid:101)ν(cid:101)µ (x) =
=

(cid:107)∇f(cid:101)µ (x)(cid:107)H−1(cid:101)µ (x)
r(cid:101)µ (x)
(cid:107)∇fµ (x) − (µ − (cid:101)µ)x(cid:107)H−1(cid:101)µ (x)
r(cid:101)µ (x)
(cid:107)∇fµ (x)(cid:107)H−1
rµ (x)

(cid:18) µ(cid:101)µ

µ (x)

+

(cid:54) µ(cid:101)µ
(cid:17) (cid:54) c + t ⇔ (cid:101)µ (cid:62) µ

(cid:16) c

3

µ(cid:101)µ

+ t

(cid:19) (cid:107)µx(cid:107)H−1
rµ (x)

µ (x)

.

− 1

c/3 + t
c + t

t =

(cid:107)µx(cid:107)H−1
rµ (x)

µ (x)

.

Hence, if (cid:101)νµ (x) (cid:54) c
3 , a condition to obtain (cid:101)ν(cid:101)µ (x) (cid:54) c is the following:

This yields the second point of the lemma. The analysis is completely analoguous for the ﬁrst.

Lemma 14 (Useful bounds for q ). Let µ > 0. Then the following hold:

∀x ∈ H,

µ(cid:107)x(cid:107)H−1
rµ (x)

µ (x)

√

(cid:54) R

µ(cid:107)x(cid:107)H−1

µ (x)

(cid:54) R(cid:107)x(cid:107).

Moreover, we can bound all of these quantities using x(cid:63)
µ :
• For any c < 1, x ∈ H, if x ∈ Dµ (c/3), then the following holds:

(cid:18)

µ(cid:107)x(cid:107)H−1
c rµ (x)

µ (x)

(cid:54) 1

3

(cid:19)

1 +

1
1 − c/3

28

+

1
1 − c/3

µ(cid:107)H−1
µ (x(cid:63)
µ )

(cid:107)µx(cid:63)
c rµ (x(cid:63)
µ )

.

• For any c < 1, x ∈ H, if Rνµ (x)√

(cid:54) c

3 , then the following holds:

µ

(cid:18)

√

R

µ(cid:107)x(cid:107)H−1
c

µ (x)

(cid:54)

(cid:115)

(cid:19) 1

1 +

1
1 − c/3

+

3

R

1
1 − c/3

√

µ(cid:107)H−1
µ (x(cid:63)
µ )

µ(cid:107)x(cid:63)
c

.

Likewise, it can be shown that under the same conditions:

R(cid:107)x(cid:107)

µ(cid:107)

(cid:54) R(cid:107)x(cid:63)
c

+

1
3

φ(− log(1 − c/3)).

Proof.The ﬁrst bound is obvious. Moreover, the fact that (cid:101)νµ (x) (cid:54) c
3 implies that t(x − x(cid:63)
1−c/3 . Thus, we get the classical bounds on the Hessian using Eq. (14):

c

1

µ ) (cid:54)

log

e−t(x−x(cid:63)

µ )H(x) (cid:22) H(x(cid:63)

µ ) (cid:22) et(x−x(cid:63)

µ )H(x).

1. Bound on µ(cid:107)x(cid:107)H−1

µ (x) . Using Eqs. (17) and (18),

µ(cid:107)x(cid:107)H−1
µ (x) = (cid:107)∇fµ (x) − ∇f (x) + ∇f (x(cid:63)
µ ) − ∇f (x(cid:63)
(cid:54) νµ (x) +
(cid:107)Hµ (x)−1/2H(xt )(x − x(cid:63)
µ )(cid:107) dt + (cid:107)∇f (x(cid:63)
µ )(cid:107)Hµ (x) , xt = tx + (1 − t)x(cid:63)
µ .

µ )(cid:107)H−1

µ (x)

(cid:90) 1

0

Now bound (cid:107)Hµ (x)−1/2H(xt )(x − x(cid:63)
µ )(cid:107) (cid:54) (cid:107)Hµ (x)−1/2 Hµ (xt )1/2(cid:107) (cid:107)x − x(cid:63)

µ(cid:107)H(xt ) and use

Eq. (17) and Eq. (14) to get:

(cid:107)Hµ (x)−1/2H(xt )(x − x(cid:63)

µ )(cid:107) (cid:54) et t(x−x(cid:63)

µ )(cid:107)x − x(cid:63)

µ(cid:107)H(x) .

Integrating this yields:

(cid:90) 1

0

(cid:107)Hµ (x)−1/2H(xt )(x − x(cid:63)
µ )(cid:107) dt (cid:54) φ(t(x − x(cid:63)
µ )) (cid:107)x − x(cid:63)

µ(cid:107)H(x) (cid:54) et(x−x(cid:63)

µ ) νµ (x).

Where the last inequality is obtained using the bounds between gradient and hessian distance
Eq. (18). Finally, using the bound on t(x − x(cid:63)

(cid:18)

µ ),

(cid:19)

(cid:115)

µ(cid:107)x(cid:107)H−1

µ (x)

(cid:54)

1 +

1
1 − c/3

νµ (x) +

1
1 − c/3

(cid:107)∇f (x(cid:63)

µ )(cid:107)H−1

µ ) .
µ (x(cid:63)

2. Bound on R(cid:107)x(cid:107). Start by decomposing

R(cid:107)x(cid:107) (cid:54) R(cid:107)x(cid:63)
µ(cid:107) + R(cid:107)x − x(cid:63)
µ(cid:107).

Now bound

Using Eq. (17), (cid:107)x − x(cid:63)

µ(cid:107) (cid:54) R√

R(cid:107)x − x(cid:63)
(cid:107)x − x(cid:63)
µ
µ(cid:107)Hµ (x) (cid:54) φ(− log(1 − c/3))νµ (x). Hence:

µ(cid:107)Hµ (x) .

R(cid:107)x(cid:107) (cid:54) R(cid:107)x(cid:63)
µ(cid:107) + φ(− log(1 − c/3))

Rνµ (x)√
µ

.

29

3. Now assume x ∈ Dµ (c/3). Using the bound on µ(cid:107)x(cid:107)H−1
µ (x) , and noting that

. We know that in particular, x ∈ Dµ (c/3) and hence:

it holds:

µ(cid:107)x(cid:107)H−1
c rµ (x)

µ (x)

(cid:54) 1

3

1
rµ (x)

(cid:18)

1 +

4. Now assume Rνµ (x)√

µ

(cid:54) c

3 .

√

R

µ(cid:107)x(cid:107)H−1

µ (x)

(cid:54)

(cid:54)

(cid:18)
(cid:18)

(cid:18)

√

µ(cid:107)x(cid:107)H−1
c

µ (x)

(cid:54)

Hence

R

Likewise:

1 +

1
1 − c/3

1 +

1
1 − c/3

1 +

1
1 − c/3
(cid:54) R(cid:107)x(cid:63)
c

µ(cid:107)

+

R(cid:107)x(cid:107)

c

(cid:54) et(x−x(cid:63)
µ )/2

1
rµ (x(cid:63)
µ )

,

(cid:19)

+

1
1 − c/3

1
1 − c/3
(cid:19) Rνµ (x)√
µ
1
+
1 − c/3

(cid:115)

+

(cid:115)
(cid:115)

(cid:19) c
(cid:19) 1

3

+

3

µ(cid:107)H−1
µ (x(cid:63)
µ )

(cid:107)µx(cid:63)
c rµ (x(cid:63)
µ )

.

1
1 − c/3

Rµ(cid:107)x(cid:63)

µ(cid:107)H−1
µ (x(cid:63)
µ )

√

µ

√

R

µ(cid:107)x(cid:63)

µ(cid:107)H−1
µ ) .
µ (x(cid:63)

√

R

µ(cid:107)H−1
µ (x(cid:63)
µ )

µ(cid:107)x(cid:63)
c

.

1
1 − c/3

φ(− log(1 − c/3)).

1
3

We can get the following simpler bounds.
Corollary 3 (Application to c = 1
7 ). Applying Lemma 14 to c = 1
7 , we get the following bounds.
Let µ > 0.
• For any x ∈ H, if x ∈ Dµ (c/3), then the following holds:

7µ(cid:107)x(cid:107)H−1
rµ (x)

µ (x)

(cid:54) 1 +

µ(cid:107)H−1
µ (x(cid:63)
µ )

8(cid:107)µx(cid:63)
rµ (x(cid:63)
µ )

.

• For any c < 1, x ∈ H, if Rνµ (x)√

(cid:54) c

µ

√

7R

3 , then the following hold:

√

µ (x)

µ(cid:107)x(cid:107)H−1
(cid:54) 1 + 8R
µ(cid:107)x(cid:63)
7R(cid:107)x(cid:107) (cid:54) 7R(cid:107)x(cid:63)
µ(cid:107) + 1.

µ(cid:107)H−1
µ ) .
µ (x(cid:63)

C.2 Proof of main theorems

In this section, we bound the number of iterations of our scheme in different cases.
Recall the proposed globalization scheme in the paper, where ANMρ (f , x, t) is a method per-
forming t successive ρ-relative approximate Newton steps of f starting at x.

30

Proposed Globalization Scheme

Phase I: Getting in the Dikin ellispoid of fλ

For k ∈ N

Start with x0 ∈ H, µ0 > 0, t, T ∈ N and (qk )k∈N ∈ (0, 1].
xk+1 ← ANMρ (fµk , xk , t)

µk+1 ← qk+1µk

Stop when µk+1 < λ and set xlast ← xk . K ← k
Phase II: reach a certain precision starting from inside the Dikin ellipsoid

Return (cid:98)x ← ANMρ (fλ , xlast , T )

Throughout this section, we will denote with K the value of k when the scheme stops, i.e. the
ﬁrst value of k such that µk+1 < λ.

Adaptive methods We start by presenting an adaptive way to select µk+1 from µk , with theoret-
ical guarantees. The main result is the following.

Proposition 6 (Adaptive, simple version). Assume that we perform phase I starting at x0 such that

√

Rνµ0 (x0 )
µ0

(cid:54) 1

7

.

Assume that at each step k , we compute xk+1 using t = 2 iterations of the ρ-relative approximate
Newton method. Then if at each iteration, we set:

µk+1 = qk+1 µk ,

qk+1 :=

1

3 + 7R(cid:107)xk+1(cid:107)
1 + 7R(cid:107)xk+1(cid:107) .

Then the following hold:

1. ∀k (cid:54) K + 1,

√

Rνµk (xk )
µk

(cid:54) 1

7 .

2. The decreasing parameter qk+1 is bounded above before reaching K :

3 + 7R(cid:107)x(cid:63)
∀k (cid:54) K, qk+1 (cid:54) 4
2 + 7R(cid:107)x(cid:63)

(cid:107)

(cid:107) (cid:54) 4

λ(cid:107)

3 + 7R(cid:107)x(cid:63)
2 + 7R(cid:107)x(cid:63)
λ(cid:107) .

µk

µk



log µ0

log 2+7R(cid:107)x(cid:63)
λ
3 +7R(cid:107)x(cid:63)

λ (cid:107)
λ (cid:107)

4

 (cid:54) (cid:106)

(3 + 11R(cid:107)x(cid:63)
λ(cid:107)) log

(cid:107)

,

µ0
λ

3. K is ﬁnite,

K (cid:54)

and Rνλ (xK+1 )
λ

√

(cid:54) 1

7 .

Proof.

Let us prove the three points one by one.

31

1. This is easily proved by induction, the keys to the induction hypothesis being:
• Using the induction hypothesis, xk ∈ Dµk (c) and hence, using Proposition 5 shows that
after two iterations of the approximate Newton scheme, νµk (xk+1 )
3 which implies
3 .
• Now using Lemma 13, we see that that since

Rνµk (xk+1 )
µk

νµk (xk )

(cid:54) 1

(cid:54) c

√

7R(cid:107)xk+1(cid:107) =

R(cid:107)xk+1(cid:107)
c

(cid:62) R

µk (cid:107)xk+1(cid:107)H−1

µk (xk+1 )

c

,

√

the hypotheses to guarantee the bound for qk+1 hold, hence

√

Rνµk+1 (xk+1 )
µk+1

(cid:54) c.

2. Using the second bullet point of Cor. 3, we see that the previous point implies

∀k (cid:54) K, 7R(cid:107)xk+1(cid:107) (cid:54) 7R(cid:107)x(cid:63)

µk

(cid:107) + 1 =⇒ qk+1 (cid:54) 4/3 + 7R(cid:107)x(cid:63)
2 + 7R(cid:107)x(cid:63)

(cid:107)
(cid:107) .

µk

µk

Now using the fact that for any k (cid:54) K , µk > λ, we can use the simple fact that (cid:107)x(cid:63)
get the desired bound for qk+1 .

λ(cid:107) (cid:62) (cid:107)x(cid:63)

µk

(cid:107) to

3. Using the previous point clearly shows the following bound:

∀k (cid:54) K + 1, µk (cid:54)

As this clearly converges to 0 when k goes to inﬁnity, K is necessarily ﬁnite. Applying this for
k = K , we see that:

(cid:32) 4

3 + 7R(cid:107)x(cid:63)
2 + 7R(cid:107)x(cid:63)

λ(cid:107)
λ(cid:107)

(cid:33)k

µ0 .

(cid:32) 4

3 + 7R(cid:107)x(cid:63)
2 + 7R(cid:107)x(cid:63)

λ(cid:107)
λ(cid:107)

(cid:33)K

µ0 .

λ (cid:54) µK (cid:54)

This shows that K (cid:54)

log

log µ0

2+7R(cid:107)x(cid:63)
λ
λ
3 +7R(cid:107)x(cid:63)
4
λ

(cid:107)
(cid:107)

.

The ﬁnal bound is obtained noting that

λ(cid:107)

2 + 7R(cid:107)x(cid:63)
λ(cid:107) = 1 +
3 + 7R(cid:107)x(cid:63)

4

1
t

,

t = 2 +

R(cid:107)x(cid:63)
λ(cid:107),

21
2

and using the classical bound:

1
log(1 + 1
t )

(cid:54) t + 1.

Finally, the fact that Rνλ (xK+1 )
(cid:54) c is just a consequence of the fact that µK+1 (cid:54) λ (cid:54) µK and
thus that λ = qµK with q (cid:62) qK+1 , which is shown to satisfy the condition in Lemma 13. Hence,
the lemma holds not only for µK+1 but also for λ.

√

λ

32

Remark 2 (µ0 ). In the previous proposition, we assume start at x0 , µ0 such that

√

Rνµ0 (x0 )
µ0

(cid:54) 1

7

.

A simple way to have such a pair is simply to select:

x0 = 0, µ0 = 7R(cid:107)∇f (0)(cid:107),

R(cid:107)∇f (0)(cid:107)
µ0

√

H−1
µ0

√

(0)

=

since Rνµ0 (x0 )
µ0

.
Alternatively, if one can approximately compute (cid:107)x(cid:107)H−1
µ (x) , one can propose the following
variant, whose proof is completely analogous.

(cid:54) R(cid:107)∇f (0)(cid:107)
µ0

Proposition 7 (Adaptive, small variant version). Assume that we perform phase I starting at x0
such that

Rνµ0 (x)√
µ0

(cid:54) 1

.

7

Then if at each iteration, we set:

(cid:114) 7

tk+1 = 7

√

R

µk

√

xk+1 · sk+1 , sk+1 ∈ LinApprox(Hµk (xk+1 ), xk+1 ,

1
7

),

6

and

µk+1 = qk+1 µk ,

qk+1 :=

1
3 + tk+1

1 + tk+1

.

Then the following hold:

1. ∀k (cid:54) K,

√

Rνµk (xk )
µk

(cid:54) 1

7 .

2. The decreasing parameter qk+1 is bounded above before reaching K :

∀k (cid:54) K, qk+1 (cid:54) sup

µ0(cid:62)µ(cid:62)λ

7

3 + 10R
3 + 10R

√
√

µ(cid:107)x(cid:63)
µ(cid:107)x(cid:63)

µ(cid:107)H−1
µ (x(cid:63)
µ )
µ(cid:107)H−1
µ (x(cid:63)
µ )

(cid:54) 7

3 + 10R(cid:107)x(cid:63)
3 + 10R(cid:107)x(cid:63)
λ(cid:107) .

λ(cid:107)

(cid:32)

K (cid:54)

3. K is ﬁnite,

and Rνλ (xK+1 )
λ

√

(cid:54) 1

7 .

(cid:33)

9
2

+ 15 sup

λ(cid:54)µ(cid:54)µ0

R

√

µ(cid:107)x(cid:63)

µ(cid:107)H−1
µ (x(cid:63)
µ )

log

µ0
λ

,

7 -approximations, if sk+1 ∈
Proof.The main thing to note is that because of the properties of 1

LinApprox(Hµk (xk+1 ), xk+1 , 1
7 ),
(1 − 1
)(cid:107)xk+1(cid:107)2
7

H−1

µk (xk+1 )

Hence,

(cid:107)xk+1(cid:107)H−1

µk (xk+1 )

(cid:54)

(cid:114) 7

6

1
7

)(cid:107)xk+1(cid:107)2

H−1

µk (xk+1 )

.

(cid:107)xk+1(cid:107)H−1

µk (xk+1 ) .

(cid:114) 4

3

(cid:54) xk+1 · sk+1 (cid:54) (1 +

√

xk+1 · sk+1 (cid:54)

33

√

Hence, tk+1 (cid:62) 7R
µk (xk+1 ) , and we can apply Lemma 13 to get the ﬁrst point.
To get the second point, we bound tk+1 above:

µk (cid:107)xk+1(cid:107)H−1

(cid:114) 4

3

tk+1 (cid:54) 7

√

R

µk (cid:107)xk+1(cid:107)H−1

µk (xk+1 ) .

(cid:114) 4

3

(cid:16)

√

µk (cid:107)x(cid:63)

µk

1 + 8R

(cid:107)H−1

µk (x(cid:63)
µk

)

Now use Cor. 3 to ﬁnd:

tk+1 (cid:54)

Thus,

√
√

7

3 + 10R
3 + 10R

qk+1 (cid:54)

(cid:17) (cid:54) 2 + 10R
µk (cid:107)x(cid:63)
µk (cid:107)x(cid:63)

(cid:107)H−1
(cid:107)H−1

µk (x(cid:63)
µk

µk

µk

µk (x(cid:63)
µk

)

µk (cid:107)x(cid:63)

µk

(cid:107)H−1

µk (x(cid:63)
µk

) .

√

)

.

Note that as long as k (cid:62) K ,

qk+1 (cid:54) sup

µ(cid:62)λ

This guarantees convergence.

7

3 + 10R
3 + 10R

√
√

µ(cid:107)x(cid:63)
µ(cid:107)x(cid:63)

µ(cid:107)H−1
µ (x(cid:63)
µ )
µ(cid:107)H−1
µ (x(cid:63)
µ )

(cid:54) 7

3 + 10R(cid:107)x(cid:63)
3 + 10R(cid:107)x(cid:63)
λ(cid:107) .

λ(cid:107)

For the last point, the proof is exactly the same as in the previous proposition.

General non-adaptive result. As mentioned in the core of the article, in practice, we do not
select qk+1 at each iteration using a safe adaptative value, but rather decrease µk+1 = qµk with a
constant q , which we see as a parameter to tune. The following result shows that for q large enough,
this is justiﬁed, and that the lower bound we get for q depends on the radius of the Dikin ellipsoid
rµ (x), instead of
R in the previous bounds, which is somewhat ﬁner and shows that if the data is
structured such that this radius is very big, then q might actually be very small.

√

µ

Proposition 8 (Fixed q ). Assume that we perform phase I starting at x0 such that

Assume we perform the method with a ﬁxed qk+1 = q , satisfying

x0 ∈ Dµ0 (

1
7

).

q (cid:62) sup

λ(cid:54)µ(cid:54)µ0

4

3 + 8

2 + 8

H−1
µ (x(cid:63)
µ )

µ (cid:107)

µ(cid:107)x(cid:63)
rµ (x(cid:63)
µ )
µ(cid:107)x(cid:63)
rµ (x(cid:63)
µ )

µ (cid:107)

.

H−1
µ (x(cid:63)
µ )

Then the following hold:

1. ∀k (cid:54) K + 1, xk ∈ Dµk ( 1
7 ).

2. K is ﬁnite,

and xK+1 ∈ Dλ ( 1
7 ).

Proof.Let us prove the two points.

K (cid:54) 1
1 − q

log

µ0
λ

,

34

1. Let us prove the result by induction. The initialization is trivial. Now assume xk ∈ Dµk ( 1
Performing two iterations of the approximate Newton method guarantees that

7 ).

as show in Proposition 5. Now using Lemma 13, we see that xk+1 ∈ Dqµk ( 1
7 ), provided that

xk+1 ∈ Dµk (

1
21

),

q (cid:62)

1

3 +

1 +

7µk (cid:107)xk+1 (cid:107)

7µk (cid:107)xk+1 (cid:107)

H−1
(xk+1 )
µk
rµk (xk+1 )
H−1
µk
(xk+1 )
rµk (xk+1 )

.

Now using Cor. 3, we get that

7µk (cid:107)xk+1(cid:107)H−1

µk (xk+1 )

rµk (xk+1 )

(cid:54) 1 +

Hence the result.

(cid:107)H−1

8µk (cid:107)x(cid:63)
rµk (x(cid:63)

µk

)

µk

µk (x(cid:63)
µk

)

(cid:54) 1 + 8 sup

λ(cid:54)µ(cid:54)µ0

µ(cid:107)H−1
µ (x(cid:63)
µ )

µ(cid:107)x(cid:63)
rµ (x(cid:63)
µ )

.

2. This point just follows, using the bound

(cid:54) 1

1−q .

1
log 1

q

C.3 Proof of Thm. 1

Using Remark 2, the fact that x0 = 0 and µ0 = 7R(cid:107)∇f (0)(cid:107), as well as the hypotheses of the
theorem, we can apply Proposition 6, and show that the number of steps K performed in the ﬁrst
phase is bounded:

K (cid:54) (cid:98)(3 + 11R(cid:107)x(cid:63)
λ(cid:107)) log(7R(cid:107)∇f (0)(cid:107)/λ)(cid:99) .
λ (cid:54) 1

√

Moreover, this proposition also shows that Rνλ (xlast )/
if

7 . Hence, we can use Proposition 5:

(cid:38)

(cid:114)

(cid:39)

(cid:24)

t (cid:62) T =

λε−1
log2
R2
ε and fλ ( ˆx) − fλ (x(cid:63)
λ ) (cid:54) ε.

(cid:62)

(cid:25)

,

log2

νλ (xlast )√
ε

then it holds νλ ( ˆx) (cid:54) √

35

D Non-parametric learning with generalized self-concordant func-
tions

In this section, the aim is to provide a fast algorithm in the case of Kernel methods which achieves
the optimal statistical guarantees.

D.1 General setting and assumptions, statistical result for regularized ERM.

In this section, we consider the supervised learning problem of learning a predictor f : X → Y from
training samples (xi , yi )1(cid:54)i(cid:54)n which we assume to be realisations from a certain random variable
Z = (X, Y ) ∈ Z = X × Y whose distribution is ρ. In what follows, for simpliﬁcation purposes,
we assume Y = R; however, this analysis can easily be adapted (although with heavier notations)
to the setting where Y = Rp . Our aim is to compute the predictor of minimal generalization error

f ∈H L(f ) := Ez∼ρ [(cid:96)z (f (x))],
inf

(29)

where H is a space of candidate solutions and (cid:96)z : R → R is a loss function comparing the
prediction f (x) to the objective y .
Kx : t ∈ X (cid:55)→ K (x, t) and the linear combinations of such functions f = (cid:80)m
Kernel methods. Kernel methods consider a space of functions HK implicitly constructed from
a symmetric positive semi-deﬁnite Kernel K : X × X → and whose basic functions are the
It is endowed with a scalar product such that: ∀x1 , x2 ∈ X , Kx1 · Kx2 = K (x1 , x2 ), and as a
consequence, HK satisﬁes the self-reprocucing property:

j=1 αj Kxj .

∀x ∈ X , ∀f ∈ H, f (x) = (cid:104)f , Kx (cid:105)H .

In order to ﬁnd a good predictor for Eq. (29), the following estimator, called the regularized ERM
estimator, is often computed:

(cid:98)fλ := arg min

(cid:98)Lλ (f ) :=

f ∈H

(cid:96)zi (f (xi )) +

(cid:107)f (cid:107)2H .

λ
2

n(cid:88)

i=1

1
n

The properties of this estimator have been studied in [13] for the square loss and [23] for
generalized self-concordant functions. In Appendix H, we recall the full setting of [23], and extend
it to include the statistical properties of the projected problem.

Assumptions

In this section, we will make the following assumptions, which are reformulations
of the assumptions of [23], which we recall in Appendix H, in order to have the statistical properties
of the regularized ERM. First, we assume that the (xi , yi ) are i.i.d. samples.
Assumption 1 (i.i.d. data). The samples (zi )1(cid:54)i(cid:54)n = (xi , yi )1(cid:54)i(cid:54)n ∈ Z n are independently and
identically distributed according to ρ.
self concordance of the mappings f (cid:55)→ (cid:96)z (f (x)) and that of L, (cid:98)L...
In the case where Y = R, we make the following assumptions on the loss, which leads to the
Assumption 2 (Technical assumptions). The mapping (z , t) ∈ Z × R (cid:55)→ (cid:96)z (t) is measurable.
Moreover,

36

• there exists R(cid:96) < ∞ such that for all z ∈ supp(Z ),

• the random variables |(cid:96)Z (0)|, |(cid:96)(cid:48)
Z (0)| are are bounded;
• The kernel is bounded, i.e. ∀x ∈ supp(X ), K (x, x) (cid:54) κ2 for a certain κ.

∀t ∈ R, |(cid:96)(3)
z (t)| (cid:54) R(cid:96) (cid:96)(cid:48)(cid:48)
z (t),
Z (0)|, |(cid:96)(cid:48)(cid:48)

Using these assumptions, we see that the following properties are satisﬁed. Deﬁne Lz (f ) :=
(cid:96)z (f (x)). Then the Lz satisfy the following properties:
• For any z ∈ Z , (Lz , {R(cid:96)Kx}) is a generalized self-concordant function in the sense of
Deﬁnition 4.
• The mapping (z , f ) ∈ Z × H (cid:55)→ Lz (f ) is measurable;
• the random variables (cid:107)LZ (0)(cid:107), (cid:107)∇LZ (0)(cid:107), Tr(∇2LZ (0)) are bounded by |(cid:96)Z (0)|, κ|(cid:96)(cid:48)

Z (0)|,

κ2 |(cid:96)(cid:48)(cid:48)

Z (0)|;

n(cid:88)

(cid:98)L =

• G := {R(cid:96)Kx : z ∈ supp(Z )} is a bounded subset of H, bounded by R = R(cid:96)κ.
sition 16 in the next appendix, L is well-deﬁned, generalized self-concordant with G . Moreover,
This shows that Assumption 7 and Assumption 8 are satisﬁed by the Lz and hence, using Propo-
the empirical loss
is also generalized self-concordant with (cid:98)G := {R(cid:96)Kxi
Finally, as in Appendix H, we make an assumption on the regularity of the problem; namely,
we assume that a solution to the learning problem exists in H.
Assumption 3 (Existence of a minimizer). There exists f (cid:63) ∈ H such that L(f (cid:63) ) = inf f ∈H L(f ).
empirical problems by adding a (cid:98)· over the quantities related to the empirical problem. We continue
We adopt all the notations from Appendix H, doing the distinction between expected an
using the standard notations for L: for any f ∈ H and λ > 0,

: 1 (cid:54) i (cid:54) n}.

Lzi ,

1
n

i=1

λ
2

(cid:98)Lλ (f ) = (cid:98)L(f ) +
(cid:107)f (cid:107)2 ,
λ
2
(cid:98)Hλ (f ) = ∇2 (cid:98)Lλ (f ) = (cid:98)H(f ) + λI
Hλ (f ) = ∇2Lλ (f ) = H(f ) + λI

(cid:107)f (cid:107)2

Lλ (f ) = L(f ) +
(cid:98)H(f ) = ∇2 (cid:98)L(f ),
H(f ) = ∇2L(f ),

Recall that (cid:98)fλ is deﬁned as the minimizer of (cid:98)Lλ .
Deﬁne the following bounds on the second order derivatives:

∀f ∈ H, b2 (f ) = sup

z∈supp(Z )

(cid:96)(cid:48)(cid:48)

z (f (x)).

37

Statistical properties of the estimator The statistical properties of the estimator (cid:98)fλ have been
studied in [23] in the case of generalized self concordance, an are reported in the main lines in
Appendix H. The statistical rates of this estimator and the optimal choice of λ is determined by two
parameters, deﬁned in Proposition 17 and which we adapt to the Kernel problem here.

λ (f (cid:63) ) , which characterizes the regularity
of the optimum. The faster bλ decreases to zero, the more regular f (cid:63) is.
• the effective dimension

• the bias bλ = (cid:107)Hλ (f (cid:63) )−1/2∇Lλ (f (cid:63) )(cid:107) = λ(cid:107)f (cid:63)(cid:107)H−1
df λ = E (cid:104)(cid:107)Hλ (f (cid:63) )−1/2∇LZ (f (cid:63) )(cid:107)2(cid:105)

(30)
This quantity characterizes the size of the space H with respect to the problem; the slower it
explodes as λ goes to zero, the smaller the size of H.

.

For more complete explanations on the meaning of these quantities, we refer to [23].

Moreover, as mentioned in Proposition 17, one can deﬁne

B(cid:63)
1 := sup

z∈supp(Z )

(cid:107)∇Lz (f (cid:63) )(cid:107), B(cid:63)
2 := sup

z∈supp(Z )

Tr(∇2Lz (f (cid:63) )), Q(cid:63) =

1(cid:112)B(cid:63)
B(cid:63)

2

, b(cid:63)
2 = b2 (f (cid:63) ). (31)

We assume the following regularity condition on the minimizer f (cid:63) , in order to get statistical
bounds.
Assumption 4 (Source condition). There exists r > 0 and g ∈ H such that f (cid:63) = Hr (f (cid:63) )g . This
implies the following decrease rate of the bias:

bλ (cid:54) Lλ1/2+r ,

L = (cid:107)g(cid:107)H .

This is a stronger assumption than the existence of the minimizer as r > 0 is crucial for our
analysis.
We also quantify the effective dimension df λ : (however, since it always holds for α = 1, this is
not, strictly speaking, an additional assumption).
Assumption 5 (Effective dimension). The effective dimension decreases as df λ (cid:54) Qλ−1/α .
If these two assumptions hold, deﬁne:

β =

α
1 + α(1 + 2r)

,

γ =

(1 + 2r)α
1 + α(1 + 2r)

.

Under these assumptions, one can obtain the following statistical rates (which can be found in
[23] or in Cor. 4).
Proposition 9. Let δ ∈ (0, 1/2]. Under Assumptions 1 to 5, when n (cid:62) N and λ = (C0/n)β , then
with probability at least 1 − 2δ ,

L( (cid:98)fλ ) − L(f (cid:63) ) (cid:54) C1n−γ log

2
δ

,

with C0 = 256(Q/L)2 , C1 = 8(256)γ (Qγ L1−γ )2 and N deﬁned in [23], and satisfying N =

O(poly(B(cid:63)
1 , B(cid:63)
2 , L, Q, R, log(1/δ))).

38

D.2 Reducing the dimension: projecting on a subspace using Nyström sub-sampling.

Computations Using a representer theorem, one of the key properties of Kernel spaces is that,
owing to the reproducing property,

(cid:40) n(cid:88)

(cid:98)fλ ∈ Hn :=

(cid:41)

αiKxi

: (αi ) ∈ Rn

.

problem in α. Indeed (cid:98)fλ = (cid:80)n
This means that solving the regularized empirical problem can be turned into a ﬁnite dimensional
i=1 αiKxi where α = (αi )1(cid:54)i(cid:54)n is the solution to the following
problem:

i=1

n(cid:88)

i=1

 M(cid:88)
n(cid:88)

j=1

i=1

α = arg min
α∈Rn

1
n

(cid:96)zi (α(cid:62)Knnei ) +

α(cid:62)Knnα,

λ
2

Knn = (K (xi , xj ))1(cid:54)i,j(cid:54)n ∈ Rn×n .

The previous problem is usually too costly to solve directly for large values of n, both in
time and memory, because of the operations involving Knn . A solution consists in looking for
a solution in a smaller dimensional sub-space HM constructed from sub-samples of the data

{ ˜x1 , ..., ˜xM } ⊂ {x1 , ..., xn}:

HM :=

˜αj K ˜xj

: ˜α ∈ RM

In this case, the minimizer (cid:98)fM ,λ = arg minf ∈HM (cid:98)Lλ (f ) can be written (cid:98)fM ,λ = (cid:80)M
where ˜α is the solution to the following problem:

j=1 ˜αj K ˜xj ,

 .

˜α = arg min
α∈RM

1
n

where

(cid:96)zi (α(cid:62)KM nei ) +

α(cid:62)KM M α,

λ
2

(cid:98)LM ,λ (β ) :=

n(cid:88)

i=1

KnM = (K (xi , ˜xj )) 1(cid:54)i(cid:54)n

1(cid:54)j(cid:54)M

, KM n = K(cid:62)
nM , KM M := (K ( ˜xi , ˜xj ))1(cid:54)i,j(cid:54)M .

previous problem in the following way. For any β ∈ RM , deﬁne fβ = (cid:80)M
implies in particular that (cid:107)fβ (cid:107)H = (cid:107)β (cid:107)RM . Then (cid:98)fM ,λ = fβM ,λ , where
Let T be an upper triangular matrix such that T(cid:62)T = KM M . One can re-parametrize the

j=1 [T†β ]j K ˜xj . This

1
n

βM ,λ = arg min
β∈RM

(cid:96)zi (e(cid:62)
i KnM T†β ) +
i KnM T†β ) is (cid:8)R(cid:96)T−(cid:62)KM nei
λ
(cid:107)β (cid:107)2 .
2

(cid:112)K (xi , xi ). Thus, (cid:98)LM is also general-
Using the properties the (cid:96)z , one easily shows that β (cid:55)→ (cid:96)zi (e(cid:62)
ized self-concordant, and the associated (cid:98)GM is bounded by R = R(cid:96)κ. It will therefore be possible
generalized self-concordant, and (cid:107)R(cid:96)T−(cid:62)KM nei(cid:107) (cid:54) R(cid:96)
to apply the second order scheme presented in this paper to approximately compute βM ,λ .
Statistics Let (cid:98)νλ,M (β ) denote the Newton decrement of (cid:98)Lλ,M at point β and PM denote the
orthogonal projection on HM . Then the following statistical result shows that provided β is a good
generalization error as the empirical risk minimizer (cid:98)fλ .
enough approximation of the optimum, and provided HM is large enough, then fβ has the same
Recall the following result proved in Proposition 19 in Appendix H.3.

(cid:9)

39

Proposition 10 (Behavior of an approximation to the projected problem). Suppose that Assump-
tions 1 to 3 are satisﬁed. Let n ∈ N, δ ∈ (0, 1/2], 0 < λ (cid:54) B(cid:63)
2 . Whenever

(cid:114)

n (cid:62) (cid:52)1

2

B(cid:63)
λ

log

1B(cid:63)
2

8(cid:3)2
λδ

,

C1

if

(cid:107)H1/2 (f (cid:63) )(I − PM )(cid:107)2 (cid:54) λ

the following holds, with probability at least 1 − 2δ .

L(fβ ) − L(f (cid:63) ) (cid:54) K1 b2
λ + K2

df λ ∨ (Q(cid:63) )2
n

√

(cid:54) λ1/2

df λ ∨ (Q(cid:63) )2
2
log
,
n
δ
R
, 126(cid:98)νM ,λ (β ) (cid:54) λ1/2
R
+ K3 (cid:98)ν 2
M ,λ (β ),

2
480

log

2
δ

C1bλ (cid:54) λ1/2

R

,

,

R(cid:107)fβ − f (cid:63)(cid:107)H (cid:54) 10,

where K1 (cid:54) 6.0e4, K2 (cid:54) 6.0e6 and K3 (cid:54) 810, C1 is deﬁned in Lemma 19, and the other
constants are deﬁned in Thm. 8.
In particular, if we apply the previous result for a ﬁxed λ, the following theorem holds (for a
proof, see Appendix H.4).
Theorem 5 (Quantitative result with source r > 0). Suppose that Assumptions 1 to 5 are satisﬁed.
Let n (cid:62) N and δ ∈ (0, 1

α
α(1+2r)+1 , and if

(cid:16)(cid:0) Q

(cid:17)

(cid:1)2 1
n

L

√

2 ]. If λ =
(cid:107)H1/2 (f (cid:63) )(I − PM )(cid:107)2 (cid:54) λ
2
L(fβ ) − L(f (cid:63) ) (cid:54) K (cid:0)Qγ L1−γ (cid:1)2
480

then with probability at least 1 − 2δ ,

, (cid:98)νM ,λ (β ) (cid:54) Qγ L1−γ n−γ /2 ,

1
nγ log

2
δ

,

R(cid:107)fβ − f (cid:63)(cid:107) (cid:54) 10,

where N is deﬁned in Eq. (42) and K (cid:54) 7.0e6.
The proof of the previous result is quite technical and can be found in Appendix H, in Thm. 9.

D.3 A note on sub-sampling techniques

Let Z be a random variable on a Polish space Z and (vz )z∈Z be a family of vectors in H such that
||v ||L∞ (Z ) := supz∈supp(Z ) (cid:107)vz (cid:107) < ∞ is bounded. Assume that z1 , ..., zn are i.i.d. samples from
Z .

Deﬁne the following trace class Hermitian operators:

A = E [vZ ⊗ vZ ] , (cid:98)A =

n(cid:88)

i=1

1
n

vzi ⊗ vzi .

Deﬁne

N A (λ) := Tr(A−1
λ A),

N A∞ (λ) := sup

z∈supp(Z )

(cid:107)A

−1/2
λ

vz (cid:107)2 .

We typically have:

N A (λ) (cid:54) N A∞ (λ) (cid:54) (cid:107)v(cid:107)2
.
i (t) = (cid:107) (cid:98)A
vzi (cid:107)2 = n (cid:0)(Gnn + tnI)−1Gnn
λ
∀1 (cid:54) i (cid:54) n, ∀t > 0, lA

We deﬁne the leverage scores associated to the points zi and A:

−1/2
t

L∞ (Z )

(cid:1)

ii ,

where Gnn = (vzi · vzj )1(cid:54)i,j(cid:54)n denotes the Gram matrix associated to the family vzi .
As in [28], deﬁnition 1, we give the following deﬁnition for leverage scores.

(32)

(33)

40

Deﬁnition 5 (q -approximate leverage scores). given t0 , a family (˜lA
i (t))1(cid:54)i(cid:54)n is said to be a family
of q -approximate leverage scores with respect to A if

∀1 (cid:54) i (cid:54) n, ∀t (cid:62) t0 ,

i (t) (cid:54) ˜lA
i (t) (cid:54) q lA
lA
i (t).

1
q

We say that a subset of m points { ˜z1 , ..., ˜zm} ⊂ {zi

: 1 (cid:54) i (cid:54) n} is:

• Sampled using q -approximate leverage scores for t if the ˜zj = zij where the ij are m i.i.d.

samples from {1, ..., n} using the probability vector pi =

(cid:98)Am := 1

m

(cid:80)m

j=1

1
npij

v ˜zj ⊗ v ˜zj .

(cid:80)n

˜lA
i (t)
˜lA

˜i=1

˜i

. In that case, we deﬁne

(t)

(cid:80)m

m

j=1 v ˜zj ⊗ v ˜zj .

• Sampled uniformly if the {ij : 1 (cid:54) j (cid:54) m} is a uniformly chosen subset of {1, ..., n} of
size m. In this case, we deﬁne (cid:98)Am := 1
In Appendix I.1, we present technical lemmas which allow us to show that if m is large enough,
the following hold:
• (cid:107)Aη (I − Pm )(cid:107)2 (cid:54) 3η , where Pm is the orthogonal projection on the subspace induced by
• (cid:98)Am,λ is equivalent to (cid:98)Aλ .
the v ˜zj ;
Remark 3 (cost of computing q -approximate leverage scores). In [30], one can show that the com-
plexity of computing q -approximate leverage scores can be achieved in: csamp = O(q2N A (λ)2 min(n, 1/λ))
time (where a unit of time is a scalar product evaluation) and O(N A (λ)2 + n) in memory.

D.4 Selecting the M Nyström points

In order for Thm. 5 to hold, we must subsample the M points such as to guarantee (cid:107)H1/2 (f (cid:63) )(I −

PM )(cid:107)2 (cid:54)

√

480 .
2λ

n

(cid:80)n

Since we must sub-sample the M points a priori, i.e. before performing the method, it is
necessary to have sub-sampling schemes which do not depend heavily on the point. Deﬁne the
covariance operator:
since (cid:98)Σ = 1
Since H(f (cid:63) ) = E [(cid:96)(cid:48)(cid:48)
Z (f (X )) KX ⊗ KX ], it is easy to see that H(f (cid:63) ) (cid:22) b(cid:63)
2Σ. Note that for Σ,
i=1 Kxi ⊗ Kxi , the leverage scores have the following form:

Σ = E [KX ⊗ KX ] .
i (t) = n (cid:0)(Knn + λnI)−1Knn
∀1 (cid:54) i (cid:54) n, lΣ
1. n (cid:62) M (cid:62) (cid:0)10 + 160N Σ∞ (η)(cid:1) log 8κ2
2. M (cid:62) (cid:0)6 + 486q2N Σ (η)(cid:1) log 8κ2
t = η , t0 ∨ 19κ2
2δ < η , n (cid:62) 405κ2 ∨ 67κ2 log 12κ2
n log n

Proposition 11 (Selecting Nyström points). Let δ > 0. Let η = min((cid:107)Σ(cid:107),
the samples { ˜x1 , ..., ˜xM } are obtained with one of the following.
ηδ using uniform sampling;
ηδ using q -approximate leverage scores with respect to Σ for
.
Then it holds, with probability at least 1 − δ :

2 ∨1) ). Assume

λ
2
1440(b(cid:63)

(cid:1)

ii .

√

δ

(cid:107)Σ1/2
η

(I − PM )(cid:107) (cid:54) 3η =⇒ (cid:107)H1/2 (f (cid:63) )(I − PM )(cid:107)2 (cid:54) λ

41

√

2
480

.

(cid:114)

Proof.The proof is a direct application of the lemmas in Appendix I.1. Indeed, note that since
Σ = E [KX ⊗ KX ], then the results can be applied with Z ← X and vz ← Kx . Indeed, from
Assumption 2, it holds:

(cid:107)Kx(cid:107)2 (cid:54) κ2 .

sup

x∈supp(X )

We can now combine Proposition 11 and Proposition 10 to obtain the following statistical
bounds for the optimizer of the projected Nyström problem βM ,λ .
Theorem 6. Suppose that Assumptions 1 to 3 are satisﬁed. Let n ∈ N, δ ∈ (0, 1/2], 0 < λ (cid:54)
2 ∨ 1)(cid:107)Σ(cid:107). Assume

2 ∧ 720
B(cid:63)

2(b(cid:63)

√

2

√

log

1B(cid:63)
2

Let η =

λ
2
1440(b(cid:63)

n (cid:62) (cid:52)1

df λ ∨ (Q(cid:63) )2
n

8(cid:3)2
B(cid:63)
log
,
C1
λ
λδ
1. n (cid:62) M (cid:62) (cid:0)10 + 160N Σ∞ (η)(cid:1) log 8κ2
2. M (cid:62) (cid:0)6 + 486q2N Σ (η)(cid:1) log 8κ2
t = η , t0 ∨ 19κ2
2δ < η , n (cid:62) 405κ2 ∨ 67κ2 log 12κ2
n log n

2 ∨1) . Assume the samples { ˜x1 , ..., ˜xM } are obtained with one of the following.
ηδ using uniform sampling;
ηδ using q -approximate leverage scores with respect to Σ for
The following holds, with probability at least 1 − 3δ .
.

2
δ

(cid:54) λ1/2

R

,

C1bλ (cid:54) λ1/2

R

,

δ

L(fβM ,λ ) − L(f (cid:63) ) (cid:54) K1 b2
λ + K2

df λ ∨ (Q(cid:63) )2
n

log

2
δ

,

R(cid:107)βM ,λ(cid:107) (cid:54) R(cid:107)f (cid:63)(cid:107) + 10,

where K1 (cid:54) 6.0e4, K2 (cid:54) 6.0e6 and K3 (cid:54) 810, C1 is deﬁned in Lemma 19, and the other constants
are deﬁned in Thm. 8.
Proof.This is simply a reformulation of Proposition 10, noting that (cid:98)νM ,λ (βM ,λ ) = 0 and that
Proposition 11 implies the condition on the Hessian at the optimum.
Provided source condition holds with r > 0, the conditions of this theorem are not void.
In order to apply Proposition 10, one needs to control (cid:98)νM ,λ (β ).
We will apply our general scheme to (cid:98)LM ,λ in order to obtain such a control.

D.5 Performing the globalization scheme to approximate βM ,λ

D.5.1 Performing approximate Newton steps

The key element in the globalization scheme is to be able to compute 1
7 -approximate Newton steps.
Note that at a given point β and for a given µ > 0 the Hessian is of the form:

(cid:98)HM ,µ (β ) =

T−(cid:62)KM nDn (β )KnM T−1 + µIM ,

1
n

where Dn (β ) = diag((di (β ))1(cid:54)i(cid:54)n ) is a diagonal matrix whose elements are given by di (β ) =

Note that we can always write

(cid:96)(cid:48)(cid:48)

zi (e(cid:62)
i KnM T−1β ).
(cid:98)HM ,µ (β ) =

n(cid:88)

i=1

1
n

ui (β )ui (β )(cid:62) + µI,

ui (β ) = (cid:112)di (β )T−(cid:62)KM nei

The gradient can be put in the following form:

42

∇ (cid:98)LM ,µ (β ) =

T−(cid:62)KM nv + µβ ,

1
n

v = ((cid:96)(cid:48)
zi (e(cid:62)
i KnM T−1β ))1(cid:54)i(cid:54)n .

Computing the gradient at one point therefore costs O(nM + M 2 ), this being the cost of
computing KnM times a vector costs O(nM ) and computing T−1 times a vector takes O(M 2 )
since T is triangular. Moreover, the cost in memory is O(M 2 + n), M 2 being needed for the saving
of T and n for the saving of the gradient; KnM times a vector can also be done in O(n) memory,
provided we compute it by blocks.

On the other hand, computing the full Hessian matrix would cost nM 2 operations, which is
un-tractable. However, computing a Hessian vector product can be done in O(nM + M 2 ) time, as
for the gradient, which suggest using an iterative solver with preconditioning.

Computing x ∈ LinApprox(A, b, ρ) through pre-conditioned conjugate gradient descent.

Assume we wish to solve the problem Ax = b where A ∈ RM ×M is a positive deﬁnite matrix
and b is a vector of RM . If one uses the conjugate gradient method starting from zero, then if xk
denotes the k-the iterate of the conjugate gradient algorithm, Theorem 6.6 in [31] shows that

xk ∈ LinApprox(A, b, ρ),

ρ = 2

(cid:32) (cid:112)Cond(A) − 1
(cid:112)Cond(A) + 1

(cid:33)k

.

λmin (A) .

(cid:101)A (cid:22) A (cid:22) 3

approximation matrix (cid:101)A such that
where Cond(A) is the condition number of the matrix A, namely the ratio λmax (A)
If
Cond(A) is large, this convergence can be very slow. The idea of preconditioning is to compute an
We then compute B a triangular matrix such that B(cid:62)B = (cid:101)A using a cholesky decomposition,
(34)
which can be done in O(M 3 ), and note that B−(cid:62)AB−1 is very well conditioned; indeed, its
condition number is bounded by 3.
Perform a conjugate gradient method to solve the pre-conditioned problem B−(cid:62)AB−1z =
B−(cid:62) b, and denote with zτ the τ -th iteration of this method. Then using the bound on the condition
number, we ﬁnd

(cid:101)A.

1
2

2

zτ ∈ LinApprox(B−(cid:62)AB−1 , B−(cid:62) b, ρ),

ρ = 2

which in turn implies that by setting xτ := B−1zτ ,

xτ ∈ LinApprox(A, b, ρ), ρ = 2

This shows that after at most τ = 3 iterations, provided (cid:101)A satisﬁes Eq. (34), xτ ∈ LinApprox(A, b, 1
The cost of this method is therefore O(M 3 + nM ) in time, and O(n + M 2 ) due to the computing
cost of ﬁnding a suitable (cid:101)A.
of the preconditioner and computing matrix vector products by block. This does not include the

7 ).

(cid:33)τ

,

3 − 1
3 + 1

(cid:32) √

√

(cid:33)τ

.

(cid:32) √

√

3 − 1
3 + 1

43

and assume 19b2 (fβ )κ2
n

Computing a suitable approximation of (cid:98)HM ,µ (β ) To compute a good pre-conditioner, we will
subsample Q points i1 , ..., iQ points from {1, ..., n}, and sketch the Hessian using these Q points.
Proposition 12 (Computing approximate newton steps). Let δ > 0. Let β ∈ RM and µ (cid:62) λ,
. Let ˜µ =

min(µ, (cid:107)H(fβ )(cid:107)). Assume one of the following properties is satisﬁed
with uniform sampling of the {i1 , ..., iQ}. We set

2δ < λ and n (cid:62) 405b2 (fβ )κ2 ∨ 67b2 (fβ )κ2 log 12b2 (fβ )κ2
log n
10 + 160N H(fβ )∞ ( ˜µ)
2. Q (cid:62) (cid:0)6 + 486q2N H(fβ ) ( ˜µ)(cid:1) log 8b2 (fβ )κ2
DQ = diag((cid:96)(cid:48)(cid:48)
(fβ (xij )))1(cid:54)j(cid:54)Q
H(fβ ) for t = ˜µ. We set DQ = diag

using q -approximate leverage scores associated to

, where the pij are the probabilities computed

1. Q (cid:62) (cid:16)

log 8b2 (fβ )κ2
˜µδ

(cid:18) (cid:96)(cid:48)(cid:48)

(fβ (xij ))

(cid:19)

(cid:17)

zij

δ

˜µδ

zij

pij

from the leverage scores.

Assume we use a pre-conditioner B such that

B(cid:62)B =

1
Q

T−(cid:62)KM QDQKQM T−1 + µIM ,

KQM = (K (xij , ˜xk )) 1(cid:54)j(cid:54)Q

1(cid:54)k(cid:54)M

.

√

√

3 + 1)/

If we perform τ = log(ρ/2)/ log((

3 − 1) iterations of the conjugate gradient
at least 1 − δ , this procedure is returns (cid:101)∆ ∈ LinApprox( (cid:98)HM ,λ (β ), ∇ (cid:98)LM ,λ (β ), ρ), and the
descent on the pre-conditioned Newton system using B as a preconditioner, then with probability
computational time is of order O(τ (M n + M 2Q + M 3 + csamp )), and the memory requirements
can be reduced to O(M 2 + n). Here csamp stands for the complexity of computing Nystrom
leverage scores, and using Remark 3 or [30], csamp = O(1) if uniform sampling is used, and
csamp = O(N H(fβ ) ( ˜µ)2/λ) if Nystrom sub-sampling is used. Note that for τ = 3, ρ = 1
7 .
Proof.Start by deﬁning the following operators:

• Kn : f ∈ H → (f (xi ))1(cid:54)i(cid:54)n ∈ Rn ;
• KM : f ∈ H → (f ( ˜xj ))1(cid:54)j(cid:54)M ∈ RM ;
• V = K ∗

M T−1 , where T is an upper triangular matrix such that T(cid:62)T = KM M = KM K ∗
M .
Note that KnV = KnM T−1 .
Now note that

∀f ∈ H, H(f ) = E [vz ⊗ vz ] ,

Since for any f ∈ H, (cid:98)H(f ) = 1

n(cid:88)

vzi ⊗ vzi ,

vz = (cid:112)(cid:96)(cid:48)(cid:48)
(cid:98)H(f ) =
1
n
z (f (x))Kx .
n K ∗
nDn (f )Kn , where Dn (f ) = diag((cid:96)(cid:48)(cid:48)
(cid:98)HM ,µ (β ) = V ∗ (cid:98)H(fβ )V + µIM .

i=1

zi (f (xi ))), we see that

Thus, the last lemma of Appendix I.1 can be applied, using the fact that (cid:107)vz (cid:107)2 (cid:54) b2 (f )κ2 , to
get that in both cases of the proposition, under the corresponding assumptions:

(cid:19)

.

(cid:18) 1

Q

1
2

(cid:19)

(cid:22) (cid:98)HM ,µ (β ) (cid:22) 3
2

(cid:18) 1

Q

T−(cid:62)KM QDQKQM T−1 + µIM

T−(cid:62)KM QDQKQM T−1 + µIM

The rest of the proposition follows from the previous discussion.

44

D.5.2 Applying the globalization scheme to control (cid:98)νM ,λ (β )

In order to apply Proposition 12 to each point β in our method, we need to have a globalized version
of the condition of this proposition.
First, we start by localizing the different values of β we will visit throughout the algorithm.

Deﬁnition 6 (path of regularized solutions). Let λ > 0, ε > 0. Deﬁne the path of regularized
solutions

(35)

(cid:98)ΓM
λ := {βM ,µ : µ (cid:62) λ} .
β ∈ RM : d(β , (cid:98)ΓM
λ ) (cid:54) ε

(cid:110)

(cid:111)

λ,ε :=

(cid:98)ΓM

And the ε approximation of this path:
(36)
Note that we always have (cid:98)ΓM
λ ⊂ BRM ((cid:107)βM ,λ(cid:107)). We now state a lemma proving that all the
values visited during the algorithm will lie in an approximation of this path.
Lemma 15. Deﬁne Let β ∈ RM such that (cid:98)νM ,µ (β ) (cid:54) µ1/2
7R for some µ (cid:62) λ. Then the following
holds:

.

β ∈ (cid:98)ΓM

.

λ, 1

6R

Proof.Bound

R(cid:107)β − βM ,µ(cid:107) (cid:54) R

µ1/2

(cid:107)β − βM ,µ(cid:107) (cid:98)HM ,µ (β )

(cid:54)

1
φ(tM (β − βM ,µ ))

R(cid:98)νM ,µ (β )

µ1/2

.

Just apply Eq. (18) to obtain R(cid:107)β − βM ,µ(cid:107) (cid:54) 1
6 .
We now introduce the following quantities which will allow to control the number of sub-
samples throughout the whole algorithm.

Deﬁnition 7. Deﬁne

λ,1/6R

• N H

• b2 := supβ∈(cid:98)ΓM
b2 (fβ ).
(λ) = supβ∈(cid:98)ΓM
• N H∞ (λ) = supβ∈(cid:98)ΓM
• (cid:107)H(cid:107) = minβ∈(cid:98)ΓM

λ,1/6R

λ,1/6R

λ,1/6R

N H(fβ ) (λ).

N H(fβ )∞ (λ).
(cid:107)H(fβ )(cid:107).

n

2δ < ˜λ and n (cid:62) 405b2κ2 ∨ 67b2κ2 log 12b2 κ2
log n

Proposition 13 (Performance of the globalization scheme). Let ε > 0, δ > 0, ˜λ = min(λ, (cid:107)H(cid:107)).
Assume 19b2 κ2
.
Assume we perform the globalization scheme with the parameters in Thm. 1, where in order to
compute any ρ approximation of a regularized Newton step, we use a conjugate gradient descent
on the pre-conditioned system, where the pre-conditioner is computed as in Proposition 12 using
if using uniform sampling

δ

1. Q (cid:62) (cid:16)
2. Q (cid:62) (cid:16)

(cid:17)
(cid:17)

10 + 160N H∞ (˜λ)
6 + 486q2N H
(˜λ)

log 8b2 κ2
˜λδ
log 8b2 κ2
˜λδ

if using Nyström leverage scores

Recall that t denotes the number of approximate Newton steps performed at for each µ in Phase
I and T denotes the number of approximate Newton steps performed in Phase II, and that using

45

(cid:112)1 ∨ (λε−1/R2 )(cid:101). Moreover, recall that K denotes the number of
Thm. 1, t = 2 and T = (cid:100)log2
steps performed in Phase I. Deﬁne

(cid:106)

(3 + 11R(cid:107)βM ,λ(cid:107)) log2 (7R(cid:107)∇ (cid:98)LM (0)(cid:107)/λ)

Nns := 2

(cid:107)

(cid:112)

+ (cid:100)log2

1 ∨ (λε−1/R2 )(cid:101).

Then with probability at least (1 − δ)Nns :
• The method presented in Proposition 12 returns a 1/7- approximate Newton step at each
• If β denotes the result of the method, (cid:98)νM ,λ (β ) (cid:54) √
time it is called in the algorithm.
ε.
• The number of approximate Newton steps computed during the algorithm is bounded by Nns ;
the complexity of the method is therefore of order O(Nns (M 2 max(M , Q)+nM +csamp (λ)))
in time and O(M Q + M 2 + n) in memory, where csamp (λ) is a bound on the complexity
associated to the computing of leverage scores (see [30] for details).

The algorithm is detailed in Appendix E, in algorithm 1. Note however that the notations are
those of the main paper, which are slightly different from the ones used here.
we are belongs to (cid:98)ΓM
Proof.If we take the globalization scheme, using the parameters of Thm. 1. Assume that all
previous approximate Newton steps have been computed in a good way. Then the β at which
λ,1/6R . Thus, the hypotheses of this proposition imply that the hypothesis of
Proposition 12 are satisﬁed; and hence, up to a (1 − δ) probability factor, we can assume that the
next approximate Newton step is performed correctly, continuing the globalization scheme in the
right way. Thus, the globalization scheme converges as in Thm. 1.

D.6 Statistical properties of the algorithm

The following theorem describes the computational and statistical behavior of our algorithm.

Proposition 14 (Behavior of an approximation to the projected problem). Suppose that Assump-
tions 1 to 3 are satisﬁed.

B(cid:63)

2

λδ

n

2 .

1B(cid:63)
2

(cid:114)

. Assume

n (cid:62) (cid:52)1

Let n ∈ N, ε > 0, δ ∈ (0, 1/2], 0 < λ (cid:54) B(cid:63)
Deﬁne ˜λ = min(λ, (cid:107)H(cid:107)) and assume 19b2 κ2
λ log 8(cid:3)2
df λ ∨ (Q(cid:63) )2
C1
log
2
n
δ
1. M (cid:62) (cid:0)10 + 160N Σ∞ (η)(cid:1) log 8κ2
2. M (cid:62) (cid:0)6 + 486q2N Σ (η)(cid:1) log 8κ2

(cid:54) λ1/2

,

R

2δ < ˜λ, n (cid:62) 405b2κ2 ∨ 67b2κ2 log 12b2 κ2
log n

δ

, and

C1bλ (cid:54) λ1/2

R

,

126

√

ε (cid:54) λ1/2
R

.

Assume that the M points ˜x1 , ..., ˜xM are obtained through Nyström sub-sampling using η =
2 ∨1) , with either

(cid:107)Σ(cid:107) ∧

λ
2
1440(b(cid:63)

√

ηδ if using uniform sampling;
ηδ if using q -approximate leverage scores for η , associated to

the co-variance operator Σ.
Assume we perform the globalization scheme as in Proposition 13, i.e. with the parameters in
Thm. 1, where in order to compute any ρ approximation of a regularized Newton step, we use a
conjugate gradient descent on the pre-conditioned system, where the pre-conditioner is computed
as in Proposition 12 using

1. Q (cid:62) (cid:16)

(cid:17)

10 + 160N H∞ (˜λ)

log 8b2 κ2
˜λδ

if using uniform sampling

46

2. Q (cid:62) (cid:16)

(cid:17)

(cid:36)

6 + 486q2N H

if using Nyström leverage scores
Let Nns be deﬁned as in Proposition 13. Recall Nns is an upper bound for the number of
approximate Newton steps performed in the algorithm. One can bound

log 8b2 κ2
˜λδ

(˜λ)

Nns (cid:54) 2

(113 + 11R(cid:107)f (cid:63)(cid:107)) log2

(cid:37)

(cid:38)

7R(cid:107)∇ (cid:98)LM (0)(cid:107)
λ

+

log2

λ1/2

Rε

(cid:39)

.

Moreover, with probability at least 1 − (Nns + 2)δ , the following holds:

L(fβ ) − L(f (cid:63) ) (cid:54) K1 b2
λ + K2

df λ ∨ (Q(cid:63) )2
n

log

+ K3 ε.

2
δ

where K1 (cid:54) 6.0e4, K2 (cid:54) 6.0e6 and K3 (cid:54) 810, C1 is deﬁned in Lemma 19, and the other
constants are deﬁned in Thm. 8.
Proof.This is a simple combination between Propositions 10, 11 and 13. To bound the number
of Newton steps Nns , one simply uses the fact that under the conditions of the theorem, R(cid:107)βM ,λ(cid:107) (cid:54)

10 + R(cid:107)f (cid:63)(cid:107).

(cid:16)

(cid:16)

Remark 4 (Complexity). Let L = b2κ2 . The complexity of the previous method using leverage
scores computed for Σ for the Nystrom projections and for H(fβ ) for choosing the Q points at the
different stages is the following. The total complexity in time will be of order:

O

Nns

nN H (λ) log(Lλ−1 δ−1 ) + b
2N Σ (λ)3 log3 (Lλ−1 δ−1 ) + L/λ b

3

2N Σ (λ)2(cid:17)(cid:17)

2

.

The memory complexity can be bounded by

2N Σ (λ)2 log2 (Lλ−1 δ−1 ) + n).
O(b

2

Here, we use the fact that H (cid:54) b2Σ.
We can now write down the previous proposition by classifying problems using Assumptions 4
and 5 and in order to get optimal rates.
Theorem 7 (Performance of the scheme using pre-conditioning). Let δ > 0. Assume Assumptions 1
to 5 are satisﬁed. Let n (cid:62) ˜N , where ˜N is characterized in the proof, λ =
Assume that the M points ˜x1 , ..., ˜xM are obtained through Nyström sub-sampling using η =
2 ∨1) , with either

(cid:1)2 1
n

(cid:16)(cid:0) Q

α
α(1+2r)+1 .

(cid:17)

L

√

λ
2
1440(b(cid:63)

1. M (cid:62) (cid:0)10 + 160N Σ∞ (η)(cid:1) log 8κ2
2. M (cid:62) (cid:0)6 + 486q2N Σ (η)(cid:1) log 8κ2

ηδ if using uniform sampling;
ηδ if using q -approximate leverage scores for η , associated to

the co-variance operator Σ.
Assume we perform the globalization scheme as in Proposition 13, i.e. with the parameters in
Thm. 1, where in order to compute any ρ approximation of a regularized Newton step, we use a
conjugate gradient descent on the pre-conditioned system, where the pre-conditioner is computed
as in Proposition 12 using

1. Q (cid:62) (cid:16)
2. Q (cid:62) (cid:16)

(cid:17)
(cid:17)

10 + 160N H∞ (λ)
6 + 486q2N H
(λ)

log 8b2 κ2
λδ
log 8b2 κ2
λδ

if using Nyström leverage scores
Let Nns be deﬁned as in Proposition 13. Recall Nns is an upper bound for the number of
approximate Newton steps performed in the algorithm. One can bound

(cid:18)(cid:108)

(cid:16)

7R(cid:107)∇ (cid:98)LM (0)(cid:107)(cid:17)(cid:109)

(cid:24)

(cid:25)

(cid:24)

(cid:25)(cid:19)

.

1
RL

+

log2

+

log2

nL2
Q2

Nns (cid:54) (227 + 22R(cid:107)f (cid:63)(cid:107))

log2

if using uniform sampling

Moreover, with probability at least 1 − (Nns + 2)δ , the following holds:

47

• all of the approximate Newton methods yield 1
7 -approximate Newton steps
• The scheme ﬁnishes, and the number of approximate Newton steps is bounded by Nns . The
total complexity of the method is therefore

• The returned β is statistically optimal:

O((nM + M 3 + M 2Q + csamp )Nns ) in time ,
L(fβ ) − L(f (cid:63) ) (cid:54) K (cid:0)Qγ L1−γ (cid:1)2

O(n + M 2 ) in memory.

1
nγ log

2
δ

,

where K is deﬁned in Thm. 5.
Proof.The proof consists mainly of combining Propositions 11 and 13 and Thm. 5.
Recall that we set λ =

α
α(1+2r)+1 .

(cid:17)

(cid:16) Q2
L2

1
n

1. Start by deﬁning ˜N such that:
• ˜N (cid:62) N where N is deﬁned in Thm. 5;
• ∀n (cid:62) ˜N , λ (cid:54) (cid:107)H(cid:107). This is possible as

α

α(1+2r)+1 is a strictly positive exponent.

• ∀n (cid:62) ˜N , 19b2∨1 κ2

n

since r > 0;

log n

2δ < λ; this is possible as soon as

α

α(1+2r)+1 < 1, i.e. this is satisﬁed

• ˜N (cid:62) 405b2 ∨ 1 κ2 ∨ 67b2 ∨ 1 κ2 log 12b2∨1 κ2
• ∀n (cid:62) ˜N ,

(cid:54) (cid:107)Σ(cid:107).

√

δ

λ
2 ∨1)
2
1440(b(cid:63)

;

We see that such a ˜N can be deﬁned explicitly.
Thm. 5 are satisﬁed except the bound on (cid:98)νM ,λ (β ).
2. Combining the assumptions on ˜N with the ones on M , we see that all the assumptions of
Proposition 11 are satisﬁed and thus that with probability at least 1 − δ , all the hypotheses for
3. Applying Proposition 13, taking
under these hypotheses,

α(1+2r)+1 , we see that

(cid:16) Q2
L2

(cid:17)

√

1
n

α

(cid:32)

ε = Qγ L1−γ n−γ /2 and λ =
(cid:18) nL2
7R(cid:107)∇ (cid:98)LM (0)(cid:107)
Q2

(cid:19)

(cid:33)(cid:37)

α
α(1+2r)+1

Nns :=2

(cid:36)
(cid:38)

+

log2

(cid:32)

1
RL

(cid:33)(cid:39)

(cid:19) rα
α(1+2r)+1

(3 + 11R(cid:107)βM ,λ(cid:107)) log2
(cid:18) nL2
Q2
.
7R(cid:107)∇ (cid:98)LM (0)(cid:107)(cid:17)(cid:109)
7R(cid:107)∇ (cid:98)LM (0)(cid:107)(cid:17)(cid:109)

(cid:18)(cid:108)
(cid:18)(cid:108)

(cid:16)
(cid:16)

log2

log2

+

(cid:24)
(cid:24)

log2

+

log2

(cid:25)
(cid:25)

nL2
Q2

nL2
Q2

+

(cid:24)
(cid:24)

log2

1
RL

+

log2

1
RL

(cid:25)(cid:19)
(cid:25)(cid:19)

.

.

Now we can bound this harshly:

Nns (cid:54) (7 + 22R(cid:107)βM ,λ(cid:107))

Now bounding R(cid:107)βM ,λ(cid:107) (cid:54) 10 + R(cid:107)f (cid:63)(cid:107), we get

Nns (cid:54) (227 + 22R(cid:107)f (cid:63)(cid:107))

4.

Finally, we use a union bound to conclude.

48

E Algorithm

Let N , M ∈ N with M (cid:54) N . In Alg. 1, leverage-scores-sampling((zi )N

i=1 , M , k , λ)

returns a subset of (zi )N
i=1 of cardinality M sampled by using (approximate) leverage scores at
scale λ > 0 and computed using the kernel k . An explicit example of an algorithm computing
computes the kernel matrix K ∈ RN ×M where Kij = k(xi , x(cid:48)
j ), with N , M ∈ N.

leverage-scores-sampling is in [30]. Moreover kernel-matrix((xi )N

i=1 , (x(cid:48)

i )M

i=1 , k)

49

Algorithm 1 Algorithm efﬁcient non-parametric learning for generalized self-concordant losses
with optimal statistical guarantees discussed in Sec. 4 of the main paper.

Input: (xi , yi )n

Return: estimated function (cid:98)g : X → R
i=1 , n ∈ N, (cid:96) loss function, k kernel function and λ > 0.
Parameters: Q, M , T ∈ N, µ0 > 0, (qk )k∈N .
Fixed parameters: t = 2 from Thm. 1, τ = 3 from Proposition 12 in Appendix D.5.1.

j=1 )

Input: α ∈ RM , λ > 0

deﬁne compute-preconditioner:

j=1 ← leverage-scores-sampling((xi )n
( ¯xj )M
K ← kernel-matrix(( ¯xj )M
i=1 , M , λ, k)
j=1 , ( ¯xj )M
T ← cholesky-upper-triangular(K)
deﬁne the function v(·) = (k( ¯x1 , ·), . . . , k( ¯xM , ·)) ∈ RM
ci ← (cid:112)(cid:96)(2) (v(xi )(cid:62)T−1α, yi ) for all i = 1, . . . , n
s=1 ← leverage-scores-sampling((i)n
(hs )Q
i=1 , Q, λ, k (cid:48) )
G ← kernel-matrix(( ¯xj )M
h=1 ) × G(cid:62) × T−1
s=1 , k)
B ← cholesky-upper-triangular( 1
Q H + λI )

H ← T−(cid:62) × G × diag((c2

i=1 , (xhs )Q
lh )Q

return B

deﬁne preconditioned-conj-grad:
Input: α ∈ RM , µ > 0, r ∈ RM , τ ∈ N, B ∈ RM ×M

deﬁne the function k (cid:48) (◦, •) as k (cid:48) (◦, •) := c◦ × c• × k(x◦ , x• ) for ◦, • ∈ {1, . . . , n}

i=1 (cid:96)(2) (v(xi )(cid:62)T−1α, yi ) (v(xi )(cid:62)T−1B−1 p) B−(cid:62)T−(cid:62) v(xi )

(cid:80)n

p ← r, s0 ← (cid:107)r(cid:107)2 , β ← 0
For i = 1, . . . , τ
z ← µB−(cid:62)B−1 p + 1
a ← s0 /(p(cid:62) z )
β ← β + ap
r ← r − az , s1 ← (cid:107)r(cid:107)2
p ← r + (s1 /s0 )p
s0 ← s1

n

return β

deﬁne appr-linear-solver:

Input: α ∈ RM , µ > 0, g ∈ RM

return B−1u

deﬁne approximate-Newton:

Input: α0 ∈ RM , µ > 0, t ∈ N

(cid:80)n

B ← compute-preconditioner(α, µ)
u ← preconditioned-conjugate-gradient(α, µ, B−(cid:62) g , τ = 3, B)

n

return αt
For k ∈ N

For j = 1, . . . , t
g ← µαj−1 + 1
i=1 (cid:96)(1) (v(xi )(cid:62)T−1αj−1 , yi ) T−(cid:62) v(xi )
αj ← αj−1 − appr-linear-solver(αj−1 , µ, g)
α0 ← 0
αk+1 ← approximate-Newton(αk , µk , t = 2)
(cid:98)α ← approximate-Newton(αlast , λ, T )

Stop when µk+1 < λ and set αlast ← αk
return (cid:98)g(·) := v(·)(cid:62)T−1 (cid:98)α

µk+1 ← qk+1µk

50

F Experiments

We present our algorithm’s performance for logistic regression on two large scale data sets: Higgs
and Susy. We have implemented our method using pytorch, and performed computations on one
node of a Tesla P100-PCIE-16GB GPU. Recall that in the case of logistic regression, (cid:96)(x,y) (t) =

log(1 + e−yt ).

In what follows, denote with n the cardinality of the data set and d the number of features of
this data set. The error is measured in terms of classiﬁcation error for both data sets. In both cases,
we pre-process the data by substracting the mean and dividing by the standard deviation for each
feature. The data sets are the following.

Susy (n = 5 × 106 , d = 18, binary classiﬁcation). We always use a Gaussian Kernel with σ = 5
for logistic loss (obtained through a grid search; note that in [29], σ = 4 is used for the square loss),
and will always use 104 Nystrom points.

Higgs

(n = 1.1 × 107 , d = 28, binary classiﬁcation). We then apply a Gaussian Kernel with
σ = 5, as in [29] (we have also performed a grid search).

For these data sets, we do not have a ﬁxed test set, and thus set apart 20% of the data set at
random to be the test set, and use the rest of the 80% to train the classiﬁer.

In practice, we perform our globally convergent scheme with the following parameters.
• We use Q = M uniform random features to compute the pre-conditioner for each approxi-
mate Newton step;
• In the ﬁrst phase, we decrease µ in a very fast way to λ by starting at µ = 1 and dividing
µ by 1000 after performing only a single approximate Newton step (using 2 iterations of
conjugate gradient descent);
• In the second phase, we perform 10 approximate Newton steps (each ANS is computed using
8 iterations of conjugate gradient descent).

Selection of λ In the introduction, we claim that in many a learning problem, the parameter λ
obtained through cross validation is often much smaller than the ones obtained in statistical bounds
which are usually of order 1√
n . This leads to very ill conditioned problems.
For both data sets, we select λ (and σ , but we omit the double tables from this paper) by
computing the test loss and classiﬁcation errors for different values of λ, and report the evolution of
these losses as a function of the parameter λ in Fig. 2 for the Higgs data set, and Fig. 3 for the Susy
data set. We see that the optimal λ yield strongly ill-conditioned problems.
(cid:98)LM ,λ . From an optimization point of view, i.e. from a point of view where the aim is to minimize
Comparison with accelerated methods Given the M Nystrom points, our aims to minimize
(cid:98)LM ,λ , we compare our method with a large mini-batch version of Katyusha accelerated SVRG (see
[4]).
Indeed, we perform this method using batch sizes of size M ; the theoretical bounds provided in [4]
show that the algorithm has linear convergence, with a time complexity of order O(nM + M 3 +
ε to reach precision ε. In the following plots, we compare both methods in terms of
passes and time.

M 2(cid:113) L
λ ) log 1

51

Figure 2:
(Left) Classiﬁcation error as a function of the regularization parameter and (Right)
test loss as a function of the regularization parameter, when performing a logistic regression with
M = 2 × 104 Nyström features on the entire Higgs data set; we select λ = 10−9 .

Figure 3:
(Left) Classiﬁcation error as a function of the regularization parameter and (Right)
test loss as a function of the regularization parameter, when performing a logistic regression with
M = 104 Nyström features on the entire Susy data set; we select λ = 10−10 .

By pass, we mean the following.
• In the case of our second-order scheme, we deﬁne a pass on the data to be one step of the
conjugate gradient descent used to compute approximate newton steps.
• In the case of Katyusha SVRG, we deﬁne a pass on the data to be either a full gradient
computation or n/M computations of the type Kτ M T −1β where T is an upper triangular
matrix, and Kτ M is a M × M kernel matrix, associated to one batch gradient.
We use this notion to measure the speed of our method as they both correspond to natural
O(nM ) operations, and incorporate the essential of the computing time. However, the second point
is often much slower to compute than the ﬁrst, due to the solving of the triangular system. Thus,
the notion of passes is to take with precaution, as a pass for the accelerated SVRG algorithm takes
much longer to run that a pass for our method. This is conﬁrmed by the time plots (see Fig. 5 for in
instance).
Comparison between the two methods - Due to the running time of K-SVRG, we compare
both methods for M = 10000 Nyström points for both data sets. We compare the performance
of these two algorithm with respect to the distance to the optimum in function values as well as
classiﬁcation error Fig. 4 for the Higgs data set, and in Fig. 5 for the Susy data set.
Note on the need for precise optimization - As noted in the introduction, we see in both Fig. 5
and Fig. 4 that precise optimization of the objective function is needed in order to get a good

52

10111010109108regularization parameter27.12027.12527.13027.13527.140classification error10111010109108regularization parameter0.539250.539300.539350.539400.539450.53950test loss101210111010109108regularization parameter19.5519.5619.5719.5819.5919.60classification error101210111010109108regularization parameter0.42360.42380.42400.42420.42440.42460.42480.4250test lossFigure 4: (Left) Distance to optimum as a function of time and (Right) distance to optimum and
classiﬁcation error as a function of the number of passes on the data when performing our second
order scheme and K-SVRG to minimize the train loss on Higgs, with 1.0 × 104 Nyström points
and λ = 10−9 .

Figure 5: (Left) Distance to optimum as a function of time and (Right) distance to optimum and
classiﬁcation error as a function of the number of passes on the data when performing our second
order scheme and K-SVRG to minimize the train loss on Susy, with 1.0 × 104 Nyström points and

λ = 10−10 .

classiﬁcation error. This justiﬁes a posteriori the use of a second order method. In particular, in
Fig. 5, one notes the difference in behavior between the two methods : the second order method
converges linearly in a fast way while the ﬁrst order method slows down because of the condition
number.
Note on ill-conditioning - First note that in order to optimize test error, one gets very poorly
conditioned problems. As predicted by the rates, we observe that K-SVRG is more sensible to
ill-conditioning than our second order scheme. Indeed, in Fig. 6, we have plotted the results for
Susy for a smaller condition number with λ = 10−8 , compared to λ = 10−10 to get optimal test
error in Fig. 5. We see that the difference in number of passes needed to reach a certain precision is
much lower when λ = 10−8 in Fig. 6, conﬁrming that K-SVRG behaves better when the condition
number is smaller.

Performance of our method.

In Table 1, we record the performance of the following methods,
taking the λ values we have obtained previously for the different data sets.
For FALKON (see [29]), we take the parameters suggested in the paper (except for the number
of Nyström points needed for Higgs, as our computational capacity is limited).

53

0100200300400500time (minutes)106105104103102101distance to optimumsecond orderK-SVRG20406080100120passes over data27.828.028.228.428.628.829.0classification error106105104103102101distance to optimumsecond orderK-SVRG050100150200time (minutes)104103102101distance to optimumsecond orderK-SVRG020406080100120passes over data19.419.619.820.020.220.420.620.821.0classification error104103102101distance to optimumsecond orderK-SVRGFigure 6: (Left) Distance to optimum as a function of time and (Right) distance to optimum and
classiﬁcation error as a function of the number of passes on the data when performing our second
order scheme and K-SVRG to minimize the train loss on Susy, with 1.0 × 104 Nyström points and

λ = 10−8 .

Method

Logistic regression with K-SVRG
Logistic regression with our scheme
Ridge Regression with FALKON ([29])

Susy
c-error M time(m)
19.64% 104
230
19.5% 104
15
19.7% 104
5

Higgs

M
104

c-error
27.82 %
26.9 % 2.5 × 104
27.16 % 2.5 × 104

time(m)
500
65
60

Table 1: Classiﬁcation error of different methods

G Solving a projected problem to reduce dimension

G.1

Introduction and notations

In this section, we give ourselves a generalized self-concordant function f whose associated subset
we denote with G . Once again, we will always omit the subscript f in the notations associated to f .

The aim of this section is the following. Given f and λ > 0, computing an approximate solution

to

x(cid:63)
λ = arg min

fλ (x),

x∈H

is often too costly. Instead, we look for a solution in a small subset of H which we see as the image
of a certain orthogonal projector P and which we denote HP . Usually, this subset will be ﬁnite
dimensional and admit an easy parametrization. Thus we will compare an approximation of x(cid:63)
λ to
an approximation of

x∗

f (Px) +

(cid:107)x(cid:107)2 .

λ
2

x∈H

P,λ = arg min

fλ (x) = arg min

x∈HP

Denote with fP the mapping x ∈ H (cid:55)→ f (Px).
eralized self-concordant function with G , fP is naturally a generalized self-concordant with
It is easy to see that, as f is a gen-
GP := PG = {Pg : g ∈ G }. Moreover, x∗

P,λ = x(cid:63)
fP ,λ .

We will adopt the following notations for the quantities related to the generalized self-concordant
function fP . Essentially, we always replace fP simply by P from our deﬁnitions in appendix.

54

050100150200time (minutes)105104103102101distance to optimumsecond orderK-SVRG050100150200time (minutes)105104103102101distance to optimumsecond orderK-SVRG• For the regularized function :

∀x ∈ H, ∀λ > 0, fP,λ (x) = fP (x) +

(cid:107)x(cid:107)2 .

λ
2

• For the Hessians

∀x ∈ H, λ > 0, HP,λ (x) = HfP ,λ (x) = PH(Px)P + λI.
• ∀h ∈ H, tP (h) := tfP (h) = t(Ph).

• For the Newton decrement:

∀x ∈ H, λ > 0, νP,λ (x) = νfP ,λ (x) = (cid:107)∇fP,λ(cid:107)H−1
P,λ (x) = (cid:107)P∇f (Px) + λx(cid:107)H−1

P,λ (x) .

• For the Dikin ellipsoid radius:

∀λ > 0, ∀x ∈ H, rP,λ (x) := rfP ,λ (x) =

1
supg∈G (cid:107)Pg(cid:107)H−1

λ,P (x)

;

• For the Dikin ellipsoid:

∀λ > 0, ∀c (cid:62) 0, DP,λ (c) := DfP ,λ (c).

Note that for any x ∈ HP , rP,λ (x) (cid:62) rλ (x).
We will now introduce the key quantities in order to compare an approximation of x∗
P,λ to an
approximation of x(cid:63)
λ .
Deﬁnition 8 (key quantities). Deﬁne the following quantities
• For any λ > 0, the source term sλ := λ(cid:107)x(cid:63)
• Given an orthogonal projector P, λ > 0, and x ∈ H, the capacity of the projector
.

λ ) = (cid:107)∇f (x(cid:63)

λ(cid:107)H−1
λ (x(cid:63)

CP (x, λ) :=

λ )(cid:107)H−1

λ ) ;
λ (x(cid:63)

(cid:107)H(x)1/2 (I−P)(cid:107)2
λ

G.2 Relating the projected to the original problem

Given x ∈ HP , our aim is to bound νλ (x) given νλ,P (x) and sλ .

Proposition 15. Let x ∈ HP . If

sλ
rλ (x(cid:63)
λ )

(cid:54) 1

4

λ , λ) (cid:54) 1
, CP (x(cid:63)
120

, νP,λ (x) (cid:54) rP,λ (x)
2

,

Then it holds:

Moreover, under these conditions,

νλ (x) (cid:54) 3(νP,λ (x) + sλ ).

• (cid:107)x − x(cid:63)

λ(cid:107) (cid:54) 7λ−1/2 (νP,λ (x) + sλ );
• λ(cid:107)x(cid:107)H−1
(cid:54) 7νP,λ (x) + 9sλ .

P,λ (x)

Proof.In this proof, introduce the following auxiliary quantity:

γλ :=

sλ
rλ (x(cid:63)
λ )

.

55

1) Start by bounding t(Px(cid:63)

λ − x(cid:63)
λ ).

It holds:

t(Px − x(cid:63)
λ ) = sup

g∈G

|g · (I − P)x(cid:63)

λ |

(cid:54) 1

rλ (x(cid:63)
λ )

(cid:54) 1

rλ (x(cid:63)
λ )

λ(cid:107)Hλ (x(cid:63)
λ )

(cid:107)(I − P)x(cid:63)
(cid:107)Hλ (x(cid:63)
λ )1/2 (I − P)Hλ (x(cid:63)
λ )1/2(cid:107) (cid:107)H
λ(cid:107)H

−1/2
λ

λ(cid:107)

(x(cid:63)
λ )x(cid:63)
rλ (x(cid:63)
λ )

= (1 + CP (x(cid:63)
λ , λ))

−1/2
λ

(x(cid:63)
λ )x(cid:63)

λ(cid:107)

= (1 + CP (x(cid:63)
λ , λ)) γλ .

2) Then bound t(x∗

P,λ − Px(cid:63)

λ ) First, bound νP,λ (Px(cid:63)

λ ):

λ ) = (cid:107)P∇fλ (Px(cid:63)
νP,λ (Px(cid:63)
(cid:54) (cid:107)∇fλ (Px(cid:63)

λ )(cid:107)Hλ,P (Px(cid:63)
λ )−1
λ )(cid:107)Hλ (Px(cid:63)
λ )−1 .
λ )(cid:107)Hλ (Px(cid:63)
λ )−1 (cid:54) et((I−P)x(cid:63)
λ )/2νλ (Px(cid:63)

λ ). Using Eq. (20), we

Using Eq. (17), we get (cid:107)∇fλ (Px(cid:63)
can bound

λ ) (cid:54) φ(t((I − P)x(cid:63)
λ )) (cid:107)(I − P)x(cid:63)
νλ (Px(cid:63)

λ(cid:107)Hλ (x(cid:63)

λ ) (cid:54) φ(t((I − P)x(cid:63)
λ )) (1 + CP (x(cid:63)
λ , λ))sλ .

Putting things together,

νP,λ (Px(cid:63)

λ ) (cid:54) et((I−P)x(cid:63)

λ )/2φ(t((I − P)x(cid:63)
λ )) (1 + CP (x(cid:63)
λ , λ))sλ .

Now

Hence,

1
rP,λ (Px(cid:63)
λ )

(cid:54)

1
rλ (Px(cid:63)
λ )

(cid:54) et((I−P)x(cid:63)
λ )/2

1
rλ (x(cid:63)
λ )

.

νP,λ (Px(cid:63)
λ )
rP,λ (Px(cid:63)
λ )

(cid:54) e˜tλ φ(˜tλ ) ˜tλ ,

˜tλ = (1 + CP (x(cid:63)
λ , λ))γλ .

Since t (cid:55)→ etφ(t) t is an increasing function whose value in 0 is 0, we ﬁnd numerically that for
2 . Hence, if (1 + CP (x(cid:63)
2 . Using Lemma 5,

10 , etφ(t) t (cid:54) 1
t = 3

λ , λ))γλ (cid:54) 3

10 , then νP,λ (Px(cid:63)
λ )
rP,λ (Px(cid:63)
λ )

(cid:54) 1

this shows that

tP (Px(cid:63)

λ − x∗

P,λ ) = t(Px(cid:63)

λ − x∗

P,λ ) (cid:54) log 2.

3) Getting a bound for t(x − x(cid:63)

if νP,λ (x) (cid:54) rP,λ (x)
2

λ ). To do so, combine the two previous bounds with the fact that
, then using Lemma 5 with fP , tP (x − x∗
P,λ ) (cid:54) log 2. Thus, if

λ , λ))γλ (cid:54) 3
(1 + CP (x(cid:63)
10

P,λ ) = t(x − x∗
, νP,λ (x) (cid:54) rP,λ (x)
2

,

then it holds

t(x − x(cid:63)
λ ) (cid:54) 3
10

+ 2 log 2.

56

4) A technical result to bound (cid:107)Hλ (x)−1/2HP,λ (x)1/2(cid:107)

Lemma 23, applied to A = H(x), we get

(cid:107)Hλ (x)−1/2HP,λ (x)1/2(cid:107) (cid:54) 1 + (cid:112)CP (x, λ).
λ , λ).

λ )CP (x(cid:63)

Then, one can easily bound CP (x, λ) (cid:54) et(x−x(cid:63)

. Using the fact that Px = x, and

5) Let us now bound νλ (x). First, decompose the term

νλ (x) = (cid:107)∇fλ (x)(cid:107)H−1

λ (x)

(cid:54) (cid:107)P∇fλ (x)(cid:107)H−1
λ (x) + (cid:107)(I − P)∇f (x)(cid:107)H−1

λ (x) .

Since x ∈ HP , (cid:107)P∇fλ (x)(cid:107)H−1

(cid:107)P∇fλ (x)(cid:107)H−1

λ (x)

λ (x) = (cid:107)∇fP,λ (x)(cid:107)H−1

λ )/2(cid:113)
1 + et(x−x(cid:63)

(cid:54) (cid:16)

(cid:17)

CP (x(cid:63)
λ , λ)

νP,λ (x).

λ (x) , and using the previous point, we get

Let us now bound the second term. We divide it into two terms:

(cid:107)(I − P)∇f (x)(cid:107)H−1

λ (x)

(cid:54) (cid:107)(I − P) (∇f (x) − ∇f (x(cid:63)

λ )) (cid:107)H−1

λ (x) + (cid:107)(I − P)∇f (x(cid:63)

λ )(cid:107)H−1

λ (x) .

The second term can be bounded in the following way:

(cid:107)(I − P)∇f (x(cid:63)

λ )(cid:107)H−1

λ (x)

(cid:54) 1√

λ

(cid:107)(I − P)H1/2
λ )(cid:107) (cid:107)∇f (x(cid:63)
λ (x(cid:63)

λ )(cid:107)H−1

λ (x(cid:63)
λ )

1 + CP (x(cid:63)
λ , λ) sλ .

(cid:54) (cid:113)

For the ﬁrst term, we proceed in the following way.

(cid:107)(I − P) (∇f (x) − ∇f (x(cid:63)

λ )) (cid:107)H−1

λ (x) = (cid:107)

H

(cid:90) 1
(cid:90) 1
(cid:54) (cid:113)
(cid:54) (cid:113)

(cid:54) 1√

0

0

−1/2
λ

(x)(I − P)H(xt )(x − x(cid:63)
λ ) dt(cid:107)
(cid:107)(I − P)H1/2 (xt )(cid:107) (cid:107)H1/2 (xt )(x − x(cid:63)
λ )(cid:107) dt
λ
CP (x(cid:63)
λ , λ) φ(t(x − x(cid:63)
λ )) (cid:107)x − x(cid:63)
CP (x(cid:63)
λ )νλ (x).

λ(cid:107)H(x(cid:63)
λ )

Hence the ﬁnal bound:

(cid:16)

1 − (cid:113)

CP (x(cid:63)

λ )(cid:17)
λ , λ) et(x−x(cid:63)

νλ (x) (cid:54) (cid:16)

120 , we see that (cid:112)CP (x(cid:63)

λ , λ) (cid:54) 1

λ , λ) et(x−x(cid:63)
λ )/2(cid:113)
1 + et(x−x(cid:63)
λ , λ) et(x−x(cid:63)
λ ) (cid:54) 1

CP (x(cid:63)
λ , λ)

Now if CP (x(cid:63)

on t(x − x(cid:63)
λ ),

(cid:17)

(cid:113)

1 + CP (x(cid:63)
λ , λ) sλ .

νP,λ (x)+

2 , and hence, using the bound

νλ (x) (cid:54) 3(νP,λ (x) + sλ ).

. We leverage the fact that νλ (x) (cid:54) 3(νP,λ (x) + sλ ) and

6) Showing the last two points

t(x − x(cid:63)
λ ) (cid:54) 3
10 + 2 log 2.

To show the ﬁrst bound, we plug in the previous results in the following equation:

(cid:107)x − x(cid:63)
λ(cid:107) (cid:54) λ−1/2(cid:107)x − x(cid:63)

λ(cid:107)Hλ (x) (cid:54)

1
φ(t(x − x(cid:63)
λ ))

λ−1/2νλ (x).

The last inequality is obtained using Eq. (18).

57

To show the second point, we use the fact that x ∈ HP to show that

λ(cid:107)x(cid:107)H−1

P,λ (x)

(cid:54) λ(cid:107)x(cid:107)H−1

λ (x)

(cid:54) λ(cid:107)x − x(cid:63)

λ(cid:107)Hλ (x) + λ(cid:107)x(cid:63)
λ(cid:107)H−1
λ (x) .

Then applying Eq. (17) and Eq. (18):

λ(cid:107)x(cid:107)H−1

P,λ (x)

(cid:54)

1
φ(t(x − x(cid:63)
λ ))

νλ (x) + et(x−x(cid:63)

λ )/2 sλ .

We then use the previous results to conclude.

G.3 Finding a good projector

Lemma 16. If for a certain η (cid:54) λ and for a certain constant C , (cid:107)H1/2

η

(x)(I − P)(cid:107)2 (cid:54) C η , then

CP (x, λ) (cid:54) C η
λ

.

Proof.This is completely direct, using the fact that H1/2 (x) (cid:22) H1/2

η

(x).

58

H Relations between statistical problems and empirical problem.

In this section, we recall and reformulate the framework from [23].

H.1 Statistical problem and ERM estimator

Let Z be a Polish space and Z be a random variable on Z with distribution ρ. Let H be a separable
Hilbert space, with norm (cid:107) · (cid:107), and let (fz )z∈Z be a family of functions on H. Our goal is to
minimize the expected risk with respect to x ∈ H:

i=1 ∈ Z n , we deﬁne the empirical risk:
Given (zi )n

x∈H f (x) := E [fZ (x)] .
inf
(cid:98)f (x) :=

n(cid:88)

fzi (x),

1
n

i=1

and consider the following estimator based on regularized empirical risk minimization given λ > 0
(note that the minimizer is unique in this case):

(cid:98)x(cid:63)
λ = arg min

x∈H

(cid:98)fλ (x) := (cid:98)f (x) +

(cid:107)x(cid:107)2 ,

λ
2

where we assume the following.
Assumption 6 (i.i.d. data). The samples (zi )1(cid:54)i(cid:54)n are independently and identically distributed
according to ρ.
We make the following assumption on the family (fz ) (this is a reformulation of Assumption 8
in [23])
Assumption 7 (Generalized self-concordance). For any z ∈ Z , there exists an associated subset
Gz ⊂ H such that (fz , Gz ) is generalized self-concordant in the sense of Deﬁnition 3.
Moreover we require the following technical assumption to guarantee that f and and its
derivatives are well deﬁned for any x ∈ H (this is a reformulation of Assumptions 3 and 4 in [23],
and the necessary conditions to obtain Proposition 3).
Assumption 8 (Technical assumptions). The mapping (z , x) ∈ Z × H (cid:55)→ fz (x) is measurable.
Moreover,
• the random variables (cid:107)fZ (0)(cid:107), (cid:107)∇fZ (0)(cid:107), Tr(∇2fZ (0)) are are bounded;
z∈supp(Z ) Gz is a bounded subset of H.
The assumptions above are usually easy to check in practice. In particular, if the support of ρ is
bounded, the mappings z (cid:55)→ (cid:96)z (0), ∇(cid:96)z (0), Tr(∇2 (cid:96)z (0)) are continuous, and z (cid:55)→ Gz is uniformly
bounded on bounded sets, then they hold.

• G := (cid:83)

Proposition 16. Under Assumptions 7 and 8, the function (f , G ) (or simply f ) is generalized
self-concordant.
Moreover, under Assumption 6, deﬁne (cid:98)G :=
Then ( (cid:98)f , (cid:98)G ) (or simply (cid:98)f ) is generalized self-concordant. Moreover, note that (cid:98)G ⊂ G .
59

n(cid:91)

Gzi .

i=1

The main regularity assumption we make on our statistical problems follows (see Assumption
5 in [23]).
Assumption 9 (Existence of a minimizer). There exists x(cid:63) ∈ H such that f (x(cid:63) ) = inf x∈H f (x).
Notations We adopt all the notations from Appendix A for f and (cid:98)f , which are generalized
self-concordant functions with associated subsets given in Proposition 16 with the following
conventions:
• For all quantities relating to f , we omit the subscript f as usual;
• For all quantities relating to (cid:98)f , we omit the subscript (cid:98)f and instead put a hat over all these
quantities. For instance:

(cid:98)H(x) := H (cid:98)f (x) =

n(cid:88)

∇2fzi (x), (cid:98)rλ (x) := r (cid:98)f ,λ (x) =

1
n

supg∈ (cid:98)G (cid:107)g(cid:107) (cid:98)H−1
1

i=1

λ (x)

, etc...
Recall the two main quantities introduced in [23] to establish the quality of our estimator (cid:98)x(cid:63)
λ (in
[23], this is a mix between Proposition 2 and Deﬁnition 3).
Proposition 17 (Bias, degrees of freedom). Suppose Assumptions 7 to 9 are satisﬁed. The following
key quantities are well deﬁned:
• the effective dimension df λ = E (cid:2)(cid:107)Hλ (x(cid:63) )−1/2∇fZ (x(cid:63) )(cid:107)2 (cid:3).
Moreover, we also introduce the following quantities:

• the bias bλ = (cid:107)Hλ (x(cid:63) )−1/2∇fλ (x(cid:63) )(cid:107);

B(cid:63)
1 := sup

z∈supp(Z )

(cid:107)∇fz (x(cid:63) )(cid:107),

B(cid:63)
2 := sup

z∈supp(Z )

Tr(∇2fz (x(cid:63) )),

Q(cid:63) =

1(cid:112)B(cid:63)
B(cid:63)

2

.

(cid:114)

We can now recall the main theorem of [23] (Theorem 4), which quantiﬁes the behavior of the
ERM estimator:
Theorem 8 (Bound for the ERM estimator). Let n ∈ N, δ ∈ (0, 1/2], 0 < λ (cid:54) B(cid:63)
2 . Whenever

df λ ∨ (Q(cid:63) )2
n

,

2

log

1B(cid:63)
2

(cid:52)2

B(cid:63)
λ

n (cid:62) (cid:52)1

then with probability at least 1 − 2δ , it holds

8(cid:3)2
λδ
f ((cid:98)x(cid:63)
λ ) − f (x(cid:63) ) (cid:54) Cbias b2
where Cbias , Cvar , (cid:3)1 (cid:54) 414, (cid:52)1 , (cid:52)2 (cid:54) 5184.

λ + Cvar

log

2
δ

(cid:54) rλ (x(cid:63) ),

2bλ (cid:54) rλ (x(cid:63) ),

df λ ∨ (Q(cid:63) )2
n

log

2
δ

,

(37)

H.2 Link between a good approximation of (cid:98)x(cid:63)
λ and x(cid:63)

In this paper, we provide an algorithm which can effectively compute a good approximation of (cid:98)x(cid:63)
whose precision with respect to the empirical problem will be characterized by (cid:98)νλ (x). The aim
(as it is a ﬁnite sum problem which can be solved). This algorithm will return a certain x ∈ H,
of the following lemma is to see how this approximation x behaves with respect to the statistical
problem.

λ

60

B(cid:63)

Lemma 17. Suppose the conditions for Thm. 8 are satisﬁed, i.e. let n ∈ N, δ ∈ (0, 1/2], 0 < λ (cid:54)
2 and suppose
Let x be an approximation of (cid:98)x(cid:63)
λ characterized by its Newton decrement (cid:98)νλ (x). If

2bλ (cid:54) rλ (x(cid:63) ).

n (cid:62) (cid:52)1

(cid:114)

B(cid:63)
λ

(cid:52)2

log

,

then with probability at least 1 − 2δ , it holds

2

1B(cid:63)
2

8(cid:3)2
λδ

df λ ∨ (Q(cid:63) )2
log
2
(cid:54) rλ (x(cid:63) ),
n
δ
(cid:98)νλ (x) (cid:54) (cid:98)rλ (x)
, (cid:98)νλ (x) (cid:54) rλ (x(cid:63) )
2
2
,
f (x) − f (x(cid:63) ) (cid:54) 14(f ((cid:98)x(cid:63)
λ ) − f (x(cid:63) )) + 30(cid:98)νλ (x)2 .
f (x) − f ((cid:98)x(cid:63)
λ ) (cid:54) (cid:104)∇f ((cid:98)x(cid:63)
λ ), x − (cid:98)x(cid:63)
λ (cid:105)H + ψ(t(x − (cid:98)x(cid:63)
λ ))(cid:107)x − (cid:98)x(cid:63)
(cid:107)∇f ((cid:98)x(cid:63)
ψ(t(x − (cid:98)x(cid:63)
λ )(cid:107)2
+
λ )) +
1
2
2

λ ((cid:98)x(cid:63)
H−1
λ )

(cid:18)

(cid:19)

(cid:54) 1

Proof.Using Eq. (16),

λ(cid:107)2
Hλ ((cid:98)x(cid:63)
λ )

(cid:107)x − (cid:98)x(cid:63)

λ(cid:107)2
Hλ ((cid:98)x(cid:63)
λ ) .

xt = (1 − t)(cid:98)x(cid:63)
λ + tx(cid:63)

1. Let us bound (cid:107)∇f ((cid:98)x(cid:63)

λ )(cid:107)H−1

(cid:107)∇f ((cid:98)x(cid:63)

λ )(cid:107)H−1

λ ((cid:98)x(cid:63)
λ )

λ ((cid:98)x(cid:63)
λ )
−1/2
λ

(cid:90) 1
(cid:90) 1

0

Now using equation Eq. (14)

Thus:

Finally, using equation Eq. (16)

0

(cid:54)

(cid:54)

(cid:107)H

(cid:107)H

−1/2
λ

((cid:98)x(cid:63)
λ )H(xt )((cid:98)x(cid:63)
λ − x(cid:63) )(cid:107) dt,
((cid:98)x(cid:63)
λ )H1/2 (xt )(cid:107) (cid:107)H1/2 (xt )((cid:98)x(cid:63)
λ − x(cid:63) )(cid:107) dt.
H(xt ) (cid:22) ett((cid:98)x(cid:63)
λ−x(cid:63) )H((cid:98)x(cid:63)
λ ),
(cid:107)∇f ((cid:98)x(cid:63)
(cid:107)∇f ((cid:98)x(cid:63)

H(xt ) (cid:22) e(1−t)t((cid:98)x(cid:63)
λ−x(cid:63) ) .
λ−x(cid:63) )/2 (cid:107)(cid:98)x(cid:63)
(cid:54) et((cid:98)x(cid:63)
λ − x(cid:63)(cid:107)H(x(cid:63) ) .
et((cid:98)x(cid:63)
λ−x(cid:63) )/2
λ − x(cid:63) ))1/2
λ(cid:107)Hλ ((cid:98)x(cid:63)

ψ(−t((cid:98)x(cid:63)

λ ((cid:98)x(cid:63)
λ )

λ ((cid:98)x(cid:63)
λ )

λ )(cid:107)H−1

λ )(cid:107)H−1

(cid:54)

(f ((cid:98)x(cid:63)
λ ) − f (x(cid:63) ))1/2 .

2. Let us bound the terms involving (cid:107)x − (cid:98)x(cid:63)

applied to (cid:98)f ,

(cid:107)x − (cid:98)x(cid:63)

λ(cid:107)Hλ ((cid:98)x(cid:63)
λ ) (cid:54) (cid:107)H1/2

λ ((cid:98)x(cid:63)
λ ) (cid:98)H

−1/2
λ

This also leads to:

t(x − (cid:98)x(cid:63)
λ ) (cid:54) 1
rλ ((cid:98)x(cid:63)
λ )
rλ ((cid:98)x(cid:63)
λ )

(cid:54) 1

λ ((cid:98)x(cid:63)
λ ) (cid:98)H
λ ((cid:98)x(cid:63)
λ ) (cid:98)H

−1/2
λ

−1/2
λ

(cid:107)H1/2

(cid:107)H1/2

λ )/2

((cid:98)x(cid:63)
λ )(cid:107) e(cid:98)t(x−(cid:98)x(cid:63)
φ((cid:98)t(x − (cid:98)x(cid:63)
λ )) (cid:98)νλ (x).
((cid:98)x(cid:63)
λ )(cid:107) (cid:107)x − (cid:98)x(cid:63)
((cid:98)x(cid:63)
λ )(cid:107) e(cid:98)t(x−(cid:98)x(cid:63)
φ((cid:98)t(x − (cid:98)x(cid:63)
λ )) (cid:98)νλ (x).

λ(cid:107) (cid:98)Hλ ((cid:98)x(cid:63)
λ )
λ )/2

λ ) Note that using Eq. (18) and Eq. (17)

61

3. Putting things together

In the end, we get

(cid:32)
(cid:18)

(cid:33)
(cid:19) (cid:32)

et((cid:98)x(cid:63)
λ−x(cid:63) )

ψ(−t((cid:98)x(cid:63)
1 +
λ − x(cid:63) ))
ψ(t(x − (cid:98)x(cid:63)
1
λ )) +
2

(f ((cid:98)x(cid:63)
λ ) − f (x(cid:63) ))

et((cid:98)x(cid:63)
λ−x(cid:63)
λ )/2(cid:107)H1/2

λ ) (cid:98)H
λ (x(cid:63)

f (x) − f (x(cid:63) ) (cid:54)

+

Moreover, we bound

t(x − (cid:98)x(cid:63)

λ ) (cid:54) e(t(x(cid:63)−(cid:98)x(cid:63)
λ )+t((cid:98)x(cid:63)
λ−x(cid:63)
λ ))/2 (cid:107)H1/2

λ ) (cid:98)H
λ (x(cid:63)

−1/2
λ

λ )/2

−1/2
λ

λ )(cid:107) e(cid:98)t(x−(cid:98)x(cid:63)
φ((cid:98)t(x − (cid:98)x(cid:63)
(x(cid:63)
λ ))
(cid:98)νλ (x)
λ )(cid:107) e(cid:98)t(x−(cid:98)x(cid:63)
φ((cid:98)t(x − (cid:98)x(cid:63)
(x(cid:63)
λ ))
rλ (x(cid:63) )

λ )/2

.

(cid:33) (cid:98)νλ (x)2 .

4. Plugging in previous results Under the assumptions of this lemma, which include the as-
sumptions of Theorem 4. in [23], we get the following bounds.
• In [23],the assumptions of Theorem 4 imply that we can use Lemma 9, which uses Lemma 8
in which we show that with probability at least 1 − δ ,

(cid:107) (cid:98)H

−1/2
λ

λ )1/2(cid:107)2 (cid:54) 2.
(x(cid:63)
λ )Hλ (x(cid:63)

• Still using the assumptions of Theorem 4, we see in the proof of this theorem that the
assumptions of Theorem 7 of [23] are satisﬁed in the case where bλ (cid:54) rλ (x(cid:63) )
, and thus that

2

t((cid:98)x(cid:63)
λ − x(cid:63)
λ ) (cid:54) log 2, t(x(cid:63)
λ − x(cid:63) ) (cid:54) log 2.

(cid:32)

(cid:33)

Plugging in all these bounds, we get

(cid:18)

ψ(t(x − (cid:98)x(cid:63)
λ )) +

et((cid:98)x(cid:63)
λ−x(cid:63) )

ψ(−t((cid:98)x(cid:63)
λ − x(cid:63) ))

et((cid:98)x(cid:63)
λ−x(cid:63)
λ )/2(cid:107)H1/2

(cid:54) 14, t(x − (cid:98)x(cid:63)
λ ) (cid:54) 6,
λ ) (cid:98)H
λ )(cid:107) e(cid:98)t(x−(cid:98)x(cid:63)
φ((cid:98)t(x − (cid:98)x(cid:63)
λ (x(cid:63)
(x(cid:63)
λ ))

−1/2
λ

λ )/2

1 +

(cid:19) (cid:32)

1
2

(cid:33)

(cid:54) 30.

H.3 Bounds when we solve a projected empirical problem

In this section, we place ourselves in the setting of Appendix G. In this section, we had argued that
for computational purposes, it was less costly to compute an approximate solution to a projected
problem.

In this section, we assume that we are going to project the regularized empirical problem, that
is solve approximately

(cid:98)fP,λ (x) = (cid:98)f (Px) +

(cid:107)x(cid:107)2 .

λ
2

x ≈ arg min

x∈H

seeing (cid:98)fP as a generalized self-concordant function. We import all the notations from this section,
keeping a (cid:98)· over all notations to mark the fact that we are projecting (cid:98)f and not f .
for a given orthogonal projection P. Recall from Appendix G that there is a natural way of

62

empirical projected problem (cid:98)νP,λ (x) := ν (cid:98)fP ,λ (x).
To quantify the quality of the approximation x, we will use the Newton decrement for the
As we see in Proposition 15, under certain conditions, bounding (cid:98)νλ (x) amounts to bounding
two terms:
• The empirical source (cid:98)sλ := λ(cid:107)(cid:98)x(cid:63)
• The projected empirical Newton decrement (cid:98)νP,λ (x).
1. Bounding the empirical source term (cid:98)sλ Start by bounding the source empirical source term
using quantities we know.
Lemma 18 (Empirical source). Let n ∈ N, δ ∈ (0, 1/2], 0 < λ (cid:54) B(cid:63)
2 . Whenever

λ ((cid:98)x(cid:63)
λ ) ,

λ(cid:107) (cid:98)H−1

(cid:114)

n (cid:62) (cid:52)1

2

B(cid:63)
λ

log

1B(cid:63)
2

8(cid:3)2
λδ

,

(cid:52)2

df λ ∨ (Q(cid:63) )2
n

The following holds, with probability at least 1 − 2δ .

log

2
δ

(cid:54) rλ (x(cid:63) ),

2bλ (cid:54) rλ (x(cid:63) ).

(cid:115)

(cid:98)sλ (cid:54) 8 bλ + 80

df λ ∨ (Q(cid:63) )2 log 2
n

δ

.

Moreover, we also have the following bound :

(cid:107)(cid:98)x(cid:63)

λ − x(cid:63)(cid:107) (cid:54) 3 λ−1/2 bλ + 8 λ−1/2

Proof.

(cid:115)

df λ ∨ (Q(cid:63) )2 log 2
n

δ

.

We ﬁrst decompose the source term into two terms, and then apply different bounds from [23]
to effectively bound it. We will use the following quantity:

(cid:98)vλ := (cid:107)H1/2
λ ) (cid:98)H
λ (x(cid:63)

−1/2
λ

λ )(cid:107)2 (cid:107)∇ (cid:98)fλ (x(cid:63)
(x(cid:63)

λ )(cid:107)H−1

λ ) .
λ (x(cid:63)

It is also deﬁned in equation (23) in [23].

1. Dividing (cid:98)sλ into two controllable terms

(cid:98)sλ = (cid:107)λ(cid:98)x(cid:63)

λ(cid:107) (cid:98)H−1

λ ((cid:98)x(cid:63)
λ )

(cid:54) (cid:107) (cid:98)H
(cid:54) (cid:107) (cid:98)H

−1/2
λ
−1/2
λ

((cid:98)x(cid:63)
((cid:98)x(cid:63)

. Decompose

λ ((cid:98)x(cid:63)
λ )(cid:107) (cid:107)λ(cid:98)x(cid:63)
λ ((cid:98)x(cid:63)
λ )(cid:107) (cid:16)(cid:107)∇fλ ((cid:98)x(cid:63)

λ(cid:107)H−1
λ ((cid:98)x(cid:63)
λ )

λ )H1/2
λ )H1/2

λ ) + (cid:107)∇f ((cid:98)x(cid:63)

λ )(cid:107)H−1

λ ((cid:98)x(cid:63)

λ )(cid:107)H−1

λ ((cid:98)x(cid:63)
λ )

(cid:17)

.

On the one hand, from the previous proof, we get

(cid:107)∇f ((cid:98)x(cid:63)

λ )(cid:107)H−1

λ ((cid:98)x(cid:63)
λ )

(cid:54) et((cid:98)x(cid:63)
λ−x(cid:63) )/2 (cid:107)(cid:98)x(cid:63)
λ−x(cid:63) )/2 (cid:16)
λ − x(cid:63)(cid:107)H(x(cid:63) )
(cid:54) et((cid:98)x(cid:63)
λ−x(cid:63) )(cid:107)(cid:98)x(cid:63)
et(x(cid:63)
(cid:54) et((cid:98)x(cid:63)
λ−x(cid:63) )/2
et(x(cid:63)
λ−x(cid:63) )

λ − x(cid:63)
φ(t((cid:98)x(cid:63)
λ )) (cid:98)vλ +
λ − x(cid:63)

(cid:32)

λ − x(cid:63)(cid:107)Hλ (x(cid:63) )

λ(cid:107)Hλ (x(cid:63)

λ ) + (cid:107)x(cid:63)
λ − x(cid:63) ))
1
φ(t(x(cid:63)

(cid:33)

bλ

.

(cid:17)

63

In the last line, we use the fact that (cid:107)(cid:98)x(cid:63)
λ ) and then bound it using Eq. (18) applied to (cid:98)f to get

λ(cid:107) (cid:98)Hλ (x(cid:63)

x(cid:63)

λ(cid:107)Hλ (x(cid:63)
λ ) (cid:54) (cid:107)H1/2

λ ) (cid:98)H
λ (x(cid:63)

λ )(cid:107) (cid:107)(cid:98)x(cid:63)
(x(cid:63)

λ −

−1/2
λ

(cid:107)(cid:98)x(cid:63)

λ − x(cid:63)

λ(cid:107) (cid:98)Hλ (x(cid:63)
λ )

(cid:54)

(cid:54)

φ((cid:98)t(x(cid:63)
λ − (cid:98)x(cid:63)
1
λ ))
λ − (cid:98)x(cid:63)
1
φ(t(x(cid:63)
λ ))

λ − x(cid:63)
(cid:107)∇ (cid:98)fλ (x(cid:63)
λ ) (cid:98)H
λ (x(cid:63)

(cid:107)H1/2

λ )(cid:107) (cid:98)H−1

On the other hand, apply successively Eq. (18) to f and (cid:98)f using the fact that (cid:98)t (cid:54) t to get

λ (x(cid:63)
λ )
−1/2
λ

λ )(cid:107) (cid:107)∇ (cid:98)fλ (x(cid:63)
(x(cid:63)

λ )(cid:107)H−1

λ ) .
λ (x(cid:63)

(cid:107)∇fλ ((cid:98)x(cid:63)

λ )(cid:107)H−1

λ ((cid:98)x(cid:63)

λ )(cid:107)H−1

λ ((cid:98)x(cid:63)
λ )

λ ) = (cid:107)∇fλ ((cid:98)x(cid:63)
λ )/2φ(t((cid:98)x(cid:63)
λ ) − ∇fλ (x(cid:63)
λ )) (cid:107)(cid:98)x(cid:63)
λ )/2φ(t((cid:98)x(cid:63)
λ − x(cid:63)
λ − x(cid:63)
λ ) (cid:98)H
λ )/2φ(t((cid:98)x(cid:63)
λ − x(cid:63)
λ )) (cid:107)H1/2
φ(t((cid:98)x(cid:63)
λ (x(cid:63)
λ ) (cid:98)H
λ − x(cid:63)
λ ))
λ − x(cid:63)
λ ))
λ (x(cid:63)

(cid:54) et((cid:98)x(cid:63)
λ−x(cid:63)
(cid:54) et((cid:98)x(cid:63)
(cid:54) et((cid:98)x(cid:63)
λ−x(cid:63)
λ−x(cid:63)
λ )/2(cid:98)vλ .
= e3t((cid:98)x(cid:63)
λ−x(cid:63)

(cid:107)H1/2

λ(cid:107)Hλ (x(cid:63)
λ )
−1/2
λ
−1/2
λ

λ )(cid:107) (cid:107)(cid:98)x(cid:63)
λ − x(cid:63)
(x(cid:63)
λ )(cid:107)2 (cid:107)∇ (cid:98)fλ (x(cid:63)
(x(cid:63)

λ(cid:107) (cid:98)Hλ (x(cid:63)
λ )
λ )(cid:107)Hλ (x(cid:63)
λ )

Putting things together:

(cid:98)sλ (cid:54) (cid:107) (cid:98)H

−1/2
λ

((cid:98)x(cid:63)

λ ((cid:98)x(cid:63)
λ )(cid:107)

λ )H1/2

(cid:32)

(cid:18)

λ−(cid:98)x(cid:63)
e3t(x(cid:63)
λ )/2

1 +

λ − (cid:98)x(cid:63)
1
φ(t(x(cid:63)
λ ))

(cid:19) (cid:98)vλ +

λ−(cid:98)x(cid:63)
et(x(cid:63)
λ )/2

λ − x(cid:63) ))
φ(t(x(cid:63)

(cid:33)

bλ

.

2. We now import the results from [23]

. The following hypotheses imply those of Thms 4 and

7 in [23]:

Let n ∈ N, δ ∈ (0, 1/2], 0 < λ (cid:54) B(cid:63)
8(cid:3)2
λδ

n (cid:62) (cid:52)1

log

2 . Whenever

, n (cid:62) (cid:52)2

df λ ∨ (Q(cid:63) )2
rλ (x(cid:63) )2

log

2
δ

, bλ (cid:54) rλ (x(cid:63) )
2

.

In particular, they imply that with probability at least 1 − 2δ :

B(cid:63)
λ

1B(cid:63)
2
2
(cid:113) df λ∨(Q(cid:63) )2 log 2
n
−1/2
λ

λ )(cid:107) (cid:54) √

2;

δ

;

• (cid:98)vλ (cid:54) 1

2 bλ + 4(cid:3)1
λ ) (cid:98)H
• (cid:107)H1/2
λ (x(cid:63)
(x(cid:63)
• t(x(cid:63) − x(cid:63)
λ ) (cid:54) log 2;
• t((cid:98)x(cid:63)
λ − x(cid:63)
λ ) (cid:54) log 2.

Hence, plugging these bounds in the previous equation, we get

(cid:98)sλ (cid:54) 8bλ + 80

df λ ∨ (Q(cid:63) )2 log 2
n

δ

.

(cid:115)

64

3. Note that in what has been done previously, we can bound:

(cid:107)(cid:98)x(cid:63)

λ − x(cid:63)

λ(cid:107)Hλ (x(cid:63)
λ ) (cid:54)

λ )) (cid:98)vλ (cid:54) bλ + 8
λ − (cid:98)x(cid:63)
1
φ(t(x(cid:63)

df λ ∨ (Q(cid:63) )2 log 2
n

δ

.

(cid:115)

Moreover,

(cid:107)x(cid:63)

λ − x(cid:63)(cid:107)Hλ (x(cid:63) ) (cid:54)

1
λ − x(cid:63) ))
φ(t(x(cid:63)

Hence:

(cid:107)(cid:98)x(cid:63)

λ − x(cid:63)(cid:107) (cid:54) 3 λ−1/2 bλ + 8 λ−1/2

(cid:115)

(cid:107)∇fλ (x(cid:63) )(cid:107)H−1

λ (x(cid:63) )

(cid:54) 2bλ .

df λ ∨ (Q(cid:63) )2 log 2
n

δ

.

2. Final bound for the projected ERM approximation In this paragraph, denote with CP (x, λ)

and (cid:98)CP (x, λ) the quantity (cid:107) (cid:98)H1/2 (x)(I−P)(cid:107)2
the quantity (cid:107)H1/2 (x)(I−P)(cid:107)2
Lemma 19. Let n ∈ N, δ ∈ (0, 1/2], 0 < λ (cid:54) B(cid:63)
2 . Whenever

λ

λ

(cid:114)

n (cid:62) (cid:52)1

2

B(cid:63)
λ

log

if

8(cid:3)2
λδ

,

C1

√

1B(cid:63)
2

df λ ∨ (Q(cid:63) )2
2
(cid:54) rλ (x(cid:63) ),
n
log
δ
, (cid:98)νP,λ (x) (cid:54) (cid:98)rP,λ (x)
CP (x(cid:63) , λ) (cid:54)
2
∧ rλ (x(cid:63) )
480
2
126
(cid:98)νλ (x) (cid:54) (cid:98)rλ (x)
, (cid:98)νλ (x) (cid:54) rλ (x(cid:63) )
2
2

.

C1bλ (cid:54) rλ (x(cid:63) ),

,

the following holds, with probability at least 1 − 2δ .

Here, C1 = 1008.

Proof.

Proceed in the following way.

1.

It is easy to see that the conditions of this lemma imply the conditions of Thm. 8. Hence, as in
the previous proofs, the following hold:

2;

−1/2
λ

λ )(cid:107) (cid:54) √

Let us now apply Proposition 15 to (cid:98)f . If

λ ) (cid:98)H
• (cid:107)H1/2
λ (x(cid:63)
(x(cid:63)
• t(x(cid:63) − x(cid:63)
λ ) (cid:54) log 2;
• t((cid:98)x(cid:63)
λ − x(cid:63)
λ ) (cid:54) log 2.
(cid:98)sλ(cid:98)rλ ((cid:98)x(cid:63)
, (cid:98)CP ((cid:98)x(cid:63)
, (cid:98)νP,λ (x) (cid:54) (cid:98)rP,λ (x)
λ , λ) (cid:54) 1
λ )
4
120
2
(cid:98)νλ (x) (cid:54) 3((cid:98)νP,λ (x) + (cid:98)sλ ),
(cid:98)t(x − (cid:98)x(cid:63)
λ ) (cid:54) 3
10

(cid:54) 1

,

Then it holds:

2.

where the second bound is obtained in the proof of this proposition. Now since

65

+ 2 log 2.

(38)

1(cid:98)rλ ((cid:98)x(cid:63)
λ )

−1/2
λ

λ−x(cid:63)
λ )/2

(cid:54) e(cid:98)t((cid:98)x(cid:63)
(cid:54) e(cid:98)t((cid:98)x(cid:63)

1(cid:98)rλ (x(cid:63)
λ ) (cid:98)H
λ )
λ (x(cid:63)
λ ) (cid:98)H
λ (x(cid:63)
λ ) (cid:98)H
λ (x(cid:63)

λ−x(cid:63)
λ )/2 (cid:107)H1/2
(cid:54) et((cid:98)x(cid:63)
λ−x(cid:63)
λ )/2 (cid:107)H1/2
= et((cid:98)x(cid:63)
λ−x(cid:63)
λ )/2 (cid:107)H1/2
−1/2
(cid:54) e(t((cid:98)x(cid:63)
λ
λ−x(cid:63)
λ−x(cid:63) ))/2 (cid:107)H1/2
λ )+t(x(cid:63)

−1/2
λ

√

λ (x(cid:63)
λ )

λ (x(cid:63)
λ )

(cid:107)g(cid:107)H−1
(cid:107)g(cid:107)H−1

g∈ (cid:98)G
g∈G

λ )(cid:107) sup
(x(cid:63)
λ )(cid:107) sup
(x(cid:63)
(x(cid:63)
λ )(cid:107)
1
λ ) (cid:98)H
rλ (x(cid:63)
λ )
λ )(cid:107)
λ (x(cid:63)
(x(cid:63)

−1/2
λ

1
rλ (x(cid:63) )

E q . (17)

Def

(cid:98)G ⊂ G

Def

E q . (17)

previous bounds

(cid:54) 2

2
rλ (x(cid:63) )

.

In a similar way, we get (cid:98)CP ((cid:98)x(cid:63)
the following conditions are satisﬁed:

λ , λ) (cid:54) 2

√

√

(cid:98)sλ
rλ (x(cid:63) )

(cid:54)

2
16
1(cid:98)rλ (x)

, CP (x(cid:63) , λ) (cid:54)
(cid:54) e(cid:98)t(x−(cid:98)x(cid:63)
(cid:98)rλ (x)

Finally, note that under these conditions,
using the previous bound and the bound on (cid:98)t(x − (cid:98)x(cid:63)
3. Let us assume

λ )/2

λ ).

2CP (x(cid:63) , λ). Thus, the conditions above are satisﬁed if

, (cid:98)νP,λ (x) (cid:54) (cid:98)rP,λ (x)
2

.

√

2
480

(cid:54) 7

rλ (x(cid:63) )

.

(39)

(cid:98)sλ
rλ (x(cid:63) )

√

2
16

(cid:54)

, CP (x(cid:63) , λ) (cid:54)

, (cid:98)νP,λ (x) (cid:54) (cid:98)rP,λ (x)
2

.

√

2
480

According to Eq. (39), and to Eq. (38), if(cid:98)νP,λ (x) + (cid:98)sλ (cid:54) rλ (x(cid:63) )
then it holds

(cid:98)νλ (x) (cid:54) (cid:98)rλ (x)
2

,
, (cid:98)νλ (x) (cid:54) rλ (x(cid:63) )
42
.
2
(cid:98)sλ (cid:54) 2rλ (x(cid:63) )
126

,

We simplify this condition as:(cid:98)νP,λ (x) (cid:54) rλ (x(cid:63) )
4. Now using the fact that under the conditions of this lemma, those of Lemma 18 are satisﬁed:

126

.

Thus, (cid:98)sλ (cid:54) 2rλ (x(cid:63) )

126

where C1 = 1008.

(cid:115)

(cid:98)sλ (cid:54) 8 bλ + 80

df λ ∨ (Q(cid:63) )2 log 2
n

δ

.

holds, provided

bλ (cid:54) rλ (x(cid:63) )
C1

, n (cid:62) C2

1

df λ ∨ (Q(cid:63) )2 log 2
rλ (x(cid:63) )2

δ

,

66

Proposition 18 (Behavior of an approximation to the projected problem). Let n ∈ N, δ ∈ (0, 1/2],
2 . Let x ∈ HP . Whenever

0 < λ (cid:54) B(cid:63)

n (cid:62) (cid:52)1

2

B(cid:63)
λ

log

1B(cid:63)
2

8(cid:3)2
λδ

,

C1

if

CP (x(cid:63) , λ) (cid:54)

(cid:114)

√

2
480

C1bλ (cid:54) rλ (x(cid:63) ),

df λ ∨ (Q(cid:63) )2
2
(cid:54) rλ (x(cid:63) ),
log
, (cid:98)νP,λ (x) (cid:54) (cid:98)rP,λ (x)
n
δ
2
df λ ∨ (Q(cid:63) )2
n

log

∧ rλ (x(cid:63) )
.
126
+ K3 (cid:98)ν 2
P,λ (x),

2
δ

The following holds, with probability at least 1 − 2δ .

f (x) − f (x(cid:63) ) (cid:54) K1 b2
λ + K2

where K1 (cid:54) 6.0e4, K2 (cid:54) 6.0e6 and K3 (cid:54) 810, C1 are deﬁned in Lemma 19, and the other
constants are deﬁned in Thm. 8.

Remark 5 (Constants). In this result, absolutely huge constants are obtained. They are (of course)
totally sub-optimal. Indeed, this analysis has been simpliﬁed by dividing the bound into blocks:
error of the empirical risk minimization with regularization, error of the projection compared to
this empirical risk minimizer. Going back and forth from empirical to statistical, from projected to
non projected induces exponential explosion of the constants. There is a way of doing the analysis
directly by projecting the statistical problem. However, in order to relate to our previous work
[23] and avoid re-doing all of our work we discarded this. If we were to perform this more direct
analysis, we could keep the constants to a reasonable level, of order 102 .

Proof.We apply Lemma 17, using the previous lemma to guarantee the conditions.

f (x) − f (x(cid:63) ) (cid:54) 14(f ((cid:98)x(cid:63)
λ ) − f (x(cid:63) )) + 30(cid:98)νλ (x)2 .

1. Under the conditions of this proposition, applying Lemma 19, the conditions of Lemma 17 are
satisﬁed. Thus,
Moreover, from the previous proof, (cid:98)νλ (x) (cid:54) 3((cid:98)νP,λ (x) + (cid:98)sλ ),
and seeing as Lemma 18 is satisﬁed,

(cid:115)

(cid:98)sλ (cid:54) 8 bλ + 80

df λ ∨ (Q(cid:63) )2 log 2
n

δ

.

This therefore yields:

(cid:98)νλ (x)2 (cid:54) 27(cid:98)νP,λ (x)2 + 1726b2
λ + 172600

df λ ∨ (Q(cid:63) )2 log 2
n

δ

.

2. Moreover, from Thm. 8, it holds:

f ((cid:98)x(cid:63)
λ ) − f (x(cid:63) ) (cid:54) 414 b2
λ + 414

df λ ∨ (Q(cid:63) )2
n

log

2
δ

.

67

3.

Putting things together:

f (x) − f (x(cid:63) ) (cid:54) K1 b2
λ + K2

We bound the constants in the theorem.

df λ ∨ (Q(cid:63) )2
n

log

2
δ

+ K3 (cid:98)ν 2
P,λ (x).

Lemma 20. Under the conditions of the previous theorem, the following hold:

•

1(cid:98)rP,λ (x)

(cid:54) 8

rλ (x(cid:63) ) ;

• λ1/2(cid:107)x − x(cid:63)(cid:107) (cid:54) 7(cid:98)νP,λ (x) + 59bλ + 568
• λ(cid:107)x(cid:107) (cid:98)H−1
(cid:54) 7(cid:98)νP,λ (x) + 72bλ + 720

(cid:113) df λ∨(Q(cid:63) )2 log 2
(cid:113) df λ∨(Q(cid:63) )2 log 2
n
n

.

δ

δ

;

In particular,

(x)

(cid:54) 11.

λ(cid:107)x(cid:107) (cid:98)H−1

P,λ (x)
(cid:98)rP,λ (x)

P,λ

Proof.Let us prove the three statements.

1. Write

1(cid:98)rP,λ (x) = supg∈ (cid:98)G (cid:107)Pg(cid:107) (cid:98)H−1
(cid:107)Pg(cid:107) (cid:98)H−1
(cid:54) sup
sup

P,λ (x)

g∈ (cid:98)G

g∈ (cid:98)G

P,λ (x) . Now

(cid:107)g(cid:107) (cid:98)H−1

λ (x)

(cid:54) e(cid:98)t(x−(cid:98)x(cid:63)

λ )/2 sup

g∈ (cid:98)G

(cid:107)g(cid:107) (cid:98)H−1

λ ((cid:98)x(cid:63)
λ ) .

Now bound

(cid:107)g(cid:107) (cid:98)H−1

λ ((cid:98)x(cid:63)
λ )

sup

g∈ (cid:98)G

(cid:54) e(cid:98)t(x(cid:63)
λ−(cid:98)x(cid:63)
λ )/2 sup

g∈ (cid:98)G

(cid:107)g(cid:107) (cid:98)H−1

λ (x(cid:63)
λ )

(cid:54) e(cid:98)t(x(cid:63)
λ−(cid:98)x(cid:63)
λ )/2 (cid:107)H1/2

λ ) (cid:98)H
λ (x(cid:63)

−1/2
λ

λ )(cid:107) sup
(x(cid:63)

g∈ (cid:98)G

(cid:107)g(cid:107)H−1

λ ) .
λ (x(cid:63)

Finally bound

.

λ (x(cid:63)
λ )

sup

g∈ (cid:98)G

1
rλ (x(cid:63) )

(cid:54) et(x(cid:63)−x(cid:63)
λ )/2

Now using the fact that under the previous assumptions t(x(cid:63) − x(cid:63)
2, we get the ﬁrst equation.

(cid:107)g(cid:107)H−1
(cid:98)t(x − (cid:98)x(cid:63)
λ ) (cid:98)H
λ ) (cid:54) 3
10 + 2 log 2 and (cid:107)H1/2
λ (x(cid:63)
(x(cid:63)
λ1/2(cid:107)x − x(cid:63)(cid:107) (cid:54) λ1/2(cid:107)x − (cid:98)x(cid:63)
λ(cid:107) + λ1/2(cid:107)(cid:98)x(cid:63)
λ − x(cid:63)(cid:107).

In order to bound λ1/2(cid:107)x − x(cid:63)(cid:107), decompose
Now use Proposition 15 to bound λ1/2(cid:107)x − (cid:98)x(cid:63)
λ(cid:107) (cid:54) 7((cid:98)νP,λ (x) + (cid:98)sλ ). Using Lemma 18, under the
conditions above,

λ − (cid:98)x(cid:63)
λ ) (cid:54) log 2,
λ ), t(x(cid:63)

λ )(cid:107) (cid:54) √

−1/2
λ

2.

(cid:115)

Hence

(cid:98)sλ (cid:54) 8 bλ + 80

df λ ∨ (Q(cid:63) )2 log 2
n
λ1/2(cid:107)x − (cid:98)x(cid:63)
λ(cid:107) (cid:54) 7(cid:98)νP,λ (x) + 56bλ + 560

(cid:115)

δ

.

df λ ∨ (Q(cid:63) )2 log 2
n

δ

.

Moreover, using again Lemma 18

68

(cid:115)

λ1/2(cid:107)(cid:98)x(cid:63)
λ − x(cid:63)(cid:107) (cid:54) 3 bλ + 8

df λ ∨ (Q(cid:63) )2 log 2
n

δ

.

Combining these two inequalities, we get:

λ1/2(cid:107)x − x(cid:63)(cid:107) (cid:54) 7(cid:98)νP,λ (x) + 59bλ + 568

3.

In order to bound λ(cid:107)x(cid:107) (cid:98)H−1
P,λ (x) , use Proposition 15 to get λ(cid:107)x(cid:107) (cid:98)H−1
Now using Lemma 18, the following bound holds:

λ(cid:107)x(cid:107) (cid:98)H−1

P,λ (x)

(cid:54) 7(cid:98)νP,λ (x) + 72bλ + 720

df λ ∨ (Q(cid:63) )2 log 2
n

δ

.

(cid:115)

(cid:115)

δ

df λ ∨ (Q(cid:63) )2 log 2
n
.
(cid:54) 7(cid:98)νP,λ (x) + 9(cid:98)sλ .

P,λ (x)

Proposition 19 (Simpliﬁcation). Let n ∈ N, δ ∈ (0, 1/2], 0 < λ (cid:54) B(cid:63)
2 . Let x ∈ HP . Whenever

n (cid:62) (cid:52)1

2

B(cid:63)
λ

log

1B(cid:63)
2

8(cid:3)2
λδ

,

C1

if

CP (x(cid:63) , λ) (cid:54)

(cid:114)

df λ ∨ (Q(cid:63) )2
log
n
, (cid:98)νP,λ (x) (cid:54)

2
480

√

(cid:54)

2
δ

√

√

λ
R

,

C1bλ (cid:54)

√

λ
R

,

then the following holds, with probability at least 1 − 2δ .

f (x) − f (x(cid:63) ) (cid:54) K1 b2
λ + K2

df λ ∨ (Q(cid:63) )2
n

λ
,
126R
+ K3 (cid:98)ν 2
P,λ (x),

log

2
δ

where K1 (cid:54) 6.0e4, K2 (cid:54) 6.0e6 and K3 (cid:54) 810, C1 are deﬁned in Lemma 19, and the other
constants are deﬁned in Thm. 8.
Moreover, in that case, R(cid:107)x − x(cid:63)(cid:107) (cid:54) 10.

H.4 Optimal choice of λ, speciﬁc source conditions

In this part, we continue to assume Assumptions 6 to 9. We present a classiﬁcation of distributions
ρ and show that we can achieve better rates than the classical slow rates, as presented in Appendix
F of [23].

H.4.1 Classiﬁcation of distributions and statistical bounds for the ERM

We use the following classiﬁcation for distributions.
Deﬁnition 9 (class of distributions). Let α ∈ [1, +∞] and r ∈ [0, 1/2].
We denote with Pα,r the set of probability distributions ρ such that there exists L, Q (cid:62) 0,

1+2r
2

• bλ (cid:54) L λ
• df λ (cid:54) Q2 λ−1/α ;

;

where this holds for any 0 < λ (cid:54) 1. For simplicity, if α = +∞, we assume that Q (cid:62) Q(cid:63) .

69

Note that given our assumptions, we always have

We also deﬁne

such that

(cid:18) Q

ρ ∈ P1,0 , L = (cid:107)x(cid:63)(cid:107), Q = B(cid:63)
(cid:19)2α ∧ 1,
1 .
∀λ (cid:54) λ1 , df λ ∨ (Q(cid:63) )2 (cid:54) Q2

λ1 =

Q(cid:63)

.

λ1/α

(40)

(41)

Interpretation of the classes

• The bias term bλ characterizes the regularity of the objective x(cid:63) . In a sense, if r is big,
then this means x(cid:63) is very regular and will be easier to estimate. The following results
reformulates this intuition.
Remark 6 (source condition). Assume there exists 0 (cid:54) r (cid:54) 1/2 and v ∈ H such that

PH(x(cid:63) )x(cid:63) = H(x(cid:63) )r v .

Then it holds:

∀λ > 0, bλ (cid:54) L λ

1+2r
2

,

L = (cid:107)H(x(cid:63) )−r x(cid:63)(cid:107).

• The effective dimension df λ characterizes the size of the space H with respect to the problem.
The higher α, the smaller the space. If H is ﬁnite dimensional for instance, α = +∞.
In this section, for any given pair (α, r) characterizing the regularity and size of the problem,
we associate
expected risk minimizer (cid:98)x(cid:63)
In [23] (see corollary 3), explicit bounds are given for the performance of the regularized
λ depending on which class ρ belongs to, i.e., as a function of α, r .
Corollary 4. Let δ ∈ (0, 1/2]. Under Assumptions 6 to 9, if ρ ∈ Pα,r with r > 0 , when n (cid:62) N
and λ = (C0/n)β , then with probability at least 1 − 2δ ,

α(1 + 2r)
α(1 + 2r) + 1

1
1 + 2r + 1/α

β =

γ =

,

.

f ((cid:98)x(cid:63)
λ ) − f (x(cid:63) ) (cid:54) C1n−γ log

2
δ

,

with C0 = 256(Q/L)2 , C1 = 8(256)γ (Qγ L1−γ )2 and N deﬁned in [23], and satisfying N =

O(poly(B(cid:63)
1 , B(cid:63)
2 , L, Q, R, log(1/δ))).

H.4.2 Quantitative bounds for the projected problem

In this part, the aim is to show that if we approximately solve the projected problem up to a certain
precision, then this approximation has the same statistical rates as the regularized ERM with the
good choice of λ. For the sake of simplicity, we will assume that r > 0.
In what follows, we deﬁne

(cid:18)

(cid:19)(cid:19)1/(1−β )

1
1 − β

A2 1
δ

,

(42)

N =

−1/β ∨

Q2
2 ∧ λ0 ∧ λ1 )
L2 (B(cid:63)

1
1 − β
Q2β , λ0 = (C1LR log 2
δ )−1/r ∧ 1 and λ1 = Q2α

2.1e4

A log

2 L2β

(Q(cid:63) )2α .

1.4e6

where A = B(cid:63)

(cid:18)

70

Theorem 9 (Quantitative result with source r > 0). Let ρ ∈ Pα,r and assume r > 0. Let δ ∈ (0, 1
Let P be an orthogonal projection, x ∈ H. If

2 ].

n (cid:62) N , λ =

then with probability at least 1 − 2δ ,

(cid:32)(cid:18) Q

(cid:33)β

√

(cid:19)2 1
, CP (x(cid:63) , λ) (cid:54)
2
L
n
480
f (x) − f (x(cid:63) ) (cid:54) K (cid:0)Qγ L1−γ (cid:1)2

, (cid:98)νP,λ (x) (cid:54) Qγ L1−γ n−γ /2

1
nγ log

2
δ

,

where N is deﬁned in Eq. (42) and K (cid:54) 7.0e6. Moreover, R(cid:107)x − x(cid:63)(cid:107) (cid:54) 10.

Proof.

Using the deﬁnition of λ1 , as soon as λ (cid:54) λ1 ,it holds: df λ ∨ (Q(cid:63) )2 (cid:54) Q2λ−1/α .
Let us formulate Proposition 19 using the fact that ρ ∈ Pα,r .
2 , x ∈ HP . Whenever

Let n ∈ N, δ ∈ (0, 1/2], 0 < λ (cid:54) B(cid:63)
8(cid:3)2
λδ

n (cid:62) (cid:52)1

B(cid:63)
λ

, C1

1B(cid:63)
2

log

2

(cid:114)

if

CP (x(cid:63) , λ) (cid:54)

The following holds, with probability at least 1 − 2δ .

, C1 Lλ1/2+r (cid:54) λ1/2

,

R

λ1/αn

(cid:54) λ1/2

Q2
2
log
δ
R
, (cid:98)νP,λ (x) (cid:54) Lλ1/2+r ,

√

2
480

f (x) − f (x(cid:63) ) (cid:54) (K1 + K3 )L2λ1+2r + K2

Q2

λ1/αn

log

2
δ

,

R(cid:107)x − x(cid:63)(cid:107) (cid:54) 10,

where all constants are deﬁned in Proposition 19.

Assume that r > 0 . Deﬁne

Then for any λ (cid:54) λ0 :

λ0 = (C1LR log

)−1/r ∧ 1.

2
δ

Lλ1/2+r (cid:54) 1

C1

√

λ
R

.

1) First, we ﬁnd a simple condition to guarantee

rλ (x(cid:63) )2λ1/α (cid:62) C2 Q2 1
n

log

2
δ

.

We see that if λ (cid:54) λ0 , then rλ (cid:62) C1Lλ1/2+r log 2
δ . Hence, this condition is satisﬁed if

λ (cid:54) λ0 , C2

1L2λ1+2r+1/α (cid:62) C2 Q2 1

n

.

Using the fact that C2 = C2
1 , we reformulate:

λ (cid:54) λ0 ,

L2λ1+2r+1/α (cid:62) Q2 1

n

.

71

2) Now ﬁx

λ1+2r+1/α =

where β = 1/(1 + 2r + 1/λ) ∈ [1/2, 1).

(cid:18) Q2
L2

(cid:19)β

.

1
n

Q2
L2

1
n

⇐⇒ λ =

Using our restatement of Proposition 18, with probability at least 1 − 2δ ,

(cid:18)

(cid:19)

L(x) − L(x(cid:63) ) (cid:54)

K1 + K3 + K2 log

2
δ

L2λ1+2r (cid:54) K log

2
δ

L2λ1+2r ,

where K = K1 + K3 + K2 (cid:54) 7.0e6 (see Proposition 18).
This result holds provided

0 < λ (cid:54) B(cid:63)
2 ∧ λ0 ∧ λ1 , n (cid:62) (cid:52)1

2

B(cid:63)
λ

log

1B(cid:63)
2

8(cid:3)2
λδ

.

(43)

Indeed, it is shown in the previous point that the other conditions are satisﬁed.
3) Let us now work to guarantee the conditions in Eq. (43).
First, to guarantee n (cid:62) (cid:52)1
, bound

1B(cid:63)
2

B(cid:63)

2

λδ

λ log 8(cid:3)2
B(cid:63)
λ

=

2

2L2β nβ

B(cid:63)
Q2β logβ 2

δ

(cid:54) 2

2L2β

B(cid:63)
Q2β nβ .

Then apply lemma 15 from [23] with a1 = 2(cid:52)1 , a2 = 16(cid:3)2
Q2β . Since β (cid:62) 1/2,
using the bounds in Thm. 8, we ﬁnd a1 (cid:54) 10400 and a2 (cid:54) 64, hence the following sufﬁcient
condition:

1 , A = B(cid:63)

2 L2β

(cid:18)

n (cid:62)

(cid:18)

(cid:19)(cid:19)1/(1−β )

2.1e4

A log

1.4e6

1
1 − β

1
1 − β

A2 1
δ

.

(cid:19)(cid:19)1/(1−β )

,

1
1 − β

A2 1
δ

Then, to guarantee the condition

λ (cid:54) B(cid:63)
2 ∧ λ0 ∧ λ1 ,

we simply need

Hence, deﬁning

n (cid:62) Q2
2 ∧ λ0 ∧ λ1 )
L2 (B(cid:63)

−1/β .

(cid:18)

(cid:18)

N =

Q2
2 ∧ λ0 ∧ λ1 )
L2 (B(cid:63)

−1/β ∨

2.1e4

A log

1.4e6

1
1 − β

we see that as soon as n (cid:62) N , Eq. (43) holds.

72

I Multiplicative approximations for Hermitian operators

In this section, we put together useful tools for approximating linear operators and solving linear
systems with regularization.
In this section, A and B will always denote positive semi-deﬁnite Hermitian operators on a
Hilbert space H, and P will denote an orthogonal projection operator. Moreover, given a positive
semi-deﬁnite operator A, and λ > 0, Aλ will stand for the regularized operator A + λI.
Lemma 21 (Equivalence of Hermitian operators). Let A and B be two semi-deﬁnite Hermitian
operators. Let λ > 0. Assume you have access to

t := (cid:107)A

−1/2
λ

(B − A)A

−1/2
λ

(cid:107).

It holds:

Moreover, if t < 1,

(cid:107)A

−1/2
λ B1/2

λ (cid:107)2 (cid:54) 1 + t ⇔ Bλ (cid:22) (1 + t)Aλ .

(cid:107)B

−1/2
λ A1/2

λ (cid:107)2 (cid:54) 1
1 − t

⇔ (1 − t)Aλ (cid:22) Bλ .

Proof.For the ﬁrst point, simply note that:

(cid:107)A

−1/2
λ B1/2

λ (cid:107)2 = (cid:107)A

−1/2

λ BλA

−1/2
λ

−1/2
λ

(B − A) A

−1/2
λ

(cid:107) (cid:54) 1 + t.

For the second point,

λ (cid:107)2 = (cid:107) (cid:16)

(cid:107)B

−1/2
λ A1/2

−1/2

λ BλA

−1/2
λ

A

I + A

−1/2
λ

(B − A) A

−1/2
λ

Moreover, we know that if (cid:107)H(cid:107) < 1 with H a Hermitian operator, then (cid:107)(I + H)−1(cid:107) (cid:54) 1
The result follows.
We will now state a technical lemma which describes how combining approximation behaves.
Lemma 22 (Combination of approximations). Let N (cid:62) 1. Let (Ai )1(cid:54)i(cid:54)N +1 be a sequence of
positive semi-deﬁnite Hermitian operators. Deﬁne

(cid:107) = (cid:107)I + A

(cid:17)−1 (cid:107) = (cid:107) (cid:16)

(cid:17)−1 (cid:107).
1−(cid:107)H(cid:107) .

For any 1 (cid:54) i, j (cid:54) N + 1, deﬁne

ti := (cid:107)A

i,λ (Ai+1 − Ai )A
−1/2

i,λ (cid:107).
−1/2

ti:j := (cid:107)A
i,λ (Aj − Ai )A

−1/2

i,λ (cid:107).
−1/2

In particular, ti = ti:i+1 . Then the following holds:

∀1 (cid:54) i (cid:54) j (cid:54) N , 1 + ti:j (cid:54)

j−1(cid:89)

(1 + tk )

k=i

Moreover, if ti < 1, then it holds:

(cid:107)A

−1/2
i+1,λ (Ai − Ai+1 )A

Hence, in that case

∀1 (cid:54) j (cid:54) i (cid:54) N , 1 + tj :i (cid:54)

−1/2
i+1,λ(cid:107) (cid:54) ti

1 − ti

j−1(cid:89)

k=i

1
1 − tk

Proof.Let us prove everything for a sequence of three operators; the rest follows by induction.
Let A1 , A2 , A3 be three positive semi-deﬁnite operators.

73

1. Bound

t1:3 = (cid:107)A

(cid:54) (cid:107)A

−1/2
1,λ
−1/2
1,λ

(A1 − A3 ) A
(A1 − A2 ) A
1,λ (cid:107) + (cid:107)A
(cid:54) t1:2 + (cid:107)A
(cid:54) t1:2 + (1 + t1:2 )t2:3 .

1,λ (cid:107)
−1/2
−1/2
−1/2
2,λ (cid:107)2 t2:3
1,λ A1/2

−1/2
1,λ

(A2 − A3 ) A

1,λ (cid:107)
−1/2

The last line comes from Lemma 21. Thus

1 + t1:3 (cid:54) 1 + t1:2 + t2:3 + t1:2 t2:3 = (1 + t1:2 )(1 + t2:3 ).

2. Let us now bound t2:1 knowing t1:2 . This will imply the rest of the lemma.
Indeed, simply note that

t2:1 = (cid:107)A
2,λ (A2 − A1 )A

−1/2

−1/2

2,λ (cid:107) (cid:54) (cid:107)A

−1/2
1,λ (cid:107)2 t1:2 .
2,λ A1/2

Using Lemma 21, if t1:2 < 1, (cid:107)A

, hence

−1/2
1,λ (cid:107)2 (cid:54) 1
2,λ A1/2
1−t1:2
t2:1 (cid:54) t1:2

1 − t1:2

.

Lemma 23 (Projection of Hermitian operators). For any Hermitian operator A and orthogonal
projection P, the following holds:

(cid:107)A

−1/2
λ

(A − PAP)A

−1/2
λ

(cid:107) (cid:54)

1 +

(cid:107)A1/2 (I − P)(cid:107)
λ

√

− 1.

(cid:32)

(cid:33)2

In particular,

Moreover, if

then it holds

(cid:107)A

−1/2
λ

(PAP + λI)1/2 (cid:107) (cid:54) 1 +

(cid:107)A1/2 (I − P)(cid:107)
λ

√

.

(cid:107)A1/2 (I − P)(cid:107)
λ

√

(cid:107)A1/2
λ

(PAP + λI)

−1/2 (cid:107)2 (cid:54)

2 − 1,

<

√
2 − (cid:16)

(cid:17)2 .

1

1 +

(cid:107)A1/2 (I−P)(cid:107)
λ

√

We also always have:

(cid:107) (PAP + λI)

−1/2 PA1/2

λ (cid:107)2 (cid:54) 1.

Proof.For any Hermitian operator A, the following computation holds:

(cid:107)A

−1/2
λ

(A − PAP)A

−1/2
λ

(cid:107) = (cid:107)A

−1/2
λ

(cid:107)

−1/2
λ

(cid:54) 2(cid:107)A

(cid:107) + (cid:107)A

−1/2
λ
−1/2
λ

(A − (I − (I − P))A(I − (I − P))A
(I − P)AA
(I − P)A(I − P)A
(cid:54) 2(cid:107)A1/2 (I − P)(cid:107)
(cid:107)A1/2 (I − P)(cid:107)2
+
λ
λ
(cid:107)A1/2 (I − P)(cid:107)
− 1.
λ

(cid:33)2

−1/2
λ

(cid:32)

=

1 +

√

√

−1/2
λ

(cid:107)

74

Lemma 24 (Relationship between approximations). Let A and B be two positive semi-deﬁnite
hermitian operators. Let λ > 0, b ∈ H and ρ > 0. If

(cid:107)A

∧ ρ

−1/2
λ

−1/2
λ

(cid:107) (cid:54) 1

then it holds:
Proof.Assume (cid:101)∆ ∈ LinApprox(Bλ , b, ˜ρ/4) for a certain ˜ρ. Decompose

(B − A)A

(cid:101)∆ ∈ LinApprox(Bλ , b, ρ/4),
,
(cid:101)∆ ∈ LinApprox(Aλ , b, ρ).
2
4
λ b − (cid:101)∆(cid:107)Aλ
λ b − B−1
λ b(cid:107)Aλ + (cid:107)B−1
(cid:54) (cid:107)A1/2
λ (A−1
λ − B−1
λ (cid:107) (cid:107)b(cid:107)A−1
+ (cid:107)A1/2
λ B
λ − B−1
λ = B−1
λ (B − A)A−1

Now using the fact that A−1

λ b − (cid:101)∆(cid:107)Aλ

(cid:54) (cid:107)A−1

λ )A1/2

(cid:107) (cid:107)B−1

(cid:107)A−1

−1/2
λ

λ ,

λ

λ b − (cid:101)∆(cid:107)Bλ .

(cid:107)A1/2

λ )A1/2

λ (A−1
λ − B−1
λ b − (cid:101)∆(cid:107)Bλ

(cid:107)B−1

Moreover,

λ (cid:107) (cid:54) (cid:107)A

= (cid:107)A

−1/2
λ
−1/2
λ

(B − A)A
(B − A)A

−1/2
λ
−1/2
λ

(cid:107) (cid:107)A1/2
λ B−1
(cid:107) (cid:107)A1/2
λ B

λ A1/2
−1/2
λ

(cid:107)2 .

λ (cid:107)

(cid:54) ˜ρ(cid:107)b(cid:107)B−1

(cid:54) (cid:107)A1/2B−1/2(cid:107) (cid:107)b(cid:107)A−1

.

λ

λ

Putting things together, and noting that from Lemma 21, (cid:107)A1/2B−1/2(cid:107)2 (cid:54)
as soon as (cid:107)A
(cid:107) < 1, it holds:

−1/2
λ

−1/2
λ

(B − A)A
(cid:101)∆ ∈ LinApprox(Aλ , b, ρ), ρ =
(B − A)A

˜ρ + (cid:107)A

1 − (cid:107)A

−1/2
λ

−1/2
−1/2
λ
λ
−1/2
λ

Choosing the right values for ˜ρ and (cid:107)A

(B − A)A
(B − A)A

−1/2
−1/2
λ
λ

(cid:107)
(cid:107) .

(cid:107) yields the result.

1−(cid:107)A

−1/2
λ

1
(B−A)A

−1/2
λ

(cid:107)

I.1 Results for Nystrom sub-sampling

Recall the notations from Appendix D.
We write without proof the following lemmas, which are just restatements of lemmas 9 and 10
of [29].
Lemma 25 (Uniform sampling). Let δ > 0. If { ˜z1 , ..., ˜zm} are sampled uniformly, then if 0 < λ <
(cid:107)A(cid:107), m (cid:54) n and

Then it holds, with probability at least 1 − δ :

m (cid:62) (cid:0)10 + 160N A∞ (λ)(cid:1) log
8(cid:107)v(cid:107)2
λδ
.
( (cid:98)A − A)A
m,λ ( (cid:98)A − (cid:98)Am ) (cid:98)A
m,λ (cid:107) (cid:54) 1
2

(cid:107) (cid:98)A

(cid:107) (cid:54) 1

−1/2
λ

L∞ (Z )

−1/2

−1/2

2

,

.

(cid:107)A

−1/2
λ

Lemma 26 (Nystrom sampling). Let δ > 0. If { ˜z1 , ..., ˜zm} are sampled using q -approximate
leverage scores for t = λ, then if t0 ∨ 19(cid:107)v(cid:107)2
, if

log n
2δ < λ < (cid:107)A(cid:107), and n (cid:62) 405(cid:107)v(cid:107)2
m (cid:62) (cid:0)6 + 486q2N A (λ)(cid:1) log

L∞ (Z ) ∨

12(cid:107)v(cid:107)2
δ

L∞ (Z ) log

67(cid:107)v(cid:107)2

L∞ (Z )

L∞ (Z )

L∞ (Z )

n

.

8(cid:107)v(cid:107)2
λδ

75

Then it holds, with probability at least 1 − δ :

Lemma 27. Let λ > 0. Assume:

( (cid:98)A − A)A
( (cid:98)A − A)A

−1/2
λ

−1/2
λ

(cid:107) (cid:54) 1

2

(cid:107) (cid:54) 1

2

,

,

(cid:107)A

−1/2
λ

(cid:107)A

−1/2
λ

(cid:107) (cid:98)A
(cid:107) (cid:98)A

−1/2

−1/2

m,λ ( (cid:98)A − (cid:98)Am ) (cid:98)A
m,λ (cid:107) (cid:54) 1
2
m,λ ( (cid:98)A − (cid:98)Am ) (cid:98)A
m,λ (cid:107) (cid:54) 1
2

−1/2

−1/2

.

.

Denote with Pm the projection on span(v ˜zj )1(cid:54)j(cid:54)m . Then the following holds:

V ∗ (cid:98)AmV + λI

(cid:17)

.

and for any partial isometry V ,

Proof.For the ﬁrst point, use the well known fact that
since the range of Pm contains that of (cid:98)Am . Thus,

(cid:16)

1
2

(cid:16)

(cid:107)A1/2

λ (I − Pm )(cid:107)2 (cid:54) 3λ,
(cid:17) (cid:22) V ∗ (cid:98)AV + λI (cid:22) 3
V ∗ (cid:98)AmV + λI
2
I − Pm (cid:54) λ (cid:98)A−1
λ (cid:98)A
λ (I − Pm )(cid:107)2 (cid:54) λ(cid:107)A1/2
( (cid:98)A − A)A
( (cid:98)A − A) (cid:98)A

=⇒ (cid:107) (cid:98)A

m,λ (cid:107)2 .
−1/2

(cid:107)A1/2

(cid:107) (cid:54) 1

m,λ ,

−1/2
λ

−1/2
λ

Now using Lemma 22,

(cid:107)A

−1/2
λ

−1/2
λ

(cid:107) (cid:54) 1.

Hence, again using Lemma 22,

and therefore, using Lemma 21,

(cid:107) (cid:98)A

−1/2

2
m,λ ( (cid:98)Am − A) (cid:98)A
λ (cid:98)A
m,λ (cid:107)2 (cid:54) 3.

(cid:107)A1/2

−1/2

−1/2

m,λ (cid:107) (cid:54) 2,

For the second point, this is only a consequence of Lemma 21. Now state two results which show
that
Lemma 28 (Uniform sampling yielding ρ-approximation). Let 0 < ρ (cid:54) 1 and δ > 0. Let b ∈ H.
If { ˜z1 , ..., ˜zm} are sampled uniformly, 0 < λ < (cid:107)A(cid:107), m (cid:54) n and

(cid:18)

(cid:19)

Then it holds, with probability at least 1 − δ :

m (cid:62)

8(cid:107)v(cid:107)2
48
5000
ρ2 N A∞ (λ)
2 +
+
log
.
ρ
λδ
x ∈ LinApprox( (cid:98)Am,λ , b, ρ/4) =⇒ x ∈ LinApprox(Aλ , b, ρ).

L∞ (Z )

In particular, with probability 1 − δ ,(cid:98)A−1

m,λ b ∈ LinApprox(Aλ , b, ρ).

76

Proof.Apply Lemma 9 from [29] with η = ρ
with probability at least 1 − δ ,

12 < 1

2 . We ﬁnd that under the conditions above,

(cid:107) (cid:98)A

m,λ ( (cid:98)A − (cid:98)Am ) (cid:98)A

−1/2

−1/2

m,λ (cid:107) (cid:54) η .

(cid:107)A

−1/2
λ

(cid:107) (cid:54) η ,

−1/2
λ

( (cid:98)A − A)A
( (cid:98)Am − A)A

−1/2
λ

(cid:107)A

Now use Lemma 22 to see that

Thus, we can apply Lemma 24 to get the desired result.

−1/2
λ

(cid:107) (cid:54) (1 + η2 ) − 1 (cid:54) 3η (cid:54) ρ/4.

Lemma 29 (Leverage scores Nystrom sampling yielding ρ-approximation). Let δ > 0.
{ ˜z1 , ..., ˜zm} are sampled using q -approximate leverage scores for t = λ, then if t0∨ 19(cid:107)v(cid:107)2
λ < (cid:107)A(cid:107), and n (cid:62) 405(cid:107)v(cid:107)2
, if

L∞ (Z )

L∞ (Z )

L∞ (Z ) ∨ 67(cid:107)v(cid:107)2

12(cid:107)v(cid:107)2
δ

L∞ (Z ) log

n

log n

2δ <

If

(cid:18)

(cid:19)

Then it holds, with probability at least 1 − δ :

m (cid:62)

8(cid:107)v(cid:107)2
24
13000q2
ρ2 N A (λ)
2 +
+
log
.
ρ
λδ
x ∈ LinApprox( (cid:98)Am,λ , b, ρ/4) =⇒ x ∈ LinApprox(Aλ , b, ρ).

L∞ (Z )

In particular, with probability 1 − δ ,(cid:98)A−1
Proof.The proof is exactly the same as that of the previous lemma, using Lemma 10 instead of
Lemma 9 in [29].

m,λ b ∈ LinApprox(Aλ , b, ρ).

77

