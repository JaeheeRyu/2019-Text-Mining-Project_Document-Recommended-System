9
1
0
2

v
o

N

1
2

]

L

C

.

s

c

[

1
v
5
1
5
9
0

.

1
1
9
1

:

v

i

X

r

a

An analysis of observation length requirements in
spoken language for machine understanding of human
behaviors

Sandeep Nallan Chakravarthula, Brian Baucom, Shrikanth Narayanan, Panayiotis Georgiou

Abstract

Automatic quantiﬁcation of human interaction behaviors based on language information has been
shown to be eﬀective in psychotherapy research domains such as marital therapy and cancer care. Existing
systems typically use a moving-window approach where the target behavior construct is ﬁrst quantiﬁed
based on observations inside a window, such as a ﬁxed number of words or turns, and then integrated over
all the windows in that interaction. Given a behavior of interest, it is important to employ the appropriate
length of observation, since too short a window might not contain suﬃcient information. Unfortunately,
the link between behavior and observation length for lexical cues has not been well studied and it is not
clear how these requirements relate to the characteristics of the target behavior construct. Therefore,
in this paper, we investigate how the choice of window length aﬀects the eﬃcacy of language-based
behavior quantiﬁcation, by analyzing (a) the similarity between system predictions and human expert
assessments for the same behavior construct and (b) the consistency in relations between predictions of
related behavior constructs. We apply our analysis to a large and diverse set of behavior codes that are
used to annotate real-life interactions and ﬁnd that behaviors related to negative aﬀect can be quantiﬁed
from just a few words whereas those related to positive traits and problem solving require much longer
observation windows. On the other hand, constructs that describe dysphoric aﬀect do not appear to be
quantiﬁable from language information alone, regardless of how long they are observed. We compare our
ﬁndings with related work on behavior quantiﬁcation based on acoustic vocal cues as well as with prior
work on thin slices and human personality predictions and ﬁnd that, in general, they are in agreement.

1 Introduction

Behavior assessment/quantiﬁcation is important Human interactions involve a number of social-cognitive
abilities of varying levels of complexity such as speech detection, language understanding and emotion
recognition, to name a few. Among these, the ability to reliably and accurately assess a person’s behavior1
by observing their verbal and non-verbal cues is a considerably complex and important one. Such a skill
is especially important for both delivery and assessment in psychotherapy research domains as varied as
Couples Therapy [1], Addiction Counseling [2] and Cancer Care [3]. In addition to the administration of
therapy by counselors to patients, human experts also evaluate patients’ behaviors during interactions in
order to provide feedback and improve therapy eﬀectiveness. Subsequently, there has been an eﬀort [4] to
automate this behavior annotation (or coding) process using machine learning so that quick and inexpensive
feedback may be provided to therapists. Previous work has shown that automated coding systems are
eﬀective at quantifying behaviors such as Negativity [5–7], Depression [8, 9] and Empathy [10–12]. However,
there are some critical aspects of this behavior assessment process which humans can handle naturally and
easily but machines still cannot, one of which is the notion of how much to observe in order to reliably assess
behavior.

Introduce notion of observation window length for behavior When assessing a person’s behavior
based on their interaction cues, humans look at factors such as the intensity of expression, context and how
frequently the behavior is observed [13]. The latter two imply that an appropriately long window is used to
observe the cues before making a judgment about the behavior; for lexical cues, we measure the length of this
observation window in terms of the number of words spoken. While some behaviors can be assessed based
on short cues, others require observations along longer time-scales. For example, one can sense that a person

1we use the term ‘behavior’ to refer to not just physical actions such as facial expressions, body gestures and speech but the
underlying state of mind that is expressed through these actions.

1

 
 
 
 
 
 
is Angry if they say something as brief as “Shut up !”, but it is diﬃcult to judge whether they are Engaged
in a discussion or not unless a longer and more involved conversation is carried out. Based on this, it is
intuitive to expect that evaluating diﬀerent behaviors would require diﬀerent observation window lengths.
Such associations have been exhibited by humans when judging human characteristics such as personality
traits [14], non-verbal behaviors [15] and group dynamics [16]. However, it is not clear as to how they
manifest in behaviors or in automated systems that quantify behaviors based on interaction cues. Unlike
emotions, which are simple and rapid [17] and can be reliably observed using short observation windows
such as a few seconds [18], a sentence [19] or a speaker turn [20]), the rich variety of human behaviors can
be much more complicated and long-ranging. Even expert coders in the ﬁeld of psychotherapy research
typically ﬁrst have to be trained according to domain-speciﬁc guidelines or manual (CIRS [21], MITI [22])
before they can start coding patients’ behaviors. This complexity can potentially give rise to uncertainty at
the time of assessment, which then necessitates longer observations in order to achieve conﬁdent and reliable
annotation. Furthermore, the annotation time-frames for coding diﬀerent behaviors can range from as small
as 30 seconds [23] to as long as 10 minutes [21], demonstrating the potential variability in observation lengths.
These facets of behavior coding demonstrate the need for as well as the potential value of a comprehensive
investigation into this topic.

Figure 1: Automated quantiﬁcation of behavior using a moving-window approach: This ﬁgure shows a typical
process for automatically quantifying behavior from lexical cues. First, during an interaction, interlocutors
express multiple behaviors, one of which is Bi , through multiple cues, one of which is language. This language,
encoded as a transcript, is decomposed into its constituent windows of text, based on the window length L.
Then, model M is used to score the text inside each window, resulting in a tra jectory of window-level scores.
Finally, a functional F is applied on the tra jectory to summarize it as a single interaction-level score

Knowledge of observation window length is important for automated behavior systems
More importantly, the knowledge of how long an automated system should observe an interaction is vital
in applications that rely on moving-window approaches. Such approaches, as shown in Figure 1, typically
ﬁrst compute a “local” behavior score within a ﬁxed-length observation window, such as a few words or
speaker turns, and then aggregate local scores from all the windows to obtain a “global” score for behavior.
These are especially useful in psychotherapy research settings [7, 10, 11] where automated systems need to
analyze “sessions”, or interactions, at ﬁne-grained scales, such as turn-level, but can only be evaluated at
session-level. In such situations, the choice of length of the observation window is important; too short a
window can result in noisy or incorrect local scores, since insuﬃcient information is being used, and as a
result, the global summary score will be inaccurate, as illustrated in the toy example in Figure 2.

Another scenario where knowledge of the observation length is critical is in multi-speaker interactions
where both speakers exhibit and inﬂuence each others’ behaviors. For example, consider an interlocutor and
their partner discussing an important problem, where the interlocutor expresses indiﬀerence towards the
problem, causing the partner to become angry and yell. While it is possible for a system to detect that the
partner is angry just by observing their last speaking turn, in order to understand that the interlocutor was
being indiﬀerent, it might need to observe several previous turns from both speakers. Recognizing diﬀerent
behaviors might thus require observations over diﬀerent time-scales.

What is the main contribution of this paper ? In this work, we present a systematic analysis
of how long the observation window of a system should be when quantifying behavior and how this varies
for diﬀerent behaviors. Speciﬁcally, we are interested in empirically identifying the minimum amount of
language information, measured in number of words, from which a behavior can be judged. By analyzing
how the quality of language-based behavior quantiﬁcation varies with the length of the observation window,
we address the following questions:

2

SpeakerPartnerB1B2BiFacial ExpressionsAcousticsLanguage......Elang(Bi)BehaviorsInteractionCuesWindowed CuesWindowLengthLjthwindowM(;ϴ)Window-levelBehavior ScoresInteraction-levelBehavior ScoreModelFunctionalFFigure 2: Toy example il lustrating the eﬀect of judging how Supportive the statement ”honey it’s not your
fault please” is at diﬀerent observation window lengths. When at least 5 words are used, the judgments are
accurate. At shorter windows, however, the results are incorrect and noisy across windows; this is especial ly
evident in 1-word windows where each word on its own cannot provide suﬃcient behavior information

1. Do diﬀerent behaviors need observations of diﬀerent lengths in order to be quantiﬁed from language

2. Is the nature or type of behavior related to the length of its required observation window

3. Are the observation length requirements for behaviors in the lexical modality similar to those in the
acoustic modality

Addressing these requires us to evaluate the system’s behavior scores at multiple window lengths, even
when ground-truth annotations might not be available to directly compare against. While evaluating
the aggregate score serves as an indirect solution to this problem, it is still limited by the choice and
appropriateness of the aggregating mechanism. For this purpose, we propose an additional metric that
evaluates with respect to the relations between the ground-truth behavior annotations. For instance, we
know that humans perceive Happiness to be similar to Satisfaction but opposite to Sadness ; ideally, an
automated system’s scores of Happiness, Satisfaction and Sadness would exhibit these relations as well. By
investigating how these metrics change with window length, we aim to identify the appropriate amounts of
language required to quantify each behavior. We also compare our ﬁndings with existing work in psychology
and machine learning and identify some general trends based on the observed similarities in results. For our
analysis, we choose the Couples Therapy corpus [1] which contains real-life interactions coded for a large
and diverse set of behaviors.

Paper roadmap This paper is structured as follows: In Sec. 2, we describe existing work in psychology
and speech processing that has addressed similar problems to ours and explain how our scope and approach
are diﬀerent from those eﬀorts. Sec. ?? contains the details of our analysis framework where we describe
our metrics and explain how we use them to arrive at conclusions about the amount of information required
to judge behaviors. Next, in Sec. 4, we describe the experimental setup used to score text for behavior
constructs, followed by Sec. 5, which describes the Couples Therapy corpus of interactions and behaviors on
which we conduct our analysis. In Sec. 6, we present the results of our analysis, followed by a discussion in
Sec. ?? of how they address the questions posed in this section. Finally, in Sec. 7, we present our conclusions
and list potential improvements and extensions to this work.

2 Related Work

Summary of thin-slices work A body of work in psychology that is related to, but not the same as,
our notion of “window length” is the one which studies Thin Slices of observed behavior [24]. It refers to
excerpts or snippets of an interaction that can be used to arrive at a similar judgment of behavior to as if the
entire interaction had been used. Essentially, it implies that an entire interaction can be replaced with just
a windowed part (which is diﬀerent from our aim of identifying the best window through which to view the
entire interaction). The eﬀect of the location of these slices has been investigated as well; the conventional
approach is to situate the slices near the start of the interaction. The eﬀectiveness of thin slices has been
observed in many applications, ranging from judging personality traits [14, 25] such as the “big ﬁve” [26] to
viewer impressions of TED talks [27] such as “funny” and “inspiring”.

Example of related thin-slices work
Notably, Carney et al. [28] studied the accuracy of impressions for Aﬀect, Extraversion and Intelligence at
diﬀerent thin-slice durations, locations, etc. Accuracy was measured as the correlation between the true value

3

HONEY IT’S NOT YOUR FAULT PLEASEFULLHONEY IT’SNOTYOUR FAULTPLEASE1-WORDIT’S NOT YOUR3-WORDHONEY IT’S NOT YOUR FAULT5-WORDIT’S NOT YOUR FAULT PLEASENOT YOUR FAULTYOUR FAULT PLEASEHONEY IT’S NOTVerySupportiveNotSupportiveSomewhatSupportiveof a construct (whether rated or self-reported) and the impression based on the thin slice, and it was observed
that, in general, accuracy increased as the slice length increased from 5 seconds to 5 minutes. Furthermore,
they found that Negative aﬀect could be assessed with similar accuracies at all slice lengths whereas Positive
aﬀect was best assessed only when thicker slices were used. These works provide an encouraging support for
analyzing the window length of behaviors along similar lines.

Similar BSP works, novelty Some works that are closely related to ours are those that have investigated
the accuracy of behavior prediction using acoustic vocal cues. Xia et al. [29] found that as the observation
window used to compute features was increased from 2 seconds to 50 seconds, the classiﬁcation accuracy
generally improved, with Negative and Positive behaviors gaining the most. Similar eﬀects were reported by
Li et al. [30] where they varied the receptive ﬁeld of a 1-dimensional Convolutional Neural Network-based
behavior classiﬁer from 3.75 seconds to 60 seconds and found that classiﬁcation improved for many behaviors,
but mainly for Negative and Blame. Other eﬀorts have addressed related aspects; for instance, Lee et al. [31]
examined whether the behavior annotation process is driven more by a gradual, causal mechanism or by
isolated salient events, which imply the use of long and short observation windows respectively.

While these works have contributed to a better understanding of the eﬀect of observation windows, they
are limited in the variety of constructs that are analyzed. In addition, they mostly focus on acoustic and vocal
cues and not enough on the linguistic characteristics. Hence, the novelty of our work lies in (1) analyzing the
eﬀect of observation lengths in the lexical modality and (2) performing this analysis using a large and diverse
set of real-life human behavior constructs. Through our analysis, we aim to uncover the relation between
the nature of the behavior and how long an automated system needs to observe the language through which
it is being expressed.

3 Problem Statement

Let’s suppose that we want to quantify the behavior content of behavior Bi in a set of data samples D, where
each sample is a sequence of words. Let Ai ∈ R|D| be the ground-truth annotations of Bi in D and let C be
a metric that is used to evaluate the quantiﬁcation results against Ai ; the higher the value of C, the better
the quantiﬁcation.

Let Elang (Bi ) represent the degree to which Bi is actually expressed in language. Let M denote a machine
learning model (e.g. Deep Neural Network) that estimates a scalar value from a sequence of words and θ
represent its learnable parameters (e.g. weights). Finally, let L represent the window length at which Bi is
observed in language and F denote a statistical functional that maps a sequence of scalar values to a single
scalar value. Then, the quality of behavior quantiﬁcation can be expressed as:
Qi = C (F (M (Elang (Bi ), L, D ; θ)), Ai )
Statement: Our goal is to identify the best window length L∗
i , for each Bi , at which Qi is maximized:
i = arg max

=⇒ L∗

(1)

Qi

L

= arg max

L

C (F (M (Elang (Bi ), L, D ; θ)), Ai )

(2)

We argue that Qi can be high only when Elang (Bi ), L, M , θ and F are all appropriate together. If even
one of them is ﬂawed or incompatible with the rest, then it would adversely aﬀect Qi , as explained below:
• Elang (Bi ): The behavior Bi must be suﬃciently expressed in the lexical channel to begin with;
otherwise, it might not be possible to observe it using lexical cues alone (for example, if it is mainly
expressed through nonverbal vocal cues).
• L: The observation window must be long enough to observe Bi ; otherwise, the incomplete information
from partial observations can lead to noisy or incorrect estimates.
• M : The model must be well-suited for capturing Bi ; otherwise, it might be severely limited in how
well it can perform. For example, quantifying a behavior that is based on the actions of both speakers
requires a model that looks at both speakers; using a single-speaker model instead might provide
incomplete information and, hence, inaccurate estimates.

4

• θ: The model must be well-trained; otherwise, its estimates might be inaccurate. This is dependent
on the training process, the amount and quality of data used to estimate parameters, etc.
• F : The aggregating functional must be well-suited for summarizing Bi ; otherwise, the resulting overall
scores might not match the ground-truth annotations Ai . For example, the functional mode, which
identiﬁes the most frequently occurring value, might not be appropriate for summarizing a behavior
that is expressed very infrequently.
Therefore, we reason that a high value of Q is indicative of all the aforementioned factors being appropriate
and a low value is indicative of a ﬂaw in at least one of these factors. Based on this, we now proceed to
analyze the variation in Q for diﬀerent behaviors as window length L is varied.

4 Methodology

Analysis roadmap This section provides the details of our methodology, starting with the process for
creating windowed samples of language from the interaction transcripts. Then, we describe the machine
learning models used in this work to score the window-level text samples for degrees of diﬀerent behavior
constructs. Following this, we describe two metrics for evaluating the quality of the models’ window-level
scores and, ﬁnally, we detail the procedure for employing these metrics to identify the appropriate window
lengths for diﬀerent behaviors.

4.1 Windowed Scoring of Text

How text is windowed To score the text Ti of an interaction containing Oi words using an observation
window of length Lj , we ﬁrst decompose it into its constituent windows. If Oi > Lj , then we get Oi − Lj + 1
using model M . Assuming that Ti = {¡s¿w1 , w2 , . . . , wOi ¡/s¿}, where ¡s¿ and ¡/s¿ represent start and end
windows, each containing Lj words; else, we get just one window. Then, each window is scored independently
respectively, there are Oi distinct observation window lengths at which it can be scored, as shown in the
ﬁrst column of Table. 1.

Observation Window
interaction-length
Oi -1 word

2-word
1-word

Window Decomposition
{<s>w1 ,w2 , ... wOi </s>}
{<s>w1 ,w2 , ... wOi−1 }, {w2 ,w3 , ... wOi </s>}
{<s>w1 ,w2}, {w2 ,w3}, ... {wOi−1 ,wOi </s>}
...
{<s>w1}, {w2}, ... {wOi </s>}

No. of Scores Resolution
1
Very Coarse
2
Coarse

Oi -1
Oi

Fine
Very Fine

Table 1: Scoring a sentence with Oi words using observation windows of diﬀerent lengths

The window with the coarsest resolution is the “interaction-length” window since it scores the entire
interaction as a whole, resulting in a single score for Ti . On the other hand, the “1-word” window provides
the ﬁnest resolution possible since a score is generated for each word of the sentence, resulting in a tra jectory
of scores for Ti . In this work, we test the following observation window lengths: {3, 10, 30, 50, 100} where 3
represents a 3-word window and interaction represents a window spanning the entire length of the interaction.
We qualitatively refer to 3 and 10 as “short” windows, 30 as “medium”-length and 50 and 100 as “long”
windows.

4.2 Behavior Quantiﬁcation Models

Why try diﬀerent model From Eqn. 2, we see that the analysis of window lengths depends on the model
M ; therefore, it is possible that the results of our analysis might be diﬀerent for diﬀerent choices of M . In
such a scenario, running an analysis using just one modeling framework might be limiting, since the results
might be reﬂective of the model and not necessarily the behaviors. This can be addressed by testing diﬀerent
models and observing traits that are speciﬁc to each model as well as traits that are consistent across diﬀerent
models, thereby providing a more comprehensive understanding about how behaviors are expressed through
language and how they are aﬀected by the observation window length. Hence, in this work, we conduct our
analysis using two diﬀerent modeling frameworks: (1) Maximum Likelihood N-gram models and (2) Deep
Neural Networks, the details of which are provided in this section.

5

4.2.1 Maximum-Likelihood N-gram
• Model
N-gram LM probability of text We use a Maximum Likelihood method closely following [32, 33]
where N-gram Language Models (LMs) are employed to map text onto a scalar value. N-gram LMs
compute the joint probability of a sequence of words or, equivalently, the probability of a word given
the context of the preceding n−1 words. Given a sequence of M words W = {w1 , w2 , . . . , wM }, its
N-gram probability is given by:
P (W ) = P (w1 , w2 , . . . , wM ) ≈ M(cid:89)

P (wj |wj−1 , wj−2 , ...wj−n+1 )

(3)

j=1

How to create successive binary partitions of data We assume that the text corpus is annotated
using ratings in the interval [1,K ] where 1 indicates the lowest degree (e.g. “absence of behavior”) and
K indicates the highest degree (ex: “strong presence of behavior”). First, we partition the corpus into
Ai and Ai such that Ai contains text whose annotations lie in the interval [1,i+1 ] while Ai contains
those in the interval (i+1,K ]. Then, we train N-gram LMs on Ai and Ai to obtain a binary classiﬁer,
denoted as the ith pair. We repeat this for all integer-thresholded partitions {Ai , Ai }∀ i ∈ {1, K − 2},
shown in Table. 2, to obtain K -2 binary classiﬁer pairs.
Pair \Rating
1st
2nd

. . .
A1

1 - 2
A1

(K-1) - K

2 - 3

A2

(K -2)th

AK−2

A2
. . .
AK−2

Table 2: Integer-thresholded partitions for behavior ratings ranging from 1 to K

How to get likelihoods from binary partitions For a text sequence W , whose behavior score, x,
we want to ﬁnd, the ith pair provides likelihoods PAi (W ) and P ¯Ai (W ):
PAi (W ) ≡ P (W | 1 ≤ x ≤ i+ 1)
P ¯Ai (W ) ≡ P (W | i+ 1 < x ≤ K )

(5)

(4)

How to get posteriors from likelihoods From these, we calculate the posterior probability of each
cumulative interval using Bayes Rule, as shown in Eq. 6. For the prior probabilities, we don’t use
any domain-speciﬁc knowledge about the behaviors of interest, instead assuming them to be uniformly
distributed i.e. X ∼ U nif orm(1, K ). We then compute posteriors for each adjacent interval using
Eq. 7, resulting in a probability mass function of the behavior score:
P (x ≤ i+ 1 | W ) ≡ P (1 ≤ x ≤ i+ 1 | W )
PAi (W )P (1 ≤ x ≤ i+ 1)
=
PAi (W )P (1 ≤ x ≤ i+ 1) + P ¯Ai (W )P (i+ 1 < x ≤ 9)
P (i < x ≤ i+ 1 | W ) = P (x ≤ i+ 1 | W ) − P (x ≤ i | W )

(6)

(7)

How to get score from posteriors Since the ith point in Eq. 7 denotes the interval [i,i+1 ], we
represent it using its midpoint i+ 1
2 . Finally, the behavior score x of text sequence W is computed as
simply the expected value:

K−1(cid:88)

i=1

x =

(i +

)P (i < x ≤ i+ 1 | W )

1
2

(8)

• Training
Speciﬁc implementation details Similar to previous work [34], we implement Maximum Likelihood
models through 3-gram LMs trained with Good-Turing discounting using the SRILM toolkit [35]. A

6

leave-one-couple-out scoring scheme is used where models are trained on data from N − 1 couples and
subsequently used to score data from the N th couple.
Ideally, we would train and test a separate
model at each window length; for example, the Ngram model at window length 100 would be trained
on sequences of 100 consecutive words. However, this is infeasible due to the curse of dimensionality in
Ngram models, where the amount of data required for training increases exponentially with the order
N [36]. Therefore, we train a single 3-gram model, i.e. at window length 3 words, and use the same
model for testing at all ﬁve window lengths. As we show in Sec. ??, this diﬀerence between training
and testing window lengths does not result in automatic degradations at longer window lengths.

Algorithm 1 Similarity-based Grouping of Behaviors
1: Calculate Rglobal = RG as in Algorithm. 3
2: for number of clusters N in {2, 3, . . . K } do
for D random initializations do
Run K-Means clustering on Rglobal with N clusters, store clustering C D
end for
Pick most frequent clustering CN = arg max

3:
4:
5:
6:

N

7:

Count(C D
N )
Calculate size disparity SN = Range(cluster sizes in CN )
8: end for
9: Pick Behavior Grouping C = arg min

D

SN

N

4.2.2 Neural Estimation
• Model
Description of model We use a recurrent modeling framework similar to the one used by Tseng et
al. [37] for classifying Negative behavior from language, but with a few changes: the Long Short-Term
Memory [38] unit is replaced with a Gated Recurrent Unit (GRU) [39] since it is similar but with fewer
trainable parameters.
In addition, we replace the word2vec [40] embeddings with ELMo [41] since
it oﬀers the advantages of deeper, contextual and character-level representations. Finally, while [37]
post-processed the system outputs using Support Vector Regression, we eschew such transformations
since we are interested in analyzing the properties of the system outputs themselves.

At runtime, given a sequence of K words W = {wj , wj+1 , . . . , wj+K−1} words from the j th observation
window, we pass their embeddings M ∈ RK xU through a GRU to obtain a ﬁxed-length hidden
representation hK ∈ RV for that window, where U and V are the dimensions of ELMo and the GRU
hidden representation respectively. The representation hK is then passed through a fully connected
layer, followed by a Relu6 [42] layer, resulting in a scalar value that represents the behavior score of
the j th window. We use the ReLU6 layer in order to ensure that our predictions are bounded, similar
to the ground truth annotations A. Our neural estimation model is shown in Figure 3.
• Training
Description of training process

We trained and tested our Neural model at two window lengths: 3-word (i.e. 3 unrolled time steps),
representing a short window, and 30-word (30 unrolled time steps), representing a medium-length
window, with zero padding performed at the end as required. We also attempted to train at long
windows (50 and 100 words) but were unable to obtain well-trained, converged models for all the
behaviors; hence, we only show results for 3-word and 30-word in this paper. A separate model was
trained for each behavior construct, without any shared parameters or layers, so as to ensure that the
results for each construct would be indicative of that construct only.

For every word, ELMo provides embeddings from 3 layers, each of size U = 1024, and their corresponding
mixing weights; we obtain a single 1024-dimensional embedding through a scaled, weighted sum of all
3 embeddings as recommended by Peters et al. [41]. These mixing weights are softmax-normalized,
similar to attention [43] weights, and, along with a scaling factor, were trained using backpropagation,
similar to all other parameters. The size of the GRU hidden representation V was tuned to be either

7

Figure 3: Neural model that estimates the behavior score of a sequence of words in a K -length observation
window

10 or 100 and the sample minibatch size was set to 64. Dropout [44] of 0.2 was applied before the fully
connected layer and we used L1 loss in conjunction with Adam [45] optimizer to train our models.

We initially used grid search for tuning the learning rate but it was observed that the rates at which
training remained stable varied greatly across diﬀerent behaviors. Addressing this in a task-agnostic
manner would necessitate a large search space and exacerbate the time and computational cost of
training. Hence, we replaced the grid search with a task-speciﬁc tuning scheme; speciﬁcally, we
implemented the Cyclical Learning Rate schedule as proposed by Smith [46]. For each behavior and
model conﬁguration, we ﬁrst performed a “range test” to determine the minimum and maximum
learning rates at which training remained stable. We then cyclically varied the learning rate between
its minimum and maximum value during training, saving a model checkpoint at the end of every
epoch when the learning rate would be at its lowest. Finally, at testing time, inspired by Huang et
al. [47], instead of using just the last or the best checkpoint, we used an ensemble average of all of them.

To further reduce the time and computational costs of training, we employed a 6-fold nested cross-validation
setup where in every test fold, four folds were used to train the model while the ﬁfth fold was used
to optimize the model hyper-parameters, learning rate range, etc. Similar to the setup with Ngram
models, we ensured that no dyad appeared in more than one fold.

4.3 Analysis

4.3.1 Metrics

1. Behavior Construct Similarity

What is BCS ? Our ﬁrst proposed metric, the Behavior Construct Similarity (BCS), is a direct
representation of Eqn. 1, with Spearman’s Correlation R as the choice of the evaluation metric C. For
behavior Bi and window length Lj , it is computed as:

BC Si (j ) = R(F (M (Elang (Bi ), Lj , D; θ)), Ai )

(9)

BCS measures how similar the system’s interaction-level scores are to the ground truth annotations,
over all the data samples; the higher the value, the more similar the two are. Hence, by observing how
it varies with window length, we can obtain an idea of which ones are appropriate for each behavior.
The procedure for computing BCS is detailed in Algorithm. 2.

We use Spearman’s Correlation since it captures monotonic similarity and is, thus, apt for relating
together ordinal variables such as the ground truth annotations Ai . For the functional F, we test 3
statistics: median, minimum and maximum ; the former because it has been shown to be useful for

8

GRUiGRUiFCih0h1hK...Estimated scoreof behavior Biin jthwindowword embedding sequence in jthwindowELMoembeddingsGRU hiddenrepresentationsDropoutReLU6mjmj+K-12:
3:
4:
5:

Algorithm 2 Behavior Construct Similarity
1: for each behavior Bi in B do
for each window length Lj in L do
for each functional Fk in F do
Initialize empty lists G, H
for each interaction Tl in T containing Ol words do
Score Tl at window length Lj to get tra jectory of scores S = {S1 , S2 , . . . Smax(1,Ol−Lj +1) }
Compute aggregate score Fk (S)
Append Fk (S) to G
Append ground-truth annotation Al
i to H
end for
Compute Spearman Correlation Rk (j ) between G and H
end for
BC Si (j ) = {Rk (j ) ∀ k}
end for
15: end for

6:
7:
8:
9:
10:

11:
12:
13:
14:

summarizing behaviors such as Negative in previous works [37, 48] and the latter two since they can
capture characteristics such as frequent/rare expression, large/small ﬂuctuations, etc. We did not use
the mean because in some instances, we observed that the system’s window-level scores were α-stable
distributed with α > 1, for which the mean is undeﬁned; the other three statistics, however, are still
deﬁned.

2. Behavior Relationship Consistency

Limitation of BCS The BCS metric performs a direct validation of the window-level scores by
evaluating their aggregate against ground-truth annotations. However, this means that it is limited by
the choice of the functional F used for aggregation, since a poor choice of F can result in inaccurate
results and, hence, low BCS, even if a suﬃciently long window is used. In such a scenario, relying on
BCS alone would lead to an incorrect conclusion that the window length is still not suﬃcient. One way
to resolve this would be to perform a similar evaluation at the window-level, without any aggregation,
but it is not possible since we do not have ground truth annotations at the window-level to compare
against.

Motivation for BRC Instead, we propose to evaluate predictions of constructs based on whether they
are related to each other the same way as the constructs themselves. This is motivated by previous
work by Thornton et al. [49] who, while studying emotional transitions, found that similar emotions
(e.g. Anger and Disgust ) frequently tend to co-occur whereas dissimilar ones (e.g. Anger and Joy ) do
not. Extending this principle to our scenario, we argue that if our system is indeed able to accurately
quantify related behaviors, then we should be able to observe those same relations in the predictions
as well. The more consistent the relations between the predictions are with the relations between the
actual behaviors, the more accurate we can consider the window-level predictions, and hence the more
appropriate the window length, to be.

Deﬁnition of BRC We formalize the above intuition in the form of our second proposed evaluation
metric, the Behavior Relationship Consistency (BRC). The BRC metric is deﬁned for a pair of
constructs and measures how close the Spearman Correlation between their window-level scores is,
over all the data samples, to the Spearman Correlation between their ground-truth annotations; the
full procedure is detailed in Algorithm. 3. For a pair of behaviors Bi and Bj and window length Lk ,
it is calculated as:
BRCi,j (k) = 1 − |Q∗
where Q∗
i,j = R(Ai , Aj )
and Q
i,j (k) = R(M (Elang (Bi ), Lk , D; θ), M (Elang (Bj ), Lk , D; θ))

i,j − Q

i,j (k)|

(10)

(cid:48)

(cid:48)

2

9

2:
3:
4:
5:

6:
7:

8:

Algorithm 3 Behavior Relationship Consistency
1: for each behavior Bi in B do
for each behavior Bj in B do
for each window length Lk in L do
Initialize empty lists Ci , Cj , Gi , Gj
for each interaction Tl in T containing Ol words do
Score Tl at window length Lk to get scores
l = {S i
S i
1 , S i
2 , . . . S i
l = {S j
S j
1 , S j
2 , . . . S j
Append S i
l to Ci , S j
l to Cj
Append ground-truth annotation Al
i to Gi , Al
i to Gj
end for
Compute Spearman Correlations RC (i, j ) between Ci and Cj , RG (i, j ) between Gi and Gj

max(1,Ol−Lk+1) } for Bi ,
max(1,Ol−Lk+1)} for Bj

9:
10:
11:

12:

BRCi,j (k) = 1 − |RC (i,j )−RG (i,j )|
2

13:
14:
15:

end for
end for
16: end for

(cid:48)

i,j in Eqn. 10 is similar to Qi from Eqn. 1; both evaluate
Limitation of BRC It can be seen that Q
the window-level scores of behavior Bi , but the former evaluates against predictions of behavior Bj
whereas the latter evaluates against the ground truth annotations of behavior Bi . Naturally, this means
that the eﬀectiveness of BRC for Bi is directly dependent on the accuracy of predictions of Bj ; the
more accurate they are, the more eﬀective it will be. For this reason, BRC is an indirect validation
of the window-level scores, unlike BCS. However, BRC can be computed directly on the window-level
scores without involving any kind of aggregation; hence, it does not suﬀer from the same limitation as
BCS. In the following section, we describe how we perform our analysis using both BCS and BRC in
a manner that combines their advantages while mitigating their disadvantages.

4.3.2 Procedure

Now that we have deﬁned the metrics with which to analyze behaviors, we proceed to employ them in the
following multi-stage fashion, depicted in the ﬂowchart in Figure 4.
• Stage 1: We start by identifying behaviors for which all the factors listed in Sec. 3, such as window
length, functional, etc. are appropriate. For such behaviors, their estimates by the system are highly
correlated with human ratings at all window lengths. Therefore, given behavior Bi and a set of
window lengths L being tested, we check whether its BCS, which was deﬁned in Eqn. 9, is greater than
a threshold Y1 at all lengths. The higher this threshold Y1 , the better estimated Bi is considered to be;
in our work, we used Y1 = 0.59 since the highest BCS that both of our models achieved was around
0.6.

Identify Bi : BC Si (j ) > Y1 ∀j ∈ L

We refer to these behaviors as “reference” behaviors, denoted using Bref . Since they are considered to
be estimated well at all window lengths, we set the appropriate window length to be the shortest one
in L.

Window length Li = min{ L }
• Stage 2: In Stage 2, we focus on behaviors whose BCS did not exceed Y1 , i.e. they were either low or
medium. This could be attributed to multiple factors - inappropriate window length, ill-suited model,
insuﬃcient expression of behavior in language, etc. However, with all other factors kept constant, any
change in BCS must be solely due to the eﬀect of the window length. Therefore, in this stage, we
observe how the BCS of a behavior changes over the window lengths in L and pick the one at which it
is maximum. Speciﬁcally, for each behavior in Stage 2, we check if its maximum BCS is signiﬁcantly

10

Figure 4: Flowchart depicting the step-by-step procedure through which we analyze the appropriate window
lengths for a target behavior Bi . First, in stages 1 and 2, we examine whether the BCS metric, which
describes how well Bi ’s predictions by the system match its annotations by human experts, contains clues
about window length. If not, then in stages 3 and 4, we check whether the BRC metric, which describes
how much the system’s predictions resemble human expert annotations in terms of Bi ’s relations with other
behaviors, can provide the required information.
If not, we make no determinations about Bi ’s window
length. Blue boxes represent standard operations while green boxes represent ﬁnal conclusions from our
analysis

larger than its minimum BCS.
Identify Bi : max{BC Si (j )} > min{BC Si (j )} (signiﬁcant, p < 0.05)
Window length Li = arg max
BC Si (j )

j

We check statistical signiﬁcance by following the recommendations outlined by Diedenhofen et al. [50].
Since we are comparing correlations between ground truth annotations and system estimates from the
same data at diﬀerent window lengths, they are considered to be dependent overlapping correlations.
Accordingly, we calculate the 95% conﬁdence interval for diﬀerences in dependent overlapping correlations
using Zou’s method [51]: the change in BCS from minimum to maximum is signiﬁcant if the interval
does not contain 0; else, it is not signiﬁcant.

11

Li*= argmax BCSi(j)Is there significant variation in BCSi(j) ?Select behavior BiSTARTCompute BCSi(j) at all window lengths j ЄLIs BCSi(j) > Y1at all window lengths j ЄL?Designate Bias reference behavior BrefLi*= min LYESCompute weighted BRCi(k) with Brefbehaviors at all window lengths kЄLYESNONOIs weighted BRCi(k) > Y2at any window length k ЄL?Li*= min {k : BRCi(k) > Y2}YESIs there significant variation in BRCi(k) ?YESNOCould not determine window length Li*  for BiNOjLi*= argmax BRCi(k)kStage 1Stage 2Stage 3Stage 4StartEnd• Stage 3: For those behaviors Bi whose BCS neither exceeded Y1 nor signiﬁcantly changed with window
length, we instead examine whether their BRC, which was deﬁned in Eqn. 10, is high at any window
length. Speciﬁcally, we check if it is higher than a threshold Y2 and pick the shortest window length
at which this is true.

Identify Bi : ∃ k ∈ L : BRCi (k) > Y2
Window length Li = min{ k ∈ L : BRCi (k) > Y2}

A value of BRC greater than Y2 indicates that Bi exhibits similar relations in the system’s scores as in
the ground truth annotations. Y2 should be as close to 1 as possible, since we would like our system’s
scores to exhibit exactly the same relations as human annotations; in our work, we use Y2 = 0.95.
BRCi (k) is computed with respect to the Bref behaviors that were identiﬁed in Stage 1 since they
are well-estimated and are, thus, more reliable than other behaviors. In cases where Bref consists of
multiple behaviors, we compute BRCi (k) as a weighted sum of individual BRCs, as shown below:

(11)

(12)

(cid:88)
(cid:80)

Bl∈Bref

BRCi (k) =

where αl (k) =

αl (k)BRCi,l (k)

BC Sl (k)
BC Sm (k)

Bm∈Bref

We see that for every behavior in Bref , its BRC with Bi is scaled by a normalized weight that
is proportional to its BCS. This is done in order to assign greater importance to better estimated
behaviors than worse ones, thereby increasing the reliability of the resulting BRC metric.
• Stage 4: This is the last stage of our analysis and the behaviors we are left with are those with low
and unchanging BCS as well as low BRC. While we could not identify appropriate window lengths (at
which BRC is very high), it is nevertheless useful to understand which lengths are better or worse than
others. Therefore, similar to Stage 2, we examine whether the BRC of Bi shows a tendency to increase
or decrease as window length changes and identify where it is maximum.
Identify Bi : max{BRCi (k)} > min{BRCi (k)} (signiﬁcant, p < 0.05)
Window length Li = arg max
BRCi (k)

k

Since we are comparing correlations between system estimates of diﬀerent behaviors at diﬀerent window
lengths, these fall under the category of independent groups [50]. Similar to Stage 2, we check statistical
signiﬁcance using 95% conﬁdence intervals as described in [51], but this time, for diﬀerences between
independent correlations.
• End: For behaviors that do not show a signiﬁcant change in BRC in of Stage 4, we do not analyze
them any further and simply conclude that we are unable to make any determinations at this point
about their window length properties.

In the next section, we describe the data corpus on which we apply our analysis.

5 Dataset

Why use Couples Therapy corpus ? The Couples Therapy pro ject [1] involved 134 real-life chronically
distressed couples attending marital therapy over a period of up to 1 year. Its dataset consists of hundreds
of realistic interactions as well as a rich and diverse set of annotations characterizing the behavior of the
participants in these interactions. This oﬀers an attractive and challenging domain for behavior analysis,
which is why we select it as the area of focus for this paper. We note, though, that the analysis and techniques
presented in this work can be also applied to other domains involving machine learning for human-derived
and human-centered attributes, such as emotion and sentiment.

12

5.1 Description of Corpus

Interaction and coding setup The corpus consists of audio and video recordings with manual transcriptions
of each couple discussing topics of marital distress in 10-minute interactions. Each couple had at least 2
interactions or “sessions”, once with each participant leading the discussion on a topic of their choice, and
the total number of sessions per couple ranged from 2 to 6. In each session, both the husband and the wife
were rated for a total of 31 CIRS [21] and SSIRS [52] behaviors by trained human annotators with a sense of
what “typical” behavior is like during these interactions. The annotators were asked to observe both verbal
and nonverbal expressions when rating each behavior independently and in many cases, diﬀerent annotators
rated diﬀerent behaviors. Each behavior in each session was rated by 3 to 9 annotators, with most of them
being rated by 3 to 4. The rating was done on a Likert scale from 1 to 9, where 1 represents “absence of
behavior”, 5 represents “some presence of behavior” and 9 represents “strong presence of behavior”. More
details about the recruitment, data collection and the annotations can be found in [1, 13].

Figure 5: Histogram of number of words per speaker per session

Speciﬁcations of data used in this paper Consistent with previous work [5, 7, 53–55], for each
participant and behavior, we take the average of the annotators’ ratings as the true rating in that session.
Therefore, each speaker’s data sample consists of the manual transcription of their utterances and their
behavior ratings in that session. Of the 31 behaviors that were rated, we discard 4 of them, such as “Is
the topic of discussion a personal issue ?” and “Is the discussion about husband’s behavior ?”, since they
are tied more to the topic of interaction and less to the couples’ behavior. We further consider only those
sessions where speakers are rated for all the 27 remaining behaviors, resulting in 1325 data samples. Since
the content and nature of interaction vary from one couple to another, the number of words spoken by a
person during the whole session ranges from around 50 to 2500, as shown in Figure 5.

5.2 Description of Behavior Codes

The 27 behaviors that will be analyzed in this work are described below:

Description of CIRS behaviors The Couples Interaction Rating System 2 (CIRS2) describes a spouse
when interacting with their partner about a problem. It consists of the following 13 behaviors:
• Acceptance: Indicates understanding, acceptance, respect for partners views, feelings and behaviors
• Perspective: Tries to understand partners views, feelings by clarifying and asking to hear them out
• Responsibility: Implies self-power over feelings, thoughts, behaviors on issue being discussed
• External: Softens criticism of partner by attributing their undesirable behaviors to external origins
• Deﬁne: Articulates problems clearly, facilitates everyone’s participation in problem solving process
• Solution: Suggests speciﬁc solutions that could solve the problem
• Negotiates: Oﬀers compromises or bargains
• Agreement: States terms of agreement, willingness to follow them with partner
• Blame: Blames, accuses, criticizes partner and uses critical sarcasm and character assassinations
• Change: Requests, demands, nags, pressures for change in partner
• Withdrawal: Generally non-verbal, becomes silent, refuses to respond, discuss, argue, defend

13

• Avoidance: Minimizes importance and denies existence of problem, diverts attention, delays discussion
• Discussion: Discusses problem, shows engagement, interest and willingness in discussing issue

Description of SSIRS behaviors The Social Support Interaction Rating System (SSIRS) measures
emotional features of the interaction and the concrete ratings of the topic of conversation. It consists of the
following 14 behaviors:

1. Aﬀectivity Ratings
• Positive: Overtly expresses warmth, support, acceptance, aﬀection, positive negotiation
• Negative: Overtly expresses rejection, defensiveness, blaming, and anger
• Anger: Expresses anger, frustration, hostility, or resentment during the interaction
• Belligerence: Quarrels, argues, verbalizes nasty comments and mean rhetorical questions
• Disgust: Shows disregard, scorn, lack of respect and makes patronizing and insulting comments
• Sadness: Cries, sighs, speaks in a soft or low tone, expresses unhappiness and disappointment
• Anxiety: Expresses discomfort and stress, answers with short yes/no responses without elaboration
• Defensiveness: Deﬂects criticism by defending self, accusing partner of similar behavior
• Aﬀection: Expresses warmth and caring for partner, speaks warmly, uses endearments
• Satisfaction: Feels satisfaction about how topic of discussion is deﬁned, discussed, and resolved
2. Features of the Interaction
• Dominance: Commands course of interaction, dominates conversation, changes sub ject frequently
• Solicits suggestions: Shows interest in and seeks partners suggestions, help in handling issue
• Instrumental support: Oﬀers positive advice for clear, concrete actions to support spouse
• Emotional support: Emphasizes feelings, builds conﬁdence, and raises self-esteem in partner

5.3 Behavioral Grouping

Descriptions show inter-behavior relationships Based on the descriptions of these behaviors, we can
see clear relations between some of them; for instance, Negative is similar to Blame and Anger but opposite
to Positive and Aﬀection, Withdrawal is similar to Avoidance but opposite to Discussion, etc. These notions
of similarity and polarity arise as a result of how humans perceive them. Using them, we can identify diﬀerent
groups of behaviors, with each group representing a concept that is common to the behaviors in it. Such
a grouping would be beneﬁcial since it can lend more interpretability to our analysis; for example, we can
investigate a link between the common trait of a group and the number of words required to express the
behaviors in it. Hence, we identify groups of related behaviors, shown below, using k-means clustering as
described in Algorithm. 1.
Grouping demonstrates existence of inter-behavior relationships Figure 6 shows the clustering
of the 27 Couples behaviors based on how similarly they were rated by human annotators, as described
in Sec. 5.3. Each cell shows the Spearman Correlation for a pair of behaviors. As expected, we observe
that Negative is positively correlated with Blame and Anger and negatively correlated with Positive and
Aﬀection. Similarly, Withdrawal
is positively correlated with Avoidance and negatively correlated with
Discussion. On the other hand, some behaviors do not appear to be strongly related to each other, such as
the pairs (Sadness, Positive ) and (Disgust,Negotiates ).

Conceptual names for behavior groups We tested diﬀerent numbers of clusters and found that 4
clusters provided the most coherent grouping. This also matches a diﬀerent study by Sevier et al. [56] on
the same dataset, which derived 4 scales of behavior using a Principal Component Analysis-based approach:
Negativity, Withdrawal, Positivity and Problem-Solving. Hence, we name our 4 groups of behaviors in similar
fashion. The ﬁrst group contains behaviors such as Discussion, Negotiates, Responsibility and Solutions.
Since these are strongly tied to a back-and-forth problem-speciﬁc style of interaction, we refer to them
as Problem-Solving behaviors. The second group consists of behaviors such as Anger, Blame, Disgust
and Negative ; hence, we refer to it as Negative. Similarly, we name the third group Positive since it
contains Aﬀection, Positive, Satisfaction, etc. The last group contains behaviors such as Anxiety, Change,
Sadness and Withdrawal. Since a ma jority of these are related to “dysphoria”, which is a state of unease or
unhappiness, we call this last group Dysphoric.

14

Figure 6: Behavior Grouping: Clustering of Couples Therapy behaviors into 4 groups based on their
similarity to each other. The cell in the (i, j ) position shows the Spearman Correlation between human
ratings of the corresponding behaviors i and j . Yellow (Blue) indicates highly positive (negative) correlation.
Non-diagonal gray cells indicate that correlation is not statistically signiﬁcant (p ¡ 0.05)

6 Results & Discussion

Analysis roadmap In this section, we present the results of our analysis on the Couples Therapy dataset.
We ﬁrst discuss the ﬁndings from each model separately following which we comment on behavior characteristics
that are shared across both models. Finally we review our ﬁndings with respect to how they address the
questions posed in Sec. 1 and how they compare with ﬁndings from other related work.

6.1 Ngram Model

6.1.1 Analysis of BCS

Figure 7 shows the BCS of the Ngram model scores at the 5 observation window length values that were
tested - 3, 10, 30, 50 and 100 words. Every behavior is represented by a tra jectory and each point on the
tra jectory represents the Spearman Correlation between ground truth annotations and the aggregated model
scores at that window length. While we tested three functionals for aggregation - minimum, median and
maximum - we only use the one that performed best, on average, across all window lengths for our analysis.
Hence, we only show the best performing functional for each behavior in the BCS plot; all correlations were
found to be statistically signiﬁcant (p < 0.05).

The best performing behaviors with the Ngram model are Acceptance and Blame, with BCS values
greater than 0.6 at nearly all window lengths; hence, we use these as reference behaviors during our analysis.
With respect to behavior groups, we see that the Negative behaviors are, on average, the best estimated
ones, followed by Positive behaviors. The BCS for Problem-Solving behaviors varies from moderate
(0.45 for Solutions ) to extremely low (0.06 for External ). Finally, with the exception of Change, the BCS
for Dysphoric behaviors is, in general, extremely low. This matches previous studies which have found
that behavioral constructs related to negative and positive aﬀect tend to be estimated well from low-level
lexical features. From these results, we can now also see that they are, in fact, much better estimable than
higher-level and more complex behaviors related to dysphoria and problem-solving. This could be due to
factors such as these behaviors not being expressed suﬃciently in language or their expression in language,

15

Figure 7: BCS for Ngram model

even if suﬃcient, being too complex to be modeled using Ngram phrases and simple statistics.

Figure 8: Sample distribution of Dominance and External scores at window lengths 3, 100 and session. In
both behaviors, the median 3-gram as well as 100-gram scores are very similar to the session-level scores,
possibly as a result of the nearly-symmetrical distribution of scores

In evaluating the choice of functionals, median appears to the best aggregation method for nearly every
behavior except for a few such as dominance and withdrawal, for whom maximum and minimum respectively
work better. This agrees with previous ﬁndings [37, 48] which also found the median to be a useful measure
of aggregate behavior. In cases where median is the best functional, we see that the BCS does not change
much even from varying from the shortest window length to the longest one possible. This, however, does
not imply that all window lengths are equally appropriate for such behaviors. As shown in Figure 8, the
scores from the Ngram model tend to be symmetrically distributed, a pattern which was also reported in [37].

16

00.10.20.30.40.50.60.7angerbelligerenceblamedefensivenessdisgustnegativeNEGATIVE00.10.20.30.40.50.60.7acceptanceaffectionagreementdefinepositivesatisfactionsupport-emotionalsupport-instrumentalPOSITIVE310305010000.10.20.30.40.50.60.7SpearmanCorrelationanxietyavoidancechangedominancesadnesswithdrawalDYSPHORIC3103050100Observation Window Length (number of words)00.10.20.30.40.50.60.7SpearmanCorrelationdiscussionexternalnegotiatesperspectiveresponsibilitysolicit-suggestionssolutionsPROBLEM-SOLVINGMinimumMedianMaximumFunctionalAs a result, any change in scores resulting from changes in the window length would not be reﬂected by
the median and, hence, the BCS would not change, giving the false impression that all windows are equally
appropriate. Therefore, in order to gain a more correct understanding of the eﬀect of window length on
behavior quantiﬁcation, we proceed to examine the BRC.

6.1.2 Analysis of BRC

Table 3 displays the Spearman Correlation between behaviors in the ground-truth annotations and in the
Ngram scores at diﬀerent window lengths. From these correlations, we calculate the BRC for each behavior
using Eqns. 10 and 11. The Ratings and Scores from Table 3 serve as Q∗ and Q
in Eqn. 10, and since
we have 2 reference behaviors, Acceptance and Blame, we use Eqn. 11 to calculate the weighted BRC where
the Reference Weights serve as the normalized weights α. This now enables us to observe changes in
the quality of the Ngram model scores which were otherwise not reﬂected in the BCS. For instance, for
the behavior Solutions, the BCS is nearly the same, around 0.45 at all the window lengths. However, when
compared to its ground truth correlations with Acceptance and Blame (0.282 and -0.184 respectively), we see
that the Ngram score correlations at 50 words (0.37 and -0.153 respectively) are more similar to them than
those at 3 words (0.323 and 0.006 respectively). Therefore, based on this, we can conclude that a window
length of 50 words is more appropriate for scoring the behavior Solutions than 3 words. In this manner,
we perform our analysis using both BCS and BRC and obtain the ﬁnal set of results which are displayed in
Figure 9. Each behavior is shown against its appropriate window length and arranged by behavior groups
so as to also demonstrate group-level patterns.

(cid:48)

6.1.3 Analysis of window lengths

Figure 9: Appropriate observation window length of diﬀerent behaviors as determined by our analysis when
quantiﬁed using an Ngram model

We see that a ma jority of behaviors tend to perform best at short window lengths, typically lesser than
10 words, with nearly half of them at just 3 words. Nearly a third of the behaviors perform best when scored
using long observation windows, such as Solutions at 50 words and Avoidance at 100 words. Among the
behavior groups, we see that Negative behaviors, on average, appear to require much shorter observation
windows than the rest. This matches our intuition about the emotional, short-term nature of these behaviors
that lends itself to brief expressions.

The remaining three groups consist of behaviors that are expressed over a range of lengths, from short
(Acceptance ) to long ones (Avoidance ). Among these, Positive and Dysphoric behaviors mostly work

17

ngramanxietysadnesschangeavoidancedominanceacceptancesupport-emotionaldefinepositivesatisfactionaffectionagreementsupport-instrumentalexternalresponsibilitysolicit-suggestionssolutionsdiscussionnegotiatesangerbelligerenceblamedefensivenessdisgustnegative3103050100Observation Window Length (number of words)BehaviorsNEGATIVEPROBLEM-SOLVINGPOSITIVEDYSPHORICBehavior GroupR

e

f

e
r
e

n

e
c

B

e

h

v
a

i

o

r

A

e
c
c

p

t

a

n

e
c

B

l

a

m

e

R

e

f

e
r
e

n

e
c

W

e

i

g

h

t

0

.

1
0
5

0

.

7
0
5

0

.

6
1
5

0

.

4
1
5

0

.

6
1
5

0

.

9
9
4

0

.

3
9
4

0

.

4
8
4

0

.

6
8
4

0

.

4
8
4

T

a

r

e
g

t

B

e

h

v
a

i

o

r

R

a

t

i

n

g

s

S

c

o

e
r

s

(

3

)

S

c

o

e
r

s

(

0
1

)

S

c

o

e
r

s

(

0
3

)

S

c

o

e
r

s

(

0
5

)

S

c

o

e
r

s

(

0
0
1

)

R

a

t

i

n

g

s

S

c

o

e
r

s

(

3

)

S

c

o

e
r

s

(

0
1

)

S

c

o

e
r

s

(

0
3

)

S

c

o

e
r

s

(

0
5

)

S

c

o

e
r

s

(

0
0
1

)

D

i

c
s

u

s
s

i

o

n

0

.

8
7
1

-

0

.

7
0
0

0

.

5
1
0

0

.

8
1
0

0

.

2
1
0

0

.

5
0
0

0

.

5
8
0

-

0

.

1
2

-

0

.

1
9
1

-

0

.

4
7
1

-

0

.

1
6
1

-

0

.

6
4
1

E

x

t

r
e

n

a

l

0

.

6
5
2

0

.

1
8
1

0

.

6
6
1

0

.

2
9
1

0

.

6
1
2

0

.

6
5
2

-

0

.

3
7
0

0

.

2
1
2

0

.

6
7
1

0

.

2
1

0

.

8
0

0

.

5
1
0

N

e

o
g

t

i

a

t

s
e

0

.

7
3
2

0

.

9
9
1

0

.

8
9
1

0

.

7
2
2

0

.

2
5
2

0

.

2
9
2

-

0

.

3
8
0

0

.

1
8
1

0

.

8
3
1

0

.

6
7
0

0

.

3
3
0

-

0

.

6
3
0

P

s
r
e

p

c
e

t

i

v

e

0

.

9
9
0

0

.

8
6
0

0

.

1
4
0

0

.

1
2
0

0

.

6
0
0

-

0

.

2
2
0

-

0

.

1
4
1

0

.

4
2
1

0

.

3
2
1

0

.

7
2
1

0

.

9
3
1

R

s
e

p

o

n

s

i

b

i
l
i

t

y

0

.

8
2
3

0

.

8
9
1

0

.

1
2
2

0

.

7
5
2

0

.

4
8
2

0

.

9
2
3

-

0

.

9
3
2

0

.

6
7
1

0

.

2
0
1

0

.

9
2
0

-

0

.

6
1
0

-

0

.

2
9
0

S

o

i
l

c

i

t

-

s

u

g
g

t
s
e

i

o

n

s

0

.

9
7
3

0

.

7
9
2

0

.

5
0
3

0

.

9
4
3

0

.

1
8
3

0

.

4
3
4

-

0

.

3
2

0

.

3
8
0

0

.

3
1
0

-

0

.

7
6
0

-

0

.

6
1
1

-

0

.

5
9
1

S

o

l

u

t

i

o

n

s

0

.

2
8
2

0

.

3
2
3

0

.

6
2
3

0

.

1
5
3

0

.

7
3

0

.

7
9
3

-

0

.

4
8
1

0

.

6
0
0

-

0

.

7
5
0

-

0

.

7
1
1

-

0

.

3
5
1

-

0

.

7
0
2

A

n

g

r
e

-

0

.

3
5
6

-

0

.

9
9
4

-

0

.

4
6
5

-

0

.

7
2
6

-

0

.

6
6

-

0

.

1
7

0

.

3
7
6

0

.

6
3
7

0

.

9
3
7

0

.

2
6
7

0

.

8
7
7

0

.

6
0
8

B

e

i
l
l

g

e
r
e

n

e
c

-

0

.

6
4
6

-

0

.

3
8
4

-

0

.

8
3
5

-

0

.

8
9
5

-

0

.

2
3
6

-

0

.

4
8
6

0

.

7
7
6

0

.

8
3
7

0

.

5
3
7

0

.

6
5
7

0

.

3
7
7

0

.

8

D

e

f

e

n

s

i

v

e

n

s
s
e

-

0

.

4
6
5

-

0

.

6
4
4

-

0

.

7
8
4

-

0

.

8
4
5

-

0

.

5
8
5

-

0

.

4
4
6

0

.

2
2
5

0

.

9
3
6

0

.

5
3
6

0

.

6
6
6

0

.

9
8
6

0

.

9
2
7

D

i

s

g

u

t
s

-

0

.

6
6

-

0

.

4
2
4

-

0

.

5
8
4

-

0

.

5
4
5

-

0

.

9
7
5

-

0

.

2
3
6

0

.

9
6

0

.

7
0
7

0

.

8
0
7

0

.

9
2
7

0

.

5
4
7

0

.

3
7
7

N

e

a
g

t

i

v

e

-

0

.

9
2
7

-

0

.

6
5
6

-

0

.

8
0
7

-

0

.

7
5
7

-

0

.

2
8
7

-

0

.

9
1
8

0

.

3
9
6

0

.

4
7
7

0

.

4
8
7

0

.

9
0
8

0

.

5
2
8

0

.

2
5
8

A

ﬀ

c
e

t

i

o

n

0

.

2
8
5

0

.

3
8
2

0

.

9
9
2

0

.

8
4
3

0

.

4
8
3

0

.

4
4

-

0

.

2
5
3

0

.

2
5
1

0

.

1
8
0

-

0

.

4
0
0

-

0

.

8
5
0

-

0

.

3
4
1

A

g

e
e
r

m

e

n

t

0

.

7
4
3

0

.

9
6
2

0

.

9
5
2

0

.

4
8
2

0

.

7
0
3

0

.

4
3

-

0

.

5
9
2

0

.

8
1
1

0

.

2
7
0

0

.

8
0
0

-

0

.

2
3
0

-

0

.

7
9
0

D

e

n
ﬁ

e

0

.

7
1
5

0

.

9
3
4

0

.

7
9
4

0

.

6
5
5

0

.

9
5

0

.

6
4
6

-

0

.

3
5
3

-

0

.

6
3
5

-

0

.

2
5
5

-

0

.

4
9
5

-

0

.

2
2
6

-

0

.

4
7
6

P

o

s

i

t

i

v

e

0

.

7
6

0

.

5
7
5

0

.

9
1
6

0

.

4
7
6

0

.

5
0
7

0

.

3
5
7

-

0

.

7
4
5

-

0

.

1
0
2

-

0

.

7
9
2

-

0

.

9
3

-

0

.

4
4

-

0

.

2
2
5

S

a

t

i

s

f

a

c

t

i

o

n

0

.

3
6
5

0

.

6
5
4

0

.

6
8
4

0

.

8
3
5

0

.

2
7
5

0

.

4
2
6

-

0

.

7
3
5

-

0

.

5
9
0

-

0

.

2
8
1

-

0

.

7
2

-

0

.

2
3

-

0

.

1
0
4

p
p
u
S

o

t
r

-

e

m

o

t

i

o

n

a

l

0

.

2
9
4

0

.

9
3
2

0

.

7
5
2

0

.

1
0
3

0

.

3
3
3

0

.

6
8
3

-

0

.

6
2
3

0

.

1
8
1

0

.

9
1
1

0

.

2
4
0

-

0

.

8
0
0

-

0

.

6
8
0

p
p
u
S

o

t
r

-

i

n

r
t
s

u

m

e

n

t

a

l

0

.

6
4

0

.

2
3

0

.

2
4
3

0

.

7
9
3

0

.

2
3
4

0

.

7
8
4

-

0

.

3
4
3

0

.

9
8
0

0

.

4
0
0

-

0

.

9
8
0

-

0

.

3
4
1

-

0

.

3
2

A

x
n

i

e

t

y

-

0

.

9
9
2

-

0

.

7
0
1

-

0

.

6
4
1

-

0

.

2
8
1

-

0

.

1
0
2

-

0

.

3
3
2

0

.

7
1

0

.

5
2
4

0

.

8
0
4

0

.

3
1
4

0

.

7
1
4

0

.

6
2
4

A

o
v

i

d

a

n

e
c

-

0

.

7
1

0

.

4
0
0

-

0

.

5
1
0

-

0

.

2
2
0

-

0

.

2
2
0

-

0

.

2
2
0

0

.

5
8
0

0

.

3
3
3

0

.

7
0
3

0

.

7
8
2

0

.

3
7
2

0

.

3
5
2

C

h

a

n

g

e

-

0

.

4
7
4

-

0

.

7
4

-

0

.

8
2
5

-

0

.

9
7
5

-

0

.

6
0
6

-

0

.

3
5
6

0

.

7

0

.

3
0
7

0

.

4
0
7

0

.

6
2
7

0

.

3
4
7

0

.

7
7

D

o

m

i

n

a

n

e
c

-

0

.

4
4
1

-

0

.

1
0
2

-

0

.

2
0
2

-

0

.

9
0
2

-

0

.

6
1
2

-

0

.

7
3
2

0

.

3
9
2

0

.

4
7
1

0

.

4
2
2

0

.

4
3
2

0

.

4
2

0

.

4
5
2

S

a

n
d

s
s
e

-

0

.

1
3
1

-

0

.

9
5
0

-

0

.

9
6
0

-

0

.

5
7
0

-

0

.

4
7
0

-

0

.

7
0

0

.

8
9
1

0

.

4
9
3

0

.

4
5
3

0

.

8
2
3

0

.

2
1
3

0

.

8
8
2

W

i

t

d
h

r

a

w

a

l

-

0

.

4
6
1

0

.

9
1
0

-

0

.

3
0
0

0

.

8
0
0

0

.

4
1
0

-

0

.

3
9
2

0

.

8
6
2

0

.

1
4
2

0

.

4
2
2

0

.

2

Table 3: Spearman Correlation between reference and target behaviors used to calculate the BRC metric
(Eqn. 10) for the Ngram model. Ratings refers to ground-truth correlations Q∗ , Scores(l) refers to Ngram
model score correlations Q
at observation window length l. All correlations statistically signiﬁcant (p < 0.05)
unless marked as -

(cid:48)

18

best at short observations (10 words or lesser). For Positive behaviors, this is likely due to their emotional
nature which, while not as brief as negative ones, is nevertheless short-term. Dysphoric behaviors, on the
other hand, are characterized by a lack of participation and expression and are thus, likely to be marked by
brief expressions, which could be why they tend to do best at short window lengths.

Finally, Problem-Solving behaviors are evenly split between either very short (3 words) or very long
windows (50 - 100 words). This is a little surprising since we would normally expect them to be mostly,
if not all, long-range due to their extended, back-and-forth nature. Upon inspection, we found that the
3-word-window behaviors in this group exhibited their highest BCS at that window length but their highest
BRC at longer window lengths. This indicates that while the model could be capturing them better at longer
windows, the aggregating functional performs better at shorter window lengths. Hence, for such behaviors,
perhaps more sophisticated and better-suited functionals could exploit the full potential of using long window
lengths.

Lastly, we note here that a-priori, it would be reasonable to expect that the Ngram model would always
prefer 3-word windows over longer ones, simply by virtue of its 3-gram training. However, we see that this
is not the case and that, in fact, more than half of the behaviors perform better at windows longer than 3
words when using the Ngram model. Therefore, this shows that our ﬁndings are also indicative of the nature
of the behaviors being modeled.

6.2 Neural Model

6.2.1 Analysis of BCS

Figure 10 shows the BCS of the Neural model scores at the two observation window lengths that were
tested, 3 and 30 words. For each behavior, a bar represents the Spearman Correlation between ground truth
annotations and the aggregated Neural model score at that window length. Similar to the Ngram model,
we used the best performing, on average, functional for our analysis and display it for each behavior in the
BCS plot; all correlations were found to be statistically signiﬁcant (p < 0.05).

Figure 10: BCS for Neural model

The best performing behavior with the Neural model is Blame, with BCS values close to 0.6 at both
window lengths; hence, it is used as the reference behavior in our analysis. In similar fashion to the Ngram
model, we see that the best estimated behaviors belong to the Negative group, followed by Positive.
However, unlike in the Ngram model, Dysphoric behaviors perform similar to the Problem-Solving ones,

19

NEGATIVEbelligerenceblameangernegativedisgustdefensiveness00.10.20.30.40.50.60.7SpearmanCorrelationPOSITIVEsatisfactionagreementsupport-emotionalsupport-instrumentalaffectionpositiveacceptance00.10.20.30.40.50.60.7SpearmanCorrelationavoidancechangeanxietydominancesadnesswithdrawalDYSPHORIC00.10.20.30.40.50.60.7SpearmanCorrelation3 words30 wordsWindow Lengthperspectiveexternalsolicit-suggestionsnegotiatesresponsibilitysolutionsPROBLEM-SOLVING00.10.20.30.40.50.60.7SpearmanCorrelationMinimumMedianMaximumFunctionalwith mostly low-to-moderate BCS. One reason for this improvement could be that the ELMo contextual
embeddings are able to provide sophisticated linguistic features required to capture such subtle behaviors,
which plain n-grams probably cannot provide.

With respect to functionals, we see that the median is once again the best performing one, observed in
almost all behaviors except for a few such as solutions and withdrawal, for whom maximum and minimum
respectively work better. We also see that the BCS in the Neural model changes noticeably as the window
length increases from 3 to 30 words, unlike in the Ngram model. One reason for this could be that in the
Ngram model, we used the same model (3-gram) at all the window lengths whereas in the Neural model,
a diﬀerent model was trained for each window length. Hence, theoretically, the variation in BCS that we
observe here could be due to the window length as well as the quality of the model trained. However, in
practice, since we tuned for the best model at each window length, we assume that the quality of training
is similar across window lengths and that the variation in BCS is mostly attributable to the window length.
As with the Ngram model, some behaviors in the Neural model, such as Change, do not exhibit noticeable
changes in BCS across window lengths; for such ones, we examine their BRC.

6.2.2 Analysis of BRC

Reference Behavior

Target Behavior

Ratings

External
Negotiates
Perspective
Responsibility
Solicit-suggestions
Solutions
Anger
Belligerence
Defensiveness
Disgust
Negative
Acceptance
Aﬀection
Agreement
Positive
Satisfaction
Support-emotional
Support-instrumental
Anxiety
Avoidance
Change
Dominance
Sadness
Withdrawal

-0.073
-0.083
-
-0.239
-0.23
-0.184
0.673
0.677
0.522
0.69
0.693
-0.75
-0.352
-0.295
-0.547
-0.537
-0.326
-0.343
0.171
0.085
0.7
0.293
0.198
-

Blame
Scores
(3)
-0.18
-0.381
0.063
-0.311
-0.388
-0.306
0.678
0.605
0.408
0.634
0.732
-0.656
-0.481
-0.464
-0.61
-0.465
-0.541
-0.571
0.369
-
0.76
0.394
0.204
-0.118

Scores
(30)
-0.134
-0.263
0.0902
-0.346
-0.42
-0.356
0.659
0.55
0.454
0.421
0.457
-0.618
-0.528
-0.347
-0.59
-0.464
-0.358
-0.39
0.349
-0.005
0.725
0.425
0.177
-0.035

Table 4: Spearman Correlation between reference and target behaviors used to calculate the BRC metric
(Eqn. 10) for the Neural model. Ratings refers to ground-truth correlations Q∗ , Scores(l) refers to Neural
model score correlations Q
at observation window length l. All correlations statistically signiﬁcant (p < 0.05)
unless marked as -

(cid:48)

Table 4 shows the Spearman Correlations between behaviors in the ground truth annotations and Neural
model scores. From these, we calculate the BRC using Eqn. 10, since we have just one reference behavior,
(Blame ). Then, using both BCS and BRC, we analyze the Neural model scores of all the behaviors, the
results of which are shown in Figure 11. As before, each behavior is shown against its appropriate window
length and arranged by behavior groups. It should be noted here that comparing just two window lengths

20

- 3 and 30 words - might not necessarily reveal which of them is the most appropriate window length for a
behavior. For instance, we might ﬁnd that 30 words is more appropriate than 3 words simply because the
most appropriate window length is actually 50, which is closer to 30 than to 3. Hence, while the results
don’t necessarily provide us with the best window length for observing a behavior, they do reveal a general
trend about whether it is primarily marked by short expressions or medium-to-long ones.

6.2.3 Analysis of window lengths

Figure 11: Appropriate observation window length of diﬀerent behaviors as determined by our analysis when
quantiﬁed using an Neural model

From Figure 11, we can see that most of the behaviors perform best at the 30-word window length,
with less than a quarter appearing to prefer the shortest window possible (3 words). For such behaviors,
this indicates that the longer context of 30 words is able to provide more useful information to the Neural
model than what a 3-word observation window could have. Among the behavior groups, we see, once again,
that Negative and Positive behaviors require, on average, shorter observation windows than the other
two groups. This matches the observations from our Ngram model as well as other studies about emotional
behaviors being primarily marked by short-term lexical expressions. Interestingly, though, we see that the
trend has slightly been altered; a smaller percentage of Negative behaviors perform better at short window
lengths with the Neural model than the Positive behaviors. We inspected the analysis metrics of Negative
behaviors to understand why this was happening, and found that while the BRC was higher at 3 words for
most of them, their BCS was higher at 30 words. This suggests that the functional median is probably better
suited for aggregating the Neural model scores at 30 words than at 3 words, and that, therefore, further
investigation may be required to ﬁnd a suitable functional at short window lengths.

In the Problem-Solving and Dysphoric groups, we see that their behaviors tend to perform, on
average, better at longer window lengths than shorter ones. This is in line with our intuition and expectations
about the nature of these behaviors being either subtle or marked by complex construction, both of which
necessitate the use of medium-to-long window lengths. Furthermore, since our Neural model uses a GRU,
which was specially designed to handle long-context dependencies, the kind of which we expect to see in
these behaviors, it makes sense that it would work better when fed information from a longer observation
window.

6.2.4 Analysis of ELMo weights

Lastly, we perform a qualitative analysis of window lengths for each behavior group, by inspecting the trained
parameters of our Neural models. Speciﬁcally, we examine the ELMo layer mixing weights that, as described

21

neuraldominanceanxietyavoidancechangesadnesswithdrawalaffectionagreementsatisfactionacceptancepositivesupport-emotionalsupport-instrumentalexternalnegotiatesresponsibilitysolicit-suggestionssolutionsbelligerenceblameangerdefensivenessdisgustnegative330Observation Window Length (number of words)BehaviorsNEGATIVEPROBLEM-SOLVINGPOSITIVEDYSPHORICBehavior Groupin Sec. 4.2.2, were used to fuse the embeddings from 3 layers, for each word, into a single input embedding.
Since these are softmax-normalized weights, they indicate relatively how much each layer was utilized in
order to estimate that behavior. Generally, it has been observed in deep neural networks that lower layers
tend to learn simpler representations such as edges in images and phrase-level information in text whereas
higher layers tend to learn more complicated representations such as ob jects in images and semantic features
in text [57, 58]. In particular, the higher layers in ELMo have been found to model complex characteristics
of language such as polysemy and semantics better than the lower layers [41]. Since our hypothesis is that
diﬀerent behavior groups are characterized by diﬀerent linguistic characteristics, we examine their ELMo
weights to see if they exhibit such patterns.

Figure 12: Scatter plot of ELMo layer mixing weights from trained Neural models of behaviors of every
group. Colored regions denote a particular layer having the largest weight among all 3 layers. For example,
the red region represents the space of all weights where layer 0’s is the largest, whereas blue represents the
space where layer 2’s weight is the largest

The 3 layers in ELMo, from lower to higher, are the input token layer and 2 bidirectional language model
(biLM) layers, which we denote as 0, 1 and 2 respectively. Their mixing weights can be denoted using the
3-tuple {(1-x1 -x2 ), x1 , x2} where x1 and x2 are the weights of layers 1 and 2 respectively. Figure 12 displays
the scatter plot of x1 and x2 , averaged over all model checkpoints, from each test fold and for each behavior
in a group. Each point (x1 ,x2 ) below the diagonal corresponds to a unique 3-tuple: for example, (0.1,0.8)
corresponds to {0.1, 0.1, 0.8}, which can be interpreted as the model using layer 2, mostly, to estimate that
behavior; similarly, (0.33,0.33) corresponds to {0.34, 0.33, 0.33} which means that the model used all 3 layers
to the same extent. We use diﬀerent colors to highlight regions where a particular layer has the largest
weight among all 3 of them; for example, the red region consists of all the points corresponding to 3-tuples
where layer 0 has the largest weight, while the blue region represents those 3-tuples where layer 2 has the
largest weight.

Firstly, we see that Problem-Solving behaviors overwhelmingly tend to use information mostly from
either just the highest layer or the top 2 layers. This means that the models used to estimate these
behaviors rely mostly on the biLM representations which, as we noted earlier, pertain to complex and
high-level characteristics of language. This agrees with our ﬁndings from the window length analysis where
Problem-Solving behaviors performed best when observed using long windows, since complex expression
patterns are diﬃcult to capture using short observations. Next, we see that Negative behaviors tend to
either use all 3 layers in near-equal proportions or lean slightly towards layer 2. This indicates that even
though they may favor shorter observations than longer ones, they are nevertheless characterized by low-level
as well as high-level linguistic features. Similarly, we see that Dysphoric behaviors are mostly clustered

22

00.20.40.60.81Layer 1 Weight00.20.40.60.81Layer 2 WeightPOSITIVE BEHAVIORS0 (token)1 (biLM-1)2 (biLM-2)Dominant Layer00.20.40.60.81Layer 1 Weight00.20.40.60.81Layer 2 WeightPROBLEM-SOLVING BEHAVIORS0 (token)1 (biLM-1)2 (biLM-2)Dominant Layer00.20.40.60.81Layer 1 Weight00.20.40.60.81Layer 2 WeightNEGATIVE BEHAVIORS0 (token)1 (biLM-1)2 (biLM-2)Dominant Layer00.20.40.60.81Layer 1 Weight00.20.40.60.81Layer 2 WeightDYSPHORIC BEHAVIORS0 (token)1 (biLM-1)2 (biLM-2)Dominant Layeraround the point (0.33,0.33), implying that their expressions involve a balanced mix of all aspects of language.
Finally, we see that Positive behaviors occupy a broad spectrum of expression characteristics, from heavily
relying on Layer 2, as evinced by the cluster around the point (0.2,0.6), to not utilizing it much, as seen
by the cluster near the point (0.4,0.2). This suggests that the expression of positive aﬀect in language is
a multifaceted and diverse phenomenon, which probably explains why we found that Positive behaviors
favored both short as well as long observation windows in both the Ngram and Neural models.

This concludes our discussion of the behavior window length results and linguistic characteristics with
respect to speciﬁc models. Next, we identify and comment on results that were observed consistently across
both modeling frameworks, in order to uncover behaviors patterns that might be potentially generalizable.

6.3 Cross-model Results

Figure 13: Comparison of highest BCS from both models at any window length for diﬀerent behaviors

1. Observation Window Length

2. Functional

3. Model

7 Conclusions & Future Work

Summary of work In this paper, we analyzed how long a system needs to observe conversational language
cues, measured in number of words, in order to quantify diﬀerent behaviors. We proposed a framework
and an inter-behavior correlation-based metric that can be used to analyze behaviors in scenarios where
ground-truth annotations are not available to compare against at every possible window length. Using a
Maximum Likelihood scoring method, we applied our analysis to the Couples Therapy dataset which contains
a rich and diverse set of behaviors observed in real-life interactions. Finally, we compared our ﬁndings with
existing work in the ﬁeld of psychology on thin slices and addressed pertinent questions related to the nature
of human behaviors.

Take-home messages from this paper Based on our analysis, we found that behaviors related to
the construct of Negativity can be well captured even with short lexical observations while those related
to Positivity require some more information and are not captured as well. On the other hand, behaviors

23

POSITIVEsatisfactionacceptanceagreementpositivesupport-instrumentalsupport-emotionalaffection00.10.20.30.40.50.60.7MaximumBCSPROBLEM-SOLVINGperspectiveresponsibilitysolicit-suggestionssolutionsnegotiatesexternal00.10.20.30.40.50.60.7MaximumBCSNgramNeuralModeldefensivenessnegativebelligerenceblameangerdisgustNEGATIVE00.10.20.30.40.50.60.7MaximumBCSchangeanxietydominanceavoidancesadnesswithdrawalDYSPHORIC00.10.20.30.40.50.60.7MaximumBCSMinimumMedianMaximumFunctionalinvolving complex deliberations or back-and-forth dialogue seem to require a much longer observation window
in order to be accurately understood while those related to dysphoric aﬀect are, in general, diﬃcult to quantify
from only text and when it is possible to do so, they require the most amount of information. Finally, when
trying to obtain an overall, aggregate quantiﬁcation of behavior based on information over a period of time,
we ﬁnd that diﬀerent behaviors seem to be best captured by diﬀerent functionals that might be indicative
of the nature of such behaviors.

How might this work be relevant to psychologists ? The ﬁndings from this work are of relevance
not only to machine learning applications that attempt to quantify the behavior content of conversational
language, but also to research studies in psychotherapy domains that deal with manual annotations of
behavior. For instance, our work, in conjunction with related work in thin slices, has shown that negative
behaviors and constructs appear to require only a small amount of information in order to be accurately
quantiﬁed. Hence, future studies might ﬁnd it beneﬁcial, both in terms of cost as well as time, to consider
which types of behaviors are of primary importance when deciding on the length of interactions to be
collected. Studies focused on negative behaviors may be able to elicit and measure such behaviors over
relatively brief periods of time whereas studies focused on positive and problem-solving behaviors likely will
require considerably longer intervals that generate larger amounts of text required for accurate text analysis.

How to improve/expand upon this work The next step in this work would be to extrinsically
evaluate its ﬁndings by applying the window length design choices in behavior estimation and classiﬁcation
systems and checking if they translate into gains in performance, over just using the same window length
for all behaviors. It is also worth investigating how the window length requirements change when employing
a multimodal analysis system that used both lexical as well as acoustic cues. As a supplement to this work,
we would like to crowdsource human annotations of how accurately humans can assess diﬀerent behaviors
using diﬀerent amounts of text information, thus conducting a study similar to those involving thin slices.
A related eﬀort in that direction would also be to test on datasets with dialog acts and utterance-level
annotations of behavior. As mentioned in Sec. ??, we also plan on investing signiﬁcant amounts of time
and resources in ﬁnding stable training environments for all the behaviors. Finally, we plan on investigating
if functionals which mimic the expected manner in which humans perceive some behaviors, such as those
involving primacy and recency [59], might be a better ﬁt for diﬀerent behavior constructs and if so, identifying
which functionals work best when quantifying which constructs.

24

References

[1] A. Christensen, D. Atkins, S. Berns, J. Wheeler, D. Baucom, and L. Simpson, “Traditional versus
integrative behavioral couple therapy for signiﬁcantly and chronically distressed married couples,”
Journal of Consulting and Clinical Psychology, vol. 72, no. 2, pp. 176–191, 2004.

[2] J. S. Baer, E. A. Wells, D. B. Rosengren, B. Hartzler, B. Beadnell, and C. Dunn, “Agency context
and tailored training in technology transfer: A pilot evaluation of motivational interviewing training for
community counselors,” Journal of substance abuse treatment, vol. 37, no. 2, pp. 191–202, 2009.

[3] M. Reblin, B. R. Baucom, M. F. Clayton, R. Utz, M. Caserta, D. Lund, K. Mooney, and L. Ellington,
“Communication of emotion in home hospice cancer care: Implications for spouse caregiver depression
into bereavement,” Psycho-Oncology, 2019.

[4] S. Narayanan and P. G. Georgiou, “Behavioral signal processing: Deriving human behavioral informatics
from speech and language,” Proceedings of the IEEE, vol. 101, no. 5, pp. 1203–1233, 2013.

[5] P. G. Georgiou, M. P. Black, A. C. Lammert, B. R. Baucom, and S. S. Narayanan, “thats aggravating,
very aggravating: Is it possible to classify behaviors in couple interactions using automatically derived
lexical features?” in International Conference on Aﬀective Computing and Intel ligent Interaction.
Springer, 2011, pp. 87–96.

[6] S. N. Chakravarthula, R. Gupta, B. Baucom, and P. Georgiou, “A language-based generative model
framework for behavioral analysis of couples’ therapy,” in Acoustics, Speech and Signal Processing
(ICASSP), 2015 IEEE International Conference on.
IEEE, 2015, pp. 2090–2094.

[7] S.-Y. Tseng, B. Baucom, and P. Georgiou, “Approaching human performance in behavior estimation in
couples therapy using deep sentence embeddings,” in Proceedings of Interspeech. August 2017, 2017.

[8] R. Gupta, N. Malandrakis, B. Xiao, T. Guha, M. Van Segbroeck, M. Black, A. Potamianos, and
S. Narayanan, “Multimodal prediction of aﬀective dimensions and depression in human-computer
interactions,” in Proceedings of the 4th International Workshop on Audio/Visual Emotion Chal lenge.
ACM, 2014, pp. 33–40.

[9] M. R. Morales, S. Scherer, and R. Levitan, “A linguistically-informed fusion approach for multimodal
depression detection,” NAACL HLT 2018, p. 13.

[10] B. Xiao, D. Can, P. G. Georgiou, D. Atkins, and S. S. Narayanan, “Analyzing the language of
therapist empathy in motivational interview based psychotherapy,” in Signal & Information Processing
Association Annual Summit and Conference (APSIPA ASC), 2012 Asia-Paciﬁc.
IEEE, 2012, pp. 1–4.

[11] J. Gibson, D. Can, B. Xiao, Z. E. Imel, D. C. Atkins, P. Georgiou, and S. Narayanan, “A deep learning
approach to modeling empathy in addiction counseling,” Commitment, vol. 111, p. 21, 2016.

[12] V. P´erez-Rosas, R. Mihalcea, K. Resnicow, S. Singh, and L. An, “Understanding and predicting
empathic behavior in counseling therapy,” in Proceedings of the 55th Annual Meeting of the Association
for Computational Linguistics (Volume 1: Long Papers), vol. 1, 2017, pp. 1426–1435.

[13] K. J. Baucom, M. Sevier, K. A. Eldridge, B. D. Doss, and A. Christensen, “Observed communication in
couples two years after integrative and traditional behavioral couple therapy: Outcome and link with
ﬁve-year follow-up.” Journal of consulting and clinical psychology, vol. 79, no. 5, p. 565, 2011.

[14] M. C. Blackman and D. C. Funder, “The eﬀect of information on consensus and accuracy in personality
judgment,” Journal of Experimental Social Psychology, vol. 34, no. 2, pp. 164–181, 1998.

[15] N. A. Murphy, J. A. Hall, M. A. Ruben, D. Frauendorfer, M. Schmid Mast, K. E. Johnson, and
L. Nguyen, “Predictive validity of thin-slice nonverbal behavior from social interactions,” Personality
and Social Psychology Bul letin, p. 0146167218802834, 2018.

[16] P. Satterstrom, J. T. Polzer, L. B. Kwan, O. P. Hauser, W. Wiruchnipawan, and M. Burke, “Thin slices
of workgroups,” Organizational Behavior and Human Decision Processes, vol. 151, pp. 104–117, 2019.

25

[17] R. F. Baumeister, K. D. Vohs, C. Nathan DeWall, and L. Zhang, “How emotion shapes behavior:
Feedback, anticipation, and reﬂection, rather than direct causation,” Personality and social psychology
review, vol. 11, no. 2, pp. 167–203, 2007.

[18] B. Schuller, M. Valster, F. Eyben, R. Cowie, and M. Pantic, “Avec 2012: the continuous audio/visual
emotion challenge,” in Proceedings of the 14th ACM international conference on Multimodal interaction.
ACM, 2012, pp. 449–456.

[19] A. B. Zadeh, P. P. Liang, S. Poria, E. Cambria, and L.-P. Morency, “Multimodal language analysis
in the wild: Cmu-mosei dataset and interpretable dynamic fusion graph,” in Proceedings of the 56th
Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), 2018, pp.
2236–2246.

[20] C. Busso, M. Bulut, C.-C. Lee, A. Kazemzadeh, E. Mower, S. Kim, J. N. Chang, S. Lee, and S. S.
Narayanan, “Iemocap: Interactive emotional dyadic motion capture database,” Language resources and
evaluation, vol. 42, no. 4, p. 335, 2008.

[21] C. Heavey, D. Gill, and A. Christensen, “Couples interaction rating system 2 (cirs2),” University of
California, Los Angeles, vol. 7, 2002.

[22] T. B. Moyers, T. Martin, J. K. Manuel, W. R. Miller, and D. Ernst, “The motivational interviewing
treatment integrity (miti) code: Version 2.0,” Retrieved from Verf¨ubar unter: www. casaa. unm. edu
[01.03. 2005], 2003.

[23] R. E. Heyman, “Rapid marital interaction coding system (rmics),” in Couple observational coding
systems. Routledge, 2004, pp. 81–108.

[24] N. Ambady and R. Rosenthal, “Thin slices of expressive behavior as predictors of interpersonal
consequences: A meta-analysis.” Psychological bul letin, vol. 111, no. 2, p. 256, 1992.

[25] S. L. Krzyzaniak, D. E. Colman, T. D. Letzring, J. S. McDonald, and J. C. Biesanz, “The eﬀect of
information quantity on distinctive accuracy and normativity of personality trait judgments,” European
Journal of Personality, 2019.

[26] R. R. McCrae, P. T. Costa Jr, and C. M. Busch, “Evaluating comprehensiveness in personality systems:
The california q-set and the ﬁve-factor model,” Journal of Personality, vol. 54, no. 2, pp. 430–446, 1986.

[27] A. Cullen and N. Harte, “Thin slicing to predict viewer impressions of ted talks,” in Proceedings of the
14th International Conference on Auditory-Visual Speech Processing, 2017.

[28] D. R. Carney, C. R. Colvin, and J. A. Hall, “A thin slice perspective on the accuracy of ﬁrst impressions,”
Journal of Research in Personality, vol. 41, no. 5, pp. 1054–1072, 2007.

[29] W. Xia, J. Gibson, B. Xiao, B. Baucom, and P. G. Georgiou, “A dynamic model for behavioral analysis
of couple interactions using acoustic features,” in Sixteenth Annual Conference of the International
Speech Communication Association, 2015.

[30] H. Li, B. Baucom, and P. Georgiou, “Linking emotions to behaviors through deep transfer learning,”
submitted to PeerJ Computer Science, 2019.

[31] C.-C. Lee, A. Katsamanis, P. G. Georgiou, and S. S. Narayanan, “Based on isolated saliency or causal
integration?
toward a better understanding of human annotation process using multiple instance
learning and sequential probability ratio test,” in Thirteenth Annual Conference of the International
Speech Communication Association, 2012.

[32] E. Frank and M. Hall, “A simple approach to ordinal classiﬁcation,” in European Conference on Machine
Learning. Springer, 2001, pp. 145–156.

[33] V. Rozgi´c, B. Xiao, A. Katsamanis, B. Baucom, P. G. Georgiou, and S. Narayanan, “Estimation of
ordinal approach-avoidance labels in dyadic interactions: Ordinal logistic regression approach,” in 2011
IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP).
IEEE, 2011,
pp. 2368–2371.

26

[34] S. N. Chakravarthula, B. Xiao, Z. E. Imel, D. C. Atkins, and P. G. Georgiou, “Assessing empathy using
static and dynamic behavior models based on therapist’s language in addiction counseling,” in Sixteenth
Annual Conference of the International Speech Communication Association, 2015.

[35] A. Stolcke, “Srilm-an extensible language modeling toolkit,” in Seventh international conference on
spoken language processing, 2002.

[36] Y. Bengio, R. Ducharme, P. Vincent, and C. Jauvin, “A neural probabilistic language model,” Journal
of machine learning research, vol. 3, no. Feb, pp. 1137–1155, 2003.

[37] S.-Y. Tseng, S. N. Chakravarthula, B. R. Baucom, and P. G. Georgiou, “Couples behavior modeling
and annotation using low-resource lstm language models.” in INTERSPEECH, 2016, pp. 898–902.

[38] S. Hochreiter and J. Schmidhuber, “Long short-term memory,” Neural computation, vol. 9, no. 8, pp.
1735–1780, 1997.

[39] K. Cho, B. van Merrienboer, D. Bahdanau, and Y. Bengio, “On the properties of neural machine
translation: Encoder–decoder approaches,” in Proceedings of SSST-8, Eighth Workshop on Syntax,
Semantics and Structure in Statistical Translation, 2014, pp. 103–111.

[40] T. Mikolov, I. Sutskever, K. Chen, G. S. Corrado, and J. Dean, “Distributed representations of words
and phrases and their compositionality,” in Advances in neural information processing systems, 2013,
pp. 3111–3119.

[41] M. Peters, M. Neumann, M. Iyyer, M. Gardner, C. Clark, K. Lee, and L. Zettlemoyer, “Deep
contextualized word representations,” in Proceedings of the 2018 Conference of the North American
Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1
(Long Papers), 2018, pp. 2227–2237.

[42] A. Krizhevsky and G. Hinton, “Convolutional deep belief networks on cifar-10,” Unpublished manuscript,
vol. 40, no. 7, pp. 1–9, 2010.

[43] A. Graves, “Generating sequences with recurrent neural networks,” arXiv preprint arXiv:1308.0850,
2013.

[44] N. Srivastava, G. Hinton, A. Krizhevsky, I. Sutskever, and R. Salakhutdinov, “Dropout: a simple way
to prevent neural networks from overﬁtting,” The journal of machine learning research, vol. 15, no. 1,
pp. 1929–1958, 2014.

[45] D. P. Kingma and J. Ba, “Adam: A method for stochastic optimization,” in International Conference
on Learning Representations, 2015.

[46] L. N. Smith, “Cyclical learning rates for training neural networks,” in 2017 IEEE Winter Conference
on Applications of Computer Vision (WACV).
IEEE, 2017, pp. 464–472.

[47] G. Huang, Y. Li, G. Pleiss, Z. Liu, J. E. Hopcroft, and K. Q. Weinberger, “Snapshot ensembles: Train
1, get m for free,” in International Conference on Learning Representations, 2017.

[48] S.-Y. Tseng, H. Li, B. Baucom, and P. Georgiou, “Honey, i learned to talk: Multimodal fusion for
behavior analysis,” in Proceedings of the 2018 on International Conference on Multimodal Interaction.
ACM, 2018, pp. 239–243.

[49] M. A. Thornton and D. I. Tamir, “Mental models accurately predict emotion transitions,” Proceedings
of the National Academy of Sciences, vol. 114, no. 23, pp. 5982–5987, 2017.

[50] B. Diedenhofen and J. Musch, “cocor: A comprehensive solution for the statistical comparison of
correlations,” PloS one, vol. 10, no. 4, p. e0121945, 2015.

[51] G. Y. Zou, “Toward using conﬁdence intervals to compare correlations.” Psychological methods, vol. 12,
no. 4, p. 399, 2007.

[52] J. Jones and A. Christensen, “Couples interaction study: Social support interaction rating system,”
University of California, Los Angeles, vol. 7, 1998.

27

[53] C.-C. Lee, M. Black, A. Katsamanis, A. C. Lammert, B. R. Baucom, A. Christensen, P. G. Georgiou, and
S. S. Narayanan, “Quantiﬁcation of prosodic entrainment in aﬀective spontaneous spoken interactions
of married couples,” in Eleventh Annual Conference of the International Speech Communication
Association, 2010.

[54] M. P. Black, A. Katsamanis, B. R. Baucom, C.-C. Lee, A. C. Lammert, A. Christensen, P. G. Georgiou,
and S. S. Narayanan, “Toward automating a human behavioral coding system for married couples
interactions using speech acoustic features,” Speech communication, vol. 55, no. 1, pp. 1–21, 2013.

[55] C.-C. Lee, A. Katsamanis, M. P. Black, B. R. Baucom, A. Christensen, P. G. Georgiou, and S. S.
Narayanan, “Computing vocal entrainment: A signal-derived pca-based quantiﬁcation scheme with
application to aﬀect analysis in married couple interactions,” Computer Speech & Language, vol. 28,
no. 2, pp. 518–539, 2014.

[56] M. Sevier, K. Eldridge, J. Jones, B. D. Doss, and A. Christensen, “Observed communication and
associations with satisfaction during traditional and integrative behavioral couple therapy,” Behavior
therapy, vol. 39, no. 2, pp. 137–150, 2008.

[57] C. Olah, A. Mordvintsev, and L. Schubert, “Feature visualization,” Distil l, vol. 2, no. 11, p. e7, 2017.

[58] G. Jawahar, B. Sagot, D. Seddah, S. Unicomb, G. I˜niguez, M. Karsai, Y. L´eo, M. Karsai, C. Sarraute,
´E. Fleury et al., “What does bert learn about the structure of language?” in 57th Annual Meeting of
the Association for Computational Linguistics (ACL), Florence, Italy, 2019.

[59] D. D. Steiner and J. S. Rain, “Immediate and delayed primacy and recency eﬀects in performance
evaluation.” Journal of Applied Psychology, vol. 74, no. 1, p. 136, 1989.

28

