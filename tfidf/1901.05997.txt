Characterizing the Use of Images in State-Sponsored Information
Warfare Operations by Russian Trolls on Twitter∗

Savvas Zannettou1 , Tristan Caulﬁeld2 , Barry Bradlyn3 ,
Emiliano De Cristofaro2 , Gianluca Stringhini4 , and Jeremy Blackburn5

1Max Planck Institute for Informatics, 2University College London, 3University of Illinois at Urbana-Champaign,
4Boston University, 5Binghamton University

9
1
0
2

v
o

N

1
2

]
I

S

.

s

c

[

2
v
7
9
9
5
0

.

1
0
9
1

:

v

i

X

r

a

Abstract

State-sponsored organizations are increasingly linked to efforts
aimed to exploit social media for information warfare and ma-
nipulating public opinion. Typically, their activities rely on a
number of social network accounts they control, aka trolls, that
post and interact with other users disguised as “regular” users.
These accounts often use images and memes, along with tex-
tual content, in order to increase the engagement and the cred-
ibility of their posts.
In this paper, we present the ﬁrst study of images shared by
state-sponsored accounts by analyzing a ground truth dataset
of 1.8M images posted to Twitter by accounts controlled by
the Russian Internet Research Agency. First, we analyze the
content of the images as well as their posting activity. Then,
using Hawkes Processes, we quantify their inﬂuence on popu-
lar Web communities like Twitter, Reddit, 4chan’s Politically
Incorrect board (/pol/), and Gab, with respect to the dissemina-
tion of images. We ﬁnd that the extensive image posting activ-
ity of Russian trolls coincides with real-world events (e.g., the
Unite the Right rally in Charlottesville), and shed light on their
targets as well as the content disseminated via images. Finally,
we show that the trolls were more effective in disseminating
politics-related imagery than other images.

1

Introduction

Social network users are constantly bombarded with digital
content. While the sheer amount of information users have ac-
cess to was unthinkable just a couple of decades ago, the way
in which people process that information has also evolved dras-
tically. Social networks have become a battleﬁeld for informa-
tion warfare, with different entities attempting to disseminate
content to achieve strategic goals, push agendas, or ﬁght ideo-
logical battles [31, 10].
As part of this tactic, governments often employ “armies”
of actors, operating from believable accounts and posting con-
tent that aims to manipulate opinion or sow public discord by
actively participating in online discussions. Previous work has

∗ To appear at the 14th International AAAI Conference on Web and Social
Media (ICWSM 2020) – please cite accordingly. Work done while ﬁrst author
was with Cyprus University of Technology.

studied the involvement of state-sponsored accounts in divi-
sive events, e.g., the Black Lives Matter movement [35] or the
2016 US elections [1, 6], highlighting how these entities can
be impactful both on the information ecosystem and in the real
world.
In today’s information-saturated society, the effective use of
images when sharing online content can have a strong inﬂu-
ence in whether content will catch people’s attention and go
viral [4, 19, 21]. Users often feel overwhelmed with how much
content they are exposed to [23], and pay attention to each
piece of information for short amounts of time, with reper-
cussion to their attention span [36]. In fact, previous research
showed that 60% of social network users re-share articles on
social media without reading them, basing their decision on
limited cues such as the title of the article or the thumbnail
image associated with it [14].
Therefore, as part of the efforts aimed to actively push agen-
das, state-sponsored accounts do not only use textual content,
but also take advantage of the expressive power of images
and pictures, e.g., using politically and ideologically charged
memes [31]. In Figure 1, we report some (self-explanatory)
examples of images pushed by state-sponsored accounts on
Twitter, showcasing their unequivocally political nature and
how they can be used to push agendas. Nonetheless, the role
of images in information diffusion on the Web has attracted
limited attention from the research community, which has thus
far mainly focused on textual content [1].
In this paper, we begin ﬁlling this gap by studying the use of
images by state-sponsored accounts, aka Russian trolls [15]. In
particular, we focus on the following research questions:
1. What content
is disseminated via images by state-
sponsored accounts?
2. Can we identify the target audience of Russian state-
sponsored accounts by studying the images they share?
3. How inﬂuential are these accounts in making images go
viral on the Web? How does this inﬂuence results com-
pare to previous characterizations that look into the spread
of news by these accounts?
Aiming to address these questions, we use an image-
processing pipeline, expanding that presented by [37], to study
images shared by state-sponsored trolls on Twitter. More pre-
cisely, we implement a custom annotation module that uses
Google’s Cloud Vision API to annotate images in the ab-

1

 
 
 
 
 
 
to other images. Finally, by comparing these results to
previous analysis focused on news [40], we ﬁnd that trolls
were slightly more inﬂuential in spreading news via URLs
than images.

2 Related Work

Trolls and politics. Previous work has focused on understand-
ing the behavior, role, and impact of state-sponsored accounts
on the US political scene. Boyd et al. [6] perform linguistic
analysis on posts by Russian state-sponsored accounts over
the course of the 2016 US election; they ﬁnd that right- and
left-leaning communities are targeted differently to maximize
hostility across the political spectrum in the USA. Stewart et
al. [35] investigate the behavior of state-sponsored accounts
around the BlackLivesMatter movement, ﬁnding that they in-
ﬁltrated both right- and left-leaning political communities to
participate in both sides of the discussions. Jensen [20] ﬁnds
that, during the 2016 US election, Russian trolls were mainly
interested in deﬁning the identity of political individuals rather
than particular information claims.
Trolls in social networks. Dutt et al. [11] analyze the ad-
vertisements purchased by Russian accounts on Facebook.
By performing clustering and semantic analysis, they identify
their targeted campaigns over time, concluding that their main
goal is to sway division on the community, and also that the
most effective campaigns share similar characteristics. Zannet-
tou et al. [39] compare a set of Russian troll accounts against
a random set of Twitter users, showing that Russian troll ac-
counts exhibit different behaviors in the use of the Twitter plat-
form when compared to random users. In follow up work, Zan-
nettou et al. [40] analyze the activities of Russian and Iranian
trolls on Twitter and Reddit, ﬁnding substantial differences be-
tween them (e.g., Russian trolls were pro-Trump, Iranian ones
anti-Trump), that their behavior and targets vary greatly over
time, and that Russian trolls discuss different topics across
Web communities (e.g., they discuss about cryptocurrencies
on Reddit but not on Twitter). Also, Spangher et al. [33] ex-
amine the exploitation of various Web platforms (e.g., social
networks and search engines), showing that state-sponsored
accounts use them to advance their propaganda by promot-
ing content and their own controlled domains. Finally, Broni-
atowski et al. [7] focus on the vaccine debate and study Twit-
ter discussions by Russian trolls, bots, and regular users. They
ﬁnd that the trolls ampliﬁed both sides of the debate, while at
the same time their messages were more political and divisive
in nature when compared to messages from bots and regular
users.

Detection & Classiﬁcation. Badawy et al. [2] use machine

learning to detect Twitter users that are likely to share content
that originates from Russian state-sponsored accounts, while
Im et al. [17] detect Russian trolls using machine learning tech-
niques, ﬁnding that these accounts are still very active on the
Web. Also, Kim et al. [22] classify Russian state-sponsored
trolls into various roles: left- or right-leaning or accounts that
pose as news outlets. By applying their technique on 3M tweets

Figure 1: Examples of politically-charged images posted by Russian
trolls.

sence of high-quality ground truth data, or for images that are
not bounded to a speciﬁc domain (e.g., memes). We then run
the new pipeline on a dataset of 1.8M images from the 9M
tweets released by Twitter in October 2018 as part of their
effort to curb state-sponsored propaganda [15]. These tweets
were posted by 3.6K accounts identiﬁed as being controlled
by the Russian Internet Research Agency (IRA). Finally, we
quantify the inﬂuence that state-sponsored trolls had on other
mainstream and alternative Web communities: namely, Twit-
ter, Reddit, Gab, and 4chan’s Politically Incorrect board (/pol/).
To do this, we use Hawkes Processes [24, 25], which allow us
to model the spread of the images across multiple Web com-
munities and assess the root cause of the image appearances.
Main Findings. Along with a ﬁrst-of-its-kind characterization
of how images are used by state-sponsored actors, our work
yields a number of interesting ﬁndings:
1. The sharing of images by the trolls coincides with real-
world events. For instance, we ﬁnd a peak in activity that
is clearly in close temporal proximity with the Unite the
Right rally in Charlottesville [34], likely suggesting their
use to sow discord during dividing events.
2. Our analysis provides evidence of their general themes
and targets. For instance, we ﬁnd that Russian trolls were
mainly posting about Russia, Ukraine, and the USA.
3. By studying the co-occurrence of these images across the
Web, we show that the same images appeared in many
popular social networks, as well as mainstream and al-
ternative news outlets. Moreover, we highlight interest-
ing differences in popular websites for each of the de-
tected entities: for instance, troll-produced images related
to US matters were mostly co-appearing on mainstream
English-posting news sites.
4. Our inﬂuence estimation results highlight that the Russian
state-sponsored trolls, despite their relatively small size,
are particularly inﬂuential and efﬁcient in pushing images
related to politics to other Web communities. In particu-
lar, we ﬁnd that Russian state-sponsored trolls were more
inﬂuential in spreading political imagery when compared

2

a)b)c)Image analysis pipeline. To analyze the images posted by
these state-sponsored accounts, we build on the image process-
ing pipeline presented by [37]. This relies on Perceptual Hash-
ing, or pHash [27], and clustering techniques [12] to group
similar images according to their visual peculiarities, yielding
clusters of visually similar images. Then, clusters are anno-
tated based on the similarity between a ground truth dataset
and each cluster’s medoid (i.e., the representative image in the
cluster). For this process, [37] use crowdsourced meme meta-
data obtained from Know Your Meme. Our effort, however,
has a broader scope as the images shared by state-sponsored
accounts are not limited to memes. Consequently, we use a
different annotation approach, relying on Google’s Cloud Vi-
sion API1 , a state-of-the-art solution in Computer Vision tasks
to gather useful insights from open-domain images (i.e., not
bounded to a speciﬁc domain like Internet memes).
Figure 2 shows the resulting extended pipeline. We perform
the “Web Detection” task using Cloud Vision API, which pro-
vides us with two very useful pieces of information for each
image: 1) a set of entities, and their associated conﬁdence
scores, that best describe the image (e.g., an image showing
Donald Trump yields an entity called “Donald Trump”); and
2) a set of URLs on the Web that the same image appeared.
To extract this information, the API leverages Google’s im-
age search functionality to ﬁnd URLs to identical and simi-
lar images. Furthermore, by extracting data from the text of
these URLs, the API provides a set of entities that are related
to the image. These two pieces of information are crucial for
our analysis as they allow us to understand the context of the
images and their appearance across the Web.
Running the pipeline. First, we extract a pHash for each im-
age using the ImageHash library.2 This reveals that there is a
substantial percentage of images that are either visually iden-
tical or extremely similar as they have the same pHashes (43%
of the images). Next, we cluster the images by calculating
all the pairwise comparisons of all the pHashes. This results
in 78,624 clusters containing 753,634 images. Then, for each
cluster, we extract the medoid, which is the image that has the
minimum average Hamming distance between all the images
in the cluster. Then, using each medoid, we perform “Web De-
tection” using the Cloud Vision API, which provides us with
a set of entities and URLs, which we assign for each image in
the cluster. This is doable since the average number of unique
images per cluster is 1.8 with a median of 1 unique image per
cluster (see Figure 3(a)).
Pipeline Evaluation. To evaluate the performance of our
pipeline, we manually annotate a random sample of 500
clusters. Speciﬁcally, the ﬁrst author of this paper manu-
ally checked the 500 random clusters and the corresponding
Cloud Vision entity with the highest conﬁdence score to assess
whether the entity is “appropriate” with respect to the images
within the cluster. We ﬁnd that the Cloud Vision API-based
annotation procedure provides an appropriate entity in 83.7%
of the clusters in the random sample. Thus, we argue this is a

1 https://cloud.google.com/vision/
2 https://github.com/JohannesBuchner/imagehash

Figure 2: Overview of our image processing pipeline.

posted by Russian trolls on Twitter, they ﬁnd that despite the
fact that trolls had multiple roles, they worked together, while
for trolls that pose as news outlets, they ﬁnd that they had mul-
tiple agendas. For instance, some were posting about violent
news to create an atmosphere of fear, while others focused on
posting highly biased political news.
Remarks. Overall, unlike previous work, we focus on con-
tent shared via images by state-sponsored accounts. Indeed, to
the best of our knowledge, ours is the ﬁrst study performing
a large-scale image analysis on a ground truth dataset of im-
ages shared by Russian trolls on Twitter. Previous research [14]
has showed that social network users usually decide what to
share and consume content based on visual cues; thus, as state-
sponsored accounts tend to post disinformation [26], studying
the images they share provides an important tool to understand
and counter disinformation.

3 Methodology

We now present our dataset and our methodology for analyzing
images posted by state-sponsored trolls on Twitter.
Dataset. We use a ground truth dataset of tweets posted by
Russian trolls released by Twitter in October 2018 [15]. The
dataset includes over 9M tweets posted by 3.6K Russian state-
sponsored accounts, and their associated metadata and media
(1.8M images). Note that the methodology employed by Twit-
ter for detecting/labeling these state-sponsored accounts is not
publicly available. That said, to the best of our knowledge, this
is the most up-to-date and the largest ground truth dataset of
state-sponsored accounts and their activities on Twitter.
Ethics. We only work with publicly available data, which was
anonymized by Twitter, and follow standard ethical guide-
lines [29]—e.g., we do not try to de-anonymize users based
on their tweets.

3

3. Clustering and MedoidCaclulation 1. pHash Extraction2. pHash-based Pairwise DistanceCalculationpHashes Clusters of images and medoids4. Cluster Annotation 5. Analysis of Annotated DatasetPairwise comparisons  of pHashesTwitterimagesRussian trollsWeb entity detection using  medoid image for each cluster Annotated clusters and images(a)

(b)

Figure 3: CDF of a) number of images per cluster (image uniqueness
is based on their pHash); and b) number of images per troll account.

Figure 4: CDF of the number of images per tweet in our dataset.

reasonable performance for the purposes of our study.

4

Image Analysis

We now present the results of our analysis. First, we per-
form a general characterization of the images posted by state-
sponsored accounts on Twitter and then an analysis of the con-
tent of the images. Also, we study the occurrence of the images
across the Web.

4.1 General Characterization

We begin by looking at the prevalence of images in tweets
by state-sponsored trolls. In Figure 3(b), we plot the CDF of
the number of images posted per conﬁrmed state-sponsored ac-
count that had at least one tweet (4.5% of the identiﬁed trolls
never tweeted). We ﬁnd that only a small percentage of these
accounts do not share images (9.7% of the Russian troll ac-
counts). Also, some accounts shared an extremely large num-
ber of images, 8% of the Russian trolls posted over 1K images.
Furthermore, we ﬁnd an average of 502.2 images per account
with a median number of images of 37.
Then, in Figure 4, we report the CDF of the number of im-
ages per tweet; we ﬁnd that 19% of tweets posted by Russian

4

(a)

(b)

Figure 5: Temporal overview of: a) all tweets and tweets with im-
ages as a percentage of all tweets; and b) all tweets with images as a
percentage of all tweets that contained at least one image.

trolls include at least one image. One explanation for this rel-
atively large fraction is that Twitter automatically generates a
preview/thumbnail image when you post a URL. Indeed, by
inspecting the URLs in the tweets, we ﬁnd that out of the 19%
of the tweets that contained images, 11.8% of them contained
automatically generated one, while the rest (7.2%) include im-
ages that are explicitly posted (i.e., not generated based on a
posted URL). That said, we include all images in our dataset
and analysis, as generated images too provide insight into the
content posted by the state-sponsored accounts, especially con-
sidering their proclivity to post “fake news” [26] and the role
images might play in catching people’s attention.
Temporal Analysis. Next, we look into how the tweets from
Russian trolls are shared over time with a particular focus on
the tweets that contain images. Figure 5(a) reports the percent-
age of tweets shared each week normalized by the number of
all tweets, while Figure 5(b) the percentages normalized by
the number of tweets that contained at least one image. The
former shows that, in the early stages of their operations (be-
fore 2016), Russian trolls were posting tweets mostly without
images, whereas, after 2016 it seems that they started post-
ing more tweets containing images. This indicates that they
started using more images in their tweets after 2016, likely be-
cause they started targeting speciﬁc foreign countries (e.g., the
US [28]), suggesting the Russian trolls might believe the use
of images can be better for pushing speciﬁc narratives.
Figure 5(b) reveals an overall increase in the use of images
after October 2016 with a peak of activity in use of images
during the week leading to the Charlottesville rally in August
2017 [34], which led to the death of one counter protester [8]
and was a signiﬁcant turning point in the use of online hate
speech and anti-Semitism in fringe Web communities [41].
This peak likely indicates that the use of images is an effec-
tive tactic used by Russian trolls to sow discord on social net-
works with respect to events related to politics, the alt-right,

100101102103104# of images0.00.20.40.60.81.0CDFAll imagesUnique Images0100101102103104105# of images per troll account0.00.20.40.60.81.0CDF01234567# images in tweet0.00.20.40.60.81.0CDF04/1407/1410/1401/1504/1507/1510/1501/1604/1607/1610/1601/1704/1707/1710/1701/1804/180.00.51.01.52.0% of tweetsAll tweetsTweets with images04/1407/1410/1401/1504/1507/1510/1501/1604/1607/1610/1601/1704/1707/1710/1701/1804/180.000.250.500.751.001.251.501.752.00% of tweets with imagesTop entity

#clusters (%) Top entity

Russia
Vladimir Putin
Donald Trump
Car
Ukraine
U.S.A.
Barack Obama
Petro Poroshenko
Document
Moscow
Hillary Clinton
Meme
Logo
Product
Public Relations
Illustration
Syria
Web page
Advertising
Police

2,783 (3.5%) Russia
1,377 (1.7%) Vladimir Putin
1,281 (1.6%) Breaking news
1,262 (1.6%) Donald Trump
1,031 (1.3%) Car
907 (1.1%) Ukraine
823 (1.0%) U.S.A.
621 (0.8%) Barack Obama
530 (0.6%) Petro Poroshenko
495 (0.6%) Logo
479 (0.6%) Moscow
461 (0.6%) Syria
456 (0.6%) Public Relations
422 (0.5%) Police
416 (0.5%) Hillary Clinton
393 (0.5%) Document
372 (0.5%) Meme
310 (0.4%) Product
295 (0.3%) Saint Petersburg
290 (0.3%)
Illustration

#images (%)

30,426 (4.0%)
15,718 (2.0%)
15,071 (2.0%)
13,807 (1.8%)
10,236 (1.3%)
10,169 (1.3%)
8,638 (1.1%)
8,380 (1.1%)
6,654 (0.9%)
6.017 (0.8%)
5,524 (0.7%)
4,540 (0.6%)
4,459 (0.6%)
4,301 (0.6%)
4,167 (0.5%)
4,060 (0.5%)
3,886 (0.4%)
3,256 (0.4%)
2,870 (0.4%)
2,862 (0.4%)

Table 1: Top 20 entities found in images shared by Russian troll ac-
counts. We report the top entities both in terms of the number of clus-
ters and of images.

and white supremacists.

4.2 Entity Analysis

We now explore the content of images with a special focus
on the entities they contain, which allows us to better under-
stand what “messages” images were used to convey. To do so,
we use the image processing pipeline presented in [37] to cre-
ate clusters of visually similar images but leverage Google’s
Cloud Vision API to annotate each cluster (as discussed in the
Methodology section).
Then, for each image, we assign the entity with the highest
conﬁdence score as returned by the Cloud Vision API. We also
associate the tweet metadata to each image (i.e., which image
appears in which tweet). The ﬁnal annotated dataset allows us
to study the popularity of entities in images posted by state-
sponsored accounts on Twitter.
Popular Entities. We ﬁrst look at the popularity of entities
for the trolls: Table 1 reports the top 20 entities that appear in
our image dataset both in terms of the number of clusters, as
well as the number of images within the clusters. We observe
that the two most popular entities for Russian trolls are refer-
ring to Russia itself (i.e., “Russia” and “Vladimir Putin” enti-
ties). Also, trolls are mainly focused on events related to Rus-
sia, Ukraine, USA, and Syria (their top entities correspond to
these countries). Moreover, several images include screenshots
of news articles (see entity “Web page”) as well as logos of
news sites (see entity “Logo”), hence indicating that these ac-
counts were sharing news articles via images. This is because
the state-sponsored accounts shared URLs of news articles,
which do not include images, hence Twitter automatically adds
the logo of the news site to the tweet. Finally, we ﬁnd a non-
negligible percentage of images and clusters that show memes,
highlighting that memes are exploited by such accounts to dis-
seminate their ideology and probably weaponized information
via memes.

Graph Visualization. To get a better picture of the spectrum of
entities and the interplay between them, we also build a graph,
reported in Figure 6, where nodes correspond to clusters of
images and each edge to the similarity of the entities between
the clusters. For each cluster, we use the set of entities from
the Google Cloud Vision API and calculate the Jaccard sim-
ilarity between each cluster. Jaccard similarity is useful here,
because it exposes meta relationships between clusters. While
images that appear within the same cluster are visually simi-
lar, there are likely to be other clusters that represent the same
subjects, but from a different visual perspective. Then, we cre-
ate an edge between clusters (weighted by their Jaccard simi-
larity) with similarities below a pre-deﬁned threshold. We set
this threshold to 0.4, i.e., we discard all edges between clusters
that have a Jaccard similarity less than 0.4, because we want
to 1) capture the main connections between the clusters and
2) increase the readability of the graph. We then perform the
following operations: 1) we run a community detection algo-
rithm using the Louvain method [5] and paint each community
with a different color; 2) we lay out the graph with the Force
Atlas2 layout [18], which takes into account weights of edges
(i.e., clusters with higher similarity will be positioned closer in
the graph); 3) for readability purposes, we show the top 30%
of nodes according to their degree in the graph; and 4) we
manually annotate the graph with representative images for
each community, allowing us to understand the content within
each community. In a nutshell, this graph allows us to under-
stand the main communities of entities pushed by the state-
sponsored accounts and how they are connected.
Main Communities. From Figure 6, we observe a large
community (sapphire) that corresponds to clusters related to
Vladimir Putin and Russia. This community is tightly con-
nected with communities related to Donald Trump/Hillary
Clinton/USA (green), Ukraine/Petro Poroshenko (light blue),
and Sergey Lavrov (gray). Also, we observe that other big
communities include logos from news outlets (pink) that are
tightly connected with communities including screenshots of
articles (brown), images of documents (light green), and var-
ious other screenshots (emerald). Other communities worth
noting are those including comics and various illustrations
(yellow) as well as images of products and advertisements (or-
ange). Overall, these ﬁndings highlight that state-sponsored
troll accounts shared many images with a wide variety of
themes, ranging from memes to news via screenshots.

4.3

Images Occurrence across the Web

Our next set of measurements analyze the co-occurrence of
the images posted by Russian state-sponsored accounts across
the greater Web. Recall that the Cloud Vision API also pro-
vides details about the appearance of an image across the Web.
This is useful when studying the behavior of state-sponsored
accounts, as it either denotes that they posted the images on
other domains too, or they obtained the image from a different
domain, or that other users on the Web posted them on other
domains too. Thus, studying the domains that shared the same
images as state-sponsored accounts allows us to understand

5

Figure 6: Overview of a subset of the clusters obtained from images shared by the troll accounts.

Domain

#clusters (%) Domain

pinterest.com
twitter.com
youtube.com
wordpress.com
ria.ru
riafan.ru
blogspot.com
livejournal.com
pikabu.ru
me.me
sputniknews.com
reddit.com
theguardian.com
rambler.ru
facebook.com
dailymail.co.uk
imgur.com
wikimedia.org
pinterest.co.uk
wikipedia.org

9,433 (12.0%)
pinterest.com
5,481 (7.0%)
twitter.com
4,132 (5.2%)
youtube.com
3,329 (4.2%)
riafan.ru
3,260 (4.1%)
ria.ru
2,734 (3.4%) wordpress.com
2,432 (3.0%)
blogspot.com
2,381 (3.0%)
sputniknews.com
2,073 (2.6%)
livejournal.com
1,984 (2.5%)
pikabu.ru
1,943 (2.4%)
rambler.ru
1,826 (2.3%) me.me
1,527 (1.9%)
theguardian.com
1,524 (1.9%)
reddit.com
1,336 (1.7%) wikimedia.org
1,271 (1.6%) wikipedia.org
1,210 (1.5%)
facebook.com
1,051 (1.3%)
dailymail.co.uk
1,027 (1.3%)
imgur.com
996 (1.2%)
cnn.com

#images (%)

76,231 (10.1%)
46,609 (6.1%)
40,540 (5.4%)
35,497 (4.7%)
31.153 (4.1%)
30,464 (4.0%)
20,890 (2.7%)
20,558 (2.7%)
20,227 (2.6%)
17,250 (2.2%)
15,227 (2.0%)
14,675 (1.9%)
14,111 (1.9%)
14,025 (1.8%)
12,897 (1.7%)
12,081 (1.6%)
12,012 (1.6%)
9,854 (1.3%)
9,381 (1.2%)
8,606 (1.1%)

Table 2: Top 20 domains that shared the same images as the trolls.
We report the top domains both in terms of number of clusters and
number of images within the clusters.

their behavior and potential impact on the greater Web. For
instance, this information can be used to detect domains that
are exclusively controlled by state-sponsored actors to spread
disinformation.
In Table 2, we report
the top domains, both in terms
of number of clusters and number images within the clus-
ters, that shared the same images as the state-sponsored ac-
counts. Unsurprisingly, the most popular domains are actu-
ally mainstream social networking sites (e.g., Pinterest, Twit-
ter, YouTube, and Facebook). Also, among the popular do-
mains we ﬁnd popular Russian news outlets like ria.ru and
riafan.ru, as well as Russian-owned social networking sites like
livejournal.com3 and pikabu.ru. This highlights the efforts by
Russian trolls to sway public opinion about public matters re-
lated to Russia. We further ﬁnd both mainstream and alterna-
tive news outlets like theguardian.com and sputniknews.com,
respectively (we use the list provided by [38] to distinguish
mainstream and alternative news outlets). This provides evi-
dence that the efforts of Russian trolls had an impact on, or
were inspired by, content shared on a wide variety of impor-
tant sites in the information ecosystem on the Web.
Next, we aim to provide a holistic view of the domains
while considering the interplay between the entities of the im-
ages and the domains that they also shared them. To do this,
we create a graph where nodes are either entities or domains
that were returned from the Cloud Vision API. An edge ex-
ists between a domain node and an entity node if an image
appearing on the domain contained the given entity. Then, we
perform the operations (1) and (2) as described in the enti-
ties analysis section (i.e., community detection and layout al-
gorithm). We do this for the images posted by the trolls and
present the resulting graph in Figure 7. This graph allows
us to understand which domains shared images pertaining to

3Although founded in the US, LiveJournal was sold to a Russian company in
2007, and all servers have been located in Russia since 2017.

6

Products -AdsDonetsk –War in DonbassSoldiers -InfantryProtestsPetroPorosenko-UkraineVladimir Putin -RussiaDonald Trump/Hillary Clinton -USASergey Lavrov -RussiaLogos/Web pagesDocuments/Screenshots of articlesComics/IllustrationsLogos/Fonts/DocumentsScreenshotsCarsMemesVarious spoke-personsFigure 7: Visualization of the interplay between entities and domains that shared matching images as the ones shared by the trolls.

various semantic entities. We ﬁnd popular Web communities
like Twitter, Pinterest, Facebook and YouTube in the middle
of the graph, constituting a separate community (light blue),
i.e., they are used for sharing images across all entities. En-
tities mainly related to Russia are shared via Russian state-
sponsored outlets like sputniknews.com (see orange commu-
nity). Entities that are related to the USA and political persons
like Donald Trump, Barack Obama, and Hillary Clinton are
part of a separate community (pink) with popular news outlets
like washingtonpost.com and nytimes.com. Finally, for mat-
ters related to Ukraine (green community) most of the images
co-appeared on popular Russian-owned social networks like
livejournal.com and pikabu.ru.
Overall, our ﬁndings indicate that the same images often
appear on both their feeds and speciﬁc domains. Thus, state-
sponsored trolls might be trying to make their accounts look
more credible and push their agenda by targeting unwitting
users on popular Web communities like Twitter.

5 Cross-Platform Inﬂuence

Our analysis above studies the occurrence of images shared by
Russian state-sponsored accounts on other domains, but does
not encapsulate the interplay between multiple communities.
In reality, the Web consists of a large number of communi-
ties that are not exclusively independent of each other, as com-
munities naturally inﬂuence each other. For instance, a Twitter
user might share an image on Twitter that she previously saw

on Reddit: in this case, we see that the Reddit community has
“inﬂuenced” the Twitter community with respect to the sharing
of that particular image.
In this section, we model and measure the interplay and
the inﬂuence across Web communities with respect to the dis-
semination of images that were also shared by Russian state-
sponsored accounts on Twitter. In other words, we aim to un-
derstand how inﬂuential the trolls were in spreading images
to other communities. To do so, we rely on Hawkes Pro-
cesses [24, 25], which allow us to estimate the probabilities
that an appearance of an image on one community is due to
that image previously occurring on the same or on another Web
community.

5.1 Hawkes Processes

Hawkes Processes are self-exciting temporal point pro-
cesses [16] that describe how events occur on a set of pro-
cesses. In our setting, events are the posting of an image, and
processes are Web communities. Generally, a Hawkes model
consists of K processes; each process has a rate of events that
dictates the frequency of the creation of events in the speciﬁc
process. The occurrence of an event on one process, causes im-
pulses to the rest of the processes, temporarily increasing the
rate of events in the other processes. The impulses comprise
two useful pieces of information: the intensity of the increase
in the rate, and how it is distributed and decays over time.
By ﬁtting a Hawkes model using Bayesian inference to data

7

Figure 8: Examples of images in the Democratic Party sample.

that describes the appearances of events in the processes, we
obtain the parameter values for the impulses. This lets us quan-
tify the overall rate of events in each process, as well as how
much previous events contribute to the rate, at a given point
in time. Naturally, we cannot possibly know what exactly trig-
gered the creation of an event on a process, however, we can
use Hawkes Processes to calculate the probability that the
cause of an event is another process in the model, as also done
by previous work [38, 37]. Note that the background rate of
the Hawkes Processes allow us to capture and model the inter-
play of external sources (i.e., platforms that we do not use in
our analysis), hence the resulting probabilities encapsulate the
inﬂuence of the greater Web via the background rates.

5.2 Datasets

Cross-Platform Dataset. We use a publicly available dataset
consisting of 160M pHashes and image URLs for all the im-
ages posted on Twitter (using 1% Streaming API), Reddit,
4chan’s /pol/, and Gab, between July 2016 and July 2017.4
Then, we select the images that have the same pHashes with
the ones shared by Russian state-sponsored accounts on Twit-
ter. For each one of these images, we ﬁnd all their occurrences
on Reddit, /pol/, Gab, and Twitter. Next, we omit images that
appear less than ﬁve times across all Web communities we
study, ultimately obtaining a set of 90K pHashes. Finally, we
annotate each pHash using the Web entities obtained from the
Cloud Vision API.
Since our dataset focuses primarily on the year before
and after the 2016 US elections, we concentrate our analy-
sis around this major event. Speciﬁcally, we want to assess
whether Russian state-sponsored accounts were more effective
in pushing images related to the Democratic Party or Repub-
lican Party. To do so, we select all the pHashes that have a
Cloud Vision Web entity corresponding to “Democratic Party,”
“Hillary Clinton,” and “Barack Obama” for the Democratic
Party, and “Republican Party” and “Donald Trump” for the
Republican Party. Using these entities, we ﬁnd 9.9K images
related to the Republican Party and 6K images related to the
Democratic Party.

4 https://zenodo.org/record/1451841.

Figure 9: Examples of images in the Republicans Party sample.

Examples. To provide an intuition on what some of the po-
litically charged images look like, we provide some examples
in Figure 8 and Figure 9 for the Democratic and the Repub-
lican party, respectively. These illustrate how Russian trolls
use images to spread disinformation: for instance, Figure 8(b)
shows Senator Robert Byrd meeting with Hillary Clinton and,
in another occasion, wearing a Ku Klux Klan robe. The im-
age with the robe is known to be fake as reported later by
Snopes [32]. We can also observe how state-sponsored ac-
counts rely on edited/photoshopped images to make speciﬁc
personalities look bad: e.g., Figure 8(a) is an edited image
aimed at reinforcing the idea that Hillary Clinton was involved
in the Pizzagate conspiracy theory (her face was edited and a
baby was added in the picture). Finally, we ﬁnd several memes
that are meant to be funny; however they have a strong political
nature and can effectively disseminate ideology. For instance,
Figure 8(c) makes fun of Hillary Clinton, while Figure 9(a) and
Figure 9(b) are clearly pro-Trump and celebrate him winning
the 2016 US elections.
Events. Table 3 summarizes the number of events for our
dataset. Note that we elect to decouple The Donald subred-
dit from the rest of Reddit mainly because of its strong polit-
ical nature and support towards Donald Trump [13]. By look-
ing at the raw numbers of events per category, we note that in
general Russian state-sponsored accounts shared more content
related to the Republican Party when compared to the Demo-
cratic Party. The same applies for all the other communities
we study: in general we ﬁnd 1.59 times more events for the
Republican Party than the Democratic Party (385K vs 242K
events). This indicates that content related to the Republican
Party was more popular in all Web communities during this
time period and that Russian state-sponsored accounts pushed
more content related to the Republican Party, likely in favor of
Donald Trump as previous research show [40].

8

a)b)c)a)b)c)Republican Party-related Images
Democratic Party-related Images
All images

/pol/

96,569
64,282
409,026

Reddit

85,457
38,602
421,115

Twitter

145,372
96,082
1,904,570

Gab

18,496
13,485
75,361

T D

21,733
17,797
72,679

Russia

18,332
12,465
231,730

Total Events

pHashes

385,959
242,713
3,114,481

9,947
6,043
90,299

Table 3: Number of events (image occurrences) for images shared by Russian state-sponsored accounts. We report the number of events on
Twitter (other users), Russian state-sponsored accounts on Twitter (Russia), Gab, /pol/, Reddit, and The Donald subreddit.

(a)

(b)

Figure 10: Inﬂuence estimation for all images shared by Russian state-sponsored accounts on Twitter: a) Raw inﬂuence between source and
destination Web communities; and b) Normalized inﬂuence (efﬁciency) of each Web community as the results are normalized by the numbers
of events created on the source community. The numbers in the cell can be interpreted as the expected percentage of events created on the
destination community because of previously occurring events on the source community.

5.3 Results

We create a Hawkes model for each pHash. Each model con-
sists of six processes, one for each of Reddit, The Donald sub-
reddit, Gab, Russian state-sponsored accounts on Twitter, and
other Twitter users. Then, we ﬁt a Hawkes model using Gibbs
sampling as described in [24] for each of the 90K pHashes.
Metrics. After ﬁtting the models and obtaining all the pa-
rameters for the models, following the methodology presented
in [37], we calculate the inﬂuence and efﬁciency that each com-
munity had to each other. The former denotes the percentage
of events (i.e., image appearances) on a speciﬁc community
that appear because of previous events on another community,
while the latter is a normalized inﬂuence metric that denotes
how efﬁcient a community is in spreading images to the other
communities irrespectively to the number of events that are
created within the community. In other words, efﬁciency de-
scribes how inﬂuential the posting of a single event to a partic-
ular community is, with respect to how it spreads to the other
communities.

Overall Inﬂuence & Efﬁciency. Figure 10 reports the in-

ﬂuence estimation results for all the events (i.e., all the im-
ages that were shared by Russian state-sponsored accounts
and have at least ﬁve occurrences across all the Web com-
munities we study). When looking at the raw inﬂuence re-
sults (Figure 10(a)), we observe that Russian state-sponsored
accounts had the most inﬂuence towards Gab (2.3%), fol-
lowed by The Donald subreddit (1.8%), and the rest of Red-
dit (1.6%), while they had the least inﬂuence to 4chan’s /pol/
(0.2%). By comparing the inﬂuence of regular Twitter users,
with respect to the dissemination of memes, to the inﬂuence

of the state-sponsored actors (see Figure 11 in extended ver-
sion of [37]5 ), we observe similar patterns. That is, regular
Twitter users were more inﬂuential on Gab (8%), followed by
The Donald (3.6%), and the rest of Reddit (2.8%), while they
had the least inﬂuence on /pol/ (0.7%). This comparison indi-
cates that Russian trolls inﬂuenced other platforms similarly to
regular Twitter users with the difference that the intensity of
their inﬂuence is substantially lower (between 3.5x-1.5x times
lower), mainly due to the fact that Russian trolls consist of a
few thousands accounts. Furthermore, when comparing the re-
sults for Twitter against previous characterizations of Russian
trolls on news URLs (see Figure 14 (a) in [40]), we ﬁnd that
actually Russian trolls were more inﬂuential in spreading news
URLs compared to images (1.29% for news URLs and 0.8%
for images).
When looking at the efﬁciency of Russian state-sponsored
accounts (last row in Figure 10(b)), we ﬁnd that they were
most efﬁcient in pushing the images on Twitter (6.5%) likely
because it is the same social network. Also, they were par-
ticularly efﬁcient in pushing images towards the rest of Red-
dit (2.9%), while again we ﬁnd that they were not very effec-
tive towards 4chan’s /pol/ (0.4%). Furthermore, we report the
overall external efﬁciency of each community towards all the
other communities (right-most column in Figure 10(b)). We
ﬁnd that the most efﬁcient platform in the ones that we study is
The Donald subreddit (68.4%), followed by the rest of Reddit
(19.5%) and the Russian state-sponsored accounts on Twitter
(11.3%). Again, by looking at previous results based on news
(see Figure 15 (a) in [40]), we observe that Russian trolls were

5Available via https://arxiv.org/abs/1805.12512

9

/pol/RedditTwitterGabT_DRussiaDestination/pol/RedditTwitterGabT_DRussiaSource0.64%9.22%7.12%0.93%2.11%93.96%4.35%12.76%3.18%2.88%86.32%1.35%7.73%8.16%13.43%94.44%5.49%2.59%0.12%0.76%64.24%0.13%0.13%0.19%0.75%67.24%9.72%0.82%4.33%1.68%86.42%1.86%2.32%0.80%1.63%0.24%/pol/RedditTwitterGabT_DRussiaTotal Ext.Destination/pol/RedditTwitterGabT_DRussiaSource9.83%0.36%1.64%1.31%4.35%2.17%93.96%19.51%2.39%2.20%0.57%13.03%86.32%1.31%3.55%0.94%0.31%0.53%94.44%1.21%0.56%6.01%0.35%0.74%64.24%3.18%0.74%1.01%68.48%2.39%67.24%10.08%21.47%25.09%9.45%11.30%86.42%0.58%0.75%6.59%2.96%0.42%(a)

(b)

Figure 11: Inﬂuence estimation for images shared by Russian state-sponsored accounts on Twitter related to the Republican party (R) and the
Democratic Party (D): a) Raw inﬂuence between source and destination Web communities; and b) Normalized inﬂuence (efﬁciency) of each
Web community as the results are normalized by the numbers of events created on the source community.

more efﬁcient in spreading news URLs compared to images
(16.95% external inﬂuence for news, while for images we ﬁnd
11.3%).
Politics-related Images. Next, we investigate how our in-
ﬂuence estimation results change when considering only the
politics-related images, and in particular the differences be-
tween the images pertaining to the Republican and Demo-
cratic Parties. Figure 11 reports our inﬂuence and efﬁciency
estimation results for the images related to the Republican
Party (R) and Democratic Party (D). NB: To assess the sta-
tistical signiﬁcance of these results, we perform a two-sample
Kolmogorov-Smirnov test to the inﬂuence distributions of the
two samples and annotate the ﬁgures with an * for cases where
p < 0.01. We make the following observations. First, Rus-
sian state-sponsored accounts were most inﬂuential in pushing
both Democratic and Republican Party-related images to Gab,
The Donald subreddit, and the rest of the Reddit, while again
were the least inﬂuential in spreading these images in 4chan’s
/pol/ (see last row in Figure 11(a)).
Second, when comparing the results for both parties, we ob-
serve that on Twitter they have more or less the same inﬂuence
for both Republicans and Democratic parties (1.3% vs 1.2%),
on Gab they were more inﬂuential in spreading Democratic
Party images when compared to Republican party (4.0% vs
3.1%). For The Donald and the rest of Reddit we observe the
opposite: they were more inﬂuential in spreading Republican
Party related images when compared to the Democratic Party
(see last row in Figure 11(a)).
Third, by looking at the efﬁciency results (Figure 11(b)),
we ﬁnd that again that Russian state-sponsored accounts were
most efﬁcient in spreading political images to big mainstream
communities like Twitter and Reddit (see last row in Fig-
ure 11(b)). Fourth, by looking at the overall external inﬂuence
of the communities (right-most column in Figure 11(b)), we
observe that again The Donald subreddit had the bigger efﬁ-
ciency (over 60% for both parties), followed by the Russian
state-sponsored accounts on Twitter and the rest of Reddit. Fi-
nally, by comparing the efﬁciency of state-sponsored trolls on

all images vs the political-related images (cf. Figure 10(b) and
Figure 11(b)), we ﬁnd that Russian state-sponsored trolls were
over 2 times more efﬁcient in spreading political-related im-
agery when compared to all the images in our dataset (11.3%
vs 23.6% and 25.7%).
Most Inﬂuential Images. Since our inﬂuence estimation ex-
periments are done with the granularity of speciﬁc pHashes,
we can also assess which images the Russian state-sponsored
accounts were more inﬂuential in spreading. To do so, we
sort the inﬂuence results for the Democratic and Republican
Parties according to the external inﬂuence that Russian state-
sponsored accounts had to all the other Web communities, and
report the top three images with the most inﬂuence. Figure 12
and Figure 13 show the three most inﬂuential images shared
by Russian state-sponsored accounts for the Democratic and
Republicans party, respectively.
Evidently, Russian state-sponsored accounts were particu-
larly inﬂuential in spreading images “against” the Democratic
Party: for instance, Figure 12(a) is an image that trolls Nancy
Pelosi, currently serving as speaker of the US House of Rep-
resentatives, while Figure 12(b) shares a political message
against Hillary Clinton’s chances during the 2016 US elec-
tions. On the other hand, the most inﬂuential images related
to the Republican Party (Figure 13) are neutral and likely aim
to disseminate pro-Trump messages and imagery.

6 Discussion & Conclusion

This paper presented a large-scale quantitative analysis of
1.8M images shared by Russian state-sponsored accounts
(“Russian trolls”) on Twitter. Our work is motivated, among
other things, by the fact that social network users tend to
put little effort into verifying information and they are often
driven by visual cues, e.g., images, for re-sharing content [14].
Therefore, as state-sponsored accounts tend to post disinfor-
mation [26], analyzing the images they share represents a cru-
cial step toward understanding and countering the spread of
false information on the Web, and its impact on society.

10

/pol/RedditTwitterGabT_DRussiaDestination/pol/RedditTwitterGabT_DRussiaSourceD: 1.94%R: 2.11%D: 8.21%R: 9.72%D: 5.10%R: 6.01%D: 3.28%R: 3.63%*D: 5.44%R: 2.82%D: 91.46%R: 92.02%*D: 7.61%R: 13.82%*D: 11.25%R: 11.47%D: 1.77%R: 3.18%D: 4.85%R: 7.52%*D: 75.57%R: 83.47%*D: 1.52%R: 1.71%D: 8.15%R: 8.55%D: 8.77%R: 8.56%*D: 13.90%R: 15.71%D: 85.95%R: 83.14%*D: 6.27%R: 6.08%*D: 4.04%R: 3.50%*D: 0.38%R: 0.36%D: 0.70%R: 0.78%D: 66.26%R: 58.91%*D: 0.42%R: 0.37%D: 0.26%R: 0.21%D: 0.14%R: 0.22%D: 3.42%R: 3.52%D: 69.23%R: 67.23%*D: 8.93%R: 13.02%D: 4.17%R: 4.11%*D: 10.97%R: 5.60%*D: 2.49%R: 2.24%D: 78.50%R: 71.63%*D: 1.84%R: 2.23%D: 4.04%R: 3.17%D: 1.34%R: 1.23%D: 1.48%R: 1.82%*D: 0.35%R: 0.31%/pol/RedditTwitterGabT_DRussiaTotal Ext.Destination/pol/RedditTwitterGabT_DRussiaSourceD: 11.89%R: 11.70%D: 0.38%R: 0.40%D: 2.27%R: 2.19%D: 1.07%R: 1.15%D: 4.90%R: 5.47%*D: 3.27%R: 2.49%D: 91.46%R: 92.02%*D: 22.87%R: 21.29%D: 2.46%R: 2.97%*D: 5.19%R: 2.92%D: 0.62%R: 0.69%D: 12.07%R: 12.79%*D: 75.57%R: 83.47%*D: 2.54%R: 1.94%D: 9.85%R: 10.26%D: 1.06%R: 1.08%D: 1.62%R: 1.28%*D: 1.95%R: 2.00%D: 85.95%R: 83.14%*D: 2.52%R: 3.57%*D: 2.70%R: 2.33%*D: 5.70%R: 6.35%D: 0.35%R: 0.35%D: 0.92%R: 0.92%D: 66.26%R: 58.91%*D: 2.99%R: 2.93%D: 0.75%R: 0.99%D: 0.69%R: 1.16%D: 64.46%R: 73.49%D: 2.39%R: 2.97%D: 69.23%R: 67.23%*D: 6.76%R: 11.08%D: 22.51%R: 27.51%*D: 23.80%R: 22.00%*D: 8.99%R: 9.93%D: 23.69%R: 25.72%D: 78.50%R: 71.63%*D: 2.62%R: 2.64%D: 4.37%R: 3.20%D: 10.31%R: 9.75%D: 4.59%R: 8.49%*D: 1.79%R: 1.64%(a)

(b)

(c)

Figure 12: Top three most inﬂuential images related to the Democratic Party shared by Russian state-sponsored accounts.

(a)

(b)

(c)

Figure 13: Top three most inﬂuential images related to the Republican Party shared by Russian state-sponsored accounts.

By extending the image processing pipeline presented
in [37], we clustered the images and annotated them using
Google’s Cloud Vision API. Our analysis shed light on the con-
tent and targets of these images, ﬁnding that Russian trolls had
multiple targets: mainly the USA, Ukraine, and Russia. Fur-
thermore, we found an overall increase in image use after 2016
with a peak in activity during divisive real-world events like the
Charlottesville rally. Finally, by leveraging Hawkes Processes,
we quantiﬁed the inﬂuence that Russian state-sponsored ac-
counts had with respect to the dissemination of images on the
Web, ﬁnding that these accounts were particularly inﬂuential
in spreading politics-related images. Also, by comparing our
results to previous analysis made on news URLs, we ﬁnd that
these actors were more inﬂuential and efﬁcient in spreading
news via URLs when compared to images.

Our ﬁndings demonstrate that state-sponsored accounts pur-
sued a political agenda, aimed at inﬂuencing users on Web
communities w.r.t. speciﬁc world events and individuals (e.g.,
politicians). Some of our ﬁndings conﬁrm previous analy-
sis performed on the text of the tweets posted by these ac-
counts [40], highlighting how state-sponsored actors post im-
ages that are conceptually similar to their text, possibly in
an attempt to make their content look more credible. Our
inﬂuence estimation also demonstrated that Russian state-
sponsored accounts were particularly inﬂuential and efﬁcient
in spreading political images to a handful of Web communi-
ties. Also considering the relatively small number of Russian
state-sponsored accounts that were actually identiﬁed by Twit-
ter, our analysis suggests that these actors need to be taken very
seriously in order to tackle online manipulation and spread of
disinformation.

Naturally, our study is not without limitations. First, our
pipeline relies on a closed-system (i.e., Cloud Vision API)
with a relatively unknown methodology for extracting entities.
However, our small-scale manual evaluation showed that the
API provides an acceptable performance for our needs. Sec-
ond, we study the images in isolation, without considering
other features of the tweets like shared text, thus we may lose
important knowledge that exists in the text like sentiment, en-
tities that are referenced, toxicity, etc. Finally, our study relies
on a dataset that is independently identiﬁed and released by
Twitter, and the methodology for identifying these accounts is
unknown and it is unclear on whether there are false positives
within the dataset.

Implications of our work. Overall, our study has several im-
plications related to the exploitation of social media by Rus-
sian state-sponsored actors, who share weaponized informa-
tion on divisive matters with the ultimate goal of sowing dis-
cord and inﬂuencing online discussions. As such, their activi-
ties should be considered as having broader impact than “just”
political campaigns, rather, as direct attacks against individu-
als and communities, since they can lead to erratic real-world
behavior outside the scope of any particular election—e.g., dis-
ease epidemics as parents are not vaccinating kids because of
disinformation [7, 26]. We also argue that the public should be
adequately informed about the existence and the strategies of
these actors, particularly their use of weaponized information
beyond just “fake news,” as a necessary step toward educating
users in how to process and digest information on the Web.
Our analysis also complements, to some extent, the Mueller
Report [28]. Although it represents the ﬁrst comprehensive
investigation of large-scale state-sponsored “information war-

11

fare,” much of the Report currently remains redacted. Even if
it is eventually released in its entirety, it is unlikely to con-
tain a quantitative understanding of how these state-sponsored
actors behaved and what kind of inﬂuence they had. Further-
more, state-sponsored attacks are reportedly still on going [3].
While still awaiting scientiﬁc study, new campaigns, including
for instance the Qanon conspiracy theory, have been launched
by Russian trolls, and at least partially supported by the use
of images initially appearing on imageboards like 4chan and
8chan [9].
Overall, our work can be beneﬁcial to policy makers, law en-
forcement, and military personnel, as well as political and so-
cial scientists, historians, and psychologists who will be study-
ing the events surrounding the 2016 US Presidential Elections
for years to come. Our scientiﬁc study of how state sponsored
actors used images in their attacks can serve to inform this type
of interdisciplinary work by providing, at minimum, a data-
backed dissection of the most notable and effective informa-
tion warfare campaign to date.
Finally, the research community can re-use the tools and
techniques presented in this paper to study image sharing
by various teams or communities on the Web, e.g., state-
sponsored accounts from other countries, bots, or any coordi-
nated campaign. In fact, Twitter recently released new datasets
for state-sponsored trolls that originate from Venezuela and
Bangladesh [30]; our techniques can be immediately be ap-
plied on this data.
Future Work. As part of future work, we plan to study the use
of news articles and social network posts from state-sponsored
accounts with a particular focus on detecting possibly doctored
images. Finally, we aim to build on top of our work to detect
domains that are controlled by state-sponsored actors and aim
to push speciﬁc (disinformation) narratives on the Web.
Acknowledgments. This project has received funding from
the European Union’s Horizon 2020 Research and Innovation
program under the Marie Skłodowska-Curie ENCASE project
(GA No. 691025). This work reﬂects only the authors’ views
and the Commission are not responsible for any use that may
be made of the information it contains.

References

[1] A. Badawy, E. Ferrara, and K. Lerman. Analyzing the Digital
Traces of Political Manipulation: The 2016 Russian Interference
Twitter Campaign. In ASONAM, 2018.
[2] A. Badawy, K. Lerman, and E. Ferrara. Who Falls for Online
Political Manipulation? In WWW Companion, 2019.
[3] J. Barnes and A. Goldman.
F.B.I. Warns of Russian In-
terference in 2020 Race and Boosts Counterintelligence Op-
erations. https://www.nytimes.com/2019/04/26/us/politics/fbi-
russian- election- interference.html, 2019.
[4] J. Berger and K. L. Milkman. What makes online content viral?
Journal of marketing research, 2012.
[5] V. D. Blondel, J.-L. Guillaume, R. Lambiotte, and E. Lefebvre.
Fast Unfolding of Communities in Large Networks. Journal of
Statistical Mechanics: Theory and Experiment, 2008(10), 2008.

[6] R. L. Boyd, A. Spangher, A. Fourney, B. Nushi, G. Ranade,
J. Pennebaker, and E. Horvitz. Characterizing the Internet
Research Agency’s Social Media Operations During the 2016
US Presidential Election using Linguistic Analyses. PsyArXiv,
2018.
[7] D. A. Broniatowski, A. M. Jamison, S. Qi, L. AlKulaib, T. Chen,
A. Benton, S. C. Quinn, and M. Dredze. Weaponized health
communication: Twitter bots and Russian trolls amplify the vac-
cine debate. American Journal of Public Health, 108(10), 2018.
[8] C. Caron. Heather Heyer, Charlottesville Victim, Is Recalled as
“a Strong Woman”. https://nyti.ms/2vuxFZx, 2017.
[9] B. Collins and Murphy.
Russian troll accounts purged
by Twitter pushed Qanon and other conspiracy theories .
https://www.nbcnews.com/tech/social-media/russian- troll-
accounts- purged- twitter- pushed- qanon- other- conspiracy-
theories- n966091, 2019.
[10] D. E. R. Denning. Information Warfare and Security. 1999.
[11] R. Dutt, A. Deb, and E. Ferrara. “Senator, We Sell Ads”: Anal-
ysis of the 2016 Russian Facebook Ads Campaign.
In Inter-
national Conference on Intelligent Information Technologies,
2018.
[12] M. Ester, H.-P. Kriegel, J. Sander, X. Xu, et al. A Density-Based
Algorithm for Discovering Clusters in Large Spatial Databases
with Noise. In KDD, 1996.
[13] C. I. Flores-Saviaga, B. C. Keegan, and S. Savage. Mobilizing
the trump train: Understanding collective action in a political
trolling community. In ICWSM, 2018.
[14] M. Gabielkov, A. Ramachandran, A. Chaintreau, and A. Legout.
Social clicks: What and who gets read on Twitter? ACM SIG-
METRICS Performance Evaluation Review, 2016.
[15] V. Gadde and Y. Roth. Enabling further research of informa-
tion operations on Twitter.
https://blog.twitter.com/ofﬁcial/
en us/topics/company/2018/enabling- further- research- of-
information- operations- on- twitter.html, 2018.
[16] A. G. Hawkes. Spectra of some self-exciting and mutually ex-
citing point processes. Biometrika, 58(1), 1971.
[17] J.
Im, E. Chandrasekharan, J. Sargent, P. Lighthammer,
T. Denby, A. Bhargava, L. Hemphill, D. Jurgens, and E. Gilbert.
Still out there: Modeling and Identifying Russian Troll Ac-
counts on Twitter. arXiv:1901.11162, 2019.
[18] M. Jacomy, T. Venturini, S. Heymann, and M. Bastian. ForceAt-
las2, a continuous graph layout algorithm for handy network vi-
sualization designed for the Gephi software. PloS one, 9(6),
2014.
[19] M. Jenders, G. Kasneci, and F. Naumann. Analyzing and pre-
dicting viral tweets. In WWW, 2013.
[20] M. Jensen. Russian Trolls and Fake News: Information or Iden-
tity Logics? Journal of International Affairs, 71(1.5), 2018.
[21] A. Khosla, A. Das Sarma, and R. Hamid. What makes an image
popular? In Proceedings of the 23rd international conference
on World wide web, 2014.
[22] D. Kim, T. Graham, Z. Wan, and M.-A. Rizoiu. Tracking the
Digital Traces of Russian Trolls: Distinguishing the Roles and
Strategy of Trolls On Twitter. arXiv:1901.05228, 2019.
[23] K. Koroleva, H. Krasnova, and O. G ¨unther. Stop spamming me!:
Exploring information overload on Facebook. In Americas Con-
ference on Information Systems, 2010.

12

[24] S. W. Linderman and R. P. Adams. Discovering Latent Network
Structure in Point Process Data. In ICML, 2014.
[25] S. W. Linderman and R. P. Adams. Scalable Bayesian Infer-
ence for Excitatory Point Process Networks. ArXiv:1507.03228,
2015.
[26] U. A. Mejias and N. E. Vokuev. Disinformation and the me-
dia: the case of Russia and Ukraine. Media, Culture & Society,
39(7), 2017.
[27] V. Monga and B. L. Evans. Perceptual Image Hashing Via Fea-
ture Points: Performance Evaluation and Tradeoffs. IEEE Trans-
actions on Image Processing, 2006.
[28] R. S. Mueller. Report On The Investigation Into Russian Inter-
ference In The 2016 Presidential Election. US Department of
Justice, 2019.
[29] C. M. Rivers and B. L. Lewis. Ethical research standards in a
world of big data. F1000Research, 3, 2014.
[30] Y. Roth. Empowering further research of potential informa-
tion operations. https://blog.twitter.com/en us/topics/company/
2019/further research information operations.html, 2019.
[31] G. Rowett. The Strategic Need to Understand Online Memes
and Modern Information Warfare Theory. In IEEE Big Data,
2018.
[32] Snopes. Senator Robert Byrd in Ku Klux Klan Garb. https:
//www.snopes.com/fact- check/robert- byrd- kkk- photo/, 2016.
[33] A. Spangher, G. Ranade, B. Nushi, A. Fourney, and E. Horvitz.
Analysis of Strategy and Spread of Russia-sponsored Content in
the US in 2017. arXiv:1810.10033, 2018.

[34] H. Spencer. A Far-Right Gathering Bursts Into Brawls. https:
//nyti.ms/2uTmIgV, 2017.

[35] L. G. Stewart, A. Arif, and K. Starbird. Examining Trolls and
Polarization with a Retweet Network. In WSDM, 2018.

[36] C. Wrzus, M. H ¨anel, J. Wagner, and F. J. Neyer. Social network
changes and life events across the life span: a meta-analysis.
Psychological bulletin, 2013.

[37] S. Zannettou, T. Caulﬁeld, J. Blackburn, E. De Cristofaro,
M. Sirivianos, G. Stringhini, and G. Suarez-Tangil. On the Ori-
gins of Memes by Means of Fringe Web Communities. In ACM
IMC, 2018.

[38] S. Zannettou, T. Caulﬁeld, E. De Cristofaro, N. Kourtellis,
I. Leontiadis, M. Sirivianos, G. Stringhini, and J. Blackburn.
The Web Centipede: Understanding How Web Communities In-
ﬂuence Each Other Through the Lens of Mainstream and Alter-
native News Sources. In ACM IMC, 2017.

[39] S. Zannettou, T. Caulﬁeld, E. De Cristofaro, M. Sirivianos,
G. Stringhini, and J. Blackburn. Disinformation Warfare: Un-
derstanding State-Sponsored Trolls on Twitter and Their Inﬂu-
ence on the Web. In WWW Companion, 2019.

[40] S. Zannettou, T. Caulﬁeld, W. Setzer, M. Sirivianos, G. Stringh-
ini, and J. Blackburn. Who Let The Trolls Out? Towards Under-
standing State-Sponsored Trolls. In WebSci, 2019.

[41] S. Zannettou, J. Finkelstein, B. Bradlyn, and J. Blackburn. A
Quantitative Approach to Understanding Online Antisemitism.
In ICWSM, 2020.

13

