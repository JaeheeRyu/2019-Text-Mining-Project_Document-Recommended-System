9
1
0
2

v
o

N

1
2

]

V

C

.

s

c

[

1
v
5
2
3
9
0

.

1
1
9
1

:

v

i

X

r

a

Simultaneous Implementation Features Extraction and Recognition Using C3D
Network for WiFi-based Human Activity Recognition

Yafeng Liu,∗ Tian Chen, Zhongyu Liu, and Enjie Ding

Internet of Things Research Center, China University of Mining and Technology, Xuzhou, China

Lei Zhang†

Xuzhou University of Technology

Yanjun Hu

School of Information and Control Engineering, China University of Mining and Technology, Xuzhou, China
(Dated: November 22, 2019)

Human actions recognition has attracted more and more people’s attention. Many technology
have been developed to express human action’s features, such as image, skeleton-based, and channel
state information(CSI). Among them, on account of CSI’s easy to be equipped and undemanding for
light, and it has gained more and more attention in some special scene. However, the relationship
between CSI signal and human actions is very complex, and some preliminary work must be done
to make CSI features easy to understand for computer. Nowadays, many work departed CSI-based
features’ action dealing into two parts. One part is for features extraction and dimension reduce,
and the other part is for time series problems. Some of them even omitted one of the two part
work. Therefore, the accuracies of current recognition systems are far from satisfactory. In this
paper, we propose a new deep learning based approach, i.e. C3D network and C3D network with
attention mechanism, for human actions recognition using CSI signals. This kind of network can
make feature extraction from spatial convolution and temporal convolution simultaneously, and
through this network the two part of CSI-based human actions recognition mentioned above can be
realized at the same time. The entire algorithm structure is simpliﬁed. The experimental results
show that our proposed C3D network is able to achieve the best recognition performance for all
activities when compared with some benchmark approaches.

I.

INTRODUCTION

Human action recognition concerns the task of auto-
matically interpreting a sample sequence to decide what
action or activity is being performed by the sub jects in
the scene [1–5]. It is a relevant topic in artiﬁcial intelli-
gence, with practical applications such as video surveil-
lance, human-computer interaction, gaming, sports arbi-
tration, sports training, smart homes, life-care systems,
among many others [6–8]. As we know, there are various
behavior recognition modalities based on diﬀerent types
of signals. Camera and video are the most traditional
and important approach for recording human activities
[9, 10]. Moreover, optical ﬂow and skeleton sequence are
always also employed as kinds of action recording tools
[11, 12]. However, all of methods mentioned above have
the fundamental limitations of requiring enough lighting
and demanding personal privacy protecting. Wearable
sensors are another popular for human activity recog-
nition due to the high recognition accuracy [13]. But
the systems based on wearable sensors require users to
take extra devices for activity recognition, which is in-
convenient and obstructive for users. Otherwise, some
sensors, such as accelerator, gyroscope and barometer,
can be embedded in mobile terminal, and the terminal

∗ yafeng@cumt.edu.cn
† lei@cumt.edu.cn

can be treated as a sensing platform for human activ-
ity recognition [14]. Some weakness also exists here. So
many sensors running in terminal will increase their bat-
tery usage, which means in some cases they are restricted
by the amount of battery power.

Comparing with conventional human activity recogni-
tion signal, Wireless sensing technology is a new sens-
ing mechanism that does not require sensor devices to
be installed or attached to the target ob ject [15]. It is
also known as a device-free sensing technology. It makes
human activity signal collecting more convenient. Intu-
itively, when a person resides in the surrounding area of a
WiFi transceiver pair, his/her body movement will aﬀect
the travel-through WiFi signals. Through analysing such
signal characteristics as coarse-grained received signal
strength indicator(RSSI) and ﬁne-grained channel state
information(CSI), diﬀerent activities can be recognized.
RSSI can be simply accepted by commercial receiver de-
vice, such as mobile phone. however RSSI accepted by
current signal receiving devices is not very stable, and
is easily aﬀected by the environment leading to spurious
detections. For this reason, it’s not suitable for human
activity recognition [16]. The most recent device-free hu-
man motion and activity recognition studies are based on
CSI instead, because CSI outperforms RSSI in its diver-
sity and stability. CSI is a kind of WiFi owning a more
informative characteristic, which has attracted more and
more attention due to the abundant and stable informa-
tion in it [17–20].

 
 
 
 
 
 
CSI is more ﬁne-grained information being tracked by
Multiple-input multiple-output(MIMO) communications
[21–25]. It contains amplitude and phase measurements
separately for each orthogonal frequency-division multi-
plexing(OFDM) subcarriers. To collection CSI informa-
tion, we need to set up a transceiver device. This de-
vice consists of the most common homes/oﬃces WiFi de-
vices(with one antenna) and any PC/laptop(with three
antennas). The homes/oﬃces WiFi devices act as the
transmitter, and the PC/laptop act as the receiver. A
person act in the certain area of a WiFi transceiver pair,
his/her body movement will aﬀect the travelthrough Wiﬁ
signals. Through analysing CSI signal characteristics,
diﬀerent activities can be recognized. However, the raw
CSI signals corresponding to human activity are hard
to obtain, since the signals are always aﬀected severely
by the wall and indoor physical environment, such as
reﬂection, diﬀraction, and scattering. To overcome this
problem, we use a convolution network to extract the key
feature. We arrange several sequential CSI signals as a
data matrix, we call it ’a CSI image’ here, and an inte-
grated action is made up by such images. So we change
this problem into a video classiﬁcation problem. An 3D
convolution network will be employed to solve this prob-
lem.
Deep learning has developed amount of network deal-
ing with spatio-temporal event, such as long-short term
memory(LSTM) [26, 27], two-stream method [28] and 3D
convolution network(3D-CNN) [29, 30]. Diﬀerent from
convolution network, LSTM does not possess feature ex-
traction ability. Some other methods, such as princi-
pal component analysis(PCA) and discrete wavelet trans-
form(DWT), are needed to extract useful CSI informa-
tion from CSI signal. For a CSI image,
it’s hard to
give out a clear deﬁnite to its optic ﬂow, and two-stream
methods should be done some changes before it suit for
these samples. We don’t discuss this here. Therefore,
we employ 3D-CNN network for dealing with our prob-
lems. 3D-CNN use a form of spatio-temporal convolution
means extracting sample features from images sequen-
tial. The convolution kernel is a three-dimensional data
square. Two dimension of the square is spatial convo-
lution, and the remaining dimension is temporal convo-
lution for time series. The spatial convolution is used
for training network for denoising of CSI signals.
Im-
ages contains redundant information. For example, if we
want to make faces recognition, we need extract faces
information from an image. The background, light ray
and human body is useless, and these information can be
thrown away by well trained convolution network. Here,
we regard signals aﬀected by the wall and indoor phys-
ical environment as useless information. With the help
of nonlinear transform of convolution network, the useful
raw CSI information can be extracted from CSI images.
The spatial attention mechanism is also employed here
to get higher accurate results.
The main contributions of this study can be summa-
rized as follows:

2

1. To the best of our knowledge, our approach is one of
the ﬁrst to integrate 3D convolution network for explor-
ing CSI temporal signals for action recognition. Diﬀerent
from LSTM approaches, we can use 3D-CNN network to
extract useful raw CSI information from original signal
without preprocessing.
2. Soft attention mechanism in the spatial convolution
and temporal convolution make our recognition process-
ing focus on important CSI signals fragments. Integrated
with 3D-CNN, the entire system beneﬁts from not only
the comprehensive entire CSI signals, but also local CSI
signals that are relevant to actions.
3. Diﬀerent from the input in LSTM, we carve the
sequential CSI signals of action samples up, and arrange
them into several windows. In the way, the CSI arrays
are changed into sorts of video-like data, and then they
can be dealt by 3D-CNN network.
The rest of this paper is organized as follows. Section
II lists out some related work of this motivation. We de-
scribe the preprocessing scheme and the 3D-CNN based
recognition approach in section III. The implementation
and evaluation are presented in section IV. We make the
discussion and conclusion in the last section.

II. RELATED WORK

We present behavior recognition based on WiFi signal
in this part and introduce the contents from signal types
and their corresponding applications. Also, we introduce
how deep learning algorithm developed to dealing with
this kind of signal.

1.

channel state information

Unlike other WiFi devices, CSI cannot be directly
measured with commercial oﬀ-the-shelf(COTS) devices.
They must be measured by modifying the device driver
on Intel 5300 network interface card (NIC), Atheros 9580,
and Atheros 9390. With these drivers’ help, CSI sig-
nals are widely available, many CSI based human ac-
tivity recognition systems have been developed in the
literature, such as Wi-Vi[31], E-eyes[24], Wihear[32],
CARM[33], Wigest[22], RT-Fall[23] and others. wang
et al.[32] used specialized directional antennas to obtain
CSI variations caused by lip movement for recognizing
spoken words. They detected and analysed CSI signals
reﬂections from moth movements, and achieved high ac-
curacy for people talking without line-of-sight. Wang et
al.[23] presented a real-time, contactless, low-cost indoor
fall detection using the commodity WiFi devices called
RT-Fall. The system can recognize the fall in the con-
dition that numerous daily activities are performed. Wu
et al.[34] proposed a TW-see system for human activ-
ity recognition. The system can realize the detection of
human action through the wall without any dedicated
device.

The problem of activity recognition is somewhat sim-
ilar to the speech recognition process, and it’s also time
series sample, essentially. Recently, people have devel-
oped many deep learning network ﬁtting for sequen-
tial data, such as recurrent neural network(RNN), long-
short term memory(LSTM), 3D convolution network(3D-
CNN)[2, 35]. Some deep learning network are also em-
ployed in dealing with CSI temporal signals. There are
two main means used before: one is arranging CSI sig-
nal into an image-like matrix, and extracting its feature
from convolution neural network(CNN) [36, 37]. The
other one is direction making CSI temporal signals as
LSTM’s inputting, and obtaining the action classiﬁca-
tion from LSTM’s output. Wang et al.[36] proposed a
sparse autoencoder network, and CSI signal features can
be learned automatically from the network. Another net-
work is needed to estimate location, activity, and gesture
recognition through these features. Gao et al.[37] trans-
formed CSI temporal signals from diﬀerent channels into
images, and learned these images features with the help
of deep learning network. In the end, a softmax regres-
sion was employed for localization and activity classiﬁ-
cation. Chen et al.[38] tried Bidireciton LSTM(BLSTM)
on the raw CSI temporal signals, and attention mecha-
nism is simultaneously added into the network for higher
accurate.
Both of the CNN and LSTM networks have their own
advantage for dealing CSI temporal signals, nevertheless
some limitations are exist in these two methods. For
CNN, the temporal features are integrated into spatial
convolution process. The temporal features are impor-
tant for action recognition. Mixing spatial and temporal
features in one convolution kernel make training step con-
fuse, and the result may be aﬀected. For LSTM, lacks
of features extraction will be needed another work de-
noising and dimension reduction. For this reason, some
other network needed to be employed for increasing ac-
curacy. Based on these considerations, we will employ
3D-CNN for CSI temporal signal spatial and temporal
features extraction. Diﬀerent from 2D convolution ker-
nel, 3D convolution kernel is formed by two dimensional
spatial dimension and one dimensional temporal dimen-
sion. Therefore, 3D-CNN can extract spatial features
and temporal features separately simultaneously. In this
way, we can realize end-to-end action recognition based
on CSI temporal signal. Some related work about 3D-
CNN is introduced follow.

2. action recognition

Nowadays, There are four main kind of action recog-
nition methods based on sequential sample. They are
respectively traditional methods, CNN based methods,
RNN based methods, and 3D-CNN based methods. Tra-
ditional methods usually utilize image’s pixel relation-
ship, and extract image features from them.
It com-
monly includes spatial-temporal interest points(STIPs),

3

histogram of gradients(HOG), histogram of optical
ﬂows(HOF) and so on[39]. Inspired by the great success
of deep learning models in computer vision tasks, many
deep learning networks have been proposed for action
recognition. The left three methods are exactly based on
three diﬀerent deep learning networks.
CNN models[28, 40] are very good at extracting spatial
appearance features from sub jects and backgrounds, but
they are diﬃcult for extracting temporal features. For
this reason, CNN based methods should learn motion
features utilizing another means, and then fuse with spa-
tial features as action expression. The most frequently
used CNN based methods are slow fusion model[40] and
two-stream model[28]. Slow fusion model extends the
connectivity of all convolutional layers in time and com-
putes activations through temporal convolutions in ad-
dition to spatial convolutions; while two-stream model
learns spatial features and motion features from two indi-
vidual networks, namely SpatialNet, which is trained on
single RGB frame and TemporalNet, which is trained on
consecutive ﬂow frames. The conﬁdence scores of Spatial-
Net and TemporalNet are fused to classify actions. Some
other improved models based on Two-stream model are
also always used as action recognition methods, such as
temporal segment network(TSN), tra jectory-pooled deep
convolutional descriptors(TDD) and so on.
RNN models[41] can not only record current obser-
vation but also store past information by hidden state,
through which they can be eﬀective in capturing tempo-
ral information. Therefore, RNN models are widely ap-
plied in action recognition to capture the motion features
in videos. The general process for GNN based methods
is that CNN-like models are needed to extract frame-
wise features before videos are input into RNN models.
The common used RNN based methods is LSTM and
gated recurrent unit(GRU). According to the general pro-
cess, RNN based methods can extract spatial-temporal
features utilizing frame-wise features. A soft attention
mechanism is sometimes used for key features extraction
throughout the video sequence.
3D-CNN model[29, 42–44] extends a 2D convolution
to the temporal domain, to extract spatial-temporal
featrues for action recognition. It gives consideration to
spatial features extraction and temporal features extrac-
tion. By this means, one side we can deal with csi sig-
nals denosing and dimensional reduction, and the other
side we can deal with sequential signals’ temporal infor-
mation. The CSI sequential signals are taken as input
directly and we don’t rely on any preprocessing. Consid-
ering additional kernel dimension having more parame-
ters than 2D CNN and RNN models, some pre-training
models will be borrowed for our networks.

III. METHODS

The main technique used in this article are WiFi-based
sensing technology and 3D convolution neural networks.

CSI is considered the new trending metric in WiFi-based
sensing technology. With the help of commodity wireless
Network Interface Controllers(NICs), CSI signals can be
extracted. CSI is a kind of collecting information that
describes how wireless signals propagate from the trans-
mitter to the receiver.

A. Channel State Information

Figure 1. CSI Action signals acquisition device

In the IEEE 802.11n/ac standards, CSI is measured
and parsed with MIMO and OFDM technology[15]. We
suppose that there are nt transmitted antenna, and based
on the MIMO model the received signal of the j antenna
yj (t) can be expressed in mathematics form:

NTx(cid:88)
i=1

hi,j (f , t) ∗ xi (t) + ηj (f , t),
yj (f , t) =
i = 1, 2, · · · , nt ; j = 1, 2, · · · , nr

(1)

where H (i, j )(f , t) is the complex valued channel fre-
quency response(CFR) for carrier frequency f measured
at time t between the transmitted antenna i and received
antenna j, which is representing CSI information. X (i)

4

is the transmitted signals of the antenna i, and etaj (t) is
an additive white Gaussian noise. Next work is totally
based on these CFR values. Let NTx and NRx represent
the number of transmitting and receiving antennas, re-
spectively. Each CSI signals contains 30 selected OFDM
subcarriers between an antenna pair, therefore each CSI
signals contains 30 matrices with dimensions NTx × NRx .
Each entry in these matrix is a CFR value at a certain
OFDM subcarrier frequency at a particular time. In con-
is 30 × NTx × NRx .
clusion, the dimension of a time-series of entire CSI signal
The wireless transmitted signals arrives at the receiver
through multiple paths,
including the LOS path and
paths reﬂected by surrounding ob jects. Supposed there
are N diﬀerent paths, H (f , t) can be given as follows
without the additive noise:

H (f , t) = exp−j 2π∆f t

ak (f , t) exp−j 2πf τk (t)

(2)

N(cid:88)

k=1

where ak (f , t) and τk (t) are the complex channel attenu-
ation and the time of ﬂight for the k th path, respectively.
exp−j 2π∆f t is phase shift caused by the carrier frequency
diﬀerence between sender and the receiver.
When a person moves in the CSI signals propagation
region, the frequency observed at the receiver will be
changed. This is called the Doppler eﬀect. Accord-
ing to the eﬀect, We divide the CFR into two cate-
gories such as static CFR and dynamic CFR as in ﬁg-
Hd (f , t) = (cid:80)
ure 3. Dynamic CFR, represented by Hd (f , t), is the
sum of paths aﬀected by human moving, and is given by
ak (f , t) exp−j 2πdk (t)/λ , where Pd is the
set of paths aﬀected by human moving. Using Hs (f ) for
static CFR, Eq.2 can be modiﬁed as
H (f , t) = exp−j 2π∆f t (Hs (f ) +
ak (f , t) exp−j 2πdk (t)

(cid:88)

k∈Pd

λ

)

k∈Pd

(3)
We suppose that every movement in the signal propa-
gation region is very slow, and therefore can be kept at
a constant speed vk (t). The length of the k th path at
time t can be written as dk (t) = dk (0) + vk (t). Then the
instantaneous CFR in time t can be deﬁned as:

(cid:88)

k,l∈Pd
k (cid:54)=l

+ φsk ) +

2|ak (f , t)al (f , t)

(4)

|H (f , t)|2 =

(cid:88)

k∈Pd

× cos(

2π(vk − vl )t
λ

+

2|Hs (f )ak (f , t)|cos(

2πvk t
λ
2π(dk (0) − dl (0))
λ

+

πdk (0)
λ

(cid:88)

k∈Pd

+ φkl ) +

|ak (f , t)|2 + |Hs (f )|2

where 2π(dk (0) − dl (0))/λ + φkl and 2πdk (0)/λ + φsk
are constants representing initial phase shift. From Eq.3
and Eq.4, we can see that the total CFR power is the

sum of a constant oﬀset and a set of sinusoids, where the
frequencies of the sinusoids is a function of the velocity
of path length changes. By measuring the frequencies

PCCeilWallFloorAP5

Figure 2. Phasor represent: The dynamic component shows the subcarrier signals between sender and receiver aﬀected by
persons with diﬀerent moving speed(in the magnitude and direction). The total CFR power is the sum of static component
and dynamic component.

alize an end-to-end human activity recognition based on
CSI temporal signals.
The 3D-CNN is very eﬀective in extracting spatial-
temporal
features from sequential signals for action
recognition. The kernels of 3D-CNN are deﬁned as 5-
dimensional tensors: F ∈ RN ×C×T ×H×W , where C is
the number of input channels, T, H and W are the tem-
poral length, height and width of these kernels, and N is
or internal feature volume is deﬁned as V ∈ RC×L×X×Y ,
the number of output channels. The input video volume
where L, X and Y are the temporal length and spatial
and width of the volume. The operation of each 3D-CNN
kernels Ff ∈ RC×T ×H×W , f = 1, · · · , N is formulated as:
Φf (l, x, y) = V ∗ Ff
V (c, l − t, x − h, y − w)Ff (c, t, h, w)

C(cid:88)

W(cid:88)

T(cid:88)

H(cid:88)

=

c=1

t=1

h=1

w=1

(5)
where l = 1, · · · , L, x = 1, · · · , X and y = 1, · · · , Y are
volume of video.
The C3D net[42] is the most widely used 3D-CNN
models, which employs traditional 3 × 3 × 3 3D homo-
geneous convolutional layer. In theory, one can train a
C3D model with 3 × 3 × 3 kernel as deep as possible to
the limitation of the machine memory and computation
power. As Fig.2, the C3D model is designed to have 8
convolution layers, 5 pooling layers, followed by two fully
connected layers and a softmax output layer. All of 3D
convolution kernels are 3 × 3 × 3 with stride 1 × 1×. All
3D pooling layers are 2 × 2× except for pool1 which has
kernel size of 1 × 2 × 2 and stride 1 × 2 × 2 with the inten-
tion of preserving the temporal information in the early
time. Each fully connected layer has 4096 output units.
In the meantime, we separately add soft attention
mechanism for spatial and temporal features extraction.
For the spatial features, we use a full connection layer
to train the diﬀerent weight matrix of each CSI frame
pixel. With this weight matrix, the important of CSI
frame pixel will be picked up, and assign larger weights to
them. For the temporal features, we use the same method
to search for important frame. The soft attention mech-
anism boost the performance of WiFi CSI based human
activity recognition.

Figure 3. Phasor represent: The dynamic component
shows the subcarrier signals between sender and receiver
aﬀected by persons with diﬀerent moving speed(in the
magnitude and direction). The total CFR power is the sum
of static component and dynamic component.

of these sinusoids and multiplying them with the car-
rier wavelength, we can obtain the speeds of path length
change.
In this way, we can build a CSI-speed model
which relates the variations in CSI power to the move-
ment speeds.

B. Three Dimensional Convolution Network

However, human activity is not a simple ob ject move-
ment, and it has complex shapes and diﬀerent body
parts moving at diﬀerent speeds. Moreover, signals
may be reﬂected through diﬀerent paths in complex
indoor environments. Therefore, we can’t simply use
CSI-speed model to map human activity detail. Tradi-
tional methods employ Time-Frequency analysis tools,
such as Short-Time Fourier Transform(STFT) or Dis-
crete Wavelet Transform(DWT), to separate the speeds
of path length change in the frequency domain for mod-
elling human activity. Here we will focus on deep learning
methods for features extraction, and use 3D-CNN to re-

Conv13×3×3Pool1 1×2×2Conv23×3×3Pool2 2×2×2Conv33×3×3Conv43×3×3Pool3 2×2×2Conv53×3×3Conv63×3×3Conv73×3×3Conv83×3×3Pool4 2×2×2Pool5 2×2×2Fc6 4096Fc7 4096SoftmaxCombined CFRDynamic ComponentStatic ComponentQI6

Figure 4. Our model is implemented in three steps: CSI measurement, sample processing and action recognition. In CSI
measurement, we collect human action signals with commercial WiFi systems. Then, we process the signal into a picture-like
format, and this part will be detailed introduced in following. Last step we will use C3D network to recognize these actions.

Recently, some research propose a kind of eﬀective
net. They separate C3D convolutional ﬁlter into many
cascaded 3D convolutional ﬁlters operating on diﬀerent
directions[29]. By this method, the number of net pa-
rameters is reduced, and comparing with 2D-CNN case,
time comsuming will not be increased so much like C3D
model. Here, time is not the main problems for dealing
with CSI temporal signals, so we simply use C3D model
for our problems.

C. Our Model

Our approach is mainly based on the two techniques
introduced above. As Fig.4, the are three steps in our
approach. The ﬁrst step is CSI signals collection(the ﬁrst
row of Fig.4), next these CSI signals will be changed into
video-volume-like data for C3D net input(the second row
of Fig.4), and at last C3D net will be used for these CSI

signals’ recognition(the last row of Fig.4).

The signal collection method will be detailed introduce
in experiment section of this article. For the second step
of our approach, unlike LSTM situation, CSI temporal
signals can’t be used in C3D model directly, since they
are all vector-like features. Therefore, as Fig4, we ar-
range CSI temporal signals into a CSI image. Then, we
use a ﬁtting windows to cut CSI frame from this CSI im-
age, and it forms a video-volume-like expression for one
human action. As a result, This kind of data can be used
in the C3D model.

In the last step, one kind of 3D-CNN network called
C3D model will be employed as our recognition model.
Since the C3D model is one of the simplest 3D-CNN moel
and eﬀective. It also can extract spatial feature and tem-
poral feature at the same time. We do not need another
method for CSI signals’ denosing and reduce demension.
The video-volume-like prepared in the second step will be
as input, and ﬁtting 3 × 3 × 3 C3D ﬁlters will be trained

BackgroundCSI signal......Frame sequential 8 convolution layers 5 pooling layers 2 full connection Softmax function3×3×3kernel...C3D networkVideo volumeconvpoolfcSoftmaxthrough C3D net and training set. With the help of this
model, our proposed approach for human action recogni-
tion will be realized.

IV. EXPERIMENT

The experiment is mainly carried out in two parts: ﬁrst
we use commercial Wiﬁ device for CSI signals collection,
second we use GPU for C3D network training.

A. CSI Signals Collection Implementation

7

B. Calculate method

The experiment calculate part will be ﬁnished through
C3D network, and the detailed ﬂowchart is shown in
Fig.6. C3D network can help us for CSI signals’ fea-
ture extraction and recognition. Before that CSI signals
should be designed to meet the C3D network input. Our
action’s CSI signal data is a 90 × N matrix, where N
is the activity time of duration, and It’s usually around
1000, as shown in Fig.7. According to the data format,
we raise our method in the left part of Fig.6: A sliding
window(the yellow box in the leftmost part of Fig.6) with
size of 100 is use for data segmentation, and the stride
is set to 8. Therefore, the CSI signals are change into a
series of 90 × 100 CSI images. All of these images are
arranged into a video-volume-data.
It contains almost
200 ∼ 300 frames in the data volume, and therefore it
will take a long time to train these original data. Given
that, We will random selected clips with 16 frames as
like data for one activity is 16 × 90 × 100, the separate
our input for C3D network. The size of video-volume-
yellow box in the middle part of the Fig.6 is just the
frame of the video volume. We divide these data equally
into training set and test set. The training of the C3D
network is to determine all the model parameters based
on the training set with true labels. The test set is used
for verifying the training eﬀect of the network. All the
experiments are carried out on a testbed equipped with
an NVIDIA Geforce GTX 1070Ti GPU card. The dura-
tion time for entire training is no more than 10 minutes,
and the test time is only about 5 seconds for entire test
set, which do not incur much system overhead.

Figure 5. signals collection environment

C.

result

1.

experiment results

The signals collection device is formed by a commer-
cial wiFi router as a transmitter and a PC with Intel
5300 NIC as a receiver. The sampling frequency is set to
500MHz. There are three transmitter antennas and one
receiver antenna in the device. Each antenna owns 30
sub-carriers. Therefore, the raw CSI signals dimension
is 90. The transmitter and the receiver are placed three
meters apart with line-of-sight(LOS) condition. The col-
lection data comes from 5 persons, and each person per-
forms 14 kinds of activities 20 times. The 14 kinds of
activities include bend, draw circle, draw tick, draw x,
drink, squat, hand clap, hand up, hand horizontal mov-
ing, kick, put oﬀ hand, run, sit and walk. During data
collection, each person performs each activity for a pe-
riod of about 20 seconds. Note that, at the beginning
and the end of an activity, the person remains station-
ary. We collected samples for 14 diﬀerent activities in
the lab environment shown in Fig.5. There are several
experiment platform and furniture around the room, and
the device is placed in the middle of the room.

To verify the eﬀectiveness of the proposed approach,
we perform a comparison with some benchmark ap-
proaches for CSI based human activity recognition. Dif-
ferent machine learning techniques have been used for
multi-class classiﬁcation based on certain features that
are extracted. Some of the popular classiﬁcation tech-
niques are Decision Tree(DT), Random Forest(RF), Gra-
dient Boosting Decision Tree(GBDT), support vector
machines (SVMs), and k-Nearest Neighbor(KNN) and
LSTM. The ﬁrst 5 techniques are traditional machine
learning methods and the last one belongs to deep learn-
ing network. However, the data we collected can’t be
used directly here, and some pretreatment must be done
before we put them into these benchmark approaches.
For traditional methods, we calculate the means of ac-
tion temporal sequence according to time variables, and
then we use PCA methods to reduce the noise of the fea-
ture of these mean values. Through the handling, We
can send them into the ﬁve traditional machine learning
methods. The calculation results are shown in the top

TXPC8

Figure 6. The CSI signals are processed into picture-like data, and by this way they can be put into C3D network.

Table I. Some methods are compared with C3D result

Type

Method
PCA + Decision Tree
PCA + Random Forest
TR PCA + GB Decision Tree
PCA + SVM
PCA + k-NearestNeighbor
LSTM
Attention + LSTM
C3D
Attention + C3D

DL

Accuracy(%)
50.28%
56.14%
65.86%
67.14%
69%
74.43%
77.57%
91.14%
93.57%

(a)bend

(b)dun

(c)walk

Figure 7. signals collection environment

half part of Table I. We can see that KNN method can
give out the highest accuracy. For LSTM method, the se-
quential samples are just right ﬁt for the LSTM network’s
input. We only need to use PCA methods to reduce the
noises and dimension of the samples, and then send them
into LSTM network directly. The results through deep
learning methods are shown in the bottom half part of
Table I. The results shown that the LSTM can get a bet-
ter result with attention mechanism, and meanwhile they
can arrive a higher accuracy comparing with traditional
ones.

2.

the C3D methods

At the last two lines of Table I are the C3D calculation
results, whose accuracy is marked with black bold font.

We can see that they outperform than other methods,
including LSTM. It is an accepted result. For traditional
methods, to achieve recognition performance and ﬁt for
employed approach, some manual feature extraction for
human activity recognition must be done in advance, and
just like the temporal sequential average methods used in
this article. However, manual feature extraction requires
export knowledge.
It is also labor intensive and time-
consuming. Besides, when the activities to be recognized
changed, the designed features may be useless. Moreover,
manual features will inevitably miss some implicit key
features. In this article, the temporal sequential methods
only records the overall trend of movement, and detail
may be omitted. Therefore, some similar actions are hard
to recognition.
On the other side, the relationship between temporal
CSI signals and human activities is nontrivial. As we
all know, LSTM network is very good at dealing with
sequential sample. However, our work with CSI signal
is quite diﬀerent from image situation, since the signal
at each time node is not equivalent to the each posture
during the action in time series CSI signal totally.
In
table I, we can see that the result calculated through
LSTM is much higher than traditional method, and it

……InputVideovolumeC3D networkConv layerPooling layerfeature mapfc layersoftmaxrecognitionOriginal samplesproves that LSTM algorithm outperform in dealing with
time series problems. Meanwhile, it’s not good enough
for the 70% 80% accuracy. The problem is that the sin-
gle CSI signal can’t stand for one posture perfectly, and
directly using the CSI signal as posture’s features is not
a good choice.

Table II. Confuse matrix

(a)LSTM

Activity
Predicted Labels
Labels BE DC DS DX DR SQ HC HU HO KK PH RN ST WK Acc.

BE
DC
DS
DX
DR
SQ
HC
HU
HO
KI
PH
RN
ST
WK

39
0
1
0
0
0
1
0
0
4
0
0
2
0

0
33
6
5
0
1
0
5
5
0
2
2
0
0

0
5
32
4
0
0
0
2
2
4
0
0
0
0

0
2
5
37
3
0
0
2
2
0
0
0
0
0

2
0
0
0
36
0
1
1
0
1
1
0
0
0

1
0
3
3
9
45
6
0
1
1
1
0
1
0

2
0
0
0
0
0
39
1
2
1
5
0
1
0

0
3
1
0
0
0
0
35
4
1
0
0
0
0

2
5
2
1
0
0
0
2
28
0
1
0
0
0

0
0
0
0
1
3
1
1
4
38
1
1
1
0

0
1
0
0
0
0
2
1
1
0
39
0
0
0

0
0
0
0
0
0
0
0
1
0
0
40
1
10

1
0
0
0
0
0
0
0
0
0
0
0
41
1

3
1
0
0
1
1
0
0
0
0
0
9
3
39

78
66
64
74
72
90
78
70
56
76
78
80
82
78

(b)LSTM with attention mechanism

Activity
Predicted Labels
Labels BE DC DS DX DR SQ HC HU HO KK PH RN ST WK Acc.

BE
DC
DS
DX
DR
SQ
HC
HU
HO
KK
PH
RN
ST
WK

46
0
0
0
0
0
2
0
3
2
0
0
0
0

0
0
32 10
0
40
2
4
0
0
3
0
0
0
4
0
3
7
0
2
0
0
0
0
0
0
0
0

0
2
3
36
2
0
0
0
1
1
0
0
0
0

0
0
0
0
36
0
0
1
1
1
1
0
0
0

1
0
2
2
8
44
3
0
0
2
1
0
0
0

0
0
0
0
0
0
42
0
2
2
4
0
0
0

0
0
4
0
2
3
2
3
2
0
0
0
0
0
38
4
10 23
1
1
3
0
0
0
0
1
0
0

0
1
0
0
0
3
2
2
0
37
0
0
1
0

0
1
0
0
1
0
12
1
0
0
41
0
0
0

0
0
0
0
0
0
0
0
0
0
0
43
0
12

0
0
0
0
0
0
0
0
0
0
0
0
47
0

3
0
0
1
1
0
0
0
0
1
0
7
1
38

92
64
80
72
72
88
84
76
46
74
82
86
94
76

(c)C3D

Activity
Predicted Labels
Labels BE DC DS DX DR SQ HC HU HO KI PH RN ST WK Acc.

BE
DC
DS
DX
DR
SQ
HC
HU
HO
KK
PH
RN
ST
WK

45
0
0
0
0
0
0
0
0
0
0
5
0
5

0
45
5
0
0
0
0
0
0
0
0
0
0
0

0
0
45
1
0
0
0
0
0
0
0
0
0
0

0
0
0
49
5
0
5
5
0
0
0
0
0
0

3
0
0
0
45
0
0
0
5
0
5
0
0
0

0
5
0
0
0
45
0
0
0
0
0
0
0
0

0
0
0
0
0
0
45
0
0
0
0
0
0
0

0
0
0
0
0
0
0
45
0
0
0
0
0
0

0
0
0
0
0
0
0
0
45
5
0
0
0
0

0
2
0
0
0
0
0
0
0
0
3
2
0
0
0
0
0
0
45 0
0 45
0
0
0
0
0
0

0
0
0
0
0
0
0
0
0
0
0
45
1
0

0
0
0
0
0
0
0
0
0
0
0
0
49
0

0
0
0
0
0
0
0
0
0
0
0
0
0
45

90
90
90
98
90
90
90
90
90
90
90
90
98
90

(d)C3D with attention mechanism

Activity
Predicted Labels
Labels BE DC DS DX DR SQ HC HU HO KK PH RN ST WK Acc.

BE
DC
DS
DX
DR
SQ
HC
HU
HO
KK
PH
RN
ST
WK

50
0
0
0
0
0
0
0
0
0
0
1
0
0

0
50
0
1
0
0
0
0
0
0
0
0
0
0

0
0
48
0
0
0
1
0
8
0
0
0
0
0

0
0
0
45
0
0
0
0
0
0
0
0
0
0

0
0
0
0
50
0
0
0
0
5
0
0
0
0

0
0
0
0
0
50
0
0
0
0
0
0
5
0

0
0
1
1
0
0
49
0
0
0
0
0
0
0

0
0
1
1
0
0
0
49
7
0
0
0
0
0

0
0
0
1
0
0
0
1
35
0
0
0
0
0

0
0
0
0
0
0
0
0
0
45
0
0
0
0

0
0
0
0
0
0
0
0
0
0
50
0
0
0

0
0
0
0
0
0
0
0
0
0
0
43
0
2

0
0
0
0
0
0
0
0
0
0
0
0
44
3

0
0
0
0
0
0
0
0
0
0
0
6
1
45

100
100
96
90
100
100
98
98
70
90
100
86
88
90

9

process has been discussed in ’calculate method’ section.
The the CSI signals can be reorganized to get better de-
scribed action’s features through C3D’s spatial convolu-
tion. The temporal convolution part has a similar func-
tion as LSTM, and they can deal with time dimension of
CSI signals. As show in Table I, the C3D network get the
best calculation result. This is a reasonable result, since
C3D network contains the traditional method’s feature
extraction function and LSTM’s dealing with time se-
ries sample ability. Therefore, C3D network outperforms
LSTM in the unintuitive time series problems.
The confusion matrix for deep learning method are
show in Table II, and they are LSTM, attention-LSTM,
C3D and attention-C3D separately. From confusion ma-
trix, we can see each individual action’s accuracy eﬀective
on overall accuracy. First of all, for the overall situation,
the C3D network outperforms LSTM network. Second,
attention mechanism is actually improving the overall ac-
curacy for each network, but some individual action ac-
curacy may get lower. We think it is also aﬀected by
the diﬀerence between CSI signals features and human
truly posture. Some improving on attention mechanism
for CSI signals will be done for the future work. Third,
some similar action may confuse during the calculation.
We design 14 kinds actions, and half of them are related
with hands. we can see from the confusion matrix that
the hand related action always make confuse with other
hands’ action. The overall accuracy is good enough, but
some detailed feature extraction methods need improving
in the future work.

3.

evaluation

We further analyse the recognition result in a statisti-
cal view. To comprehensively evaluate the classiﬁcation
result, the following metrics have been widely used: 1)
False Positive Rate(FP) indicates the ratio of falsely se-
lected activities as another activity. 2) Precision(PR) is
deﬁned as
T P +F P , where TP is the ratio of a correctly la-
beled activity. 3) Recall(RE) is
T P +F N , where FN is the
false negative rate. 4) F1-score(F1) is another evaluation
metric, deﬁned as 2∗P R∗RE

T P

T P

P R+RE .

Figure 8. The evaluation result calculated by C3D network
and C3D network with attention mechanism.

For this reason, we think C3D may be a better choice
than these original CSI signal’s feature. The speciﬁc

We examine the precision, recall and F1-score for C3D
network and C3D network with attention mechanism as

10

posed to ﬁgure out this discrimination model through
C3D network. We design some experiments to compare
C3D network with traditional machine learning method
and LSTM. Traditional machine leaning methods only
can extract the CSI signals’ features itself, but omit the
signals’ time direction feature. The other side, LSTM
can solve time dimension very well, but is weak to form
CSI signals’ features. The results showed that C3D net-
work can extract features and make recognition simulta-
neously. Therefore, it outperforms than all other meth-
ods on dealing with these CSI signal data. Totally, the
proposed C3D network for dealing with CSI signals are
eﬃcient, compact, and extremely simple to use.

illustrated in Fig. 8. Since the false positive rate is nearly
zero for all actions, we will show them in Fig. 8. The
precision and recall is almost 90% accuracy except one
or two action, and the results for C3D network with at-
tention mechanism get higher accuracy except the hand
horizontal moving action. The hand horizontal moving
action has a low recognition accuracy, since it is easy to
confuse with other hand part action. Meanwhile, it lasts
not very long enough time, and has little eﬀect on CSI
signal. Therefore, it’s also easy to be eﬀected by noise.
So some special treating for this action must be done in
the future work.
From evaluate analysis, the result indicates that C3D
network and C3D network with attention mechanism can
not only accurately but also comprehensively recognize
these diﬀerent activities with low miss and error rates.

V. CONCLUSION

In this work we try to address the problem of ac-
tion recognition based on CSI signal features. We pro-

[1] J. K. Aggarwal and M. S. Ryoo, ACM Comput. Surv.
43, 16:1 (2011).
[2] S. Ji, W. Xu, M. Yang, and K. Yu, IEEE Transactions
on Pattern Analysis and Machine Intelligence 35, 221
(2013).
[3] J. Schmidhuber, Neural Networks 61, 85 (2015).
[4] M. Vrigkas, C. Nikou,
and I. Kakadiaris, Fron-
tiers in Robotics and Artiﬁcial Intelligence 2 (2015),
10.3389/frobt.2015.00028.
[5] S. Herath, M. Harandi, and F. Porikli, Image and Vision
Computing 60, 4 (2017), regularization Techniques for
High-Dimensional Data Analysis.
[6] X. Wang, Pattern Recognition Letters 34, 3 (2013), ex-
tracting Semantics from Multi-Spectrum Video.
[7] S. S. Rautaray and A. Agrawal, Artiﬁcial Intelligence Re-
view 43, 1 (2015).
[8] T. Wang, Z. Wang, D. Zhang, T. Gu, H. Ni, J. Jia,
X. Zhou, and J. Lv, ACM TIST 8, 6:1 (2016).
[9] X. Yang and Y. Tian, IEEE Transactions on Pattern
Analysis and Machine Intelligence 39, 1028 (2017).
[10] M. Mario, IEEE Sensors Journal 19, 1487 (2019).
[11] Y. Hsieh and Y. Jeng, IEEE Access 6, 6048 (2018).
[12] Z. Yang, Y. Li, J. Yang, and J. Luo, IEEE Transactions
on Circuits and Systems for Video Technology 29, 2405
(2019).
[13] M. Amjadi, K.-U. Kyung, I. Park, and M. Sitti, Ad-
vanced Functional Materials 26, 1678 (2016).
[14] Z. Chen, Q. Zhu, Y. C. Soh, and L. Zhang, IEEE Trans-
actions on Industrial Informatics 13, 3070 (2017).
[15] W. Wang, A. X. Liu, M. Shahzad, K. Ling, and S. Lu,
IEEE Journal on Selected Areas in Communications 35,
1118 (2017).
[16] Y. Gu, F. Ren, and J. Li, IEEE Internet of Things Jour-
nal 3, 796 (2016).

[17] W. Wang, A. X. Liu, M. Shahzad, K. Ling, and S. Lu,
in Proceedings of the 21st Annual International Confer-
ence on Mobile Computing and Networking , MobiCom
’15 (ACM, New York, NY, USA, 2015) pp. 65–76.
[18] Z. Wang, K. Jiang, Y. Hou, Z. Huang, W. Dou, C. Zhang,
and Y. Guo, IEEE Access 7, 78772 (2019).
[19] M. A. A. Al-qaness, M. Abd Elaziz, S. Kim, A. A. Ewees,
A. A. Abbasi, Y. A. Alha j, and A. Hawbani, Sensors 19
(2019), 10.3390/s19153329.
[20] Y. Ma, G. Zhou, and S. Wang, ACM Comput. Surv. 52,
46:1 (2019).
[21] W. Zhang, S. Zhou, L. Yang, L. Ou, and Z. Xiao, IEEE
Transactions on Vehicular Technology 68, 7890 (2019).
[22] H. Abdelnasser, K. A. Harras, and M. Youssef, in 2015
IEEE Conference on Computer Communications Work-
shops (INFOCOM WKSHPS) (2015) pp. 17–18.
[23] Y. Wang, K. Wu, and L. M. Ni, IEEE Transactions on
Mobile Computing 16, 581 (2017).
[24] Y. Wang, J. Liu, Y. Chen, M. Gruteser, J. Yang, and
H. Liu, Proceedings of the Annual International Confer-
ence on Mobile Computing and Networking, MOBICOM
(2014), 10.1145/2639108.2639143.
[25] A. Virmani and M. Shahzad, in Proceedings of the 15th
Annual International Conference on Mobile Systems, Ap-
plications, and Services, MobiSys’17, Niagara Fal ls, NY,
USA, June 19-23, 2017 (2017) pp. 252–264.
[26] F. J. Ordez and D. Roggen, Sensors 16 (2016),
10.3390/s16010115.
[27] A. Ullah, K. Muhammad, J. Del Ser, S. W. Baik, and
V. H. C. de Albuquerque, IEEE Transactions on Indus-
trial Electronics 66, 9692 (2019).
[28] H. Wang and L. Wang,
in 2017 IEEE Conference
on Computer Vision and Pattern Recognition (CVPR)
(2017) pp. 3633–3642.

[29] H. Yang, C. Yuan, B. Li, Y. Du, J. Xing, W. Hu, and
S. J. Maybank, Pattern Recognition 85, 1 (2019).
[30] J. C. Nez, R. Cabido, J. J. Pantrigo, A. S. Montemayor,
and J. F. Vlez, Pattern Recognition 76, 80 (2018).
[31] Z. Hu, G. Huang, Y. Hu, and Z. Yang, in 2017 IEEE
International Conference on Image Processing (ICIP)
(2017) pp. 4402–4406.
[32] G. Wang, Y. Zou, Z. Zhou, K. Wu, and L. M. Ni, IEEE
Transactions on Mobile Computing 15, 2907 (2016).
[33] W. Wang, A. X. Liu, M. Shahzad, K. Ling, and S. Lu,
in Proceedings of the 21st Annual International Confer-
ence on Mobile Computing and Networking , MobiCom
’15 (ACM, New York, NY, USA, 2015) pp. 65–76.
[34] X. Wu, Z. Chu, P. Yang, C. Xiang, X. Zheng,
and
W. Huang, IEEE Transactions on Vehicular Technology
68, 306 (2019).
[35] K. Greﬀ, R. K. Srivastava, J. Koutnk, B. R. Steunebrink,
and J. Schmidhuber, IEEE Transactions on Neural Net-
works and Learning Systems 28, 2222 (2017).

11

[36] J. Wang, X. Zhang, Q. Gao, H. Yue, and H. Wang, IEEE
Transactions on Vehicular Technology 66, 6258 (2017).
[37] Q. Gao, J. Wang, X. Ma, X. Feng, and H. Wang, IEEE
Transactions on Vehicular Technology 66, 10346 (2017).
[38] Z. Chen, L. Zhang, C. Jiang, Z. Cao, and W. Cui, IEEE
Transactions on Mobile Computing 18, 2714 (2019).
[39] S. Prince, Computer Vision: Models Learning and Infer-
ence (Cambridge University Press, 2012).
[40] A. Karpathy, G. Toderici, S. Shetty, T. Leung, R. Suk-
thankar, and L. Fei-Fei, in 2014 IEEE Conference on
Computer Vision and Pattern Recognition (2014) pp.
1725–1732.
[41] S. Youseﬁ, H. Narui, S. Dayal, S. Ermon, and S. Valaee,
IEEE Communications Magazine 55, 98 (2017).
[42] D. Tran, L. D. Bourdev, R. Fergus, L. Torre-
sani,
and M. Paluri, CoRR abs/1412.0767 (2014),
arXiv:1412.0767.
[43] J. Carreira and A. Zisserman, CoRR abs/1705.07750
(2017), arXiv:1705.07750.
[44] D. Tran, H. Wang, L. Torresani, J. Ray, Y. Le-
Cun, and M. Paluri, CoRR abs/1711.11248 (2017),
arXiv:1711.11248.

