Large-scale Multi-view Subspace Clustering in Linear Time

Zhao Kang,1 Wangtao Zhou,1 Zhitong Zhao,1 Junming Shao,1 Meng Han,2Zenglin Xu,1 ,3

1School of Computer Science and Engineering, University of Electronic Science and Technology of China, China
2School of Management and Economics, University of Electronic Science and Technology of China, China
3Centre for Artiﬁcial Intelligence, Peng Cheng Lab, Shenzhen 518055, China
{zkang,zlxu}@uestc.edu.cn

9
1
0
2

v
o

N

1
2

]

G

L

.

s

c

[

1
v
0
9
2
9
0

.

1
1
9
1

:

v

i

X

r

a

Abstract

A plethora of multi-view subspace clustering (MVSC) meth-
ods have been proposed over the past few years. Researchers
manage to boost clustering accuracy from different points of
view. However, many state-of-the-art MVSC algorithms, typ-
ically have a quadratic or even cubic complexity, are inefﬁ-
cient and inherently difﬁcult to apply at large scales. In the
era of big data, the computational issue becomes critical. To
ﬁll this gap, we propose a large-scale MVSC (LMVSC) al-
gorithm with linear order complexity. Inspired by the idea of
anchor graph, we ﬁrst learn a smaller graph for each view.
Then, a novel approach is designed to integrate those graphs
so that we can implement spectral clustering on a smaller
graph. Interestingly, it turns out that our model also applies to
single-view scenario. Extensive experiments on various large-
scale benchmark data sets validate the effectiveness and efﬁ-
ciency of our approach with respect to state-of-the-art clus-
tering methods.

Introduction

During the last decade, subspace clustering has been ex-
plored extensively due to its promising performance (Huang
et al. 2019; Peng, Kang, and Cheng 2017). Basically, it is
based on the assumption that the data points are drawn from
multiple low-dimensional subspaces and each group ﬁts into
one of the low-dimensional subspaces. Signiﬁcant progress
has been made to uncover such underlying subspaces.
Among the various subspace clustering approaches, spec-
tral clustering based methods such as low-rank representa-
tion (LRR) (Liu et al. 2013; Kang, Peng, and Cheng 2015)
and sparse subspace clustering (SSC) (Elhamifar and Vidal
2013) have achieved great success. SSC ﬁnds a sparse repre-
sentation of data points while LRR ﬁnds the low-rank struc-
ture of subspace. Afterwards, the spectral clustering algo-
rithm is performed on such new representation (Ng, Jordan,
and Weiss 2002). For n data points, the ﬁrst step typically
takes O(n2 ) or O(n3 ) time while the second step requires
O(n3 ) or at least O(n2k) time, where k is the number of
clusters. Therefore, these two time comsuming steps debar
the application of subspace clustering from large-scale data
sets.

Copyright c(cid:13) 2020, Association for the Advancement of Artiﬁcial
Intelligence (www.aaai.org). All rights reserved.

Recently, much work have been developed to accelerate
subspace clustering. For example, (Wang et al. 2014) pro-
poses a data selection approach; a scalable SSC based on
orthogonal matching pursuit is developed in (You, Robin-
son, and Vidal 2016); a fast solver for SSC is proposed in
(Pourkamali-Anaraki and Becker 2018); accelerated LRR is
also developed (Fan et al. 2018; Xiao et al. 2015); a sam-
pling approach is used in (Li and Zhao 2017). Unfortunately,
they all target for single-view scenario and can not deal with
multi-view data.
With the advance of information technology, many prac-
tical data come from multiple channels or appear in muliple
modalities, i.e., the same object can be described in differ-
ent kinds of features (Kang et al. 2019c; Yin et al. 2018;
Zhan et al. 2018; Liu et al. 2019; Yao et al. 2019; Tang
et al. 2019). Different feature sets often characterize dif-
ferent yet complementary information. For instance, an im-
age can have heterogeneous features, such as Garbor, HoG,
SIFT, GIST, and LBP; an artical can be written in different
languages. Accordingly, it is crucial to develop multi-view
learning method to incorporate the useful informaiton from
different views.
There has been growing interest in developing multi-
view subspace clustering (MVSC) algorithms. For example,
MVSC (Gao et al. 2015) method learns a graph represen-
tation for each view and all graphs are assumed to share a
unique cluster matrix; (Cao et al. 2015) applies a diversity
term to explore the complementarity information; (Luo et al.
2018) explores both consistency and speciﬁcity of different
views; (Wang et al. 2019; Zhang et al. 2017) perform MVSC
in latent representation; (Brbi ´c and Kopriva 2018) imposes
both low-rank and sparsity constraints on the graph; (Kang
et al. 2019a; Kang et al. 2020) learn one partition for each
view and them perform a fusion in partition space. In terms
of clustering accuracy, these methods are attractive. Never-
theless, their computation efﬁciency, at least O(n2k) com-
plexity, is largely ignored. In the era of big data, it has been
witnessed that many applications involve large-scale multi-
view data. Therefore, their severely unaffordable complexity
precludes MVSC from real-world applications with big data.
Moreover, existing single-view subspace clustering acceler-
ation techniques do not apply to multi-view scenarios owing

 
 
 
 
 
 
to the heterogeneous nature of multi-view data.
To ﬁll the gap, we need to address two challenges. First,
how to avoid constructing n × n graph for multi-view data,
which requires a lot of time and storage. Second, how to
circumvent the computationally expensive step—–spectral
clustering with O(n2k) complexity. To this end, we in-
troduce a novel large-scale multi-view subspace clustering
(LMVSC) method, which can greatly reduce the computa-
tional complexity as well as improve the clustering perfor-
mance. First, to address the large-scale graph construction
problem, we select a small number of the instances as an-
chors, and then a smaller graph is built for each view. Sec-
ond, a novel way to integrate mutiple graphs is designed so
that we can implement spectral clustering on a small graph.
The contributions of this paper mainly include
• We make the ﬁrst attempt towards large-scale multi-view
subspace clustering. Our proposed method can be solved
in linear time, while existing methods demand square or
cubic time in the number of data instance.
• Extensive experiments in terms of data sets and compar-
ison methods validate the effectiveness and efﬁciency of
the proposed algorithm.
• As a special case, our method also applies to single-view
subspace clustering task. It gives promising performance
on very large-scale data.

Preliminaries

Multi-view Subspace Clustering

min

(cid:107)X − X S (cid:107)2
F + αf (S )

Subspace clustering expresses each data point as a linear
combination of other points and minimizes the reconstruc-
tion loss to obtain the combination coefﬁcient. This coef-
ﬁcient is treated as the similarity between corresponding
points. Mathematically, given data X ∈ Rd×n with d fea-
tures and n samples, subspace clustering solves the follow-
ing problem (Liu et al. 2013; Elhamifar and Vidal 2013;
Kang et al. 2019d):
(1)
where f (·) is a certain regularizer function, α > 0 is a bal-
straints require that all Sij is nonnegative and (cid:80)
ance parameter, and 1 is a vector with all ones. The con-
S is often called similarity graph matrix. It is obvious that S
has size of n × n, which poses a big challenge with scalabil-
ity to large-scale data sets.
Recently, multi-view subspace clustering (MVSC) has at-
tracted much attention. Basically, for multi-view data X =

s.t. S ≥ 0, S 1 = 1,

j Sij = 1.

Z

[X 1 ; · · · ; X i ; · · · ; X v ] ∈ R

, MVSC (Gao et al.
2015; Cao et al. 2015; Kang et al. 2019c) solves:

i=1

v(cid:80)

di×n

(cid:107)X i − X iS i (cid:107)2
F + αf (S i ) s.t. S i ≥ 0, S i1 = 1.

(2)
With different forms of f , Eq. (2) gives solutions with differ-
ent properties. For example, (Wang et al. 2016b) encourages
similarity between pairs of graphs; (Cao et al. 2015) empha-
sizes the complementarity. With these multiple graphs, (Gao

v(cid:88)

i=1

min

S i

et al. 2015) assumes they produce the same clustering; (Cao
et al. 2015; Wang et al. 2016b) perform spectral clustering
on averaged graph. All these MVSC methods take at least
O(n2k) time, hence they are generally computational pro-
hibtive.

Anchor Graph

Anchor graph is an efﬁcient way to construct the large-scale
graph matrix (Chen and Cai 2011; Liu, He, and Chang 2010;
Wang et al. 2016a; Han, Xiong, and Nie 2017). Speciﬁcally,
a subset of m points (m (cid:28) n) A = [a1 , a2 , · · · , am ] ∈
Rd×m are chosen based on a k-means clustering or random
sampling on the original data set. Each point plays a role as
a landmark which approximates the neighborhood structure.
Then, a sparse afﬁnity matrix Z ∈ Rn×m is constructed
between the landmarks and the original data as follows:

Zij =

j (cid:48) ∈<i> Kδ (xi ,aj (cid:48) ) ,
Kδ (xi ,aj )

j ∈< i >

otherwise

(3)

(cid:40)

(cid:80)

0,

where < i > indicates the indexes of r (r < m) nearest an-
chors around xi . The function Kδ (·) is the commonly used
Gaussian kernel with bandwidth parameter δ . Subsequently,
the doubly-stochastic similarity matrix S ∈ Rn×n , the sum-
mation of each column and each row equal to one, is built
based on:

S = ˆZ ˆZ (cid:62) where
where Σ is a diagonal matrix with the entry Σii =

ˆZ = ZΣ−1/2 ,

(4)

(cid:80)n

j=1 Zj i .

We can see that the above graph construction approach is
extremely efﬁcient since only O(mn) distances are consid-
ered. However, this heuristic strategy has one inherent draw-
back which is always ignored, i.e., quality of the built graph
heavily depends on the exponential function. In general, it
might not be appropriate to the structure of the data space
(Shakhnarovich 2005; Kang et al. 2019b). Furthermore, it
is also challenging to ﬁnd a good kernel parameter without
any supervision information. Consequently, the downstream
task might not be able to obtain a good performance.
To obtain a similarity measure tailored to the problem at
hand, a principled way is to learn it from data (Kang et al.
2019e). In this paper, inspired by the idea of anchor graph,
we use a smaller graph Z to approximate the full n × n
graph matrix. Different from existing approach, we adap-
tively learn Z from raw data by solving an optimization
problem.

Methodology

For large-scale data, there is much redundancy in the data; a
small number of the instances are sufﬁcient for reconstruct-
ing the underlying subspaces. Similar to anchor strategy, we
use a smaller matrix Z i ∈ Rn×m (m (cid:28) n) to approximate
the full n × n matrix S i . Speciﬁcally, a set of m landmarks
Ai = {aj }j∈[1,m] (cluster centers) are obtained through k-
means clustering on the data X i . Then, Z i is built between
the landmarks Ai and X i .

Rather than using the handcrafted similarity measure (3),
we learn Z i from data. For multi-view data, we solve the
following problem:

v(cid:88)

(cid:80)

i S i

v

min

Z i

(cid:107)X i − Ai (Z i )(cid:62)(cid:107)2
F + α(cid:107)Z i(cid:107)2
s.t. 0 ≤ Z i , (Z i )(cid:62)1 = 1.

i=1

F

(5)

Compared to Eq. (2), we only consider the similarities be-
tween mn points instead of n2 , so the complexity is remark-
ably reduced. Problem (5) can be easily solved via convex
quadratic programming.
Given a set of v graphs {Z i }i∈[1,v ] , our goal is to merge
them so as to make use of the rich information contained in v
views. Obviously, it is not feasible to run spectral clustering
using {Z i }i∈[1,v ] since their size is not n × n anymore. For
multi-view case, we can follow a similar procedure as the
traditional method, i.e., apply Eq. (4) to restore the n × n
graph S i and then take average,

¯S =

.

(6)

In the sequel, spectral clustering is implemented to achieve
embedding Q ∈ Rn×k , i.e.,

T r(Q(cid:62) ¯SQ)

s.t. Q(cid:62)Q = I.

(7)

max

Q

Its solution is the k eigenvectors corresponding to the largest
k eigenvalues of ¯S . However, the eigen decomposition takes
at least O(n2k) time.
In this paper, relying on proposition 1 (Chen and Cai
2011; Affeldt, Labiod, and Nadif 2019), we present an al-
ternative approach to compute the k left singular vectors of
the concatenated matrix ¯Z ∈ Rn×mv ,

v

1√

¯Z =

[ ˆZ 1 , · · · , ˆZ i , · · · , ˆZ v ].

Proposition 1. Given a set of

(8)
Since mv (cid:28) n, using ¯Z instead of ¯S naturally reduces the
computational cost of Q.
similarity matrices
{S i }i∈[1,v ] , each of them can be decomposed as Z i (Z i )(cid:62) .
Deﬁne ¯Z = 1√
v [Z 1 , · · · , Z i , · · · , Z v ] ∈ Rn×mv , its Singu-
lar Value Decomposition (SVD) is U ΛV (cid:62) (with U (cid:62)U = I,
V (cid:62)V = I). We have,

max

Q(cid:62)Q=I

T r(Q(cid:62) ¯SQ) ⇔ min

Q(cid:62)Q=I,H

|| ¯Z − QH (cid:62) ||2

F .

(9)

And, the optimal solution Q∗ is equal to U .
Proof. Based on the second term of Eq. (9), one can eas-
ily show that H ∗ = ¯Z (cid:62)Q. Plugging the value of H ∗ into
Eq. (9), the following equivalences hold
|| ¯Z − QH (cid:62) ||2

min

Q(cid:62)Q=I,H

Q(cid:62)Q=I

F ⇔ min
⇔ max
⇔ max

Q(cid:62)Q=I

F

|| ¯Z − QQ(cid:62) ¯Z ||2
T r(Q(cid:62) ¯Z ¯Z (cid:62)Q)
T r(Q(cid:62) ¯SQ).

Q(cid:62)Q=I

Moreover,

¯S = ¯Z ¯Z (cid:62) = (U ΛV (cid:62) )(U ΛV (cid:62) )(cid:62)
= U Λ(V (cid:62)V )ΛU (cid:62)
= U Λ2U (cid:62) .

Thereby the left singular vectors of ¯Z are the same as the
eigenvectors of ¯S .

Algorithm 1: LMVSC algorithm

Input: Multi-view data matrix X 1 , · · · , X v , cluster number
k , parameter α, anchors {Ai }i∈[1,v ] .
Step 1: Solve problem (5);
Step 2: Construct ¯Z as (8);
Step 3: Compute Q by performing SVD on ¯Z .
Output: Run k-means on Q to achieve ﬁnal clustering.

The complete procedures for our LMVSC method is sum-
marized in Algorithm 1.
Claim 1. As a special case, the proposed LMVSC algorithm
is also suitable to single-view data.

Proof. It is easy to observe that our proposed precedure also
works for single view data, i.e., v = 1 and ¯Z = ˆZ in Eq.
(8).

Complexity Analysis

LMVSC beneﬁts from the low complexity of the anchor
strategy for the graph construction and the eigen decomposi-
tion on a low-dimensional matrix. Speciﬁcally, the construc-
tion of Z i s takes O(nm3 v), where n is the number of data
points, m is the number of anchors (m (cid:28) n), v is the view
number. In experiments, we apply the built-in matlab func-
with complexity O(m3 ) is used (Ye and Tse 1989). It is
tion quadprog to solve (5), in which inter-point method
worth mentioning that the construction of Z i s can be eas-
ily parallelized over multiple cores, leading to more efﬁcient
computation. The computational cost for the Q embedding
induces a computational complexity of O(m3 v3 + 2mvn).
In addition, we need additional O(nmtp) for the k-means
at the begining for anchor selection and O(nk2 t) for the
last k-means on Q, where t is the number of iterations,
k is the number of clusters, p =
di is the summation
of feature dimensions. Accordingly, our proposed LMVSC
method costs time only linear in n.

v(cid:80)

i=1

Experiment on Multi-View Data

In this section, we conduct extensive experiments to assess
the performance of the proposed method on real-world data
sets.

Data Sets

We perform experiments on several benchmark data sets:
Handwritten, Caltech-101, Reuters, NUS-WIDE-Object.
Speciﬁcally, Handwritten consists of handwritten digits of 0

Table 1: Description of the data sets. The dimension of features is shown in parenthesis.

View
1
2
3
4
5
6
Data points
Class number

Handwritten
Caltech7/20
Proﬁle correlations (216)
Gabor(48)
Fourier coefﬁcients (76)
Wavelet moments (40)
Karhunen coefﬁcients (64) CENTRIST (254)
Morphological (6)
HOG (1984)
Pixel averages (240)
GIST (512)
Zernike moments (47)
LBP (928)
2000
1474/2386
10
7/20

Reuters
NUS
English (21531)
Color Histogram (65)
French (24892)
Color moments (226)
German (34251) Color correlation (145)
Italian (15506)
Edge distribution (74)
Spanish (11547) Wavelet texture (129)
-
-
18758
30000
6
31

Table 2: Clustering performance on Caltech7 data. “-” de-
notes some unreasonable values that are close to zero.

Method
E3SC(1)
E3SC(2)
E3SC(3)
E3SC(4)
E3SC(5)
E3SC(6)
SSCOMP(1)
SSCOMP(2)
SSCOMP(3)
SSCOMP(4)
SSCOMP(5)
SSCOMP(6)
AMGL
MLRSSC
MSC IAS
LMVSC

Acc
0.3060
0.3256
0.4261
0.5509
0.5122
0.5244
0.2619
0.3759
0.3256
0.5604
0.4715
0.5000
0.4518
0.3731
0.3976

0.7266

NMI
0.1993
0.2648
0.2534
0.4064
0.3863
0.4273
0.0640
0.2017
0.1354
0.4631
-
0.3688
0.4243
0.2111
0.2455

0.5193

Purity
0.3677
0.3582
0.4573
0.5787
0.5598
0.5773
0.2958
0.4023
0.3460
0.6357
-
0.5577
0.4674
0.4145
0.4444

0.7517

Time (s)
10.32
9.84
18.19
82.43
27.80
42.86
0.84
0.82
1.50
5.35
2.11
3.00
20.12
22.26
57.18
135.79

to 9 from UCI machine learning repository. Caltech-101 is a
data set of images for object recognition. Following previous
work, two subsets, caltech7 and caltech20, are used. Reuters
contains documets written in ﬁve different languages and
their translations. A subset written in English and all their
translations are used here. NUS-WIDE-Object (NUS) is also
a object recognition database. More detailed information
about these data is shown in Table 1.

Clustering Evaluation

We compare LMVSC1 with several recent work as stated
bellow:

Efﬁcient Solvers

for Sparse Subspace Clustering

(E3SC): recent subspace clustering method proposed in
each view and report its performance E3SC(·) .
(Pourkamali-Anaraki and Becker 2018). We run E3SC on

Scalable Sparse Subspace Clustering by Orthogonal
Matching Pursuit (SSCOMP): one of the representative

large-scale subspace clustering method developed in (You,
Robinson, and Vidal 2016). We run it on each single view.

Parameter-Free Auto-Weighted Multiple Graph Learn-

ing (AMGL): one of the state-of-the-art multi-view spectral
clustering method proposed in (Nie et al. 2016).

Multi-view Low-rank Sparse Subspace Clustering

1Code is available at https://github.com/sckangz/LMVSC

Table 3: Clustering performance on Caltech20 data.

Method
E3SC(1)
E3SC(2)
E3SC(3)
E3SC(4)
E3SC(5)
E3SC(6)
SSCOMP(1)
SSCOMP(2)
SSCOMP(3)
SSCOMP(4)
SSCOMP(5)
SSCOMP(6)
AMGL
MLRSSC
MSC IAS
LMVSC

Acc
0.2531
0.2670
0.2921
0.4107
0.4162
0.4845
0.1928
0.2511
0.1945
0.3667
0.2930
0.3567
0.3013
0.2821
0.3127

0.5306

NMI
0.2670
0.3248
0.3358

0.5576

0.4834
0.4976
0.1359
0.2338
0.1319
0.4487
0.1681
0.3735
0.4054
0.2670
0.3138
0.5271

Purity
0.2657
0.2993
0.3282
0.4908
0.4644
0.5444
0.2087
0.2955
0.2360
0.4288

0.6010

0.4346
0.3164
0.3039
0.3374
0.5847

Time (s)
27.71
26.81
46.69
180.70
61.32
99.62
1.66
1.56
2.75
10.02
4.45
5.88
77.63
607.28
93.87
342.97

(MLRSSC): recent method (Brbi ´c and Kopriva 2018)
which outperforms many multi-view spectral clustering
methods, e.g., (Xia et al. 2014).

Multi-view Subspace Clustering with Intactness-aware

Similarity (MSC IAS): another recent multi-view subspace
clustering method proposed in (Wang et al. 2019), which
reports improvements against a number of MVSC methods,
e.g., (Zhang et al. 2017).
All our experiments are implemented on a computer with
a 2.6GHz Intel Xeon CPU and 64GB RAM, Matlab R2016a.
For fair comparison, we follow the experimental setting de-
scribed in respective papers and tune the parameters to ob-
tain the best performance. We search anchor number m in
the range [k , 50, 100]. The motivation behind this is that the
number of points required for revealing the underlying sub-
spaces should not be less than the number of subspaces, i.e.,
the cluster number k . In addition, some other factors, such
as subspace’s intrinsic dimensions, noise level could also
inﬂuence the anchor number. We select α from the range
[0.001, 0.01, 0.1, 1, 10]. We assess the performance with ac-
curacy (Acc), normalized mutual information (NMI), purity,
and running time.

Experimental Results

Tables 2-6 show the clustering results of all methods on
ﬁve data sets. Due to out-of-memory issue, many compar-
ison methods, e.g., E3SC and MLRSSC, are not appliable

Table 4: Clustering performance on Handwritten data.

Method
E3SC(1)
E3SC(2)
E3SC(3)
E3SC(4)
E3SC(5)
E3SC(6)
SSCOMP(1)
SSCOMP(2)
SSCOMP(3)
SSCOMP(4)
SSCOMP(5)
SSCOMP(6)
AMGL
MLRSSC
MSC IAS
LMVSC

Acc
0.7555
0.5790
0.7300
0.4530
0.7555
0.6345
0.4680
0.3640
0.2430
0.3005
0.5370
0.3115
0.8460
0.7890
0.7975

0.9165

NMI
0.7491
0.5279
0.7226
0.4623
0.7153
0.6189
0.3939
0.2913
0.1536
0.3191
0.4911
0.1586

0.8732

0.7422
0.7732
0.8443

Purity
0.8385
0.6200
0.8095
0.5290
0.8345
0.6825
0.5485
0.4475
0.4475
0.3250
0.6640
0.3670
0.8710
0.8375
0.8755

0.9165

Time (s)
29.94
20.80
19.47
16.07
31.52
18.69
2.21
2.07
2.39
0.81
2.23
1.37
67.58
52.44
80.78
10.55

Table 5: Clustering performance on Reuters data.

Method
SSCOMP(1)
SSCOMP(2)
SSCOMP(3)
SSCOMP(4)
SSCOMP(5)
AMGL
MSC IAS
LMVSC

Acc
0.2714
0.2730
0.2735
0.2719
0.3128
0.1672
0.5063

0.5890

NMI
-
-
-
-
-
-
0.2759

0.3346

Purity
-
-
-
-
-
-
0.6031

0.6145

Time (s)
4721.90
5070.80
4524.10
4515.70
4162.40
32397
7015.7
130.73

to Reuters and NUS data sets which are more than 10,000
samples. Therefore, they are not listed in Table 5 and Ta-
ble 6. This demonstrates that the proposed method is low
space complexity. In terms of accuracy, our method con-
stantly outperforms others and the average improvement is
at least 7% over the second highest value. For NMI and pu-
rity, our method achieves comparable or even better perfor-
mance than the other methods. This demonstrates the con-
sistency and stability of our method. Taken Caltech7 and
Handwritten as examples, we visualize their clustering re-
sults based on t-SNE technique in Figure 1. We can clearly
observe the cluster pattern.
For running time comparison, our method can ﬁnish all
data sets in several minutes. Our method is up to several or-
ders of magnitude faster than other multi-view methods on
large data sets. For example, MSC IAS consumes almost 13

Table 6: Clustering performance on NUS data.

Method
SSCOMP(1)
SSCOMP(2)
SSCOMP(3)
SSCOMP(4)
SSCOMP(5)
MSC IAS
LMVSC

Acc
0.1153
0.1206
0.1138
0.0950
0.0756
0.1548

0.1553

NMI
0.0755
-
-
-
-

0.1521

0.1295

Purity
0.1412
-
-
-
-
0.1675

0.1982

Time (s)
53.63
102.64
90.88
64.26
83.83
45386
165.39

hours to ﬁnish NUS data, while our method only takes 3
minutes. Note that the state-of-the-art scalable method SS-
COMP takes extremely long time to run reuters data, which
is due to the data’s high dimensionality. According to our
observation, the ﬂuctuation of our execution time is mainly
caused by the number of anchors. For instance, Handwritten
achieves the best results when the fewest anchors are used,
i.e., cluster number 10. Consequently, it takes the shortest
time.

(a) Caltech7

(b) Handwritten

Figure 1: Visualization of some clustering results.

Robustness Study

To examine the robustness of our method, we perform addi-
tional experiments by adding three commonly seen kinds of
sists of gray-scale images of size 28 × 28. There are 70000
noise. We choose the famous MNIST database, which con-
samples in total. Since it is a single-view data, we construct a
multi-view data set by adding different levels of noise to dif-
ferent views. Speciﬁcally, we ﬁrst corrupt MNIST data by
adding Gaussian noise with variance 0.01, 0.03, and 0.05;
Salt&Pepper noise with density 0.05, 0.1, and 0.2; Speckle
noise with variance 0.05, 0.1, and 0.15, respectively. Then,
we concatenate them to form three three-view data sets.
Some example images are shown in Figures 2-4. For these
large data sets, we ﬁnd that only SSCOMP and our method

-80-60-40-20020406080-60-40-200204060801234567-80-60-40-20020406080-80-60-40-2002040608012345678910Table 7: Clustering performance on MNIST data.

Noise

Gaussian

Speckle

Salt&Pepper

Method
SSCOMP(1)
SSCOMP(2)
SSCOMP(3)
LMVSC
SSCOMP(1)
SSCOMP(2)
SSCOMP(3)
LMVSC
SSCOMP(1)
SSCOMP(2)
SSCOMP(3)
LMVSC

Acc
0.4300
0.4465
0.4418

0.5565

0.4417
0.4560
0.4556

0.5920

0.4536
0.4513
0.4816

0.5889

NMI
0.4726
0.4749
0.4754

0.5096

0.4770
0.4795
0.4855

0.5178

0.4782
0.4822
0.4923

0.5374

Purity
0.5965
0.5908
0.5920

0.6282

0.5967
0.5995
0.6043

0.6183

0.5964
0.6021
0.6307

0.6598

Time (s)
1120.80
1151.50
1130.30
55.17
1314.90
1287.70
1303.40
66.01
1181.90
1306.10
1302.00
73.33

can handle it, while other recent multi-view clustering meth-
ods fail.
According to the results in Table 7, our method obtains
better performance than SSCOMP in all metrics. This also
veriﬁes the advantage of multi-view learning which can ex-
ploit the complementarity of multi-view data. In terms of
computation time, our method is 20 times faster than SS-
COMP.

Figure 3: Some sample images of MNIST are shown in the
1st column. Corresponding noisy images contaminated by
Speckle noise with variance 0.05, 0.1, and 0.15, are dis-
played in the 2nd, 3rd, and 4th column, respectively.

Figure 2: Some sample images of MNIST are shown in the
1st column. Corresponding noisy images contaminated by
Gaussian noise with variance 0.01, 0.03, and 0.05, are dis-
played in the 2nd, 3rd, and 4th column, respectively.

Parameter Analysis

During the experiments, we tune two parameters based on
grid search, i.e., the number of anchors and α. Taking Hand-
written and MNIST data sets as examples, we show our
model’s sensitivity to their values in Figures 5 and 6. We can
see that α tends to have a small value since the performance
is degraded when α increases. We have similar observation
for anchor number. Too many anchors will make themselves
less representative and introduce extra errors. Consequently,
the performance will be deteriorated.

Experiment on Single-View Data

We use two large-scale single-view data sets to demon-
strate the performance of our algorithm on single-view

Figure 4: Some sample images of MNIST are shown in the
1st column. Corresponding noisy images contaminated by
Salt&Pepper noise with density 0.05, 0.1, and 0.2, are dis-
played in the 2nd, 3rd, and 4th column, respectively.

Table 8: Description of single-view data sets.

Data Sets
RCV1
CoverType

#Feature
1979
54

#Classes
103
7

#Sample
193844
581012

Table 9: Results of single-view data sets.

Data Sets Method
Acc
KM
0.1846
RCV1
LMVSC 0.1929
CoverType KM
0.2505
LMVSC 0.3862

NMI
0.2447
0.2600
0.0617
0.1273

Purity
0.2550
0.2985
0.3039
0.4889

Time (s)
1569.07
1035
156.72
2755.8

Figure 5: Sensitivity analysis of parameters for our method over Handwritten data set.

Figure 6: Sensitivity analysis of parameters for our method over MNIST data set.

data. Speciﬁcally, RCV1 consists of newswire stories from
Reuters Ltd. CovType contains instances for predicting for-
est cover type from cartographic variables. Their statistics
are summarized in Table 8. We found that SSCOMP can not
ﬁnish within 24 hours, while many other large-scale sub-
space clustering (Fan et al. 2018; Xiao et al. 2015) methods
run out of memory. For comparison, we include k-means
(KM) as baseline. From Table 9, we can see that our method
improves the performance signiﬁcantly. Considering the size
of data, our computation time is still reasonable. For Cover-
Type, KM runs fast since the cluster number is quite small.

Conclusion

In this paper, we propose a novel multi-view subspace clus-
tering algorithm. To the best of our knowledge, LMVSC is
the very ﬁrst effort of addressing the large-scale problem of
multi-view subspace clustering. Our proposed method has
a linear computation complexity, while maintaining high
level of clustering accuracy. Speciﬁcally, for each view, one
smaller graph is built between the raw data points and the
generated anchors. Then, a novel integration mechanism is
designed to merge those graphs, so that the eigen decom-
position can be accelerated signiﬁcantly. Furthermore, our
proposed method also applies to single-view data. Extensive
experiments on real data sets verify the effectiveness, efﬁ-
ciency, and robustness of the proposed method against other
state-of-the-art techniques.

ACKNOWLEDGMENT

This paper was in part supported by Grants from the Nat-
ural Science Foundation of China (Nos. 61806045 and
61572111) and Fundamental Research Fund for the Central
Universities of China (No. ZYGX2017KYQD177).

References

[Affeldt, Labiod, and Nadif 2019] Affeldt, S.; Labiod, L.;
and Nadif, M.
2019.
Spectral clustering via ensem-
ble deep autoencoder learning (sc-edae).
arXiv preprint
arXiv:1901.02291.
[Brbi ´c and Kopriva 2018] Brbi ´c, M., and Kopriva, I. 2018.
Multi-view low-rank sparse subspace clustering. Pattern
Recognition 73:247–258.
[Cao et al. 2015] Cao, X.; Zhang, C.; Fu, H.; Liu, S.; and
Zhang, H. 2015. Diversity-induced multi-view subspace
clustering. In CVPR, 586–594.
[Chen and Cai 2011] Chen, X., and Cai, D. 2011. Large
scale spectral clustering with landmark-based representa-
tion. In AAAI.
[Elhamifar and Vidal 2013] Elhamifar, E., and Vidal, R.
2013. Sparse subspace clustering: Algorithm, theory, and
applications. IEEE transactions on pattern analysis and ma-
chine intelligence 35(11):2765–2781.
[Fan et al. 2018] Fan, J.; Tian, Z.; Zhao, M.; and Chow, T. W.
2018. Accelerated low-rank representation for subspace
clustering and semi-supervised classiﬁcation on large-scale
data. Neural Networks 100:39–48.

[Gao et al. 2015] Gao, H.; Nie, F.; Li, X.; and Huang, H.
2015. Multi-view subspace clustering. In ICCV, 4238–4246.
[Han, Xiong, and Nie 2017] Han, J.; Xiong, K.; and Nie, F.
2017. Orthogonal and nonnegative graph reconstruction for
large scale clustering. In IJCAI, 1809–1815.
[Huang et al. 2019] Huang, W.; Yin, M.; Li, J.; and Xie, S.
2019. Deep clustering via weighted k-subspace network.
IEEE Signal Processing Letters 26:1628–1632.
[Kang et al. 2019a] Kang, Z.; Guo, Z.; Huang, S.; Wang, S.;
Chen, W.; Su, Y.; and Xu, Z. 2019a. Multiple partitions
aligned clustering. In IJCAI, 2701–2707.
[Kang et al. 2019b] Kang, Z.; Pan, H.; Hoi, S. C.; and Xu,
Z. 2019b. Robust graph learning from noisy data.
IEEE
Transactions on Cybernetics 1–11.
[Kang et al. 2019c] Kang, Z.; Shi, G.; Huang, S.; Chen, W.;
Pu, X.; Zhou, J. T.; and Xu, Z. 2019c. Multi-graph fusion for
multi-view spectral clustering. Knowledge-Based Systems.
[Kang et al. 2019d] Kang, Z.; Wen, L.; Chen, W.; and Xu, Z.
2019d. Low-rank kernel learning for graph-based clustering.
Knowledge-Based Systems 163:510–517.
[Kang et al. 2019e] Kang, Z.; Xu, H.; Wang, B.; Zhu, H.; and
Xu, Z. 2019e. Clustering with similarity preserving. Neuro-
computing 365:211–218.
[Kang et al. 2020] Kang, Z.; Zhao, X.; Shi; Peng, c.; Zhu, H.;
Zhou, J. T.; Peng, X.; Chen, W.; and Xu, Z. 2020. Parti-
tion level multiview subspace clustering. Neural Networks
122:279–288.
[Kang, Peng, and Cheng 2015] Kang, Z.; Peng, C.; and
Cheng, Q. 2015. Robust subspace clustering via smoothed
rank approximation.
IEEE Signal Processing Letters
22(11):2088–2092.
[Li and Zhao 2017] Li, J., and Zhao, H. 2017. Large-scale
subspace clustering by fast regression coding. In IJCAI.
[Liu et al. 2013] Liu, G.; Lin, Z.; Yan, S.; Sun, J.; Yu, Y.; and
Ma, Y. 2013. Robust recovery of subspace structures by low-
rank representation. IEEE transactions on pattern analysis
and machine intelligence 35(1):171–184.
[Liu et al. 2019] Liu, X.; Zhu, X.; Li, M.; Wang, L.; Zhu, E.;
Liu, T.; Kloft, M.; Shen, D.; Yin, J.; and Gao, W. 2019. Mul-
tiple kernel k-means with incomplete kernels. IEEE trans-
actions on pattern analysis and machine intelligence.
[Liu, He, and Chang 2010] Liu, W.; He, J.; and Chang, S.-F.
2010. Large graph construction for scalable semi-supervised
learning. In ICML, 679–686.
[Luo et al. 2018] Luo, S.; Zhang, C.; Zhang, W.; and Cao, X.
2018. Consistent and speciﬁc multi-view subspace cluster-
ing. In AAAI.
[Ng, Jordan, and Weiss 2002] Ng, A. Y.; Jordan, M. I.; and
Weiss, Y. 2002. On spectral clustering: Analysis and an
algorithm. In NIPS, 849–856.
[Nie et al. 2016] Nie, F.; Li, J.; Li, X.; et al. 2016. Parameter-
free auto-weighted multiple graph learning: A framework
for multiview clustering and semi-supervised classiﬁcation.
In IJCAI, 1881–1887.

[Peng, Kang, and Cheng 2017] Peng, C.; Kang, Z.; and
Cheng, Q. 2017. Subspace clustering via variance regu-
larized ridge regression. In CVPR, 2931–2940.
[Pourkamali-Anaraki and Becker 2018] Pourkamali-
Anaraki, F., and Becker, S. 2018. Efﬁcient solvers for sparse
subspace clustering. arXiv preprint arXiv:1804.06291.
[Shakhnarovich 2005] Shakhnarovich, G. 2005. Learning
task-speciﬁc similarity. Ph.D. Dissertation, Massachusetts
Institute of Technology.
[Tang et al. 2019] Tang, C.; Zhu, X.; Liu, X.; and Wang, L.
2019. Cross-view local structure preserved diversity and
consensus learning for multi-view unsupervised feature se-
lection. In Proceedings of the AAAI Conference on Artiﬁcial
Intelligence, volume 33, 5101–5108.
[Wang et al. 2014] Wang, S.; Tu, B.; Xu, C.; and Zhang, Z.
2014. Exact subspace clustering in linear time. In AAAI.
[Wang et al. 2016a] Wang, M.; Fu, W.; Hao, S.; Tao, D.; and
Wu, X. 2016a. Scalable semi-supervised learning by ef-
ﬁcient anchor graph regularization. IEEE Transactions on
Knowledge and Data Engineering 28(7):1864–1877.
[Wang et al. 2016b] Wang, Y.; Zhang, W.; Wu, L.; Lin, X.;
Fang, M.; and Pan, S. 2016b. Iterative views agreement: an
iterative low-rank based structured optimization method to
multi-view spectral clustering. In IJCAI, 2153–2159. AAAI
Press.
[Wang et al. 2019] Wang, X.; Lei, Z.; Guo, X.; Zhang, C.;
Shi, H.; and Li, S. Z. 2019. Multi-view subspace cluster-
ing with intactness-aware similarity. Pattern Recognition
88:50–63.
[Xia et al. 2014] Xia, R.; Pan, Y.; Du, L.; and Yin, J. 2014.
Robust multi-view spectral clustering via low-rank and
sparse decomposition. In AAAI.
[Xiao et al. 2015] Xiao, S.; Li, W.; Xu, D.; and Tao, D. 2015.
Falrr: A fast low rank representation solver. In CVPR, 4612–
4620.
[Yao et al. 2019] Yao, S.; Yu, G.; Wang, J.; Domeniconi, C.;
and Zhang, X. 2019. Multi-view multiple clustering.
In
IJCAI.
[Ye and Tse 1989] Ye, Y., and Tse, E. 1989. An extension of
karmarkar’s projective algorithm for convex quadratic pro-
gramming. Mathematical programming 44(1-3):157–179.
[Yin et al. 2018] Yin, M.; Gao, J.; Xie, S.; and Guo, Y. 2018.
Multiview subspace clustering via tensorial t-product rep-
resentation.
IEEE Transactions on Neural Networks and
Learning Systems 30(3):851–864.
[You, Robinson, and Vidal 2016] You, C.; Robinson, D.; and
Vidal, R. 2016. Scalable sparse subspace clustering by or-
thogonal matching pursuit. In CVPR, 3918–3927.
[Zhan et al. 2018] Zhan, K.; Niu, C.; Chen, C.; Nie, F.;
Zhang, C.; and Yang, Y. 2018. Graph structure fusion for
multiview clustering. IEEE Transactions on Knowledge and
Data Engineering.
[Zhang et al. 2017] Zhang, C.; Hu, Q.; Fu, H.; Zhu, P.; and
Cao, X. 2017. Latent multi-view subspace clustering. In
CVPR, 4279–4287.

