9
1
0
2

v
o

N

1
2

]

P
S

.

s
s

e
e

[

2
v
4
1
4
8
0

.

1
1
9
1

:

v

i

X

r

a

COM PAR I SON O F D EE P LEARN ING MOD EL S ON T IM E SER I E S
FOR ECA ST ING : A CA SE S TUDY O F D I S SO LV ED OXYG EN
PR ED IC T ION

A PR E PR IN T

Hongqian Qin

University of Pennsylvania
Philadelphia, PA, 19104
hongqian@upenn.edu

November 22, 2019

AB STRACT

Deep learning has achieved impressive prediction performance in the ﬁeld of sequence learning
recently. Dissolved oxygen prediction, as a kind of time-series forecasting, is suitable for this
technique. Although many researchers have developed hybrid models or variant models based on
deep learning techniques, there is no comprehensive and sound comparison among the deep learning
models in this ﬁeld currently. Plus, most previous studies focused on one-step forecasting by using a
small data set. As the convenient access to high-frequency data, this paper compares multi-step deep
learning forecasting by using walk-forward validation. Speciﬁcally, we test Convolutional Neural
Network (CNN), Temporal Convolutional Network (TCN), Long Short-Term Memory (LSTM),
Gated Recurrent Unit (GRU), Bidirectional Recurrent Neural Network (BiRNN) based on the real-
time data recorded automatically at a ﬁxed observation point in the Yangtze River from 2012 to 2016.
By comparing the average accumulated statistical metrics of root mean square error (RMSE), mean
absolute error (MAE), and coefﬁcient of determination in each time step, We ﬁnd for multi-step
time series forecasting, the average performance of each time step does not decrease linearly. GRU
outperforms other models with signiﬁcant advantages.

Keywords Deep Learning · LSTM · TCN · GRU · BiRNN · Dissolve Oxygen · Time series Forecasting · Walk Forward
Validation

 
 
 
 
 
 
A PR E PR IN T - NOV EMB ER 22 , 2019

1

Introduction

The quick development of industrialization and urbanization generates threats of water security. Long-term water
contamination, sudden pollution incidents, plus the complicated distribution of sewage outlets and water intakes, put
high pressure on both the aquaculture and the drinking water supply. To prevent such issues, we need a reliable model
to predict the trend of change in water quality parameters. Dissolved oxygen (DO), a necessity for all aquatic animals,
is considered an essential indicator of the water quality parameters. This paper will speciﬁcally discuss the univariate
time series forecasting.
Because of the development of techniques on real-time monitoring plus the increased investment in water quality
management from the government, the approach of getting the data of water quality parameters has changed from
low-frequency manual collection with low precision to high-frequency automatic collection with high accuracy. The
relative cheap access to a tremendous amount of data shifts the focus on DO forecasting from limited data to big
non-obvious seasonal data. The tech being applied also develops from stochastic models to non-linear models, and then
to deep learning models. The history of the development of time series prediction is as follows:

1. Stochastic models, such as Autoregressive Integrated Moving Average (ARIMA), Kalman ﬁlter, and their
variants, were popular on time series forecasting. They guarantee to converge to a unique optimal solution
[1], and they are simple for understanding and implementation. However, it cannot handle the non-linear
relationship among the data.

2. As a data-driven and self-adaptive non-linear model [2][3], Artiﬁcial Neural Networks (ANNs) has the
advantages that no need of statistical hypothesis for model application or a speciﬁc model form [4]. It can
approximate any continuous function with impressive performance [5]. Although ANNs and its variants have
become the hotspot on time series prediction in recent decades, it suffers from the challenges of the selection
of proper parameters, and its ﬁnal result cannot guarantee to be converged as the globally optimal.

3. Another technology, Support Vector Regression (SVR), receives growing attention in the ﬁeld of time series
forecasting. It is the application of Support Vector Machine (SVM) on regression analysis. It covers advantages
both from stochastic methods and ANNs. It is good at handling non-linear and non-stationary relationships
among data without the request of statistical hypothesis [1], and its solution is always globally optimal [4].
SVR has been proven to outperform neural networks [1]. But its main drawback is when dealing with the large
training set, it needs enormous computation, which makes it computationally expensive [6].

4. Currently, deep learning has achieved the state-of-the-art in many time series forecasting tasks. As a neural
network architecture with more hidden layers and more neurons, deep learning can automatically extract and
handle more complex features and dependency relationships. For example, LSTM can catch both long-term and
short-term dependencies in a sequence; BiRNNs can build the model automatically from both past information
and future information. Besides that, deep learning is also able to deal with multiple inputs and outputs [7].
While its drawback is to tune the hyperparameters.

The remaining of this paper is structured as follows: Section 2 introduces the related studies on dissolved oxygen or
water quality parameters forecasting. Section 3 describes the ﬁve deep learning models we test in this study. We show

2

A PR E PR IN T - NOV EMB ER 22 , 2019

the detail of the experiment in Section 4. Section 5 includes our discussion of limitations and future work. Finally, we
conclude in Section 6.

2 Related Work

In the ﬁeld of DO or water quality parameter prediction, some researchers were interested in ANNs, their variants,
or hybrid models. Sirilak Areerachakul [8], Najah [9], Majer [10], Diamantopoulou [11], Emamgholizadeh[12]
experimented with ANNs; Yan Jiang [13] employed BP neural network; Chen Siyu [14] amalgamated an improved
artiﬁcial bee colony (IABC) algorithm and BP neural network; Zhenbo Li [15] blended gray model with BP neural
network; Yalin Liu [16] tried fuzzy neural network; Ahmed [17] combined a feed-forward neural network (FFNN)
model and a radial basis function neural network (RBFNN) model; L.Zhang [18], W.Deng [19] integrated ARIMA and
RBFNN.

Some other researchers studied water quality parameter forecasting using SVR, SVM, their variants, or hybrid models. Ji
Xiaoliang [20] found SVM worked well in hypoxic river systems; M.Liu [21] utilized improved SVM in nonpoint source
polluted river; Shuangyin Liu [22][23] mixed Wavelet Analysis (WA) and Least Squares Support Vector Regression
(LSSVR). Y.Xiang [24] utilized the least squares support vector machine (LS-SVM) combined with particle swarm
optimization (PSO); Chakraborty [25] built a hybrid model integrating regression tree and SVM. Chen Li [26] proposed
the model merging the least squares support vector machine (LSSVR), BP neural network, and grey model (GM).

Although all authors claimed their methods achieved great results, what the models they compared were the original
ARIMA or ANNs, and the speciﬁc settings made it hard to generalize to other water systems. Recently, as the big data
era, the cost of access to an enormous amount of data has decreased, while the computational power has increased.
That provides the opportunity for deep learning models to outperform the conventional time series models. CNN [27],
LSTM [28-31] were tried recently and were proved get better prediction capability than traditional machine learning
or statistical methods in the ﬁeld of time series forecasting problems. Another approach called gated recurrent unit
(GRU) was created as a modiﬁcation of LSTM with a simpler structure [32]. It was proved to have better prediction
performance than RNN [33] and was applied widely on sequence prediction problems [34][35]. Schuster and Paliwal
[36] proposed bidirectional RNN to enable the information passed both in the forward direction and backward direction.
Merging the bidirectional RNN with LSTM and GRU was originally designed for problems like text and speech
recognition. Recently however, researches showed BiRNN also worked well on time series data prediction problems

[37-39].

Plus, the machine learning department of Carnegie Mellon University introduced a generic convolutional architecture,
temporal convolutional network [40], for sequence modeling. That was developed based on CNN by applying dilated
convolution and residual block. They claimed it outperformed LSTM in a broad sequence modeling tasks.

To the best of our knowledge, the GRU, BiRNN, and TCN have not been researched on the prediction of DO. In
this paper, we compare and analyze the prediction performance of CNN, LSTM, GRU, BiLSTM, BiGRU, and TCN.
The following sections of this paper are as follows: Section 2 introduces the theories and architectures of these
methodologies. Section 3 describes the experiment and explains its result. Section 4 provides the conclusion of our
analysis.

3

A PR E PR IN T - NOV EMB ER 22 , 2019

3 Methodologies

Because of the fast development of computational power while the slower improvement of RAM, statistical models
have the power to optimize parameters fast while they have to be memory efﬁcient simultaneously. Hence, the focus of
statistics and machine learning are converting to methods with more nonlinearity like deep networks from traditional
methods like linear models and kernel methods [41]. In this section, we will introduce ﬁve types of deep learning
methods on time series prediction. They are LSTM, GRU, BiRNN, CNN, and TCN.

3.1 Long Short Term Memory (LSTM)

LSTM has become the state-of-the-art model for many sequence problems such as analysis of machine translation [42],
audio detection [43], and video recognition [44]. It was designed by Sepp Hochreiter and Jürgen Schmidhuber in 1997
[45] to solve the vanishing gradient problem during optimization [46] in RNN by adding forget gate, input gate, and
output gate into the hidden layer.

Figure 1 shows the inner structure of the hidden state. There are three gates in each cell: forget gate, input gate, and
output gate. Each gate consists of a sigmoid activation plus a point-wise multiplication operation [46]. The forget
gate evaluates how much information should be forgotten from the cell state, which constructs the long-term memory.
The two inputs of the forget gate come from the output of the previous hidden state Ht−1 , and the input of the current
state Xt . We combine the two inputs with their weights plus their biases and then pass the result through the sigmoid
activation function (1) to estimate whether to keep or remove the information. Its result ranges from 0 to 1, where 0
indicates to forget the information completely, while 1 indicates to maintain that ultimately.

The input gate is to decide how much new information should be added to the cell. The sigmoid function (2) does the
same thing as it does in the forget gate. Then we create a candidate memory cell ˜Ct ∈ Rn×h and using tanh function
(3) to address how much memory in the old cell ˜Ct−1 ∈ Rn×h should be maintained. We utilize tanh activation
function here because tanh allows negative results, which means the cell state can either add or drop information. The
red part of Figure 1 presents the updating of the cell state. By combining the multiplications between Ft and Ct−1 , It
and ˜Ct , we get the new cell state Ct (4).
The output gate is to update the hidden state Ht ∈ Rn×h . Still, we employ a sigmoid function (5) to ﬁgure out what
information needs retain from the previous hidden state and the new input of the current state. Passing the updated cell
state Ct to tanh function, we determine what to add or drop. Multiplying the tanh function and the sigmoid function,
we ﬁnally get the new hidden state (6).

Ft = σ(Wxf Xt + Whf Ht−1 + bf )

It = σ(Wxf Xi + WhiHt−1 + bi )

˜Ct = tanh(WxiXt + WhiHt−1 + bi )
Ct = Ft (cid:12) Ct−1 + It (cid:12) ˜Ct

Ot = σ(WxoXt + WhoHt−1 + bo )

4

(1)

(2)

(3)

(4)

(5)

A PR E PR IN T - NOV EMB ER 22 , 2019

Ht = Ot (cid:12) tanh(Ct )

(6)

Figure 1: LSTM Structure

3.2 Gated Recurrent Unit

GRU was proposed by Cho et al. [47] in 2014. It also addresses the vanishing gradient problem of RNN. Unlike LSTM,
however, it only has two gates: reset gate and update gate. The reset gate handles the short-term dependencies by
deciding which past information should be dropped. Update gate is responsible for the long-term dependencies by
controlling the amount of information passed to the future.

Assuming there are h hidden units, n examples, and d inputs. Given the time step t, the input of the current state is
Xt ∈ Rn × d, and the input from the previous hidden state is Ht−1 ∈ Rn × d . Xt and Ht−1 are multiplied by their
own weights Wxz ∈ Rn × d, Whz ∈ Rh × h respectively and then their summation is passed by a sigmoid activation
function to get the reset gate Rt ∈ Rn × d ranging from 0 to 1 (7) and the update gate Zt ∈ Rn × d ranging from 0 to 1
(8). To get the candidate hidden state referred as the red part of Figure 2, we pass the summation to a tanh activation
function, and replacing Ht−1 by Ht−1 × Rt to reduce the inﬂuence of previous states [41]. The yellow part of Figure 2
shows the process of getting the new Ht . We need to incorporate the update gate, determine the extent of the inﬂuence
from the previous state Ht−1 , and the new candidate state ˜Ht . Equation (10) shows the ﬁnal updating step.

Rt = σ(WxrXt + WhrHt−1 + bz )

Zt = σ(WxzXt + WhzHt−1 + bz )

˜Ht = tanh(WxhXt + Whh (RtHt−1 ) + bz )
Ht = ZtHt−1 + (1 − Zt ) ˜Ht

5

(7)

(8)

(9)

(10)

A PR E PR IN T - NOV EMB ER 22 , 2019

Figure 2: GRU Structure

3.3 Bidirectional Recurrent Neural Network

BiRNN was proposed to predict the target through both past and future information. As Figure 3 shows, it inserts an
additional hidden layer in the RNN architecture for passing information in a backward direction, and then the output
would be created by merging the information from two hidden layers, which is a combination of inputs from the
opposite direction. To avoid vanishing gradient problem in RNN, researchers usually use bidirectional LSTM (BiSTM)
and bidirectional GRU (BiGRU) instead of BiRNN. BiLSTM and BiGRU were initially designed for text, handwritten,
or speech recognition, classiﬁcation, and prediction. While recently, some researchers ﬁnd it has excellent performance
on 1-dimensional time series data forecasting, such as stock market prediction [37], trafﬁc speed prediction [38], and
salary prediction [39]

Figure 3: Bidirectional RNN Structure

6

A PR E PR IN T - NOV EMB ER 22 , 2019

3.4 Convolutional Neural Network

The CNN has achieved remarkable performance on images classiﬁcation or recognition. Its success in two dimensions
let some researchers explore 1D CNN on data regression problems. In the ﬁeld of time series, its prediction only
depends on local regions of independent variables rather than connecting to the input span over the long term. 1D
CNN works well on extracting features from short ﬁxed-length input of the whole data set and their locations have
low relevance [48]. After the 1D CNN layer, we pass the result to a max pooling layer to prevent overﬁtting. It was
developed to TCN with larger receptive ﬁeld.

3.5 Temporal Convolutional Network

The ‘TCN’ in this paper speciﬁcally refers to the architecture proposed by the paper An Empirical Evaluation of Generic
Convolutional and Recurrent Networks for Sequence Modeling published in 2014 [40]. After a series of experiments
comparing with LSTM and GRU, the author claimed the TCN outperformed in a wide range of sequence modeling
tasks.

Its main characteristics include: 1) parallelism, 2) ﬂexible receptive ﬁeld size, 3) stable gradients, 4) low memory
requirement for training, and 5) ﬂexible length of inputs. There are two different points between the TCN and CNN: 1)
The output of TCN has the same length as the input; 2) There is no leakage from the future information to the past in
TCN. To achieve the ﬁrst point, TCN uses 1D fully convolutional network architecture [49]. To get the second point, it
applies causal convolutions.

TCN employs dilated convolutions as the causal convolutions [50] to build a long effective history size, in other words,
to get a large receptive ﬁeld. Figure 4 provides an example of a dilated causal convolution with a kernel size of 2 and
a dilation factor of [1,2,4]. TCN applied residual blocks [51] to keep the stabilization of the deep network. In each
residual bock, there are two layers of dilated causal convolution, weight normalization [52], rectiﬁed linear unit (ReLU)
[53], and spatial dropout [54]. Plus, a 1*1 convolution is added in each residual block to ensure the output size is the
same as the input size.

4 Experiment

4.1 Data and data preprocessing

The data used in this experiment is obtained from a real-time water-quality monitoring station in the Yangtze River
Basin of Jiangsu Province, Wuxi. The dissolved oxygen was recorded automatically each 30 min from 12.a.m. Aug 1st,
2012 to 9 p.m. May 8th, 2016, with a total of 64411 items. The distribution of the raw data is shown in Figure 5.
Zeros are believed to be the errors made by monitor. There are 49 zeros among the whole items, which indicates that
the rate of zero is 0.76%. Considering its low percentage of occurrence and randomly dispersed distribution, we remove
them to avoid their inﬂuence on the model training. Tremendous amount of inputs with unscaled data will slow down
the speed of learning and convergence of the deep learning model. Hence, we utilize the Min-Max strategy as the
equation (11) to normalize the data and map its range from 0 to 1. Then We split the data into 90% training set and 10%

7

A PR E PR IN T - NOV EMB ER 22 , 2019

Figure 4: TCN Structure. The left part is the dilated causal convolution with Kernel size=2 and dilation factors d=1,2,4. The right
part is the residual block. The 1*1 convolution is to make sure the residual input and output have the same dimensions

Figure 5: Raw data of dissolved oxygen

8

A PR E PR IN T - NOV EMB ER 22 , 2019

testing set. Moreover, among the training data, we set 10% validation set.

y = (x − min)/(max − min)

(11)

4.2 Model design

4.2.1 Loss function and Optimizer

When we train the models in this study, we choose Mean Squared Error (MSE) as the loss function and Adam as the
optimizer. Adam is created speciﬁcally for training deep learning models through combining the beneﬁts of Adaptive
Gradient algorithm (AdaGrad) [55] working well with sparse gradients, and Root Mean Square Propagation (RMSProp)
[56] doing well on non-stationary objectives.

4.2.2 Hypperparameters

We tune the hyperparameters of each model based on their keras packages. The amount of epochs of each model is 20.
For LSTM, GRU, BiLSTM, and BiGRU, we set their units, batch size, validation split as 50, 128, and 0.1, respectively.
For TCN, we set its dilations [1,2,4,8,16,32], with the kernel size of 3 and amount of ﬁlters as 4. For CNN, ﬁlters,
kernel sizes, and validation split are 16, 3, 0.1.

4.3 Model estimation

4.3.1 Walk-forward Validation

Many previous studies said little about their test model. That would easily make readers misunderstand the statistical
metrics and conclusions. In this paper, We employ walk-forward validation to predict the observations of the next ten
time steps, given the observations of the previous ten time steps. That is, using the previous ﬁve hours DO level to
predict the DO level of the next ﬁve hours. There are two kinds of windows in walk-forward validation. In this paper,
we choose the expanding window. As the ﬁrst row of Figure 6 shows, we ﬁrst input ﬁve hours of known data to predict
data from the hour 6 to hour 10. The second row presents that we expand the input from ﬁve hours to ten hours, and
then forecast the data for the next ﬁve hours. All predictions based on the model referred by the yellow squares in ﬁgure
6, is stored and ﬁnally be compared with the known value.

Figure 6: Test Structure.

9

(cid:118)(cid:117)(cid:117)(cid:116) 1
n(cid:88)
n(cid:88)

t=1

n

1
n

t=1

(cid:80)n−1
(cid:80)n−1

M AE =

|yt − ˆyt |

t=0 (yt − ˆyt )2
t=0 (yt − ¯yt )2

(12)

(13)

(14)

4.3.2 Statistical Metrics

A PR E PR IN T - NOV EMB ER 22 , 2019

To test the predicting capability of the models, we select three statistical metrics and compare their values in the testing
set. They are Root Mean Square Error (RMSE), Mean Absolute Error (MAE), and Coefﬁcient of Determination R2 .
For RMSE and MAE, closer to 0 means better performance, and for R2 , closer to 1 is better.

RM SE =

(yt − ˆyt )2

R2 = 1 −

4.3.3 Results

Figure 7, 9 show that the prediction capability of all models falls dramatically after the time step of 2.5 hours and 4
hours. That is, the difference of reliability and accuracy between hour 3 and hour 2.5 is much larger than that between
hour 2.5 and 2.0. It indicates that the attenuation of predicting performance is not a linear process. According to Figure
7,8,9, GRU has the lowest average RMSE and MAE of each time step in the test set. GRU also has the highest average
R2 in each time step. That presents strong evidence to support GRU outperforms other models in this study. We also
ﬁnd LSTM has the highest RMSE, MAE, while lowest R2 on average, which indicates its performance is the worst
among the six models.

Figure 7: Average RMSE for each time step in the test set

10

A PR E PR IN T - NOV EMB ER 22 , 2019

Figure 8: Average MAE for each time step in the test set

Figure 9: Average R2 for each time step in the test set

11

A PR E PR IN T - NOV EMB ER 22 , 2019

5 Limitations and Future Works

One limitation of this study is the quality of the raw data. Although we have dropped those zeros, it is hard for us to
ﬁgure out those errors larger than zero. We see in Figure 5, there are several extreme values, and the values between
18000 to 20000 and 31000 to 32000 generate two horizontal bottom lines, which looks confusing. Although deep
learning models have the advantage of dealing with those extreme values or error values, if the total error rate of the
data is high, it is risk to believe the trained models are reliable enough.

Another limitation is, although the expanding window test method is believed a scientiﬁc and excellent testing approach
in time series forecasting. In the actual application, however, during the model testing process, we need to consider to
update the training model as the window moves forward because more known data is available. While that requires a
much stronger computational power.

Finally, we tried some different combinations of hyperparameters and found their results did not change a lot. Hence,
we choose big batch sizes, small units, and ﬁlters to accelerate the speed of training. Currently, there is still no gold
standard of tuning the hyperparameter. While we believe a more rigorous proof or evidence might need to provide.
Also, the result might be different if the length of input or output changes. The relationship among them needs more
discussion in the future.

6 Conclusion

In this paper, we propose a sound comparison of predicting dissolved oxygen between multi-step deep learning models
that predict DO levels of the next ﬁve hours, given the data of the last ﬁve hours. We analysed the performance of each
model through walk-forward validation. It was observed that 1) GRU has the best performance, while LSTM has the
lowest performance; 2) The attenuation of predicting performance of each model is not linear. The prediction capability
of all models falls dramatically after 2.5 hours and 4 hours. That reminds the researchers to pay attention to the time
nodes to prevent possible risks caused by the predictions with suddenly increased error rate.

References

[1] N. I. Sapankevych and R. Sankar. “Time Series Prediction Using Support Vector Machines: A Survey”. In: IEEE
Computational Intelligence Magazine 4.2 (May 2009), pp. 24–38. DOI: 10.1109/MCI.2009.932254.

[2] M.Y. Hu G. Zhang B.E. Patuwo. “Forecasting with artiﬁcial neural networks: The state of the art”. In: Interna-
tional Journal of Forecasting 14 (1998), pp. 35–62.

[3] Ruhul Sarker Joarder Kamruzzaman Rezaul Begg. “Artiﬁcial Neural Networks in Finance and Manufacturing”.
In: Idea Group Publishing (2006).

[4] Ratnadip Adhikari and R. K. Agrawal. “An Introductory Study on Time Series Modeling and Forecasting”. In:
CoRR abs/1302.6613 (2013). arXiv: 1302.6613. URL: http://arxiv.org/abs/1302.6613.

[5] H. White K. Hornik M. Stinchcombe. “Multilayer feed-forward networks are universal approximators”. In: Neural
Networks 2 (1989), pp. 359–366.

12

A PR E PR IN T - NOV EMB ER 22 , 2019

[6] Lijuan Cao and Francis Eng Hock Tay. “Support vector machine with adaptive parameters in ﬁnancial time series
forecasting”. In: IEEE transactions on neural networks 14 6 (2003), pp. 1506–18.

[7] Francesca Lazzeri. “3 reasons to add deep learning to your time series toolkit”. In: (Feb. 2019). URL: https:
//www.oreilly.com/ideas/3-reasons-to-add-deep-learning-to-your-time-series-toolkit.

[8] Sirilak Areerachakul, Prem Junsawang, and Auttapon Pomsathit. “Prediction of Dissolved Oxygen Using Artiﬁcial
Neural Network”. In: vol. 5.

[9] Al-Mahfoodh Najah, Ahmed Elshaﬁe, and Othman A Karim. “Prediction of Johor River Water Quality Parame-
ters Using Artiﬁcial Neural Networks”. In: European Journal of Scientiﬁc Research 28 (Jan. 2009).

[10] Holger R. Maier and Graeme C. Dandy. “The Use of Artiﬁcial Neural Networks for the Prediction of Water
Quality Parameters”. In: 1996.

[11] Maria Diamantopoulou, V.Z. Antonopoulos, and D. Papamichail. “The use of a neural network technique for the
prediction of water quality parameters of Axios River in Northern Greece”. In: European Water 11 (Jan. 2005),
pp. 55–62.

[12] Samad Emamgholizadeh et al. “Prediction of water quality parameters of Karoon River (Iran) by artiﬁcial
intelligence-based models”. In: International Journal of Environmental Science and Technology 11 (Apr. 2013),
pp. 645–656. DOI: 10.1007/s13762-013-0378-x.

[13] Jing Yan et al. “Study on Prediction Model of Dissolved Oxygen about Water Quality Monitoring System
Based on BP Neural Network”. In: Advanced Materials Research 912-914 (Apr. 2014), pp. 1407–1411. DOI:
10.4028/www.scientiﬁc.net/AMR.912-914.1407.

[14] Siyu Chen et al. “Water Quality Prediction Model of a Water Diversion Project Based on the Improved Artiﬁcial
Bee Colony–Backpropagation Neural Network”. In: Water 10 (June 2018), p. 806. DOI: 10.3390/w10060806.

[15] Zhenbo Li et al. “An Improved Gray Model For Aquaculture Water Quality Prediction”. In: Intelligent Automation,
Soft Computing 18 (2012), pp. 557–567.

[16] Liu Yalin, Wei Yaoguang, and Chen Yingyi. “Dissolved Oxygen Prediction Model Which Based on Fuzzy
Neural Network”. In: IFIP Advances in Information and Communication Technology 420 (Jan. 2014). DOI:
10.1007/978-3-642-54341-8-57.

[17] A. A. Masrur Ahmed. “Prediction of dissolved oxygen in Surma River by biochemical oxygen demand and
chemical oxygen demand using the artiﬁcial neural networks (ANNs)”. In: Journal of King Saud University -
Engineering Sciences 12 (May 2014). DOI: 10.1016/j.jksues.2014.05.001.

[18] Lei Zhang, Guangxin Zhang, and Rong Li. “Water Quality Analysis and Prediction Using Hybrid Time Series and
Neural Network Models”. In: 2016.

[19] Weihui Deng et al. “Water quality prediction based on a novel hybrid model of ARIMA and RBF neural network”.
In: 2014 IEEE 3rd International Conference on Cloud Computing and Intelligence Systems. Nov. 2014, pp. 33–40.
DOI: 10.1109/CCIS.2014.7175699.

13

A PR E PR IN T - NOV EMB ER 22 , 2019

[20] Xiaoliang Ji et al. “Prediction of dissolved oxygen concentration in hypoxic river systems using support vector
machine: a case study of Wen-Rui Tang River, China”. In: Environmental science and pollution research
international 24 (May 2017). DOI: 10.1007/s11356-017-9243-7.

[21] Mei Liu and Jun Lu. “Support vector machine-an alternative to artiﬁcial neuron network for water quality
forecasting in an agricultural nonpoint source polluted river?” In: Environmental science and pollution research
international 21 (June 2014). DOI: 10.1007/s11356-014-3046-x.

[22] Shuangyin Liu et al. “Prediction of dissolved oxygen content in river crab culture based on least squares
support vector regression optimized by improved particle swarm optimization”. In: Computers and Electronics in
Agriculture 95 (July 2013), pp. 82–91. DOI: 10.1016/j.compag.2013.03.009.

[23] Shuangyin Liu et al. “A Hybrid WA-CPSO-LSSVR Model for Dissolved Oxygen Content Prediction in Crab
Culture”. In: Eng. Appl. Artif. Intell. 29 (Mar. 2014), pp. 114–124. ISSN: 0952-1976. DOI: 10.1016/j. engap-
pai.2013.09.019. URL: http://dx.doi.org/10.1016/j.engappai.2013.09.019.

[24] Y. Xiang and L. Jiang. “Water Quality Prediction Using LS-SVM and Particle Swarm Optimization”. In: 2009
Second International Workshop on Knowledge Discovery and Data Mining. Jan. 2009, pp. 900–904. DOI:
10.1109/WKDD.2009.217.

[25] Tanujit Chakraborty, Ashis Chakraborty, and Zubia Mansoor. “A hybrid regression model for water quality
prediction”. In: (Nov. 2018).

[26] Chen Li et al. “A hybrid model for dissolved oxygen prediction in aquaculture based on multi-scale features”.
In: Information Processing in Agriculture 5.1 (2018), pp. 11–20. ISSN: 2214-3173. DOI: https://doi.org/
10.1016/j.inpa.2017.11.002. URL: http://www.sciencedirect.com/science/article/pii/ S2214317317301208.

[27] Xuxiang Ta and Yaoguang Wei. “Research on a dissolved oxygen prediction method for recirculating
aquaculture systems based on a convolution neural network”. In: Computers and Electronics in Agricul-
ture 145 (2018), pp. 302–310. ISSN: 0168-1699. DOI: https://doi.org/10.1016/j.compag.2017.12.037. URL:
http://www.sciencedirect.com/science/article/pii/S016816991730786X.

[28] C. Li et al. “Dissolved oxygen time series prediction based on LSTM deep learning neural network”. In: Interna-
tional Agricultural Engineering Journal 27 (June 2018), pp. 353–359.

[29] Ping Liu et al. “Analysis and Prediction of Water Quality Using LSTM Deep Neural Networks in IoT
Envi- ronment”. In: Sustainability 11.7 (2019), pp. 1–14. URL: https://EconPapers.repec.org/RePEc:gam:
jsusta:v:11:y:2019:i:7:p:2058-:d:220636.

[30] Hu Zhuhua et al. “A Water Quality Prediction Method Based on the Deep LSTM Network Considering Correlation
in Smart Mariculture”. In: Sensors (Switzerland) 19 (Mar. 2019). DOI: 10.3390/s19061420.

[31] Y. Wang et al. “Water quality prediction method based on LSTM neural network”. In: 2017 12th International
Conference on Intelligent Systems and Knowledge Engineering (ISKE). Nov. 2017, pp. 1–5. DOI: 10.1109/
ISKE.2017.8258814.

[32] Kyunghyun Cho et al. “Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine
Translation”. In: (June 2014). DOI: 10.3115/v1/D14-1179.

14

A PR E PR IN T - NOV EMB ER 22 , 2019

[33] Junyoung Chung et al. “Empirical Evaluation of Gated Recurrent Neural Networks on Sequence Modeling”. In:
ArXiv abs/1412.3555 (2014).

[34] Jiandong Zhao et al. “Travel Time Prediction: Based on Gated Recurrent Unit Method and Data Fusion”. In:
IEEE Access PP (Oct. 2018), pp. 1–1. DOI: 10.1109/ACCESS.2018.2878799.

[35] R. Zhao et al. “Machine Health Monitoring Using Local Feature-Based Gated Recurrent Unit Networks”. In:
IEEE Transactions on Industrial Electronics 65.2 (Feb. 2018), pp. 1539–1548. DOI: 10.1109/TIE.2017. 2733438.

[36] Mike Schuster and Kuldip K. Paliwal. “Bidirectional recurrent neural networks”. In: IEEE Trans. Signal Processing
45 (1997), pp. 2673–2681.

[37] Khaled A. Althelaya, El-Sayed M. El-Alfy, and Salahadin Mohammed. “Evaluation of bidirectional LSTM
for short-and long-term stock market prediction”. In: 2018 9th International Conference on Information and
Communication Systems (ICICS) (2018), pp. 151–156.

[38] Zhiyong Cui, Ruimin Ke, and Y. Wang. “Deep Bidirectional and Unidirectional LSTM Recurrent Neural Network
for Network-wide Trafﬁc Speed Prediction”. In: (Jan. 2018).

[39] Zhongsheng Wang Shinsuke Sugaya Dat P.T. Nguyen. “Salary Prediction using Bidirectional-GRU-CNN Model”.
In: (Mar. 2019).

[40] Shaojie Bai, J. Zico Kolter, and Vladlen Koltun. “An Empirical Evaluation of Generic Convolutional and Recurrent
Networks for Sequence Modeling”. In: ArXiv abs/1803.01271 (2018).

[41] Aston Zhang et al. Dive into Deep Learning. http://www.d2l.ai. 2019.

[42] Thang Luong et al. “Addressing the Rare Word Problem in Neural Machine Translation”. In: ArXiv abs/1410.8206
(2014).

[43] Erik Marchi et al. “Multi-resolution Linear Prediction Based Features for Audio Onset Detection with Bidirec-
tional LSTM Neural Networks”. In: Proceedings - ICASSP, IEEE International Conference on Acoustics, Speech
and Signal Processing (June 2014). DOI: 10.1109/ICASSP.2014.6853982.

[44] Jeff Donahue et al. Long-term Recurrent Convolutional Networks for Visual Recognition and Description.
2014.arXiv: 1411.4389.

[45] Sepp Hochreiter and Jürgen Schmidhuber. “Long Short-term Memory”. In: Neural computation 9 (Dec. 1997), pp.
1735–80. DOI: 10.1162/neco.1997.9.8.1735.

[46] Christopher Olah. “Understanding LSTM Networks”. In: (2015). URL: https://colah.github.io/posts/ 2015-08-
Understanding-LSTMs/.

[47] Kyunghyun Cho et al. “Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine
Translation”. In: (June 2014). DOI: 10.3115/v1/D14-1179.

[48] Nils Ackermann. “Introduction to 1D Convolutional Neural Networks in Keras for Time Sequences”. In: (Sept.
2018). URL: https://blog.goodaudience.com/introduction-to-1d-convolutional-neural- networks-in-keras-for-time-
sequences-3a7ff801a2cf.

[49] Jonathan Long, Evan Shelhamer, and Trevor Darrell. Fully Convolutional Networks for Semantic Segmentation.
2014. arXiv: 1411.4038 [cs.CV].

15

A PR E PR IN T - NOV EMB ER 22 , 2019

[50] Fisher Yu and Vladlen Koltun. Multi-Scale Context Aggregation by Dilated Convolutions. 2015. arXiv: 1511.
07122 [cs.CV].

[51] Kaiming He et al. Deep Residual Learning for Image Recognition. 2015. arXiv: 1512.03385 [cs.CV].

[52] Tim Salimans and Diederik P. Kingma. “Weight Normalization: A Simple Reparameterization to Accelerate
Training of Deep Neural Networks”. In: NIPS. 2016.

[53] Vinod Nair and Geoffrey E. Hinton. “Rectiﬁed Linear Units Improve Restricted Boltzmann Machines”. In: ICML.
2010.

[54] Nitish Srivastava et al. “Dropout: A Simple Way to Prevent Neural Networks from Overﬁtting”. In: Journal of
Machine Learning Research 15 (June 2014), pp. 1929–1958.

[55] John Duchi, Elad Hazan, and Yoram Singer. “Adaptive Subgradient Methods for Online Learning and Stochastic
Optimization”. In: Journal of Machine Learning Research 12 (July 2011), pp. 2121–2159.

[56] T. Tieleman and G. Hinton. In: Lecture 6.5 - RMSProp, COURSERA: Neural Networks for Machine Learning.
Technical report (2012).

16

