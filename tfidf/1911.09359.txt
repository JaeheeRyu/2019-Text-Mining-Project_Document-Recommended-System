Noname manuscript No.
(will be inserted by the editor)

Multi-Scale RCNN Model for Financial Time-series
Classiﬁcation

Liu Guang · Wang Xiao jie · Li Ruifan

9
1
0
2

v
o

N

1
2

]

G

L

.

s

c

[

1
v
9
5
3
9
0

.

1
1
9
1

:

v

i

X

r

a

Received: date / Accepted: date

Abstract Financial time-series classiﬁcation (FTC) is
extremely valuable for investment management. In past
decades, it draws a lot of attention from a wide extent
of research areas, especially Artiﬁcial Intelligence (AI).
Existing researches ma jorly focused on exploring the ef-
fects of the Multi-Scale (MS) property or the Temporal
Dependency (TD) within ﬁnancial time-series. Unfortu-
nately, most previous researches fail to combine these
two properties eﬀectively and often fall short of accu-
racy and proﬁtability. To eﬀectively combine and uti-
lize both properties of ﬁnancial time-series, we propose
a Multi-Scale Temporal Dependent Recurrent Convo-
lutional Neural Network (MSTD-RCNN) for FTC. In
the proposed method, the MS features are simultane-
ously extracted by convolutional units to precisely de-
scribe the state of the ﬁnancial market. Moreover, the
TD and complementary across diﬀerent scales are cap-
tured through a Recurrent Neural Network. The pro-
posed method is evaluated on three ﬁnancial time-series
datasets which source from the Chinese stock market.
Extensive experimental results indicate that our model
achieves the state-of-the-art performance in trend clas-
siﬁcation and simulated trading, compared with classi-
cal and advanced baseline models.

1 Introduction

Financial time-series classiﬁcation (FTC) is highly im-
portant for investors. It emerges attention from wide
research ﬁelds, especially the Artiﬁcial Intelligence (AI)
[27]. The classical ﬁnancial theory, Eﬀective Market Hy-
pothesis (EMH)
[40], suggests that every piece of in-

Center for Intelligence of Science and Technology (CIST),
Department of Computer Science, Beijing University of Posts
and Telecommunications, Beijing, China

formation in the ﬁnancial market aﬀect the movements
of the corresponding security price. Thus, numerous re-
searches have investigated the impact of historical ﬁ-
nancial data for the future security price. Due to a large
amount of constantly produced ﬁnancial data, analyz-
ing these data consumes massive labor work from the
human expert. Consequently, the technologies which
can automatically process these data have been widely
explored [53, 26, 35].
From the property of time-series, existing researches
on FTC can be divided into the Multi-Scale (MS) -
oriented methods and the Temporal Dependency (TD)
-oriented methods.
For MS-oriented methods, existing researches focus
on extracting the MS features from ﬁnancial time-series.
As we know, the high scale of ﬁnancial time-series fea-
tures reﬂects the trend information of the ﬁnancial mar-
ket in the long run, while the low scale ﬁnancial time-
series features embody the short-term trend informa-
tion. The methods with only single-scale features ne-
glect the information on other scales. Accordingly, these
single-scale methods often fail to accurately describe
the current state of time-series movement. Unsurpris-
ingly, these methods tend to misjudge the category of
ﬁnancial time-series data. In order to describe ﬁnancial
time-series precisely, its MS-property should be consid-
ered. In the ﬁnancial area, the MS-property of ﬁnan-
cial time-series has been extensively investigated [11].
By the similarity measured on multiple scales, the fu-
ture price of given security can be estimated by ﬁnding
similar history price sequence across diﬀerent ﬁnancial
markets [43]. In the AI community, few studies have
explored the MS-property of ﬁnancial time-series. The
most prior work, ScaleNet [19], decomposes the time-
series into diﬀerent scales by Wavelet transform. Then,
it extracts features from each scale by diﬀerent Neu-

 
 
 
 
 
 
2

Liu Guang et al.

ral networks to make a prediction. More recently, Cui
et al. [10] use Convolutional Neural Network (CNN)
to improve the feature extraction ability. Although the
above methods have achieved remarkable improvement
compared to the methods only with single-scale fea-
tures, these works overpass the TD within the ﬁnancial
time-series.
For TD-oriented methods, the non-linear models are
often used due to the nonstationary of ﬁnancial time-
series. Most previous researches use classical models in
modeling classiﬁcation. For example, Kim [27] uses the
Support Vector Machine (SVM) to predict the stock
price index. Compared to the Neural Network (NN), it
achieves comparable results under their experiment set-
ting. It is notable that these models are not speciﬁcally
designed for modeling the TD. More recently, the deep
learning models [14] are introduced to improve the fea-
ture extraction and representation from ﬁnancial time-
series. For instance, Recurrent Neural Network (RNN),
which can handle TD eﬀectively, is often used in this
scenario [36]. However, the above methods only use
single-scale features and ignore the MS-property of ﬁ-
nancial time-series. Consequently, they are not capable
to describe the current state of the ﬁnancial market
precisely.
The MS and TD property of ﬁnancial time-series
and the subtle relation between these properties make
the FTC very challenging. Very few works have inves-
tigated the eﬀect of employing both properties of ﬁ-
nancial time-series for FTC. Recently, State Frequency
Memory (SFM) [24] integrate Long-Short Term Mem-
ory (LSTM) and Discrete Fourier Transform (DFT) to
model the multiple frequency properties in stock price
sequence. However, the DFT need pre-deﬁned parame-
ters which are very tricky and can not be learned auto-
matically. In addition, the DFT is not a suitable choice
for nonstationary ﬁnancial time-series. Therefore, a new
method for FTC which can eﬀectively utilize both prop-
erties of ﬁnancial time-series is needed.
To address the above problem, this paper proposes
a Multi-Scale Temporal Dependent Recurrent Convo-
lutional Neural Network (MSTD-RCNN) for ﬁnancial
time-series classiﬁcation. The proposed model is an ef-
fective end-to-end model which can learn its parameters
automatically. The ma jor contributions of this paper
are summarized as follows:

(1) We propose a novel method for FTC which combine
and utilize both MS and TD properties of ﬁnancial
time-series. The proposed method integrates CNN
and RNN to handle two diﬀerent properties in ﬁ-
nancial time-series.
(2) MS features are extracted with CNN units from the
single-scale input of ﬁnancial time-series sequence.

The parameters for each CNN units are learned au-
tomatically. There are no needs for tuning prede-
ﬁned parameters, which is critical for methods like
DFT.
(3) Diﬀerent scales features are fused with an RNN.
Beneﬁted from its structure in handling TD, the
RNN can explore and learn the dependency across
diﬀerent scales.
(4) To evaluate MSTD-RCNN, we build three minute-
level index price datasets, which are sourced from
the China stock market. According to the ﬁnancial
time-series shares identical structure and properties,
it is feasible to expand our methods to the global
ﬁnancial markets.

The experimental results demonstrate that our model
achieves superior performance compared to some clas-
sical and state-of-the-art baseline models in both ﬁnan-
cial time-series classiﬁcation and simulated trading.
The rest of this paper is organized as follows: Sec-
tion 2 introduces Financial Time-series Prediction, the
Multi-Scale (MS) property of time-series and GRU, Sec-
tion 3 illustrates the formulation of FTC and architec-
ture of MS-RCNN, Section 4 gives the experimental
settings, Section 5 describes the experimental results
and analysis, Section 6 shows the conclusions and fu-
ture works.

2 Related works

2.1 Financial Time-series Prediction

Financial time-series prediction is essential for devel-
oping eﬀective trading strategies in the ﬁnancial mar-
ket [32]. In past decades, it has attracted widespread at-
tention from researchers of many areas, especially the
Artiﬁcial Intelligence (AI) community [27]. These re-
searches mainly focus on a speciﬁcal market, e.g., the
stock market [34, 46], the foreign exchange market [18,
7, 12], and the futures market [57, 30]. Unsurprisingly,
it is very challenging due to their irregular and noisy
environment.
From the perspective of the learning target, exist-
ing researches can be divided into the regression ap-
proaches and classiﬁcation approaches. For the regres-
sion approaches, they treat this task as a regression
problem [4, 47], aiming to predict the future value of ﬁ-
nancial time-series. While the classiﬁcation-oriented ap-
proaches treat this as a classiﬁcation problem [23, 25],
focusing on ﬁnancial time-series classiﬁcation (FTC).
In most cases, the classiﬁcation approaches achieve
higher proﬁts than the regression ones [34]. Accord-

Multi-Scale RCNN Model for Financial Time-series Classiﬁcation

3

ingly, the eﬀectiveness of various approaches in FTC
has been widely explored [9, 37, 54, 42].

multiple categories, thereby improving the feature’s ex-
pressive for category information.

2.2 Multi-scale of ﬁnancial time-series

The Multi-Scale (MS) property for time-series classi-
ﬁcation has been widely studied [45, 43, 50, 56, 10, 55].
The concept of MS are often used for Computer Vision
(CV) tasks [16], i.e., image ob ject detection [5]. An im-
age is a sample formed by sampling the ob jects in the
real world at a certain pixel level. Images in large-scale
provide global features, images in small-scale provide
local features. The MS of an image can provide more
detailed information than single-scale features.
Similar to images, time-series also typically have
MS-property. Previous works mainly focus on predict-
ing the future value or movement direction based on
the assumption that the movement pattern of ﬁnancial
time-series will repeat itself. Thus, time-series similar-
ity analysis approaches have been extensively investi-
gated, i.e., discrete wavelet transform [19]. Among these
approaches, the use of MS-property is one of the key
factors to measure the similarity between time-series
sequences. Since the MS-property is very eﬀective to
characterize a time-series.
The way to analyze ﬁnancial data draw more chal-
lenging due to their non-stationary characteristic and
noisy environment in the ﬁnancial market. Therefore,
this paper focuses on predicting ﬁnancial time-series
movement direction by utilizing the MS-property.

2.3 Temporal dependency of ﬁnancial time-series

Previous researches have explored the eﬀectiveness of a
method who can classify ﬁnancial time-series based on
their Temporal Dependency (TD). Traditionally, these
researches can be divided into three categories: the feature-
oriented methods, the model-oriented methods, and the
integrated methods.
For the feature-oriented methods, the key factor is
to extract eﬀective features from the ﬁnancial time-
series data. Statistical-based approaches, such as Prin-
cipal Component Analysis (PCA) [53] and Information
Gain (IG) [33], are often used. These methods can help
to improve the performance of a given model by re-
moving the low relevant features. Some studies have
introduced fuzzy logic to transform them into more ex-
pressive representations [20, 6, 2]. Since these data are
mainly numerical which are weakly expressive for cat-
egory information. These researches transform the real
value in a feature into a probability distribution over

For the model-oriented methods, they focus on im-
proving the ﬁtting ability of the model. Traditionally,
Support Vector Machine (SVM) and Neural Network
(NN) are thought to be very eﬀective for ﬁnancial time-
series classiﬁcation [26]. Due to the excessive parameter
size, they are easily over-ﬁtting to the training set. As
a result, Extreme Learning Machine (ELM) [17] and
Random Forest (RF) [44] is introduced for ﬁnancial
time-series classiﬁcation. ELM can speed up training
and improve generalization performance through ran-
domly generated hidden layer units. RF ensembles mul-
tiple trees to achieve better prediction and generaliza-
tion performance than a single model. In more recent,
some pioneer researches have explored the eﬀectiveness
of deep learning models in ﬁnancial time-series classiﬁ-
cation [38, 1, 14]. Since deep learning models have many
successful applications in Computer Vision (CV) [31]
and Natural Language Processing (NLP) [29]. For in-
stance, TreNet [36] integrates Convolutional Neural Net-
work (CNN) and Long-Short Term Memory (LSTM)
for trend prediction.

For the integrated methods, they often integrate
multiple artiﬁcial intelligence or statistical-based tech-
niques into a pipeline method for ﬁnancial time-series
classiﬁcation. Some studies integrate the text classiﬁ-
cation [13, 48] and sentiment analysis [4] in NLP with
a classiﬁcation model to determine the direction of the
securities price movement. Kim and Han [28] have pro-
posed feature selection methods based on Genetic Algo-
rithm (GA) combined with a NN model to select useful
features to predict the trend of stock price. Teixeira
et al. [52] have used the technical indicators, which of-
ten are used in technical analysis, as the representa-
tion of ﬁnancial data and feed them into the classiﬁca-
tion model for FTC. Durn-Rosal et al. [49] have used
piecewise linear regression based turning points to seg-
ment the target sequence, and then use a NN to predict
these points. In this work, we explore the eﬀects of deep
learning models integrate statical-based method (down-
sampling) in FTC.

3 Model

In this section, we provide the formal deﬁnition of the
ﬁnancial time-series classiﬁcation. Then, we present the
proposed MSTD-RCNN model.

4

3.1 Problem formulation

In this paper, we focus on classifying sequence of ﬁnan-
cial time-series data into diﬀerent categories by their
movement direction. The price of a given security in the
ﬁnancial market is often a sequence of univariable data
sequence. A ﬁnancial time-series dataset is denoted as
D = {(xi , yi )}N , where N is the number of samples in
the dataset, xi ∈ RT is the ith sample with length T
and yi ∈ R is the corresponding label. Each sequence of
time-series is denoted as x = {x1 , x2 , , xT }, where xj is
the value at j th time-step and T is the length of time
steps.
As a result, FTC is to build a nonlinear map func-
tion from an input time-series xi to predict a class label
yi formula:

yi = f (xi ),
(1)
where f (·) is the nonlinear function we aim to learn.
Financial Time-series Classiﬁcation (FTC) emerge
attentions from researchers of various ﬁelds. However, it
is very challenging due to two ma jor diﬃculties. Firstly,
strategies/studies require Multi-Scale (MS) features to
describe the state of the ﬁnancial market. Secondly, the
Temporal Dependency (TD) features of diﬀerent scales
are needed to be fused to make an accuracy classiﬁca-
tion.
To address these problems for FTC, we propose a
Multi-Scale Temporal Dependent Recurrent Convolu-
tional Neural Network (MSTD-RCNN) model. The pro-
posed model transform the input sequence into MS se-
quences, extract features from each scale, fuses these
features and outputs the predicted category. Thus, the
proposed model is an eﬀective end-to-end model for
FTC.

3.2 Model architecture

The architecture of MSTD-RCNN is depicted in Fig. 1.
Our model mainly has three components: the transform
layer, the feature layer, and the fusion layer. The ma jor
functions of the three layers are described as follows:

1 For the transform layer, the input sequence is trans-
formed into MS sequences. Speciﬁcally, the down-
sampling transformations in the time domain are
used.
2 For the feature layer, diﬀerent convolutional units
are used to extract features from each scales. In this
end, convolution units of diﬀerent scales are inde-
pendent of each other. The feature maps of the con-
volution output will be padded to the same length
and then concatenated together.

Liu Guang et al.

3 For the fusion layer, we feed the padded and con-
catenated feature maps to the GRU. The output of
the GRU passes through the fully connected layers
and the softmax layer to produce the ﬁnal output.

Thus, our MSTD-RCNN model is a complete end-to-
end system where all parameters are jointly trained
through backpropagation.

3.2.1 Transform layer

In this layer, the single-scale input sequence is trans-
formed into multiple new sequences with diﬀerent scales.
Here, the down-sampling is used to generate sketches of
ﬁnancial data at diﬀerent scales. This MS time-series
is potentially crucial to the prediction quality for this
task. Furthermore, they can complement each other.
High scale features reﬂect slow trends and low scale
features exhibit subtle changes in fast trends.
Suppose there is a input sequence x = {x1 , x2 , , xT },
and the down-sampling rate is d. Then every dth data
points is keep in the new sequence xd = {xd , x2d , , xmd },
where m = T /d is the length of sequence xd . Through
this method, multiple new sequences are generated with
diﬀerent down sampling rates, e.g., s = 1, 2, , d. For
simplify, we use X to denote the generated sequences
{x1 , x2 , , xd}.

3.2.2 Feature layer

This layer takes MS sequences as the input and outputs
the concatenated features, which are extracted from
each scale. It has two ma jor components: the convo-
lutional units and the concatenates operation.
Convolutional units. The CNN units, which are of-
ten used as a feature extractor in Computer Vision [16],
are used to extract feature maps from sequences with
diﬀerent scales. Speciﬁcally, 1-dimension CNN is used
to process these newly generated sequences. These CNN
units share the same ﬁlter size and number across all
these sequences. Note that, with the same settings, higher
scale sequence would get a larger receptive ﬁeld than
the original sequence. Through this means, each output
of the convolution operation captures the features with
a diﬀerent receptive ﬁeld from the original sequence.
An advantage of this process is that by down-sampling
the input sequence instead of increasing the ﬁlter size,
we can greatly reduce the computation in the convolu-
tional units.
Let xd to denote the dth scale time-series. The cor-
j ∈ Rk is used to extract
responding kernel weights wd
features from the input sequence. Here, k is the window
size. For instance, the feature cd
j,i is calculated by
j ∗ xd

i:i+k−1 + bd

j ) .

cd

j,i = fa (wd

(2)

Multi-Scale RCNN Model for Financial Time-series Classiﬁcation

5

Fig. 1 The architecture of MSTD-RCNN.

Here, ∗ indicates the convolution operation, bd
j ∈ R is a
bias term and fa (·) is a non-linear function such as the
Rectiﬁed Linear Unit (ReLU). This ﬁlter is applied to
the sequence {xd
(m−k+1:m) } to produce
j ∈ R(m−k+1) as follow
a feature map cd

(1:k) , xd
(2:k+1) , , xd

(3)

(j,1) , cd
(j,2) , , cd
(j,m−k+1) ] .

cd
j = [cd
Here, the pooling layers are not used. Since the trans-
form layer does similar work as the pooling layers. The
pooling layers increasing the receptive ﬁeld [22]. While
the transform layer transforms the original sequence
into diﬀerent time-scales before the feature extraction.
We believe the transformation before the feature ex-
traction can achieve similar eﬀects with pooling layers.
Concatenation operation. This operation concatenates
the feature maps of diﬀerent scales. Due to the diﬀer-
ent lengths of feature maps, padding is needed before
concatenation.
Since m = T /d, the length of the feature map de-
creases with scale increasing. For the convenience of cal-
culation, we unify the feature maps of diﬀerent scales to
the same length, that is, the feature map length T −k+1
when d = 1. We align the feature maps of other scales
to T − k + 1 length by zero-padding. For example, the
alignment of the feature map for scale d is as follow
ad
j = [z1 , z2 , , zT −m , cd
(4)
j ∈ RT −k+1 is the padded feature map gener-
where ad
ated by jth kernel for d scale and z1 , z2 , , z(T −m) are
zeros sequence with length T − m.
Next, the padded feature maps are concatenated
into a feature matrix. The concatenating process is de-
scribed as following
1 )T , · · · , (a1
1 )T , · · · , (ad
E = [(a1
l )T , (a2
l )T ]T ,

j,1 , cd
(j,2) , , cd
(j,m−k+1) ] ,

where E ∈ R(d×l)×(T −k+1) is the feature matrix with
length T −k+1, l is the number of convolutional kernels.

3.2.3 Fusion layer

The fusion layer fuses the features from multi-scales and
generates a prediction. The output of the feature layer is
similar to the language model in Natural Language Pro-
cessing, which has Temporal Dependency (TD) among
each node. The ma jor diﬀerence is that the sequences
from diﬀerent scales have diﬀerent ﬁelds of view. To fuse
these features, we need a model that captures this de-
pendency and variety. The Recurrent Neural Networks
(RNN) is often used as an encoder in Machine Learning
Translation [8]. It can capture the complex dependency
in diﬀerent languages. Hence, we use the RNN model
to process the feature maps in this case.
Recurrent Neural Networks (RNN) have been suc-
cessfully applied in machine translation [51]. The struc-
ture of RNNs are good at handling a variable-length se-
quence input by having a recurrent hidden state whose
activation at each time is dependent on that of the pre-
vious time.
Similar to a traditional neural network, we can use a
modiﬁed backpropagation algorithm Backpropagation
Through Time (BPTT) to train an RNN [41]. Unfor-
tunately, it is diﬃcult to train RNN to capture long-
term dependencies because the gradients tend to either
vanish or explode [3]. Hochreiter and Schmidhuber [21]
proposed a long short-term memory (LSTM) unit and
Cho et al. [8] proposed a gated recurrent unit (GRU) to
deal with the Problem eﬀectively. To this end, we use
GRU to process the feature matrix.

(5)

6

Liu Guang et al.

A Gated Recurrent Unit (GRU) makes each recur-
rent unit to adaptively capture dependencies of diﬀer-
ent time scales. The parameters can be updated by the
following equations

rt = σ(Wr xt + Ur ht−1 ) ,

zt = σ(Wz xt + Uz ht−1 ) ,

h(cid:48)

t = tanh(Whxt + Uh (rt (cid:12) ht−1 )) ,

(6)

(7)

(8)

ht = (1 − zt )ht−1 + zth(cid:48)
t ,
(9)
where σ(·) denotes the logistic sigmoid function, (cid:12) de-
notes the element-wise multiplication, rt denotes the
reset gate, zt denotes the update gate and h(cid:48)
t denotes
the candidate hidden layer. In this paper, we apply the
GRU as the feature summarize layer for stock trend
prediction.
Given the feature vector et , the hidden states at the
tth time-step can be calculated by

ht = fen (et , ht−1 ; θen ) ,
(10)
where ht ∈ Rq is the hidden state of the encoder at
time t, q is the size of hidden state, et ∈ Rd×l is the tth
column in the matrix E, fen (·) is a non-linear function,
and θen is the parameters of encoder function. There
are many choice for encode the sequence of numerical
data. In this case, we use GRU as the non-linear func-
tion. The output of GRU is hT −k+1 ∈ Rq is deemed as
the encoding of the multiple scales of input sequence.

The feature vector output by the GRU is passing
through the multiple fully connected layers, and then
a softmax activation layer to obtain a probability dis-
tribution of diﬀerent classes. The softmax activation
function is calculated as follow

p(cid:48)

i =

eoi(cid:80)C
j=1 eoj

where oi indicates the result of the ith output node,
and C is the number of categories.
The cross-entropy loss function is used to measure
the diﬀerence between our predicted classiﬁcation dis-
tribution p(cid:48)
t,i and real distribution pt,i :

Jθ = − 1
N

pt,j log(p(cid:48)

t,j ) ,

(12)

N(cid:88)

C(cid:88)

t=1

j=1

4 Experimental settings

In this section, we ﬁrst give the details of datasets.
Then, we introduce the baseline models in comparative
evaluation. Last, the evaluation metrics are illustrated.

4.1 Datasets

We ﬁrst describe the data source of the datasets. Then,
we explain how to choose the threshold for the label
and the window size for window sliding.
Three high-frequency stock index datasets are col-
lected from the Chinese stock market.

– SH000001: Shanghai Stock Exchange (SSE) Com-
posite Index. Prepared and published by SSE in-
dex is the authoritative statistical indices widely fol-
lowed and used at home and abroad to measure the
performance of China’s securities market.
– SZ399005: Shenzhen Stock Exchange Small & Medium
Enterprises (SME Boards) Price Index. SME play
an important role in the economic and social de-
velopment of China. They foster economic growth,
generate employment and contribute to the devel-
opment of a dynamic private sector.
– SZ399006: ChiNext Price Index is a NASDAQ-style
board of the Shenzhen Stock Exchange. It aims to
attract innovative and fast-growing enterprises, es-
pecially high-tech ﬁrms. Its listing standards are less
stringent than those of the Main and SME Boards
of the Shenzhen Stock Exchange.

The data in the dataset begins on January 1, 2016, and
ends on December 30, 2016. There are a total of 58,000
data points. The window slicing is applied for the data
augmentation [10]. There are 48,000 of data points are
used as training sets, 5000 are used as veriﬁcation sets,
and 5000 are used as testing sets.
There are three categorical values, they are deﬁned as
follows

−1 ∆xt ≤ −δ

0 −δ < ∆xt < δ
∆xt ≥ δ
1

Here, c = 0 means the price of the security in the next
time-step is still, c = 1 means the price is going upward
and c = −1 means the price is moving downward, δ is
the threshold and ∆xt is the change value compared to
the previous time-step, it calculated by
∆xt = xt − xt−1 .

(14)

,

(11)

yt =

.

(13)

where θ is represent all the parameter of the model, N
is the total number of samples.

To select the threshold for each dataset, we analyzed
the distribution of each dataset. As shown in Fig. 2

Multi-Scale RCNN Model for Financial Time-series Classiﬁcation

7

and Table 3, the distribution of price change on the
development set on each dataset are mostly clustered
around zero. We choose the threshold which can make
each category on the development set distribute equally.
As a result, the threshold is set to 0.3, 0.2 and 0.8 for
SH000001, SZ399006, and SZ399005. The distribution
of each dataset is shown in Table 1.
To select the window size for sliding windows. We
use the Random Forest(RF) to train on the training set
and evaluate the development set under diﬀerent win-
dow size setting. As shown in Table 2, the window size
T = 30 makes the superior performance for RF. There-
fore, the window size set to 30.

In order to avoid excessive correlation between these
datasets, we calculate the Pearson Correlation Coef-
ﬁcient (PCC) for the data of these three data sets.
P CCX Y = −1 indicates that X and Y has a nega-
tive correlation. P CCX Y = 1 indicates that X and Y
has a positive correlation. P CCX Y = 0 indicates that
X and Y has no correlation. Table 4 lists the results of
PCC, which indicate that there are no strong correla-
tions (< 0.5) between each pair of datasets.

4.2 Baselines

There are six baseline models are used. Firstly, two clas-
sical models in FTC are given. Then, four advanced
models for FTC are illustrated.

– Support Vector Machine (SVM) [27]. It pro jects the
input data into a higher dimensional space by the
kernel function and separates diﬀerent classes of data
using a hyperplane. The trade-oﬀ between margin
and misclassiﬁcation errors is controlled by the reg-
ularization parameter.
– Random Forest (RF) [26]. It belongs to the category
of ensemble learning algorithms. It uses the decision
tree as the base learner of the ensemble. The idea
of ensemble learning is that a single classiﬁer is not
suﬃcient for determining the class of test data. After
the creation of n trees, when testing data is used,
the decision on which the ma jority of trees come
up with is considered as the ﬁnal output. This also
avoids the problem of over-ﬁtting.
– Fuzzy Deep Neural Network (FDNN) [14]. FDNN
uses fuzzy-neural layers and fully connected layers
to learn the fuzzy representation and neural rep-
resentation separately. Then, these two representa-
tions are fused by a two-layer fully connected layer.
The fused representations are fed to a softmax acti-
vation to get the trend to predict results.

– TreNet [36]. TreNet hybrids LSTM and CNN for
stock trend classiﬁcation. Firstly, LSTM learning
the dependencies in historical trend sequence, and
CNN learning the local features from raw data of
time-series. Then, these extracted features fused by
a fully connected layer to generate a prediction.
– State-Frequency Memory Recurrent Neural Networks
(SFM) [24]. It allows separating dynamic patterns
across diﬀerent frequency components and their im-
pacts on modeling the temporal contexts of input
sequences. By jointly decomposing memorized dy-
namics into state frequency components, the SFM
is able to oﬀer a ﬁne-grained analysis of temporal se-
quences by capturing the dependency of uncovered
patterns in both time and frequency domains.
– Multi-Scale CNN (MS-CNN) [10]. MS-CNN uses dif-
ferent convolutional units to extract features from
each time-scale of data. Then, these features are
fused by a two-layer fully connected layers. In most
cases, this model achieves better performance than
the regular convolutional neural network in time-
series classiﬁcation.

The parameters of our model are selected by the per-
formance on the validation set. The maximum epoch is
set to 100. The model is trained by Adam optimization
algorithm with the learning rate 0.0005. The batch size
is set to 32. There are l = 3 time-scales. Convolution
unit for each time-scale has 16 ﬁlters, the number of
hidden units in GRU is set to 48 (3 × 16).

4.3 Evaluation metrics

Accuracy, F-score(F1) and Confusion Matrix(CM) are
used as the classiﬁcation metrics to evaluate the mod-
els. And the accumulated proﬁt is used to evaluate the
proﬁtability of the models.
Accuracy and F1 are calculated based on Confusion
Matrix which has four components: True Positive(TP),
True Negative(TN), False Positive(FP) and False Neg-
ative(FN). CM shows for each pair of classes (cid:104)c1 , c2 (cid:105),
how many samples from c1 were incorrectly assigned to
c2 .
Accuracy is the rate of correct prediction and is calcu-
lated as the formula in Equation 15. Equation 15.

acc =

T P + T N
N
Here, N is the total number of samples in dataset. We
next explain the weighted average of F1. The calcula-
tion of F1 is displayed in Equation 16.
2 × P × R
P + R

f 1 =

.

,

(15)

(16)

8

Liu Guang et al.

Fig. 2 (a)The histogram of price change on development set of SH000001. (b)The histogram of price change on development
set of SZ399006. (c)The histogram of price change on development set of SZ399005.

Table 1 Ratio of categories on each datasets.

SH000001

SZ399005

SZ399006

Category(trend) Train(%) Dev(%) Test(%) Train(%) Dev(%) Test(%) Train(%) Dev(%) Test(%)
Downward(↓)
Still(-)
Upward(↑)

36.60
26.99
36.41

35.28
29.24
35.48

35.78
32.42
31.80

33.04
32.16
34.80

35.66
31.90
32.44

41.68
18.67
39.65

36.76
31.60
31.64

40.31
18.50
41.19

32.82
33.16
34.02

Table 2 Window size on each development sets.

SH000001

SZ399005

SZ399006

window size

10
20
30
40
50

Acc

0.5132
0.5212
0.5256
0.5210
0.5222

F1

Acc

F1

0.5123
0.4842
0.4718
0.5214
0.5018
0.4905
0.5252
0.5064
0.4938
0.5207
0.4856
0.4703
0.5212
0.4998
0.4842
Bold numbers indicate the best results

Acc

0.5874
0.5974
0.5996
0.5950
0.5934

F1

0.5826
0.5932
0.5956
0.5901
0.5877

Table 3 Statical features of each development sets.

SH000001

SZ399005

SZ399006

Mean
Std

0.0368
1.1915

0.0016
0.9429

0.0081
3.6840

Table 4 PCC results between each pair of datasets.

SH000001

SZ399005

SZ399006

change value of index ∆xt . For each trading signal gen-
erated by the model, we will execute the buy-in or sell-
out one unit of security. For the upward and downward
categories, we will make a proﬁt if the prediction is cor-
rect, and if it is wrong, we will suﬀer losses. For the still
category, the change value ∆xt = 0 is set to zero. The
transaction cost is set to zero. The accumulated proﬁt
P . is calculated by

N(cid:88)

t=1

SH000001
SZ399005
SZ399006

1.00
-
-

0.58
1.00
-

0.42
0.42
1.00

P . =

I (c(cid:48)
t , ct ) × ∆xt .

(19)

where R is the recall and P is the precise, which are
calculated as follows:

Here, P . indicates the proﬁt representing the change
points, I (c(cid:48)
t , ct ) is an indicator function, which equals 1
when the c(cid:48)
t = ct , otherwise 0.

P =

T P
T P + F P

,

R =

T P
T P + F N

.

(17)

(18)

The simulated trading algorithm is calculated based
on the predicted result c(cid:48)
t , the real trend ct and the

5 Results and Analysis

First, the models performance is compared with the
baseline models on three datasets. Then, the eﬀects of
the feature layer in extracting Multi-Scale (MS) fea-
tures are analyzed. Third, the eﬀects of the fusion layer

Multi-Scale RCNN Model for Financial Time-series Classiﬁcation

9

in capturing Temporal Dependent (TD) are analyzed.
Next, the proﬁtability of models is evaluated by simu-
lated trading. Last, the reason for driving improvement
in proﬁtability is analyzed through the confusion ma-
trix.

due to the eﬃciency of our MSTD-RCNN in captur-
ing TD.

5.2 Eﬀects of multi-scale features

5.1 Comprehensive evaluation

Financial time-series classiﬁcation is a challenging task
and a minor improvement usually leads to large po-
tential proﬁts [47]. To demonstrate the eﬀectiveness of
our MSTD-RCNN model, we compare it against the six
baseline models on three datasets. The results are listed
in Table 5 The t-test results between our model and
other models are listed in Table 6. Examining the ex-
perimental results, we reach the following conclusions.

To illustrate the eﬀects of MSTD-RCNN in employ-
ing Multi-Scale (MS) property, we evaluate our MSTD-
RCNN under diﬀerent scale settings. Since MSTD-RCNN
uses the convolutional unit to extract features from dis-
tinct scales. The eﬀects on feature extraction can be
evaluated by the classiﬁcation performance when using
diﬀerent scale settings. Hence, our model is evaluated
with scale settings as follows: (1), (1, 2), (1, 2, 3). (1) in-
dicates our model using original single-scale data. (1, 2)
suggest our model using scale 1 and 2. (1, 2, 3) denotes
our model using three corresponding scales.
1 Our model achieves the best performance in both
Table 7 lists the classiﬁcation results of MSTD-RCNN
accuracy and F1. From the perspective of accuracy,
with diﬀerent scale settings. The accuracy and F1 of
our model achieves the best results in all three datasets.
MSTD-RCNN are rising with the increasing of scale
Especially, MSTD-RCNN rises the accuracy of 3.07%,
number. The (1, 2, 3) MSTD-RCNN outperforms the
3.00%, and 2.13% higher than the best baseline mod-
(1, 2) MSTD-RCNN, and the (1, 2) MSTD-RCNN out-
els on SH000001, SZ399005, and SZ399006. From
performs the (1) MSTD-RCNN. These are mainly due
the perspective of F1, our model also achieves the
to the eﬀect of MS features. These features from diﬀer-
best performance on these three datasets. Especially,
ent scales can complement each other. Moreover, MSTD-
MSTD-RCNN has 4.14%, 2.21%, and 2.86% improve-
RCNN achieves higher classiﬁcation performance than
ment compared to the best baseline models on SH000001,
baselines even if only using single-scale data. The (1)
SZ399005, and SZ399006. In addition, the t-test re-
MSTD-RCNN achieves a higher level of accuracy and
sults suggest that the results of our model are sig-
F1 than baselines on three datasets. This is likely due
niﬁcantly diﬀerent from the ones of other models.
to the eﬀect of the convolutional units in feature ex-
2 Our MSTD-RCNN model can eﬀectively extract MS
traction.
features from ﬁnancial time-series. First of all, all
models share the same single-scale input. Only SFM,
MS-CNN and MSTD-RCNN are designed to utilize
the MS-property of time-series. As a result, these
models achieve a higher level of accuracy and F1
than other baseline models in most cases. It can be
concluded that the FTC models, which can utilize
MS-property, are more eﬀective than the single-scale
ones. Secondly, our model achieves the best accu-
racy and F1 performance among these three models.
That indicates our MSTD-RCNN is more eﬀective
in extract MS features than the other two models.
3 Our MSTD-RCNN model is very eﬀective in cap-
turing Temporal Dependency (TD) within ﬁnan-
cial time-series. MSTD-RCNN has a signiﬁcantly
higher level of classiﬁcation performance than MS-
CNN. These two models have a similar structure.
Especially, they both transform input single-scale
sequence into MS sequences and then use CNN to
extract MS features. For fuse features, MSTD-RCNN
uses the GRU and the MS-CNN uses NN. We can
conclude that the performance improvement is likely

To show the eﬀects of Temporal Dependency (TD), we
compare the classiﬁcation performance of MS-CNN and
our model under diﬀerent scale settings. MS-CNN and
our MSTD-RCNN share similar structure in feature ex-
traction. Hence, the diﬀerences in classiﬁcation perfor-
mance are ma jorly due to the diﬀerent level of eﬃciency
in capturing TD.
Fig. 3.a shows the accuracy results on three datasets.
Firstly, the performance of MSTD-RCNN is rising with
the increasing of scale numbers on all three datasets.
While the MS-CNN has no signiﬁcant trend in perfor-
mance in most cases. That is likely due to the fusion
layer in MSTD-RCNN can eﬀectively fuse the features
with TD. MS-CNN uses fully connected layers to fuse
features. While MSTD-RCNN uses a GRU to fuse fea-
tures. Secondly, with sample input, the MSTD-RCNN
has a higher level of accuracy than MS-CNN on all three
datasets. The fusion layer can capture the temporary

5.3 Eﬀects of temporal dependency

10

Liu Guang et al.

Table 5 Results on the three datasets.

Model

SVM
RF
TreNet
FDNN
SFM
MS-CNN
MSTD-RCNN

SH000001

SZ399005

SZ399006

ACC

0.5150
0.5230
0.5238
0.5232
0.5296
0.5334
0.5498

F1

ACC

F1

0.5181
0.5038
0.4960
0.5196
0.5116
0.5054
0.5250
0.5120
0.5134
0.5245
0.5122
0.5035
0.5227
0.5254
0.5232
0.5287
0.5198
0.5201
0.5506
0.5454
0.5516
Bold numbers indicate the best results

ACC

0.5620
0.5732
0.5964
0.5930
0.5960
0.6006
0.6134

F1

0.5357
0.5654
0.5857
0.5510
0.5740
0.5954
0.6124

Table 6 The t-test results between MSTD-RCNN and other baseline models.

MSTD-RCNN (×10−3 )

SVM

0.8***

RF

0.1***

TreNet

0.2***

FDNN

2.5***

SFM

0.5***

MS-CNN

0.7***

p-value < 0.01: ***, p-value < 0.05: **, p-value < 0.1: *

Table 7 Eﬀects of multi-scale features.

Model

(1) MSTD-RCNN
(1, 2) MSTD-RCNN
(1, 2, 3) MSTD-RCNN

SH000001

SZ399005

SZ399006

ACC

0.5356
0.5454
0.5498

F1

ACC

0.5373
0.5330
0.5442
0.5414
0.5506
0.5454
Bold numbers indicate the best results

F1

0.5210
0.5410
0.5516

ACC

0.6070
0.6122
0.6134

F1

0.6040
0.6112
0.6124

dependency which is very important for time-series clas-
siﬁcation. We can conclude that MSTD-RCNN is more
eﬀective than MS-CNN to capture the TD.
Fig. 3.b shows the F1 results on three datasets.
There are similar observations as the Fig. 3.a.

5.4 Simulated trading

The ultimate goal of ﬁnancial time-series classiﬁcation
is to make a proﬁt. To estimate the models’ proﬁtability,
we use a simulated trading algorithm (Equation 19) to
evaluate these models based on their predictions on the
testing sets. Table 8 lists the simulated trading results
on three datasets. The proﬁtability of these models is
compared to the baseline strategy Buy & Hold (B&H)
strategy. This B&H suggests that buy in the security
at the beginning and sell out at the end.
MSTD-RCNN achieves the highest proﬁt on all three
datasets. Especially, 61.44, 126.06 and 149.54 higher
than the most proﬁtable baseline model. Note that B&H
strategy suﬀers losses due to the market is in a down-
ward trend, while all models can make a proﬁt. The
results show that our model can not only be more ac-
curate in classiﬁcation but also more proﬁtable than
baseline models.
Next, the confusion matrix of our model is analyzed
to ﬁnd the cause of proﬁtability improvement.

Table 8 proﬁts of simulated trading.

Strategies

SH000001

SZ399005

SZ399006

B&H
SVM
RF
TreNet
FDNN
SFM
MS-CNN
MS-RCNN

-233.13
1172.68
1241.96
1330.30
1231.07
1316.16
1358.41
1419.85

-221.48
1177.10
1247.25
1255.96
1273.94
1265.90
1262.12
1400.93

-557.53
7225.94
7260.03
7373.50
7377.40
7459.88
7427.14
7609.42

Bold numbers indicate the best results

5.5 Confusion Matrix

To ﬁnd the reason for proﬁtability improvement, we an-
alyze its confusion matrix of our MSTD-RCNN on three
datasets. For comparison, we also demonstrate the con-
fusion matrix of MS-CNN. Since MS-CNN shares a sim-
ilar structure with our model and it achieves almost
the highest classiﬁcation and proﬁt performance among
baseline models.
There are three categories of ﬁnancial time-series:
still(0), downward(1) and upward(2). Due to the im-
paction to proﬁtability, the samples in downward and
upward categories have a higher level of importance
than the still ones. As a result, the error classifying
these two categories will make the model suﬀer from
losses in the simulated trading. In contrast, the error

Multi-Scale RCNN Model for Financial Time-series Classiﬁcation

11

Fig. 3 (a)The accuracy results with diﬀerent scales on three datasets. (b)The F1 results with diﬀerent scales on three datasets.

(a)

(b)

classifying the still category to the other two categories
have no harm to the model’s proﬁtability.
Fig. 4 shows the confusion matrix of MSTD-RCNN
and MS-CNN on SH000001, SZ399005, and SZ399006.
The ma jor observations are listed as follows:

– For the error classifying ”upward” to ”downward”,
MSTD-RCNN has fewer occurrences than MS-CNN.
For instance, MSTD-RCNN only error classiﬁes 153
sequences on SH000001, which is 23 sequences less
than MS-CNN.
– For the error classifying ”downward” to ”upward”,
MSTD-RCNN also has fewer occurrences than MS-
CNN. For example, MSTD-RCNN error classiﬁes
236 sequences on SZ399005, which is 50 sequences
less than MS-CNN.
– For the ”upward” and ”downward”, MSTD-RCNN
achieves a higher level of precision than MS-CNN.
Such as on SZ399006, MSTD-RCNN achieves ”down-
ward” precision 0.600 and ”upward” precision 0.608,
which are 6.2% and 7.0% higher than MS-CNN.

MSTD-RCNN has higher upward and downward clas-
siﬁcation accuracy and lower error classifying number
between upward and downward than MSTD-RCNN.
Those are the accounting for that our model achieves
higher proﬁtability than MSTD-RCNN. Moreover, it is
likely causing our model to achieve the highest prof-
itability in simulated trading.

6 Conclusion and future works

This paper proposes a Multi-Scale Recurrent Convolu-
tional Neural Network, denoted MSTD-RCNN, for ﬁ-
nancial time-series classiﬁcation. The proposed method

(a)

(b)

(c)

(d)

(e)

(f )

Fig. 4 (a)(c)(e) show the confusion matrix of MSTD-RCNN
on SH000001, SZ399005 and SZ399006. (b)(d)(f ) illustrate
the confusion matrix of MSTD-CNN on SH000001, SZ399005
and SZ399006.

12

Liu Guang et al.

can eﬀectively combine and utilize Multi-Scale (MS)
and Temporal Dependency (TD). The convolutional units
are integrated to simultaneously extract MS-features,
and a GRU is used to capture the TD across multiple
scales. This enables the classiﬁcation of time-series with
MS-property by feedforwarding a single time-scale in-
put sequence through the network, which results in a
very eﬀective end-to-end classiﬁer. The proﬁtability of
our model is also evaluated by a simulated trading algo-
rithm. Extensive experimental results suggest that our
MSTD-RCNN achieves state-of-the-art performance in
ﬁnancial time-series classiﬁcation.
In the future, we prepare to explore three potential
directions to improve our MSTD-RCNN. First, diﬀer-
ent structure of feature extractors, such as the most
recently Transformer [15] is likely an even more eﬀec-
tive structure than CNN. Second, the attention mech-
anism [39] can be introduced to handle the long-term
dependency which cannot be handled by RNN. Third,
multi-source of information can be used, especially tex-
tual information such as news.

Acknowledgements This research was funded in part by
the National Social Science Fund of China No. 2016ZDA055,
and in part by the Discipline Building Plan in 111 Base No.
B08004. The authors thank NVIDIA’s donations of their pro-
viding GPUs used for this research. The authors would also
like to thank the editor and anonymous reviewers for their
precious comments on improving this article.

References

1. Akita, R., Yoshihara, A., Matsubara, T., Uehara, K.:
Deep learning for stock prediction using numerical and
textual information. In: Computer and Information Sci-
ence (ICIS), 2016 IEEE/ACIS 15th International Confer-
ence on, pp. 1–6. IEEE (2016)
2. Atsalakis, G.S., Valavanis, K.P.: Forecasting stock mar-
ket short-term trends using a neuro-fuzzy based method-
ology. Expert systems with Applications 36(7), 10696–
10707 (2009)
3. Bengio, Y., Simard, P., Frasconi, P.: Learning long-term
dependencies with gradient descent is diﬃcult.
IEEE
transactions on neural networks 5(2), 157–166 (1994)
4. Bollen, J., Mao, H., Zeng, X.: Twitter mood predicts the
stock market. Journal of computational science 2(1), 1–8
(2011)
5. Cai, Z., Fan, Q., Feris, R.S., Vasconcelos, N.: A uniﬁed
multi-scale deep convolutional neural network for fast ob-
ject detection.
In: European Conference on Computer
Vision, pp. 354–370. Springer (2016)
6. Chang, P.C., Liu, C.H.: A tsk type fuzzy rule based sys-
tem for stock price prediction. Expert Systems with ap-
plications 34(1), 135–144 (2008)
7. Cheung, Y.W., Chinn, M.D., Pascual, A.G., Zhang, Y.:
Exchange rate prediction redux: new models, new data,
new currencies. Journal of International Money and Fi-
nance (2018)

8. Cho, K., Van Merri¨enboer, B., Gulcehre, C., Bah-
danau, D., Bougares, F., Schwenk, H., Bengio, Y.: Learn-
ing phrase representations using rnn encoder-decoder
for statistical machine translation.
arXiv preprint
arXiv:1406.1078 (2014)
9. Choudhry, R., Garg, K.: A hybrid machine learning sys-
tem for stock market forecasting. World Academy of Sci-
ence, Engineering and Technology 39(3), 315–318 (2008)
10. Cui, Z., Chen, W., Chen, Y.: Multi-scale convolutional
neural networks for time series classiﬁcation.
arXiv
preprint arXiv:1603.06995 (2016)
11. Dacorogna, M.M., Gauvreau, C.L., M¨uller, U.A., Olsen,
R.B., Pictet, O.V.: Changing time scale for short-term
forecasting in ﬁnancial markets. Journal of Forecasting
15(3), 203–227 (1996)
12. Das, S.R., Mishra, D., Rout, M.: A hybridized elm-jaya
forecasting model for currency exchange prediction. Jour-
nal of King Saud University-Computer and Information
Sciences (2017)
13. De Fortuny, E.J., De Smedt, T., Martens, D., Daelemans,
W.: Evaluating and understanding text-based stock price
prediction models.
Information Processing & Manage-
ment 50(2), 426–441 (2014)
14. Deng, Y., Ren, Z., Kong, Y., Bao, F., Dai, Q.: A hier-
archical fused fuzzy deep neural network for data clas-
siﬁcation. IEEE Transactions on Fuzzy Systems 25(4),
1006–1012 (2017)
15. Devlin, J., Chang, M.W., Lee, K., Toutanova, K.: Bert:
Pre-training of deep bidirectional transformers for lan-
guage understanding. arXiv preprint arXiv:1810.04805
(2018)
16. Eigen, D., Fergus, R.: Predicting depth, surface normals
and semantic labels with a common multi-scale convolu-
tional architecture. In: Proceedings of the IEEE Inter-
national Conference on Computer Vision, pp. 2650–2658
(2015)
17. Fern´andez, C., Salinas, L., Torres, C.E.: A meta extreme
learning machine method for forecasting ﬁnancial time
series. Applied Intelligence 49(2), 532–554 (2019)
18. Frankel, J.A., Froot, K.A.: Chartists, fundamentalists,
and trading in the foreign exchange market. The Ameri-
can Economic Review 80(2), 181–185 (1990)
19. Geva, A.B.: Scalenet-multiscale neural-network architec-
ture for time series prediction.
IEEE Transactions on
neural networks 9(6), 1471–1482 (1998)
20. Guan, H., Dai, Z., Zhao, A., He, J.: A novel stock
forecasting model based on high-order-fuzzy-ﬂuctuation
trends and back propagation neural network. PloS one
13(2), e0192366 (2018)
21. Hochreiter, S., Schmidhuber, J.: Long short-term mem-
ory. Neural computation 9(8), 1735–1780 (1997)
22. Howard, A.G., Zhu, M., Chen, B., Kalenichenko, D.,
Wang, W., Weyand, T., Andreetto, M., Adam, H.: Mo-
bilenets: Eﬃcient convolutional neural networks for mo-
bile vision applications. arXiv preprint arXiv:1704.04861
(2017)
23. Hsieh, T.J., Hsiao, H.F., Yeh, W.C.: Forecasting stock
markets using wavelet transforms and recurrent neural
networks: An integrated system based on artiﬁcial bee
colony algorithm. Applied soft computing 11(2), 2510–
2525 (2011)
24. Hu, H., Qi, G.J.: State-frequency memory recurrent neu-
ral networks.
In: International Conference on Machine
Learning, pp. 1568–1577 (2017)
25. Huang, C.J., Yang, D.X., Chuang, Y.T.: Application of
wrapper approach and composite classiﬁer to the stock

Multi-Scale RCNN Model for Financial Time-series Classiﬁcation

13

SIGMOD international conference on Management of
data, pp. 647–658. ACM (2006)
44. Patel, J., Shah, S., Thakkar, P., Kotecha, K.: Predicting
stock and stock price index movement using trend de-
terministic data preparation and machine learning tech-
niques. Expert Systems with Applications 42(1), 259–268
(2015)
45. Peng, C.K., Hausdorﬀ, J., Havlin, S., Mietus, J., Stan-
ley, H., Goldberger, A.: Multiple-time scales analysis of
physiological time series under neural control. Physica
A: Statistical Mechanics and its Applications 249(1-4),
491–500 (1998)
46. Saad, E.W., Prokhorov, D.V., Wunsch, D.C.: Compar-
ative study of stock trend prediction using time delay,
recurrent and probabilistic neural networks. IEEE Trans-
actions on neural networks 9(6), 1456–1470 (1998)
47. Schumaker, R.P., Chen, H.: Textual analysis of stock
market prediction using breaking ﬁnancial news: The
azﬁn text system. ACM Transactions on Information
Systems (TOIS) 27(2), 12 (2009)
48. Shynkevich, Y., McGinnity, T., Coleman, S., Belatreche,
A.: Predicting stock price movements based on diﬀerent
categories of news articles.
In: Computational Intelli-
gence, 2015 IEEE Symposium Series on, pp. 703–710.
IEEE (2015)
49. Song, Y., Lee, J.W., Lee, J.: A study on novel ﬁlter-
ing and relationship between input-features and target-
vectors in a deep learning model for stock price predic-
tion. Applied Intelligence pp. 1–15 (2018)
50. Stopar, L., Skraba, P., Grobelnik, M., Mladenic, D.:
Streamstory: Exploring multivariate time series on multi-
ple scales. IEEE Transactions on Visualization and Com-
puter Graphics (2018)
51. Sutskever, I., Vinyals, O., Le, Q.V.: Sequence to sequence
learning with neural networks.
In: Advances in neural
information processing systems, pp. 3104–3112 (2014)
52. Teixeira, L.A., De Oliveira, A.L.I.: A method for au-
tomatic stock trading combining technical analysis and
nearest neighbor classiﬁcation. Expert systems with ap-
plications 37(10), 6885–6890 (2010)
53. Tsai, C.F., Hsiao, Y.C.: Combining multiple feature se-
lection methods for stock prediction: Union, intersection,
and multi-intersection approaches. Decision Support Sys-
tems 50(1), 258–269 (2010)
54. Wang, Y., Choi, I.C.: Market index and stock price di-
rection prediction using machine learning techniques: an
empirical study on the kospi and hsi. arXiv preprint
arXiv:1309.7119 (2013)
55. Wang, Z., Yan, W., Oates, T.: Time series classiﬁcation
from scratch with deep neural networks: A strong base-
line. In: Neural Networks (IJCNN), 2017 International
Joint Conference on, pp. 1578–1585. IEEE (2017)
56. Yang, J., Nguyen, M.N., San, P.P., Li, X., Krishnaswamy,
S.: Deep convolutional neural networks on multichan-
nel time series for human activity recognition. In: Ijcai,
vol. 15, pp. 3995–4001 (2015)
57. Zirilli, J.S.: Financial prediction using neural networks.
International Thomson Computer Press (1996)

trend prediction. Expert Systems with Applications
34(4), 2870–2878 (2008)
26. Kara, Y., Boyacioglu, M.A., Baykan, ¨O.K.: Predicting
direction of stock price index movement using artiﬁcial
neural networks and support vector machines: The sam-
ple of the istanbul stock exchange. Expert systems with
Applications 38(5), 5311–5319 (2011)
27. Kim, K.j.: Financial time series forecasting using sup-
port vector machines. Neurocomputing 55(1-2), 307–319
(2003)
28. Kim, K.j., Han, I.: Genetic algorithms approach to fea-
ture discretization in artiﬁcial neural networks for the
prediction of stock price index. Expert systems with Ap-
plications 19(2), 125–132 (2000)
29. Kim, Y.: Convolutional neural networks for sentence
classiﬁcation.
In: Proceedings of the 2014 Conference
on Empirical Methods in Natural Language Processing
(EMNLP), pp. 1746–1751 (2014)
30. Kim, Y., Ahn, W., Oh, K.J., Enke, D.: An intelligent
hybrid trading system for discovering trading rules for the
futures market using rough sets and genetic algorithms.
Applied Soft Computing 55, 127–140 (2017)
31. Krizhevsky, A., Sutskever, I., Hinton, G.E.: Imagenet
classiﬁcation with deep convolutional neural networks.
In: Advances in neural information processing systems,
pp. 1097–1105 (2012)
32. Lee, C.M., Ready, M.J.: Inferring trade direction from
intraday data. The Journal of Finance 46(2), 733–746
(1991)
33. Lee, M.C.: Using support vector machine with a hybrid
feature selection method to the stock trend prediction.
Expert Systems with Applications 36(8), 10896–10904
(2009)
34. Leung, M.T., Daouk, H., Chen, A.S.: Forecasting stock
indices: a comparison of classiﬁcation and level estima-
tion models. International Journal of Forecasting 16(2),
173–190 (2000)
35. Li, X., Xie, H., Wang, R., Cai, Y., Cao, J., Wang, F., Min,
H., Deng, X.: Empirical analysis: stock market prediction
via extreme learning machine. Neural Computing and
Applications 27(1), 67–78 (2016)
36. Lin, T., Guo, T., Aberer, K.: Hybrid neural networks for
learning the trend in time series. In: Proceedings of the
Twenty-Sixth International Joint Conference on Artiﬁcial
Intelligence, IJCAI-17, pp. 2273–2279 (2017)
37. Lin, Y., Guo, H., Hu, J.: An svm-based approach for stock
market trend prediction. In: Neural Networks (IJCNN),
The 2013 International Joint Conference on, pp. 1–7.
IEEE (2013)
38. Liu, C., Hou, W., Liu, D.: Foreign exchange rates fore-
casting with convolutional neural network. Neural Pro-
cessing Letters 46(3), 1095–1119 (2017)
39. Liu, G., Wang, X.: A numerical-based attention method
for stock market prediction with dual information. IEEE
Access 7, 7357–7367 (2019)
40. Malkiel, B.G., Fama, E.F.: Eﬃcient capital markets: A
review of theory and empirical work. The journal of Fi-
nance 25(2), 383–417 (1970)
41. Mozer, M.C.: A focused backpropagation algorithm for
temporal pattern recognition. Complex Systems 3, 349–
381 (1989)
42. O’Connor, N., Madden, M.G.: A neural network ap-
proach to predicting stock exchange movements using ex-
ternal factors. Knowledge-Based Systems 19(5), 371–378
(2006)
43. Papadimitriou, S., Yu, P.: Optimal multi-scale patterns
in time series streams. In: Proceedings of the 2006 ACM

