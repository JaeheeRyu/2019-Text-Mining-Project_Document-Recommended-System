All-Spin Bayesian Neural Networks

Kezhou Yang, Akul Malhotra, Sen Lu, and Abhronil Sengupta, Member, IEEE

1

9
1
0
2

v
o

N

1
2

]

T
E

.

s

c

[

2
v
8
2
8
5
0

.

1
1
9
1

:

v

i

X

r

a

Abstract—Probabilistic machine learning enabled by the
Bayesian formulation has recently gained signiﬁcant attention
in the domain of automated reasoning and decision-making.
While impressive strides have been made recently to scale up the
performance of deep Bayesian neural networks, they have been
primarily standalone software efforts without any regard to the
underlying hardware implementation. In this paper, we propose
an “All-Spin” Bayesian Neural Network where the underlying
spintronic hardware provides a better match to the Bayesian
computing models. To the best of our knowledge, this is the ﬁrst
exploration of a Bayesian neural hardware accelerator enabled
by emerging post-CMOS technologies. We develop an experimen-
tally calibrated device-circuit-algorithm co-simulation framework
and demonstrate 23.6× reduction in energy consumption against
an iso-network CMOS baseline implementation.
Index Terms—Neuromorphic Computing, Bayesian Neural
Networks, Magnetic Tunnel Junction.

I . IN TRODUC T ION

Probabilistic inference is at the core of decision-making in
the brain. While the past few years have witnessed unprece-
dented success of deep learning in a plethora of pattern recog-
nition tasks (complemented by advancements in dedicated
hardware designs for these workloads), these problem spaces
are usually characterized by the availability of large amounts
of data and networks that do not explicitly represent any
uncertainty in the network structure or parameters. However,
as we strive to deploy Artiﬁcial Intelligence platforms in
autonomous systems like self-driving cars, decision-making
based on uncertainty is crucial. Standard supervised backprop-
agation based learning techniques are unable to deal with such
issues since they do not overtly represent uncertainly in the
modelling process. To circumvent these problems, Bayesian
deep learning has recently been gaining attention where deep
neural networks are trained in a probabilistic framework fol-
lowing the classic rules of probability, i.e. Bayes’ theorem. In
the Bayesian formulation, the network is visualized as a set of
plausible models (assuming prior probability distributions on
its parameters, for instance, synaptic weights). Given observed
data,
the posterior probability distributions are learnt
that
best explains the observed data. The key distinction between
standard deep networks and Bayesian deep networks is the
fact that network parameters in the latter case are modelled
as probability distributions. It is worth noting here that the
probability distributions are usually modelled by Gaussian
processes characterized by a mean and variance [1]. Utilizing
probability distributions to model network parameters allows

Manuscript received November, 2019.
All authors contributed equally to this work. The authors are with the School
of Electrical Engineering and Computer Science, Department of Materials Sci-
ence and Engineering, The Pennsylvania State University, University Park, PA
16802, USA. A. Malhotra is also afﬁliated with Birla Institute of Technology
and Science, Pilani, Rajasthan 333031, India. E-mail: sengupta@psu.edu.

us to characterize the network outputs by an uncertainty
measure (variance of the distribution), instead of just point
estimates in a standard network. These uncertainty measures
can therefore be used by autonomous agents for decision
making and self-assessment in the presence of continuous
streaming data.
This paper explores a hardware-software co-design ap-
proach to accelerate Bayesian deep learning platforms through
the usage of spintronic technologies. Recent research has
demonstrated the possibility of mimicking the primitives of
standard deep learning frameworks – synapses and neurons
by single magnetic device structures that can be operated at
very low terminal voltages [2]–[5]. Further, being non-volatile
in nature, spintronic devices can be arranged in crossbar
architectures to realize “In-Memory” dot-product computing
kernels – thereby alleviating the memory access and memory
leakage bottlenecks prevalent in CMOS based implementa-
tions [6], [7]. As mentioned earlier, the key distinction between
Bayesian and standard deep learning is the requirement of
sampling from probability distributions and inference based
on sampled values. Interestingly, scaled nanomagnetic devices
operating at room temperature are characterized by thermal
noise resulting in probabilistic switching. We propose to
leverage the inherent device stochasticity of spintronic devices
to generate samples from Gaussian probability distributions
by drawing insights from statistical Central Limit Theorem.
Further, our paper also elaborates on a cohesive design of a
spintronic Bayesian processor that leverages beneﬁts of spin-
based Gaussian random number generators and spintronic “In-
Memory” crossbar architectures to realize high-performance,
energy efﬁcient hardware platforms. We believe the drastic re-
ductions in circuit complexity (single devices emulating synap-
tic scaling operations, crossbar architectures implementing
“In-Memory” dot-product computing kernels and leveraging
device stochasticity to sample from probability distributions)
and low operating voltages of spintronic devices make them a
promising path toward the realization of Probabilistic Machine
Learning enabled by the Bayesian formulation.

I I . PR EL IM INAR I E S : BAYE S IAN N EURA L N E TWORK S

Before going into the technical details of the work, we
would like to ﬁrst discuss the preliminaries of Bayesian Neural
Networks and the main computationally expensive operations
pertaining to their hardware implementation. As shown in Fig.
1, a particular layer of a neural network consists of a set
of neurons receiving inputs (sensory information or previous
layer of neurons) through synaptic weights, W. Bayesian
Neural Networks consider the weights of the network, W, to
be latent variables characterized by a probability distribution,
instead of point estimates. More speciﬁcally, each weight in

 
 
 
 
 
 
2

contribute signiﬁcantly to the total area and power consump-
tion of the hardware. Further, the random numbers need to
be sampled from a Gaussian distribution, thereby increasing
the complexity of the circuit. We will discuss the hardware
costs for CMOS implementations of such Gaussian random
number generators in the following sections along with their
limitations, followed by our proposal of nanomagnetic random
number generators that can serve as the basic building blocks
of such Bayesian Neural Networks.

• Dot-Product Operation Between Inputs and Sampled

Synaptic Weights: A common aspect of any standard deep
learning framework is the fact that forward propagation of
information through the network involves a signiﬁcant amount
of memory-intensive operations. The dot-product operation
between the synaptic weights and inputs for inference involves
the compute energy along with memory access and memory
leakage components. For large-scale problems and correspond-
ingly large-scale models, CMOS memory access and memory
leakage can be almost ∼ 50% of the total energy consumption
proﬁle [12].
The situation is further worsened in a Bayesian deep
network since each synaptic weight is characterized by two
parameters (mean and variance of the probability distribution),
thereby requiring double memory storage. However, the dot-
product operation does not occur directly between the inputs
and these parameters. In fact, for each inference operation the
synaptic weights (typically assumed constant during inference
for non-probabilistic networks and implemented by memory
elements in hardware) are repeatedly updated depending on
sampled values from the Gaussian probability distribution.
Hence, direct utilization of crossbar based “In-Memory” com-
puting platforms enabled by non-volatile memory technologies
(discussed in details later) for alleviating the memory access
and memory fetch bottlenecks is not possible and therefore
require a signiﬁcant rethinking.
In the following sections, we sequentially expand on each
of these points and propose a spin-based neural processor
that merges deterministic and stochastic devices as a potential
pathway to enable Bayesian deep learning that can be orders of
magnitude more efﬁcient in contrast to state-of-the-art CMOS
implementations.

I I I . S P IN TRON IC D EV IC E D E S IGN

A. Magnetic Tunnel Junction - True Random Number Gener-
ator Design
The basic device structure under consideration is the Mag-
netic Tunnel Junction (MTJ), which consists of two nanomag-
nets sandwiching a spacer layer (typically an oxide such as
MgO). The magnetization of one of the layers is magnetostati-
cally “pinned” in a particular direction while the magnetization
of the other layer can be manipulated by a spin current or
an external magnetic ﬁeld. The two layers are denoted as
the “Pinned” layer (PL) and “Free” layer (FL). Depending
on the relative orientation of the two magnets, the device
exhibits a high-resistance anti-parallel (AP) state (when the
magnetizations of the two layers have opposite direction) and
a low-resistance parallel (P) state (when the magnetizations of

Fig. 1.
In a Bayesian framework, each synaptic weight is represented by a
Gaussian probability distribution. The core computing kernel for a particular
layer during inference is a dot-product between the inputs and a synaptic
weight matrix sample drawn from the individual probability distributions.
Learning involves the determination of the mean and variances of the
probability distributions using Bayes’ formulation.

such a framework is a random number drawn from a posterior
probability distribution (characterized by a mean and variance)
that is conditioned on a prior probability distribution and the
the observed datapoints, D (incoming patterns to the network).
Hence, during inference, each incoming data pattern will get
propagated through the synaptic weights, each of which is
characterized by a probability distribution. Hence, as shown in
Fig. 1, the ﬁnal output of the neurons of a particular layer will
also be described by a probability distribution characterized by
a mean and variance (the uncertainty measure).
Bayesian Neural Networks correspond to the family of deep
learning networks where the weights are ‘learnt’ using Bayes’
rule. The learning process here involves the estimation of
the mean and variance of the weight posterior distribution.
Following Bayes’ rule, the posterior probability can be written
as,

(1)

P (W|D) =

P (D|W)P (W)
P (D)

where, P (W) denotes the prior probability (probability of the
latent variables before any data input to the network). P (D |W)
is the likelihood, corresponding to the feedforward pass of
the network. In order to make the above posterior probability
density estimation tractable, two popular approaches are –
Variational Inference methods [8] or Markov Chain Monte
Carlo methods [9]. However,
in this paper, we focus on
Variational Inference methods due to its scalability to large-
scale problems [10]. Variational Inference methods usually
approximates the posterior distribution by a Gaussian distribu-
tion, q(W, θ), characterized by parameters, θ = (µ, σ ), where
µ and σ represent the mean and standard deviation vectors
for the probability distributions representing P (W|D) [11].
To summarize, the main hardware design space concerns in
Bayesian Neural Networks can be categorized as follows:

• Gaussian Random Number Generation: Central to the

entire framework, both in the learning as well as the inference
process, is the random number generation corresponding to
the synaptic weights. Given current large model sizes char-
acterized by over a million synapses, coupled with the fact
that random draws need to performed multiple times for
each synaptic weight, random number generator circuits would

*        Determining themeanandvarianceofthe weight probability distributionsLearningInputWeightsOutputs3

high quality random numbers in presence of process, voltage
and temperature (PVT) variations. In the earlier scenario of
a standard MTJ, device-to-device variations can result
in
deviations of the bias current required for 50% switching
probability,
thereby degrading the quality of the random
number generation process. Our scheme is depicted in Fig.
2, where a magnet with Perpendicular Magnetic Anisotropy
(PMA) lies on top of the heavy-metal. The device operation
is divided into three stages. During an initial “Reset” stage,
a current ﬂowing through the heavy metal results in in-plane
spin injection in the magnet and orients it along the hard-
axis for a sufﬁcient magnitude of the “reset” current. The
magnet is then allowed to relax to either of the two stable
states in presence of thermal noise – the switching probability
being 50% since the hard-axis is a meta-stable orientation
point for the magnet. In this case, device-to-device variations
only causes change in the critical current required for biasing
the magnet close to the meta-stable orientation and does not
skew the probability distribution to a particular direction (as in
the standard MTJ case). Hence, by maintaining a worst-case
critical value of the heavy-metal “reset” current, quality of the
random number generation process can be preserved even in
the presence of PVT variations. Further, the “reset” current
does not ﬂow through the tunneling oxide layer (unlike the
standard MTJ case) and therefore reliability of the oxide layer
is not a concern in this scenario [20]. Note that our device
operation is validated by recent experiments of holding the
magnet to its meta-stable hard-axis orientation for performing
Bennett clocking in the context of nanomagnetic logic [21].
SHE based energy-efﬁcient switching also results in reduction
of the energy consumption involved in the random number
generation process.
The probabilistic switching characteristics of the MTJ can
be analyzed by Landau-Lifshitz-Gilbert (LLG) equation with
additional term to account for the spin-orbit torque generated
by spin-Hall effect at the ferromagnet-heavy metal interface
[22],
where, (cid:98)m is the unit vector of FL magnetization, γ =
is the gyromagnetic ratio for electron, α is Gilbert’s
damping ratio, Hef f is the effective magnetic ﬁeld including
the shape anisotropy ﬁeld for elliptic disks, Ns = Ms V
is the number of spins in free layer of volume V (Ms is
saturation magnetization and µB is Bohr magneton), and
Is = θSH (AM T J /AHM )Iq is the input spin current (AM T J
and AHM are the MTJ and HM cross-sectional area, θSH is
the spin-Hall angle and Iq is the charge current ﬂowing through
the HM underlayer). Thermal noise is included by an addi-
tional thermal ﬁeld [14], Hthermal =
where G0,1 is a Gaussian distribution with zero mean and
unit standard deviation, KB is Boltzmann constant, TK is the
temperature and δt is the simulation time-step.
Considering a worst-case “reset” current of 140µA for a
duration of 1ns, the energy consumption involved in using a
20kB T barrier magnet (calibrated to experimental measure-
ments reported in [23]) as a TRNG is ∼ 57f J /bit (I 2Rt

= −γ ( (cid:98)m × Hef f ) + α( (cid:98)m × d (cid:98)m
dt

( (cid:98)m × Is × (cid:98)m) (2)

(cid:113) α

d (cid:98)m

dt

1+α2

2KB TK
γµ0Ms V δt

G0,1 ,

2µB µ0

) +

1
qNs

µB

Fig. 2. The TRNG device structure is shown. Reset current (IQ ) ﬂowing
through the heavy-metal (HM) results in in-plane spin current (IS ) injection
for the MTJ “free layer” (FL). After switching to the in-plane meta-stable
position, the magnet relaxes to either of the two stable states with 50%
probability.

the two layers have the same direction). These two states are
stabilized by an energy barrier determined by the anisotropy
and volume of the magnet.
Let us now consider the switching of the magnet from one
state to another by the application of an external current.
The switching process is inherently stochastic at non-zero
temperatures due to thermal noise [14]. In the presence of
an external current, the probability of switching from one
state to the other is modulated depending on the magnitude
and duration of the current. True random number generator
(TRNG) can be designed using such a device by biasing the
magnet at the “write” current corresponding to a switching
probability of 50%. Note that CMOS based TRNGs suffer
from high energy consumption and circuit design complexity
[15]. Proposals and experimental demonstrations of MTJ based
TRNG have been shown in Refs. [16], [17]. MTJ based
TRNGs are characterized by low area footprint and compati-
bility with CMOS technology.
In this paper, we consider a spin-orbit coupling enabled
device structure (Fig. 2). It consists of the MTJ stack lying
on top of a heavy-metal (HM) underlayer. The device “read”
is performed through the MTJ stack between terminals T1
and T3. However, the device “write” is performed by passing
current through the heavy-metal underlayer between terminals
T2 and T3. Input current ﬂowing through the heavy-metal
results in spin-injection at the interface of the magnet and
heavy-metal due to spin-Hall effect (SHE) [18] and thereby
causes switching of the MTJ “free layer” [19]. The device has
the following advantages:
• The decoupled “write” and “read” current paths is ad-
vantageous from the perspective of peripheral circuit design
to avoid “read”-“write” conﬂicts since the associated circuits
can be optimized independently.
• Such devices offer 1-2 orders of magnitude energy ef-
ﬁciency in comparison to other memristive technologies as
well as standard spin-transfer torque MRAMs. This is due
to the fact that in such spin-orbit coupling based systems,
every incoming electron in the “write” current path repeatedly
scatters at the interface of the magnet and heavy metal and
transfers multiple units of spin angular momentum to the
ferromagnet lying on top.
Usage of SHE based switching enables us to use an alter-
native TRNG design [20] that has the potential to produce

FLPLHM eeeeISIQ1T2T3T“Reset” Current“Read” CurrentTunneling oxide (MgO)InitialIntermediateFinaltimeResetRead4

Fig. 4. Device characteristics are shown for a 20× 0.6nm3 magnet calibrated
to experimental measurements [24]. The device characteristics illustrate that
the programming current magnitude is directly proportional to the amount of
conductance change [6].

domain wall position determines the magnitude of the MTJ
conductance. The MTJ conductance varies linearly with the
domain wall position since it determines the relative proportion
of the area of the Parallel and Anti-Parallel domains of the
MTJ (Fig. 4(b)). Since such a device can be programmed
to multi-level resistive states and are characterized by low
switching current requirements and linear device behavior
(device conductance change varies in proportion to magnitude
of programming current), they are an ideal ﬁt for implementing
crossbar based “In-Memory” computing platforms (discussed
in next section). We will refer to this device as a DW-MTJ for
the remainder of this text. Experimentally, a multi-level DW
motion based resistive device was recently shown to exhibit
15-20 intermediate resistive states [27].
It is worth noting here that the device structure in Fig.
3(a) can be used as a neuron by interfacing with a Reference
MTJ (Fig. 3(b)) [6]. The resistive divider can drive a CMOS
transistor where the output drive current would be a linear
function of the input current ﬂowing through the heavy metal
layer of the device, thereby mimicking the functionality of a
Saturated Linear Functionality by ensuring that the transistor
operates in the saturation regime [6]. The simulation parame-
ters, provided in Table II, were used for the rest of this text
for DW-MTJ unless otherwise stated. The parameters were
obtained magnetometric measurements of CoFe-Pt nanostrips
[25], [26].

TABLE II. DW-MTJ Device Simulation Parameters

Parameters

Ferromagnet Thickness
Grid Size
Heavy Metal Thickness
Domain Wall Width
Saturation Magnetization, Ms
Spin-Hall Angle, θ
Gilbert Damping Factor, α
Exchange Correlation Constant, A
Perpendicular Magnetic Anisotropy, Ku2
Effective DMI constant, D

Value

0.6nm
4 × 1 × 0.6nm3
3nm
7.6nm
700 KA/m

0.07
0.3

1 × 10−11 J/m
4.8 × 105 J/m3
−1.2 × 10−3 J/m2

IV. A LL -S P IN BAYE S IAN N EURA L N E TWORK S

A. Spin-Based Gaussian Random Number Generator
Gaussian random number generation task is a hardware-
expensive process. CMOS based designs for Gaussian random
number generators would usually require large number of
registers, linear feedback circuits, etc. For instance, a recent
work for a CMOS based Gaussian RNG implementation

Fig. 3.
(a) DW-MTJ: Magnitude of current ﬂowing through the HM, J ,
causes a proportionate displacement, ∆x, in the DW position, which causes
a change, ∆G, in the device conductance between terminals T1 and T3. (b)
The same device can be used as a neuron by interfacing with a Reference
MTJ. The current provided by the output transistor, Iout is a saturated linear
function of the input current, Iin .

TABLE I. MTJ Device Simulation Parameters

Parameters

Free layer area
Free layer thickness
Heavy-metal thickness, tHM
Saturation magnetization, MS
Spin-Hall angle, θSH
Gilbert damping factor, α
Energy barrier, EB
Temperature, TK

Value

π

4 × 100 × 40nm2
1.2nm
2nm

1000 KA/m [23]
0.3 [23]
0.0122 [23]

20 KB T
300K

energy consumption) [20], which is almost 2× lower than
standard MTJ based TRNG.

B. Domain Wall Motion Based Magnetic Devices - Multi-
Level Non-Volatile Memory Design
The mono-domain magnet discussed above is characterized
by only two stable states. For a magnet with elongated shape,
multiple domains can be stabilized in the FL, thereby leading
to the realization of multiple stable resistive states. Such a
domain-wall (DW) MTJ consists of a domain wall separating
the two oppositely magnetized regions and the domain wall
position is programmed to modulate the MTJ resistance (due
to variation in the relative proportion of P and AP domains in
the device) [6].
We consider SHE based domain wall motion dynamics also
in magnet-heavy metal bilayers. In magnetic heterostructures
with high perpendicular magnetocrystalline anisotropy, spin-
orbit coupling and broken inversion symmetry stabilizes chi-
ral domain walls through Dzyaloshinskii-Moriya interaction
(DMI) [25], [26]. Such an interfacial DMI at the magnet-
heavy metal interface results in the formation of a N ´eel domain
wall. When an in-plane charge current is injected through
the heavy metal, the accumulated spins at the magnet-heavy
metal interface results in N ´eel domain-wall motion. The device
structure is shown in Fig. 3(a), where a current of magnitude,
J , ﬂowing through the HM layer results in a conductance
change, ∆G, between terminals T1 and T3. As shown in
Fig. 4(a), for a given programming time duration, the current
ﬂowing through the HM underlayer, causes DW displacement
proportional
to its magnitude. Note that
the device char-
acteristics are obtained by performing micromagnetic LLG
simulations by dividing the magnet into multiple grids. The

-40-200204022.533.544.5Domain Wall Position (nm)Pinned LayersPLTunneling OxideHM FL∆GJ∆xT2T3T10200400600010203040Domain Wall Displacement (nm)4 ns3 ns2 ns1 ns(a)(b)(c)Current through HM (µA)Device Conductance (x10e-7 mho)HM FLPLNeuron MTJFLPLVDDIoutIinT1T2T3ReferenceMTJ(b)-40-200204022.533.544.5Domain Wall Position (nm)Pinned LayersPLTunneling OxideHM FL∆GJ∆xT2T3T10200400600010203040Domain Wall Displacement (nm)4 ns3 ns2 ns1 ns(a)(a)(b)Current through HM (µA)Device Conductance (x10e-7 mho)HM FLPLNeuron MTJFLPLVDDIoutIinT1T2T3ReferenceMTJ5

(a) Outline of a 2 × 2 array utilizing spin based devices interfaced with an accumulator to implement a Gaussian RNG. The probability distributions
Fig. 5.
of random numbers generated from such an array are shown in the extreme right by using a sum of N random variables (rows of the array). We use 8-bit
representation and 100,000 samples to plot the distribution.

reports 1780 registers and 528.69mW power consumption for
a 64-parallel Gaussian random number generator task [28].
Let us now discuss our proposal of spin-based Gaussian ran-
dom number generator. In the previous section, we discussed
the design of a spintronic TRNG. An array of TRNGs can
be used for sampling from a uniform probability distribution.
In order to generate a Gaussian probability distribution from a
uniform one, we draw inspiration from statistical Central Limit
Theorem, discussed in Box 1. The key result of Central Limit
Theorem that we utilize is that the sum of a large number of
independent and identically distributed (i.i.d) random variables
is approximately Normal.

Box 1: Central Limit Theorem

Fig. 6.
“In-Memory” computing primitive where an array of spin synapses
implement the dot-product kernel.

while Box 1 discussions are equally valid for a CMOS based
TRNG, it will be an order of magnitude more area and power
consuming than our proposed spin-based TRNG (as explained
in Section III).

B. Dot-Product Operation Between Inputs and Sampled
Synaptic Weights
Let us ﬁrst discuss the operation of DW-MTJ enabled
spintronic crossbar arrays as an energy-efﬁcient mechanism
to realize the dot-product computing kernel. Assuming each
synapse to be represented by a DW-MTJ, as shown in Fig.
6, they can be arranged in a crossbar structure. Each row of
the array is driven by an analog voltage (output of Digital-to-
Analog converters – DACs) that corresponds to the magnitude
of the input. The current ﬂowing through each synapse is
scaled by the conductance of the device and due to Kirchoff ’s
law, all
these currents get summed up along the column,
thereby realizing the dot-product kernel. Note that negative
synaptic weights can be also mapped by using two horizontal
lines per input (driven by ‘positive’ and ‘negative’ supply
voltages). In case a particular synaptic weight is ‘positive’
(‘negative’), then the corresponding conductance in the ‘pos-
itive’ (‘negative’) line is set in accordance to the weight. The
resultant currents get summed up along the column and pass as
the input “write” current through the spin-neuron. Consecutive
“write” and “read” cycles of the spin-neurons will implement
multiple iterations of the Bayesian network. The analog output
current provided by the spin-neuron is then converted to a

n

σ2

Let {X1 , X2 , ..., Xn} be a random sample of n i.i.d random
variables drawn from a distribution (which may not be Nor-
mal) of mean µ and variance σ2 . Then, the probability den-
sity function of the sample average, Sn = X1+X2+...+Xn
approaches a Normal distribution with mean µ and variance
n as n increases.
Our proposed design is illustrated in Fig. 5 which depicts a
possible array implementation [20] of our spin-based TRNGs.
Each spin device is interfaced with an access transistor. Rows
sharing a Reset-Line can be driven simultaneously. Hence,
random numbers can be generated in the entire array in
parallel. The timing diagram is shown in Fig. 5. Each row can
be read by asserting a particular word-line (WL) and sensing
the bit-line voltage (BL). For an m × n array, each row-read
produces an n-bit number generated from a uniform probabil-
ity distribution. By interfacing the array with an accumulator,
that averages all the generated random numbers, we are able to
produce random numbers drawn from a Normal distribution.
Note that
the hardware overhead for this process would
be high for applications that require precise sampling from
Gaussian distributions, since the convergence takes place only
for inﬁnite samples. However, for machine learning workloads
considered herein, performance of such platforms are usually
resilient to approximations in the underlying computations. For
instance, Fig. 5 shows that even with 8-bit representation and 3
random variables drawn from uniform probability distribution,
we are able to achieve an approximate Gaussian distribution.
While Gaussian probability distributions are primarily used in
such algorithms, non-Gaussian weight distributions can be also
designed by using the Gaussian function as a basis. Note that,

BL[1]BL[2]GNDWL[1]WL[2]Peripherals & AccumulatorResetResetRelaxReadReset LineWLBLtimeHistogramRandom VariablePLFLPLFLPLFLPLFLN = 3N = 8N = 2Spin Neurons+Analog-to-Digital ConverterV1+. . .. . .. . .. . .. . .. . .. . .Layer InputsSpintronic Synapse Crossbar Array (   )Digital-to-Analog ConverterV1-Vm+Vm-Layer Outputsdigital format using the Analog-to-Digital Converters (ADCs).
The digital outputs can be latched to provide inputs for the fan-
out crossbar arrays. The energy-efﬁciency of the system stems
mainly from two factors:
• The input write resistance of the spintronic neurons
are low (being magneto-metallic devices) and they inherently
require very low currents for switching. This enables the cross-
bar arrays of spintronic synapses to be operated at low terminal
voltages (typically 100mV ). Further, spintronic neurons are
inherently current-driven and thereby do not require costly
current to voltage converters unlike CMOS and other emerging
technology (Resistive Random Access Memory, Phase Change
Memory, among others) based implementations.
• Since, spin devices are inherently non-volatile technolo-
gies, the ability to perform the costly Multiply-Accumulate
operations in the memory array itself enables us to address
the issues of von-Neumann bottleneck.
However, in the context of Bayesian deep networks, even
for the inference stage, the synaptic weights are not constant
but are updated depending on sampled values from a Gaussian
distribution. Assuming we are able to generate samples from
a Normal distribution by using the device-circuit primitives
proposed earlier, the computations in a Bayesian network can
be partitioned in an appropriate fashion such that the beneﬁts
of spin-based “In-Memory” computing can be still utilized.
This is explained in Box 2.
Realizing that a Normal distribution with a particular mean
and variance is equivalent to a scaled and shifted version of
a Normal distribution with zero mean and unit variance, we
partition the inference equation as shown in (3). The constant
parameters µjk and σjk (highlighted in red) represent the
mean and variance of the probability distribution of the cor-
responding synaptic weight and can be therefore implemented
by DW-MTJ based memory devices from a hardware imple-
mentation perspective. The resultant system consists of two
crossbar arrays for storing the mean and variance parameters
respectively. While the inputs of a particular layer are directly
applied to the crossbar array storing the mean values, they
are scaled by the random numbers generated from the RNG
unit (outputs normalized to provide random numbers with zero
mean and unit variance) described previously for the crossbar
array storing the variance values. Note that the crossbar array
storing the mean values need to be evaluated only once during
the inference process for a particular input while repeated eval-
uations are performed on the variance crossbar array for each
set of sampled random numbers. Typical CMOS neuromorphic
architectures are characterized by much higher movement of
weight data than input data to compute the inference operation
[29]. Our proposal of computation partition, explained in
Box 2, enables us to leverage the “In-Memory” computing
primitives for storing the probability distribution parameters
while parallely computing energy-efﬁcient dot-products in-situ
between inputs and stochastic weights. It is worth noting here
that the crossbar column outputs are read sequentially in order
to ensure that the random numbers sampled for the synaptic
weights of each column are independent. Such a sequential
column read is currently a common practice in crossbar based
deep learning architectures [30], [31].

6

S(cid:88)

i=1

Box 2: Computations Involved in Inference Operation

Once all
the posterior distributions are learnt (µ and σ
parameters of the weight distributions), the network output
corresponding to input, x, should be obtained by averaging
the outputs obtained by sampling from the posterior distri-
bution of the weights, W [10]. The output of the network,
y , is therefore given by,

y = EP (W|D) [f (x,W)] ≈ Eq(W,θ) [f (x,W)] ≈ 1
S

f (x,Wi )

(3)
where, f (x,W) is the network mapping for input x and
weights, W. Using the Variational Inference method, we
approximate the weight distribution by Gaussian func-
tions. The approximation is performed over S independent
Monte-Carlo samples drawn from the Gaussian distribution,

q(W, θ).

Considering just a single layer and neglecting the neural
transfer function, f (x,Wi ) for the j -th neuron can be de-
composed into,

(cid:88)
(cid:88)
(cid:88)

k

k

f (x,Wi

j ) =

=

=

xk .N (µjk , σjk )

xk .(µjk + σjk .N (0, 1))

(4)

xk .µjk +

xk .N (0, 1).σjk

(cid:88)

k

k

where, k is the dimensionality of the input x and N (µjk , σjk )
represent a particular sample drawn from a Normal proba-
bility distribution with mean, µjk , and variance, σjk .

V. R E SU LT S AND D I SCU S S ION

A hybrid device-circuit-algorithm co-simulation framework
was developed to evaluate the performance of the proposed
All-Spin Bayesian hardware. The magnetization switching
characteristics of the mono-domain and multi-domain MTJ
was simulated in MuMax3, a GPU accelerated micro-magnetic
simulation framework [32]. Non-Equilibrium Green’s Function
(NEGF) based transport simulation framework [33] was used
for modelling the MTJ resistance variation with oxide thick-
ness and applied voltage. The obtained device characteristics
from MuMax3 and SPICE simulation tools was used in al-
gorithm level simulator, PyTorch, to evaluate the functionality
of the circuit. The performance of this design was tested for
a standard digit recognition problem on the MNIST dataset
[34] (60,000 training samples and 10,000 test samples of
handwritten digits (0-9)). A two layer fully connected neural
network was used, with each hidden layer having 200 neurons
(784 × 200 × 200 × 10). The probability distributions were
learnt using the ‘Bayes by Backprop’ algorithm [35]1 , which
learns the optimal Gaussian distribution by minimizing the KL
divergence from the true probability distribution. The prior dis-
tribution on the weights used for training was a scaled mixture
of two gaussian functions. The network was trained ofﬂine

1 The
related
code
bayes- by- backprop.

can

be

found

at

https://github.com/nitarshan/

to obtain the values of the mean and standard deviation of
the probability distributions of the weights. Subsequently they
were mapped to the conductances of the DW-MTJ devices.
The baseline idealized software network was trained with an
accuracy of 98.63% over the training set and 97.51% over the
testing set (averaged over 10 sampled networks).
The device parameters used in this work have been tabulated
in the previous section. 20kB T barrier height magnet was used
in the Gussian RNG unit. We considered 4-bit representation
in the DW-MTJ weights and 3-bit discretization in the neuron
output. Note that, as explained in the previous section, our
neuronal devices mimic a saturating linear functionality and
our network was trained with such a transfer function itself.
Considering a minimum sensing and programming displace-
ment of 20nm for the DW location, we consider our cross-
point and neuronal devices to be 320nm and 160nm in
length. From our micromagnetic simulations, we observe the
critical current required to switch the neuronal device from
one edge to the other is 40µA for a time duration of 1ns.
Assuming a crossbar supply voltage of 100mV , the synaptic
weight corresponding to unity value is mapped to 2.5KΩ.
Note that the lower supply voltage is enabled by the low
current requirement of the magneto-metallic spin neurons.
In addition to reducing the system-level power consumption,
lower supply voltage minimizes the variation of MTJ AP
resistance with applied voltage during the read operation. We
consider 300% TMR in the DW-MTJ weights of the crossbar
array. Considering such device-level behavioral characteristics,
non-idealities and constraints, the test accuracy of the network
was 97.02% (averaged over 10 samples).
In order to estimate the system-level energy consumption,
we consider the core RNG and crossbar energy consumption
along with peripheral circuitry like ADC and DAC2 . We
evaluate the energy consumption for a single image inference
and a particular network sample. The crossbar read latency
was assumed to be 10ns (for each column read) while the
ADC throughput was 1ns. For the RNG, DAC and ADC
units, we considered 8-bit precision and 3 variables were
used for the accumulation process in the Normal distribution
sampling. We would like to mention here that we assumed 8-
bit precision for the energy calculations in order to achieve
a fair comparison with numbers reported in Ref. [10] for
an iso-network CMOS architecture. However, from functional
viewpoint, lower bit-precision ∼ 4 bits was observed to be
sufﬁcient. The total energy consumption of our proposed “All-
Spin” was evaluated to be 804.3nJ per classiﬁcation, which
is 23.6× energy efﬁcient in contrast to the baseline CMOS
implementation [10].
Note that while limited bit-precision and device resistance
ratio in ON and OFF states are concerns that might limit algo-
rithm recognition accuracy, circuit level solutions like mapping
the computation across multiple devices have been explored
[36]. Further, resistive crossbars are usually characterized by
limited fan-in – much smaller than neuron fan-in in typical
deep networks due to non-idealities, parasitics and sneak-paths

2 The energy consumption for the peripheral circuitry were included from
typical numbers considered in literature [30], [31] and can be found at https://
github.com/Aayush-Ankit/puma- simulator/blob/training/include/constants.py.

7

[37]. Hence, mapping a practically sized network requires
mapping synapses of a neuron across multiple crossbars [30],
[31]. Such architectural level innovations can be easily inte-
grated with our current proposal.

V I . SUMMARY

In summary, we proposed the vision of an “All-Spin”
Bayesian neural processor that has the potential of enabling
orders of magnitude hardware efﬁciency (area, power, energy
consumption) in contrast to state-of-the-art CMOS implemen-
tations. Our core proposal can be easily extended to incorpo-
rate innovations in the material stack through the exploration
of novel device physics (like Voltage-Controlled Magnetic
Anisotropy (VCMA) effect [38], the Magneto-electric effect
[39]) and spin textures (like skyrmions [40]).
Computing frameworks, so far, have mainly segregated
deterministic and stochastic computations. Standard determin-
istic deep learning frameworks enabled by spintronic devices
and other post-CMOS technologies have been explored. In
such scenarios, device-level non-idealities are usually treated
as a disadvantage. More recently, stochasticity inherent in
such devices have been exploited for computing to implement
stochastic versions of their deterministic counterparts [41],
[42] (driven by the motivation that devices can be scaled down
to single bit instead of multi-bit representations due to proba-
bilistic domain encoding of information). Device stochastic-
ity has been also used in other unconventional computing
platforms like Ising computing, combinatorial optimization
problems, among others [43]. Note that prior work on using
magnetic devices for Bayesian Inference engines have been
proposed [44], [45] which are mainly used for implementing
Bayes’ rule for simple prediction tasks in directed acyclic
graphs and do not have relevance or overlap with Bayesian
deep networks. Bayesian deep learning is a unique computing
framework that necessitates the merger of both deterministic
(dot-product evaluations of sampled weights and inputs) and
stochastic computations (sampling weights from probability
distributions) - thereby requiring a signiﬁcant rethinking of
the design space across the stack from devices to circuits and
algorithms.

R E FER ENC E S
[1] Y. Gal, “Uncertainty in deep learning,” Ph.D. dissertation, PhD thesis,
University of Cambridge, 2016.
[2] A. Sengupta and K. Roy, “Encoding neural and synaptic functionalities
in electron spin: A pathway to efﬁcient neuromorphic computing,”
Applied Physics Reviews, vol. 4, no. 4, p. 041105, 2017.
[3] A. Sengupta, G. Srinivasan, D. Roy, and K. Roy, “Stochastic inference
and learning enabled by magnetic tunnel junctions,” in 2018 IEEE
International Electron Devices Meeting (IEDM).
IEEE, 2018, pp. 1–4.
[4] J. Grollier, D. Querlioz, and M. D. Stiles, “Spintronic nanodevices for
bioinspired computing,” Proceedings of the IEEE, vol. 104, no. 10, pp.
2024–2039, 2016.
[5] M. Romera, P. Talatchian, S. Tsunegi, F. A. Araujo, V. Cros, P. Bor-
tolotti, J. Trastoy, K. Yakushiji, A. Fukushima, H. Kubota, S. Yuasa,
M. Ernoult, D. Vodenicarevic, T. Hirtzlin, N. Locatelli, D. Q. Querlioz,
and J. Grollier, “Vowel recognition with four coupled spin-torque nano-
oscillators,” Nature, vol. 563, no. 7730, p. 230, 2018.
[6] A. Sengupta, Y. Shim, and K. Roy, “Proposal for an all-spin artiﬁcial
neural network: Emulating neural and synaptic functionalities through
domain wall motion in ferromagnets,” IEEE transactions on biomedical
circuits and systems, vol. 10, no. 6, pp. 1152–1160, 2016.

8

back gaussian random number generators,” in 2017 IEEE International
Conference on Computer Design (ICCD).
IEEE, 2017, pp. 289–296.
[29] Y. Chen, T. Luo, S. Liu, S. Zhang, L. He, J. Wang, L. Li, T. Chen,
Z. Xu, N. Sun, and O. Temam, “DaDianNao: A machine-learning super-
computer,” in Proceedings of the 47th Annual IEEE/ACM International
Symposium on Microarchitecture.
IEEE Computer Society, 2014, pp.
609–622.
[30] A. Ankit, I. E. Hajj, S. R. Chalamalasetti, G. Ndu, M. Foltin, R. S.
Williams, P. Faraboschi, W.-m. W. Hwu, J. P. Strachan, K. Roy et al.,
“PUMA: A programmable ultra-efﬁcient memristor-based accelerator
for machine learning inference,” in Proceedings of the Twenty-Fourth
International Conference on Architectural Support for Programming
Languages and Operating Systems. ACM, 2019, pp. 715–731.
[31] A. Shaﬁee, A. Nag, N. Muralimanohar, R. Balasubramonian, J. P. Stra-
chan, M. Hu, R. S. Williams, and V. Srikumar, “ISAAC: A convolutional
neural network accelerator with in-situ analog arithmetic in crossbars,”
ACM SIGARCH Computer Architecture News, vol. 44, no. 3, pp. 14–26,
2016.
[32] A. Vansteenkiste, J. Leliaert, M. Dvornik, M. Helsen, F. Garcia-Sanchez,
and B. Van Waeyenberge, “The design and veriﬁcation of mumax3,” AIP
advances, vol. 4, no. 10, p. 107133, 2014.
[33] X. Fong, S. K. Gupta, N. N. Mojumder, S. H. Choday, C. Augustine,
and K. Roy, “KNACK: A hybrid spin-charge mixed-mode simulator for
evaluating different genres of spin-transfer torque MRAM bit-cells,” in
Simulation of Semiconductor Processes and Devices (SISPAD), 2011
International Conference on.
IEEE, 2011, pp. 51–54.
[34] Y. LeCun, L. Bottou, Y. Bengio, P. Haffner et al., “Gradient-based
learning applied to document recognition,” Proceedings of the IEEE,
vol. 86, no. 11, pp. 2278–2324, 1998.
[35] C. Blundell, J. Cornebise, K. Kavukcuoglu, and D. Wierstra, “Weight
uncertainty in neural networks,” arXiv preprint arXiv:1505.05424, 2015.
[36] I. Boybat, M. Le Gallo, S. Nandakumar, T. Moraitis, T. Parnell,
T. Tuma, B. Rajendran, Y. Leblebici, A. Sebastian, and E. Eleftheriou,
“Neuromorphic computing with multi-memristive synapses,” Nature
communications, vol. 9, no. 1, p. 2514, 2018.
[37] J. Liang and H.-S. P. Wong, “Cross-point memory array without cell
selectors – Device characteristics and data storage pattern dependencies,”
IEEE Transactions on Electron Devices, vol. 57, no. 10, pp. 2531–2538,
2010.
[38] P. K. Amiri and K. L. Wang, “Voltage-controlled magnetic anisotropy
in spintronic devices,” in Spin, vol. 2, no. 03. World Scientiﬁc, 2012,
p. 1240002.
[39] K. J. Franke, B. Van de Wiele, Y. Shirahata, S. J. H ¨am ¨al ¨ainen,
T. Taniyama, and S. van Dijken, “Reversible electric-ﬁeld-driven mag-
netic domain-wall motion,” Physical Review X, vol. 5, no. 1, p. 011010,
2015.
[40] S. Woo, K. Litzius, B. Kr ¨uger, M.-Y. Im, L. Caretta, K. Richter,
M. Mann, A. Krone, R. M. Reeve, M. Weigand, P. Agrawal, I. Lemesh,
M.-A. Mawass, P. Fischer, M. Klui, and G. S. D. Beach, “Observation of
room-temperature magnetic skyrmions and their current-driven dynamics
in ultrathin metallic ferromagnets,” Nature materials, 2016.
[41] A. Sengupta, M. Parsa, B. Han, and K. Roy, “Probabilistic deep spiking
neural systems enabled by magnetic tunnel junction,” IEEE Transactions
on Electron Devices, vol. 63, no. 7, pp. 2963–2970, 2016.
[42] G. Srinivasan, A. Sengupta, and K. Roy, “Magnetic tunnel junction based
long-term short-term stochastic synapse for a spiking neural network
with on-chip STDP learning,” Scientiﬁc reports, vol. 6, p. 29545, 2016.
[43] K. Roy, A. Sengupta, and Y. Shim, “Perspective: Stochastic magnetic
devices for cognitive computing,” Journal of Applied Physics, vol. 123,
no. 21, p. 210901, 2018.
[44] R. Faria, K. Y. Camsari, and S. Datta, “Implementing bayesian networks
with embedded stochastic MRAM,” AIP Advances, vol. 8, no. 4, p.
045101, 2018.
[45] Y. Shim, S. Chen, A. Sengupta, and K. Roy, “Stochastic spin-orbit torque
devices as elements for bayesian inference,” Scientiﬁc reports, vol. 7,
no. 1, p. 14101, 2017.

[7] A. Sengupta, A. Ankit, and K. Roy, “Performance analysis and bench-
marking of all-spin spiking neural networks (special session paper),”
in 2017 International Joint Conference on Neural Networks (IJCNN).
IEEE, 2017, pp. 4557–4563.
[8] R. Houthooft, X. Chen, Y. Duan, J. Schulman, F. De Turck, and
P. Abbeel, “VIME: Variational information maximizing exploration,” in
Advances in Neural Information Processing Systems, 2016, pp. 1109–
1117.
[9] C. Andrieu, N. De Freitas, A. Doucet, and M. I. Jordan, “An introduction
to MCMC for machine learning,” Machine learning, vol. 50, no. 1-2,
pp. 5–43, 2003.
[10] R. Cai, A. Ren, N. Liu, C. Ding, L. Wang, X. Qian, M. Pedram,
and Y. Wang, “VIBNN: Hardware acceleration of bayesian neural
networks,” in Proceedings of the Twenty-Third International Conference
on Architectural Support for Programming Languages and Operating
Systems. ACM, 2018, pp. 476–488.
[11] Z. Ghahramani and M. J. Beal, “Propagation algorithms for variational
Bayesian learning,” in Advances in neural information processing sys-
tems, 2001, pp. 507–513.
[12] A. Ankit, A. Sengupta, P. Panda, and K. Roy, “RESPARC: A recon-
ﬁgurable and energy-efﬁcient architecture with memristive crossbars for
deep spiking neural networks,” in Proceedings of the 54th Annual Design
Automation Conference 2017. ACM, 2017, p. 27.
[13] J. Slonczewski, “Currents, torques, and polarization factors in magnetic
tunnel junctions,” Physical Review B, vol. 71, no. 2, p. 024411, 2005.
[14] W. Scholz, T. Schreﬂ, and J. Fidler, “Micromagnetic simulation of
thermally activated switching in ﬁne particles,” Journal of Magnetism
and Magnetic Materials, vol. 233, no. 3, pp. 296–304, 2001.
[15] K. Yang, D. Fick, M. B. Henry, Y. Lee, D. Blaauw, and D. Sylvester,
“16.3 A 23Mb/s 23pJ/b fully synthesized true-random-number generator
in 28nm and 65nm CMOS,” in 2014 IEEE International Solid-State
Circuits Conference Digest of Technical Papers (ISSCC).
IEEE, 2014,
pp. 280–281.
[16] D. Vodenicarevic, N. Locatelli, A. Mizrahi, J. S. Friedman, A. F. Vincent,
M. Romera, A. Fukushima, K. Yakushiji, H. Kubota, S. Yuasa, S. Ti-
wari, J. Grollier, and D. Querlioz, “Low-energy truly random number
generation with superparamagnetic tunnel junctions for unconventional
computing,” Physical Review Applied, vol. 8, no. 5, p. 054045, 2017.
[17] A. Fukushima, T. Seki, K. Yakushiji, H. Kubota, H. Imamura, S. Yuasa,
and K. Ando, “Spin dice: A scalable truly random number generator
based on spintronics,” Applied Physics Express, vol. 7, no. 8, p. 083001,
2014.
[18] J. Hirsch, “Spin Hall effect,” Physical Review Letters, vol. 83, no. 9, p.
1834, 1999.
[19] L. Liu, C.-F. Pai, Y. Li, H. Tseng, D. Ralph, and R. Buhrman, “Spin-
torque switching with the giant spin Hall effect of tantalum,” Science,
vol. 336, no. 6081, pp. 555–558, 2012.
[20] Y. Kim, X. Fong, and K. Roy, “Spin-orbit-torque-based spin-dice: A
true random-number generator,” IEEE Magnetics Letters, vol. 6, pp. 1–
4, 2015.
[21] D. Bhowmik, L. You, and S. Salahuddin, “Spin Hall effect clocking of
nanomagnetic logic without a magnetic ﬁeld,” Nature nanotechnology,
vol. 9, no. 1, p. 59, 2014.
[22] J. C. Slonczewski, “Conductance and exchange coupling of two ferro-
magnets separated by a tunneling barrier,” Physical Review B, vol. 39,
no. 10, p. 6995, 1989.
[23] C.-F. Pai, L. Liu, Y. Li, H. Tseng, D. Ralph, and R. Buhrman, “Spin
transfer torque devices utilizing the giant spin Hall effect of tungsten,”
Applied Physics Letters, vol. 101, no. 12, p. 122404, 2012.
[24] S. Emori, E. Martinez, K.-J. Lee, H.-W. Lee, U. Bauer, S.-M. Ahn,
P. Agrawal, D. C. Bono, and G. S. Beach, “Spin Hall torque magne-
tometry of Dzyaloshinskii domain walls,” Physical Review B, vol. 90,
no. 18, p. 184427, 2014.
[25] S. Emori, U. Bauer, S.-M. Ahn, E. Martinez, and G. S. Beach, “Current-
driven dynamics of chiral ferromagnetic domain walls,” Nature materi-
als, vol. 12, no. 7, pp. 611–616, 2013.
[26] E. Martinez, S. Emori, N. Perez, L. Torres, and G. S. Beach, “Current-
driven dynamics of Dzyaloshinskii domain walls in the presence of in-
plane ﬁelds: Full micromagnetic and one-dimensional analysis,” Journal
of Applied Physics, vol. 115, no. 21, p. 213909, 2014.
[27] S. Lequeux, J. Sampaio, V. Cros, K. Yakushiji, A. Fukushima, R. Mat-
sumoto, H. Kubota, S. Yuasa, and J. Grollier, “A magnetic synapse:
Multilevel spin-torque memristor with perpendicular anisotropy,” Scien-
tiﬁc Reports, vol. 6, 2016.
[28] R. Cai, A. Ren, L. Wangy, M. Pedramy, and Y. Wang, “Hardware
acceleration of bayesian neural networks using RAM based linear feed-

