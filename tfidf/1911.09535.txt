9
1
0
2

v
o

N

1
2

]

A

M

.

s

c

[

1
v
5
3
5
9
0

.

1
1
9
1

:

v

i

X

r

a

AG EN T PROB ING IN TERAC T ION PO L IC I E S

A PR E PR IN T

Siddharth Ghiya1 , Oluwafemi Azeez1 , Brendan Miller1

1 Carnegie Mellong University
Pittsburgh, PA, USA

November 22, 2019

AB STRACT

Reinforcement learning in a multi agent system is difﬁcult because these systems are inherently
non-stationary in nature. In such a case, identifying the type of the opposite agent is crucial and
can help us address this non-stationary environment. We have investigated if we can employ some
probing policies which help us better identify the type of the other agent in the environment. We’ve
made a simplifying assumption that the other agent has a stationary policy that our probing policy is
trying to approximate. Our work extends Environmental Probing Interaction Policy framework to
handle multi agent environments.

1

Introduction

Real world environments often have multiple agents. In such a case, considering the actions of the other actors in the
environment becomes crucial. In many of the scenarios, the identity of other agents may not be known. Therefore,
identifying the type of other agent is useful so that we can make a more informed decision taking into account the
nature of behaviour of other agents in the environment.
Tremendous progress has been made in the ﬁeld of multi-agent reinforcement learning. However most of these works
have considered scenarios which are either only collaborative or scenarios where the type of the opposite agent is
already known. On the contrary, in real world scenarios, agents often have to work in environments where there might
be other unknown agents present. In such scenarios, we can perform actions and observe how the other agent in the
environment reacts to our actions to form a belief about the type of the other agent.
In this work we have proposed a classiﬁer-RL based framework which helps us identify the type of the opposite agent
present in the environment. The aim of this paper is to learn a "probing" policy which our agent might perform, observe
the interactions which would include the reactions of the other agent in the environment.

2 Related Work

2.1 Multi Agent reinforcement learning

MARL is one of the more widely studied topics in the ﬁeld of reinforcement learning. Independent Q-Learning[1]
represents some of the earliest works in this ﬁeld. In Independent Q-Learning, agents in the environment are trained
with the assumption that the other agent is part of the environment. Naturally, this fails if we increase the number of
agents in the environment due to non-stationarity in the environment.
More recently, there has been some work in MARL which can be classiﬁed under the "centralised training and decen-
tralised execution paradigm"[2][3][4][5]. In most of these works, a centralised critic is maintained during the training
process which accounts for the non-stationarity in the environment. In all of these works, no communication is assumed
between the agents. There has been some work in which agents learn to communicate with each other[6][7][8][9] which
helps counter the non-stationarity of the environment. Some of the works don’t explicitly make assumptions about the
type of other agents in the environment. In this context, there has been work where an agent actively tries to modify the
learning behaviour of the opposite agent[10] or employs recursive reasoning to reason about the behaviour of other
agents in the environment[11].

 
 
 
 
 
 
A PR E PR IN T - NOV EMB ER 22 , 2019

Our work falls more under the paradigm of agent modelling[12]. [13][14] try to reason about the type of the other agent
in the environment by observing their behaviour while carrying out their own policy. As they collect more data about
the opposite agent in the environment, they condition their policy on the belief of the other agent and are able to achieve
better generalization against different kinds of agents. To the best of our knowledge, in none of the available literature,
attempts have been made to learn a policy which helps to identify the type of other agent present in the environment. To
this end, we want to propose a separate policy, which when executed helps identify the type of the other agent in the
environment. We speculate that, having information about the type of the other agent in the environment might help us
in conditioning our policy and generalise better against different types of agents.

2.2 Non stationary environments

Multi-Agent systems are inherently non-stationary in nature. A lot of literature is available for dealing with non-stationary
environments in single agent reinforcement learning. Many of these approaches use meta learning[15][16][17][18] to
adapt to such non stationary environments. [19] explicitly identify the properties of the environments which make it
non-stationary and then use the obtained information to condition their policy. A more recent approach [20] on dealing
with non-stationary environments is learning a probing policy to identify the dynamics of the environment and then
using this information to make an informed decision. We intend to use similar probing policies to help us better identify
the type of other agent present in the environment.

2.3 Classiﬁer

Our work does classiﬁcation on trajectories, which are sequences of states. Recurrent neural networks are suitible for
working with sequences. Long Short-Term Memory [21] is probably the most popular type of RNN, and it’s what we
use in our initial version of the classiﬁer.
Gated Recurrent Units [22] are an improved RNN and more computationally efﬁcient RNN. We may experiment with
replacing our LSTM based classiﬁer with a GRU.

3 Method

Our approach involves iteratively training an RL agent and a classiﬁer. The RL agent is expected to take steps that
can help the classiﬁer detect the opposing agent correctly. While training the RL agent, the trajectory of the agent and
opposing agent is collected. The RL agent follows an initial policy while the opposing agent’s behaviour is based on
the actions of the agent. Those trajectory is the fed as data to the classiﬁer to try and predict which agent behaves that
way. In the iterations when the RL agent is trained, the accuracy of the prediction is used as a reward for the RL agent.
During the iteration where the classiﬁer is trained however and the classiﬁer is trained by back-propagating the loss of
the classiﬁer. Another important detail is that the agents are randomly chosen at each step. The episode length is a ﬁxed
number of time step since there’s currently no concept of ending here.

3.1 Environment

The environment used is a modiﬁcation of openAI gym [23] frozen lake environment. The environment was originally
designed for a single agent navigating a grid world. The game ends whenever the agent either reaches a goal or falls into
a hole. For the purpose of our experiment we modiﬁed the environment to use the holes as obstacles. Also the game has
no concept of goal, it’s only goal is to probe it’s opponent and hence classify it properly. We also made the environment
multi agent, i.e the agent and opponent agent take actions simultaneously. The state representation was designed to be
easily to be used as input to the RL agent and the classiﬁer. We used a matrix of size 4x4 with a value of 1 wherever
there’s an obstacle, 2 for where the agent is located and 3 for where the opponent is located. Rendering is done on the
command line.

3.2 Opposing Agent

The opposing agent’s policy is dependent on the main agent’s actions. An agent could take four actions in the frozen
lake gridworld, which are LEFT, RIGHT, UP and DOWN. The opposing agent can be deterministic or stochastic. when
it’s behaviour is deterministic, it could be

1. diagonal: This opposing agent would always take 90 degree anti-clockwise direction of the agent’s direction.
So if the agent goes LEFT, this agent would go DOWN.
2. opposite: This opposing agent would always go the opposite direction of the original agent. So if the agent
goes LEFT, then it would go RIGHT.

2

A PR E PR IN T - NOV EMB ER 22 , 2019

Figure 1: Opposing agents

3. opposite-diagonal: This opposing agent would always take 90 degree clockwise direction of the agent’s
direction. So if the agent goes LEFT, this agent would go UP.
4. follower: This opposing agent always follow the original agent. So if the agent goes LEFT, this agent would
always go LEFT.

The opposing agents could also be stochastic. This means an action could be categorically sampled from a distribution
with more weights on each of the options listed above. i.e, stochastic diagonal would give more weights to the diagonal
direction in the probability distribution sampled from while an opposite agent would have more weights on the opposite
direction of the agent. We then have a ﬁnal opposing agent which just behaves randomly. This framework of opposing
agents could be generalized to a scenario where we can always create 2n + 1 opposing agent given n actions the original
agent can take. The conjecture is that stochastic opposing agents would prove very challenging to the agent, it would be
easier to detect deterministic agents than stochastic ones.
The classiﬁer takes in a trajectory speciﬁed as a world map W with a value of 1 wherever there’s an obstacle, 2 for
where the agent is located and 3 for where the opponent is located

3.3 RL Agent

We will use Proximal Policy Optimization Algorithm [24] for training the agent to follow a probing policy. The reward
for the agent would be the classiﬁcation error of the classiﬁer.
Right now, we have a simple 2 layer fully connected layer which takes in the current state of the world. We have to
separate heads for policy and value.
The training procedure which we plan to use is speciﬁed in Fig. 2.

Figure 2: Diagram of the overall architecture

3

4 Experiments and Results

A PR E PR IN T - NOV EMB ER 22 , 2019

Figure 3: World Model
The yellow dot is the probing policy agent. The red dot ithe opposing agent. The blue blocks are obstacles that neither
agent can cross.

Currently, we have a basic trajectory classiﬁer implemented using an LSTM that runs on the sequence of states. It runs
on a ﬁxed world pictured in ﬁgure 3. In this experiment, a 4x4 world is used. There are some blocks in the center which
must navigated around.
We randomly generate 1000 episodes of 128 time steps each. Then we train our network using the Adam optimizer with
a learning rate of 0.001 for 20 epochs.
Each step was treated as a one trajectory move and the 128 steps as 128 batch. Average reward of the RL agent and Loss
of the classiﬁer of the 128 batch was then plotted against 1000 timestep. An experiment was setup for only determinsitic
opposing agents, deterministic and stochastic opposing agents and a random agent was used as a baseline.
Fig 6a and Fig 6b shows the baseline loss and reward for a random agent policy. It is evident that the classiﬁer and RL
agent performs worse than the when the RL agent and classiﬁer improves each other. It is also clearly obvious in Fig 5b
and Fig 5a that the Agent and classiﬁer ﬁnds it challenging to learn which is because the stochastic opposing agents are
very hard to predict especially the random agent. Since this is a work in progress and our initial experiment is just using
a step, We would proceed with improving these results by giving the Classiﬁer and RL agents full trajectory information
and also freeze the the classiﬁer while training the RL agent and vice versa just as it is done when training generative
adversarial networks.

(a) Loss against Timestep for the deterministic opposing
agent

(b) Reward against Timestep for the deterministic opposing
agent

Figure 4: Loss and Reward For Deterministic Opposing Agent Alone

4

A PR E PR IN T - NOV EMB ER 22 , 2019

(a) Loss against Timestep for the deterministic and stochas-
tic opposing agent

(b) Reward against Timestep for the deterministic and
stochastic opposing agent

Figure 5: Loss and Reward For Stochastic and Deterministic Opposing Agent

(a) Loss against Timestep for random agent policy and de-
terministic and opposing agent

(b) Reward against Timestep for random agent policy and
deterministic opposing agent

Figure 6: Loss and Reward For Random Opposing Agent

5

A PR E PR IN T - NOV EMB ER 22 , 2019

References

[1] Ardi Tampuu, Tambet Matiisen, Dorian Kodelja, Ilya Kuzovkin, Kristjan Korjus, Juhan Aru, Jaan Aru, and Raul
Vicente. Multiagent cooperation and competition with deep reinforcement learning. CoRR, abs/1511.08779, 2015.
[2] Ryan Lowe, Yi Wu, Aviv Tamar, Jean Harb, Pieter Abbeel, and Igor Mordatch. Multi-agent actor-critic for mixed
cooperative-competitive environments. CoRR, abs/1706.02275, 2017.
[3] Jakob N. Foerster, Gregory Farquhar, Triantafyllos Afouras, Nantas Nardelli, and Shimon Whiteson. Counterfac-
tual multi-agent policy gradients. CoRR, abs/1705.08926, 2017.
[4] Peter Sunehag, Guy Lever, Audrunas Gruslys, Wojciech Marian Czarnecki, Vinícius Flores Zambaldi, Max
Jaderberg, Marc Lanctot, Nicolas Sonnerat, Joel Z. Leibo, Karl Tuyls, and Thore Graepel. Value-decomposition
networks for cooperative multi-agent learning. CoRR, abs/1706.05296, 2017.
[5] Tabish Rashid, Mikayel Samvelyan, Christian Schröder de Witt, Gregory Farquhar, Jakob N. Foerster, and Shimon
Whiteson. QMIX: monotonic value function factorisation for deep multi-agent reinforcement learning. CoRR,
abs/1803.11485, 2018.
[6] Sainbayar Sukhbaatar, Arthur Szlam, and Rob Fergus. Learning multiagent communication with backpropagation.
2016.
[7] Yedid Hoshen. Vain: Attentional multi-agent predictive modeling, 2017.
[8] Jakob N. Foerster, Yannis M. Assael, Nando de Freitas, and Shimon Whiteson. Learning to communicate with
deep multi-agent reinforcement learning, 2016.
[9] Igor Mordatch and Pieter Abbeel. Emergence of grounded compositional language in multi-agent populations,
2017.
[10] Jakob N. Foerster, Richard Y. Chen, Maruan Al-Shedivat, Shimon Whiteson, Pieter Abbeel, and Igor Mordatch.
Learning with opponent-learning awareness, 2017.
[11] Ying Wen, Yaodong Yang, Rui Luo, Jun Wang, and Wei Pan. Probabilistic recursive reasoning for multi-agent
reinforcement learning. In International Conference on Learning Representations, 2019.
[12] Stefano V. Albrecht and Peter Stone. Autonomous agents modelling other agents: A comprehensive survey and
open problems. Artiﬁcial Intelligence, 258:66–95, May 2018.
[13] Aditya Grover, Maruan Al-Shedivat, Jayesh K. Gupta, Yura Burda, and Harrison Edwards. Learning policy
representations in multiagent systems, 2018.
[14] Maruan Al-Shedivat, Trapit Bansal, Yuri Burda, Ilya Sutskever, Igor Mordatch, and Pieter Abbeel. Continuous
adaptation via meta-learning in nonstationary and competitive environments, 2017.
[15] Chelsea Finn, Pieter Abbeel, and Sergey Levine. Model-agnostic meta-learning for fast adaptation of deep
networks, 2017.
[16] Jane X Wang, Zeb Kurth-Nelson, Dhruva Tirumala, Hubert Soyer, Joel Z Leibo, Remi Munos, Charles Blundell,
Dharshan Kumaran, and Matt Botvinick. Learning to reinforcement learn, 2016.
[17] Yan Duan, John Schulman, Xi Chen, Peter L. Bartlett, Ilya Sutskever, and Pieter Abbeel. Rl2 : Fast reinforcement
learning via slow reinforcement learning, 2016.
[18] Tianhe Yu, Chelsea Finn, Annie Xie, Sudeep Dasari, Tianhao Zhang, Pieter Abbeel, and Sergey Levine. One-shot
imitation from observing humans via domain-adaptive meta-learning, 2018.
[19] Wenhao Yu, Jie Tan, C. Karen Liu, and Greg Turk. Preparing for the unknown: Learning a universal policy with
online system identiﬁcation, 2017.
[20] Wenxuan Zhou, Lerrel Pinto, and Abhinav Gupta. Environment probing interaction policies, 2019.
[21] Sepp Hochreiter and Jürgen Schmidhuber. Long short-term memory. Neural Computation, 9:1735–1780, 1997.
[22] Kyunghyun Cho, Bart van Merrienboer, Caglar Gulcehre, Dzmitry Bahdanau, Fethi Bougares, Holger Schwenk,
and Yoshua Bengio. Learning phrase representations using rnn encoder-decoder for statistical machine translation.
ArXiv, abs/1406.1078, 2014.
[23] Greg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman, Jie Tang, and Wojciech
Zaremba. Openai gym. arXiv preprint arXiv:1606.01540, 2016.
[24] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization
algorithms, 2017.

6

