Exploiting Oxide Based Resistive RAM Variability
for Probabilistic AI Hardware Design

Akul Malhotra, Sen Lu, Kezhou Yang, and Abhronil Sengupta, Member, IEEE

1

9
1
0
2

v
o

N

1
2

]

T
E

.

s

c

[

2
v
5
5
5
8
0

.

1
1
9
1

:

v

i

X

r

a

Abstract—Uncertainty plays a key role in real-time machine
learning. As a signiﬁcant shift from standard deep networks,
which does not consider any uncertainty formulation during its
training or inference, Bayesian deep networks are being currently
investigated where the network is envisaged as an ensemble of
plausible models learnt by the Bayes’ formulation in response to
uncertainties in sensory data. Bayesian deep networks consider
each synaptic weight as a sample drawn from a probability
distribution with learnt mean and variance. This paper elaborates
on a hardware design that exploits cycle-to-cycle variability of
oxide based Resistive Random Access Memories (RRAMs) as a
means to realize such a probabilistic sampling function, instead
of viewing it as a disadvantage.
Index Terms—Neuromorphic Computing, Bayesian Neural
Networks, Resistive Random Access Memory.

I . IN TRODUC T ION

While Bayesian deep learning has shown promise to serve
as a pathway for enabling Probabilistic Machine Learning,
the algorithms have been primarily developed without any
insights regardring the underlying hardware implementation.
Bayesian techniques are more computationally expensive than
their non-Bayesian counterparts, thereby limiting their training
and deployment
in resource-constrained environments like
wearables and mobile edge devices. In addition to the standard
von-Neumann bottleneck [2] prevalent in current deep learning
networks (where memory access and memory leakage can
account for signiﬁcant portion of the total energy consumption
proﬁle), Bayesian deep learning involves repeated sampling of
network weights from learnt probability distributions (in most
cases, Gaussian distributions are used which are much more
hardware expensive than uniform probability distributions)
and inference based on the sampled weights. For instance,
implementing just a single synapse would involve a costly
CMOS Gaussian random number generator circuit. Repeated
parameter sampling and evaluation for just a single inference
operation worsens the von-Neumann bottleneck issue further.
With deep networks involving more than a million parameters
coupled with the necessity of performing mathematical oper-
ations on sampled probabilistic data, the projected hardware
costs for a typical CMOS implementation would be humon-
gous. Thus there is a dire need to rethink hardware designs
for such probabilistic machine learning frameworks ground-

Manuscript received November, 2019.
The authors are with the School of Electrical Engineering and Computer
Science, Department of Materials Science and Engineering, The Pennsylvania
State University, University Park, PA 16802, USA. A. Malhotra is also
afﬁliated with Birla Institute of Technology and Science, Pilani, Rajasthan
333031, India. E-mail: sengupta@psu.edu.

up where the core hardware units are better matched to the
models of computation.
Our paper elaborates on a cohesive design of an RRAM-
based Bayesian processor that leverages beneﬁts of cycle-to-
cycle variability for gaussian random number sampling and
“In-Memory” crossbar architectures to realize energy efﬁcient
hardware primitives that have the potential of enabling prob-
abilistic Artiﬁcial Intelligence. While there have been some
recent efforts at incorporating the RRAM stochastic switching
processes for neuromorphic algorithms, the applications are
either extremely simple machine learning tasks [3] or do not
exploit the stochasticity for computing [4]. The concept of
utilizing probability distributions obtained from RRAM cycle-
to-cycle variations for deep learning applications have been not
explored before.

I I . U T I L I Z ING CYC LE - TO -CYCL E RRAM VAR IAB I L I TY
FOR PROBAB I L I TY D I STR IBU T ION SAM P L ING

Metal oxide RRAMs are emerging as an alternative to
traditional CMOS based memories in non-Boolean computing
due to their high density, CMOS compatibility and low power
consumption. A metal oxide RRAM is a two-terminal device
whose resistance can be changed upon applying a voltage
across the terminals (see Fig. 1). The ability to program the
device in multiple resistance states can be exploited to use
the device as a multi-bit memory. The device consists of an
oxide layer sandwiched between two electrode layers. Some
materials used for the oxide layer are H f Ox , T iOx , AlOx and
N iOx and materials used for electrodes are T i and P t.
The metal oxide RRAM has two primary resistance states,
the high resistance state (HRS) and the low resistance state
(LRS). When the resistance of the RRAM switches from HRS
to LRS, the event is called a SET process and when the
resistance switches from LRS to HRS, the event is called a RE-
SET process. The SET and RESET processes of metal oxide
RRAMs can be simpliﬁed by studying them as the growth and
rupture of a single conduction ﬁlament. The distance between
the tip of the ﬁlament and the opposite electrode, or the gap
length (g ), is the primary variable governing the device I − V
characteristics. During the SET process, the oxygen ions drift
to the anode interface, creating a conductive oxygen vacancy
path which leads to the growth of the conduction ﬁlament,
reducing the gap length. During the RESET process,
the
reverse electric ﬁeld drives the oxygen ions back to recombine
with the vacancies, breaking down the conduction ﬁlament

 
 
 
 
 
 
2

Fig. 1. (a) A 1T-1R structure where the magnitude and sign of VReset/Set controls the device resistance. (b) I − V characteristics of our RRAM model based
on parameters reported in Ref. [1] for a bilayer T iOx /H f Ox material stack with 3.3nm thick H f Ox layer. (c) Probability distribution of the cycle-to-cycle
variation of the RESET resistance. 1,000 datapoints have been used to plot the probability distribution.

and increasing the gap length. The current ﬂowing through
the device is generalized using the following equation [1],

I = Io . exp (− g
go

). sinh (

V
Vo

)

(1)

where, Io , Go and Vo are ﬁtting parameters, g is the gap
length and V is the voltage across the device. The exponential
dependence on the gap length arises due to the various
tunneling mechanisms present in the device [1]. We used an
experimentally calibrated publicly accessible RRAM model
[5] and our simulation parameters correspond to a bilayer
T iOx /H f Ox material stack with 3.3nm thick H f Ox layer.
The parameters are tabulated below. Please refer to Ref. [1]
for a description of the ﬁtting parameters.

TABLE I. RRAM Device Simulation Parameters

Parameters

Value

g0
V0
I0
v0
γ0
β0
α

0.15nm
0.35V
0.2mA
5 × 106m/s

20.8 (SET), 24.9 (RESET)
4.8 (SET), 19.0 (RESET)

1

The RRAM RESET process is a gradual one and exhibits
cycle-to-cycle resistance variation as depicted in Fig. 1(c).
This is due to the random oxygen-vacancy ﬁlament dissolution
process. While prior works have reported similar variation
studies, they have typically considered this as a non-ideality. In
this article, we utilize such a probability distribution sampling
function for Bayesian network implementation. It is worth
noting here that the distribution is not strictly Gaussian (log-
normal is a better ﬁt [4], [6]). However, we did not observe
the minor deviation from the Gaussian distribution to impact
the network accuracy. The network can be also inherently
trained assuming log-normal probability priors to account for
the device constraints. In this work, we have assumed Gaussian
probability priors for training.

I I I . SY ST EM D E S IGN AND R E SU LT S

In a Bayesian Neural Network, the synaptic weights, W, are
typically characterized by Gaussian probability distributions.
Once all
the posterior distributions are learnt (µ and σ
parameters of the weight distributions) [7], the network output
corresponding to input, x, should be obtained by averaging
the outputs obtained by sampling weights from the posterior

y = EP (W|D) [f (x,W)] ≈ Eq(W,θ) [f (x,W)] ≈ 1
S

f (x,Wi )

S(cid:88)

i=1

distribution of the weights, W [8]. The output of the network,
y , is therefore given by,

(2)
where, P (D |W) is the likelihood, corresponding to the feed-
forward pass of the network, f (x,W) is the network mapping
for input x and weights, W. The approximation is performed
over S independent Monte-Carlo samples drawn from the
Gaussian distribution, q(W, θ), characterized by parameters,
θ = (µ, σ ), where µ and σ represent the mean and standard
deviation vectors for the probability distributions representing
P (W|D) [11]. We consider the Variational Inference method
[9], [10] for training the network in this work.
Hence, the core computation in a Bayesian network is a dot-
product between an input vector, x, and samples drawn from
a Gaussian probability distribution, N (µ, σ ). It can be shown
by simple algebraic manipulations, that the dot-product for a
single neuron is equivalent to the summation of dot-products
x.µ(cid:48) and [xN (a, b)].σ (cid:48) where, the probability distribution of
RRAM cycle-to-cycle resistance variation is characterized by
mean a and variance b (µ(cid:48) = µ − a/b and σ (cid:48) = σ/b). While
the ﬁrst dot-product can be easily performed in a standard
RRAM crossbar array, the implementation of the second term
is shown in Fig. 2. We utilize a current-to-voltage converter to
scale each input to the crossbar array by a factor sampled from

Fig. 2. RRAM Based Bayesian Neural Network Implementation. The rows
of the crossbar array are driven by scaled inputs where the scaling factor
is a sampling from a Normal distribution. The sampling operation can be
simply implemented by an RRAM device interfaced with an ampliﬁer and
Sample-Hold circuit.

GNDVGateVReset/SetTop electrodeBottom electrodeMetal oxideTop electrodeBottom electrodeMetal oxideResetSetOxygen VacancyOxygen Atom1Conductance (1e-6 mho)ProbabilityDistribution01210-2102Current (uA)Voltage (V)(a)(b)(c)Analog-to-Digital ConverterV1. . .. . .. . .. . .. . .. . .RRAM Synapse Crossbar Array (   ) storing variances    Digital-to-Analog ConverterV2Vm      . . .x1xm         Gaussian Probability Distribution Sampling              S/HS/HS/HWL[0]SL[0]WL[1]WL[n]SL[n]SL[1]x2a Normal distribution. The ampliﬁcation factor of this stage is
designed such that the maximum read-voltage of the crossbar
array is limited to 0.3V (to ensure linear RRAM I − V charac-
teristics). The crossbar array stores the parameters of the array
σ (cid:48) . Each column is read sequentially by the array peripherals
in order to ensure that independent random samples are being
used for each array element. The current ﬂowing through each
cross-point device is scaled by the conductance of the device
and due to Kirchoff ’s law, all these currents get summed up
along the column, thereby realizing the dot-product kernel.
The analog current outputs drive interfaced Analog-to-Digital
converters (ADCs) to provide output to the fan-out neurons.
A hybrid device-circuit-algorithm co-simulation framework
was developed and the design was tested for a standard
digit recognition problem on the MNIST dataset [12]. The
neural network used in this work consisted of 2 hidden layers
each with 200 neurons. The probability distributions were
learnt using the ‘Bayes by Backprop’ algorithm [13]. We
considered 4-bit representation in the RRAM devices in the
cross-point array and 3-bit discretization in the Analog-to-
Digital converter output (interfaced with the crossbar array).
We considered RRAM resistance programming ranges to be
16. Note that cycle-to-cycle read variations will be also present
in the crossbar RRAM resistances. However, such variation
was observed to produce negligible accuracy drop. This is
also consistent with prior work since it is well known that
neural networks are resilient to minor ﬂuctuation in its synaptic
weights. Our proposal exploits the read resistance variation
and ampliﬁes its effect to scale the dot-product calculation ac-
cordingly. Using our hybrid hardware-algorithm co-simulation
framework,
the test accuracy of the network was 96.09%
(averaged over 10 samples). The baseline idealized software
network was trained with an accuracy of 97.51% over the
testing set (averaged over 10 sampled networks).
Since, RRAM devices are inherently memory elements, the
ability to perform the costly dot-product operations (one of
the main computationally expensive operations in a neural
network hardware) in the memory array itself enables us to
address the issues of von-Neumann bottleneck. Prior work
has revealed that RRAM based “In-Memory” architectures
can potentially achieve 1-2 orders of magnitude improvement
in energy consumption in comparison to a baseline CMOS
implementation [14], [15]. Memory access and memory leak-
age constitute a signiﬁcant portion of the total system energy
consumption in CMOS implementations and the proportion
increases signiﬁcantly as the network size increases. Addi-
tionally, the memory access latency increases with increasing
model size, thereby increasing the memory leakage energy
as well. While these beneﬁts remain valid in this scenario,
the Gaussian random number generation adds to the hardware
complexity. CMOS based implementations of Gaussian ran-
dom number generators rely on a signiﬁcant number of linear
feedback circuits which are extremely hardware expensive.
For instance, a recent work for a CMOS based 64-parallel
Gaussian RNG reports 1780 registers and 528.69mW power
consumption [16]. In stark contrast, our implementation uses
simply a single RRAM device and a Sample and Hold circuit
to implement the probability distribution sampling. The total

3

power consumption for our RRAM based Gaussian random
number generation is 32.54mW for a similar 64 parallel
generation task, which is 16.25× lower than the baseline
CMOS implementation. It is worth noting here that device
level non-idealities in other technologies can be harnessed as
well for such probabilistic AI hardware. This work is based
on Ref. [17] which explored the design for spintronic tech-
nologies. This work explores the design for RRAM devices
which requires a signiﬁcant rethinking due to different intrinsic
device physics and operating voltage/current characteristics
and conditions.

IV. SUMMARY

In conclusion, we have provided a vision for an RRAM
based “In-Memory” computing primitive for Bayesian neural
hardware that utilizes simple device-circuit primitives and
leverages RRAM stochasticity as a computing resource, in-
stead of viewing it as a disadvantage. Such hardware-software
co-design of Bayesian neural network models can potentially
lead to real-time decision making in autonomous agents in the
presence of uncertainties.

R E FER ENC E S
[1] Z. Jiang, Y. Wu, S. Yu, L. Yang, K. Song, Z. Karim, and H.-S. P. Wong,
“A compact model for metal–oxide resistive random access memory
with experiment veriﬁcation,” IEEE Transactions on Electron Devices,
vol. 63, no. 5, pp. 1884–1892, 2016.
[2] M. A. Zidan, J. P. Strachan, and W. D. Lu, “The future of electronics
based on memristive systems,” Nature Electronics, vol. 1, no. 1, p. 22,
2018.
[3] M. Suri, V. Parmar, A. Kumar, D. Querlioz, and F. Alibart, “Neuro-
morphic hybrid RRAM-CMOS RBM architecture,” in 2015 15th Non-
Volatile Memory Technology Symposium (NVMTS).
IEEE, 2015, pp.
1–6.
[4] T. Dalgaty, M. Payvand, F. Moro, D. R. Ly, F. Pebay-Peyroula, J. Casas,
G. Indiveri, and E. Vianello, “Hybrid neuromorphic circuits exploiting
non-conventional properties of RRAM for massively parallel
local
plasticity mechanisms,” APL Materials, vol. 7, no. 8, p. 081125, 2019.
[5] Z. Jiang and H.-S. P. Wong, “Stanford University Resistive-Switching
Random Access Memory (RRAM) Verilog-A Model,” Oct 2014.
[Online]. Available: https://nanohub.org/publications/19/1
[6] D. Garbin, O. Bichler, E. Vianello, Q. Rafhay, C. Gamrat, L. Perniola,
G. Ghibaudo, and B. DeSalvo, “Variability-tolerant convolutional neu-
ral network for pattern recognition applications based on OxRAM
synapses,” in 2014 IEEE International Electron Devices Meeting. IEEE,
2014, pp. 28–4.
[7] Y. Gal and Z. Ghahramani, “Bayesian convolutional neural net-
works with Bernoulli approximate variational inference,” arXiv preprint
arXiv:1506.02158, 2015.
[8] R. Cai, A. Ren, N. Liu, C. Ding, L. Wang, X. Qian, M. Pedram,
and Y. Wang, “VIBNN: Hardware acceleration of bayesian neural
networks,” in Proceedings of the Twenty-Third International Conference
on Architectural Support for Programming Languages and Operating
Systems. ACM, 2018, pp. 476–488.
[9] R. Houthooft, X. Chen, Y. Duan, J. Schulman, F. De Turck, and
P. Abbeel, “VIME: Variational information maximizing exploration,” in
Advances in Neural Information Processing Systems, 2016, pp. 1109–
1117.
[10] M. I. Jordan, Z. Ghahramani, T. S. Jaakkola, and L. K. Saul, “An
introduction to variational methods for graphical models,” Machine
learning, vol. 37, no. 2, pp. 183–233, 1999.
[11] Z. Ghahramani and M. J. Beal, “Propagation algorithms for variational
Bayesian learning,” in Advances in neural information processing sys-
tems, 2001, pp. 507–513.
[12] Y. LeCun, L. Bottou, Y. Bengio, P. Haffner et al., “Gradient-based
learning applied to document recognition,” Proceedings of the IEEE,
vol. 86, no. 11, pp. 2278–2324, 1998.
[13] C. Blundell, J. Cornebise, K. Kavukcuoglu, and D. Wierstra, “Weight
uncertainty in neural networks,” arXiv preprint arXiv:1505.05424, 2015.

4

[14] A. Ankit, I. E. Hajj, S. R. Chalamalasetti, G. Ndu, M. Foltin, R. S.
Williams, P. Faraboschi, W.-m. W. Hwu, J. P. Strachan, K. Roy et al.,
“PUMA: A programmable ultra-efﬁcient memristor-based accelerator
for machine learning inference,” in Proceedings of the Twenty-Fourth
International Conference on Architectural Support for Programming
Languages and Operating Systems. ACM, 2019, pp. 715–731.
[15] A. Shaﬁee, A. Nag, N. Muralimanohar, R. Balasubramonian, J. P. Stra-
chan, M. Hu, R. S. Williams, and V. Srikumar, “ISAAC: A convolutional
neural network accelerator with in-situ analog arithmetic in crossbars,”
ACM SIGARCH Computer Architecture News, vol. 44, no. 3, pp. 14–26,
2016.
[16] R. Cai, A. Ren, L. Wangy, M. Pedramy, and Y. Wang, “Hardware
acceleration of bayesian neural networks using RAM based linear feed-
back gaussian random number generators,” in 2017 IEEE International
Conference on Computer Design (ICCD).
IEEE, 2017, pp. 289–296.
[17] K. Yang, A. Malhotra, S. Lu, and A. Sengupta, “All-Spin Bayesian
Neural Networks,” arXiv preprint arXiv:1911.05828, 2015.

