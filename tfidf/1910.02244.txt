9
1
0
2

v
o

N

1
2

]

G

L

.

s

c

[

2
v
4
4
2
2
0

.

0
1
9
1

:

v

i

X

r

a

Paper under review

Y ET ANOTH ER BU T MOR E E FFIC I EN T B LACK -BOX
ADVER SAR IA L AT TACK :
T I L ING AND EVO LUT ION
S TRAT EG I E S

Laurent Meunier∗†, Jamal Atif∗, Olivier Teytaud†

laurentmeunier@fb.com, jamal.atif@dauphine.fr, oteytaud@fb.com

AB STRAC T

We introduce a new black-box attack achieving state of the art performances. Our
approach is based on a new objective function, borrowing ideas from (cid:96)∞ -white
box attacks, and particularly designed to ﬁt derivative-free optimization require-
ments. It only requires to have access to the logits of the classiﬁer without any
other information which is a more realistic scenario. Not only we introduce a new
objective function, we extend previous works on black box adversarial attacks
to a larger spectrum of evolution strategies and other derivative-free optimization
methods. We also highlight a new intriguing property that deep neural networks
are not robust to single shot tiled attacks. Our models achieve, with a budget lim-
ited to 10, 000 queries, results up to 99.2% of success rate against InceptionV3
classiﬁer with 630 queries to the network on average in the untargeted attacks set-
ting, which is an improvement by 90 queries of the current state of the art. In the
targeted setting, we are able to reach, with a limited budget of 100, 000, 100% of
success rate with a budget of 6, 662 queries on average, i.e. we need 800 queries
less than the current state of the art.

1

IN TRODUC T ION

Despite their success, deep learning algorithms have shown vulnerability to adversarial attacks (Big-
gio et al., 2013; Szegedy et al., 2014), i.e. small imperceptible perturbations of the inputs, that lead
the networks to misclassify the generated adversarial examples. Since their discovery, adversarial
attacks and defenses have become one of the hottest research topics in the machine learning commu-
nity as serious security issues are raised in many critical ﬁelds. They also question our understanding
of deep learning behaviors. Although some advances have been made to explain theoretically (Fawzi
et al., 2016; Sinha et al., 2017; Cohen et al., 2019; Pinot et al., 2019) and experimentally (Good-
fellow et al., 2015; Xie et al., 2018; Meng & Chen, 2017; Samangouei et al., 2018; Araujo et al.,
2019) adversarial attacks, the phenomenon remains misunderstood and there is still a gap to come
up with principled guarantees on the robustness of neural networks against maliciously crafted at-
tacks. Designing new and stronger attacks helps building better defenses, hence the motivation of
our work.
First attacks were generated in a setting where the attacker knows all the information of the network
(architecture and parameters). In this white box setting, the main idea is to perturb the input in
the direction of the gradient of the loss w.r.t.
the input (Goodfellow et al., 2015; Kurakin et al.,
2016; Carlini & Wagner, 2017; Moosavi-Dezfooli et al., 2016). This case is unrealistic because
the attacker has only limited access to the network in practice. For instance, web services that
propose commercial recognition systems such as Amazon or Google are backed by pretrained neural
networks. A user can query this system by sending an image to classify. For such a query, the user
only has access to the inference results of the classiﬁer which might be either the label, probabilities
or logits. Such a setting is coined in the literature as the black box setting. It is more realistic but
also more challenging from the attacker’s standpoint.

∗Universit Paris-Dauphine, PSL Research University, CNRS, LAMSADE, Paris, France
†Facebook AI Research, Paris, France

1

 
 
 
 
 
 
Paper under review

As a consequence, several works proposed black box attacks by just querying the inference results
of a given classiﬁer. A natural way consists in exploiting the transferability of an adversarial attack,
based on the idea that if an example fools a classiﬁer, it is more likely that it fools another one (Pa-
pernot et al., 2016a). In this case, a white box attack is crafted on a fully known classiﬁer. Papernot
et al. (2017) exploited this property to derive practical black box attacks. Another approach within
the black box setting consists in estimating the gradient of the loss by querying the classiﬁer (Chen
et al., 2017; Ilyas et al., 2018a;b). For these attacks, the PGD attack (Kurakin et al., 2016; Madry
et al., 2018a) algorithm is used and the gradient is replaced by its estimation.
In this paper, we propose efﬁcient black box adversarial attacks using stochastic derivative free
optimization (DFO) methods with only access to the logits of the classiﬁer. By efﬁcient, we mean
that our model requires a limited number of queries while outperforming the state of the art in terms
of attack success rate. At the very core of our approach is a new objective function particularly
designed to suit classical derivative free optimization. We also highlight a new intriguing property
that deep neural networks are not robust to single shot tiled attacks. It leverages results and ideas
from (cid:96)∞ -attacks. We also explore a large spectrum of evolution strategies and other derivative-free
optimization methods thanks to the Nevergrad framework (Rapin & Teytaud, 2018).

Outline of the paper. We present in Section 2 the related work on adversarial attacks. Section 3
presents the core of our approach. We introduce a new generic objective function and discuss two
practical instantiations leading to a discrete and a continuous optimization problems. We then give
more details on the best performing derivative-free optimization methods, and provide some insights
on our models and optimization strategies. Section 4 is dedicated to a thorough experimental analy-
sis, where we show we reach state of the art performances by comparing our models with the most
powerful black-box approaches on both targeted and untargeted attacks. We also assess our models
against the most efﬁcient so far defense strategy based on adversarial training. We ﬁnally conclude
our paper in Section 5.

2 R ELAT ED WORK

Adversarial attacks have a long standing history in the machine learning community. Early works
appeared in the mid 2000’s where the authors were concerned about Spam classiﬁcation (Biggio
et al., 2009). Szegedy et al. (2014) revives this research topic by highlighting that deep convolu-
tional networks can be easily fooled. Many adversarial attacks against deep neural networks have
been proposed since then. One can distinguish two classes of attacks: white box and black box
attacks. In the white box setting, the adversary is supposed to have full knowledge of the network
(architecture and parameters), while in the black box one, the adversary only has limited access to
the network: she does not know the architecture, and can only query the network and gets labels,
logits or probabilities from her queries. An attack is said to have suceeded (we also talk about Attack
Success Rate), if the input was originally well classiﬁed and the generated example is classiﬁed to
the targeted label.
The white box setting attracted more attention even if it is the more unrealistic between the two.
The attacks are crafted by by back-propagating the gradient of the loss function w.r.t. the input. The
problem writes as a non-convex optimization procedure that either constraints the perturbation or
aims at minimizing its norm. Among the most popular ones, one can cite FGSM (Goodfellow et al.,
2015), PGD (Kurakin et al., 2016; Madry et al., 2018a), Deepfool (Moosavi-Dezfooli et al., 2016),
JSMA (Papernot et al., 2016b), Carlini&Wagner attack (Carlini & Wagner, 2017) and EAD (Chen
et al., 2018).
The black box setting is more realistic, but also more challenging. Two strategies emerged in the
literature to craft attacks within this setting: transferability from a substitute network, and gradient
estimation algorithms. Transferability has been pointed out by Papernot et al. (2017). It consists in
generating a white-box adversarial example on a fully known substitute neural network, i.e. a net-
work trained on the same classiﬁcation task. This crafted adversarial example can be transferred to
the targeted unknown network. Leveraging this property, Moosavi-Dezfooli et al. (2017) proposed
an algorithm to craft a single adversarial attack that is the same for all examples and all networks.
Despite the popularity of these methods, gradient estimation algorithms outperform transferability
methods. Chen et al. (2017) proposed a variant of the powerful white-box attack introduced in (Car-

2

Paper under review

lini & Wagner, 2017), based on gradient estimation with ﬁnite differences. This method achieves
good results in practice but requires a high number of queries to the network. To reduce the number
of queries, Ilyas et al. (2018a) proposed to rely rather on Natural Evolution Strategies (NES). These
derivative-free optimization approaches consist in estimating the parametric distribution of the min-
ima of a given objective function. This amounts for most of NES algorithms to perform a natural
gradient descent in the space of distributions (Ollivier et al., 2017).
In (Al-Dujaili & O’Reilly,
2019), the authors propose to rather estimate the sign of the gradient instead of estimating the its
magnitude suing zeroth-order optimization techniques. They show further how to reduce the search
space from exponential to linear. The achieved results were state of the art at the publication date.
In Liu et al. (2019), the authors introduced a zeroth-order version of the signSGD algorithm, stud-
ied its convergence properties and showed its efﬁciency in crafting adversarial black-box attacks.
The results are promising but fail to beat the state of the art. In Tu et al. (2019), the authors intro-
duce the AutoZOOM framework combining gradient estimation and an auto-encoder trained ofﬂine
with unlabeled data. The idea is appealing but requires training an auto-encoder with an available
dataset, which an additional effort for the attacker. Besides, this may be unrealistic for several use
cases. More recently, Moon et al. (2019) proposed a method based on discrete and combinatorial
optimization where the perturbations are pushed towards the corners of the (cid:96)∞ ball. This method is
to the best of our knowledge the state of the art in the black box setting in terms of queries budget
and success rate. We will focus in our experiments on this method and show how our approaches
achieve better results.
Several defense strategies have been proposed to diminish the impact of adversarial attacks on net-
works accuracies. A basic workaround, introduced in (Goodfellow et al., 2015), is to augment the
learning set with adversarial attacks examples. Such an approach is called adversarial training in the
literature. It helps recovering some accuracy but fails to fully defend the network, and lacks theoret-
ical guarantees, in particular principled certiﬁcates. Defenses based on randomization at inference
time were also proposed (Lecuyer et al., 2018; Cohen et al., 2019; Pinot et al., 2019). These meth-
ods are grounded theoretically, but the guarantees cannot ensure full protection against adversarial
examples. The question of defenses and attacks is still widely open since our understanding of this
phenomenon is still in its infancy. We evaluate our approach against adversarial training, the most
powerful defense method so far.

3 M ETHOD S

3 .1 G ENERAL FRAM EWORK

Let us consider a classiﬁcation task X (cid:55)→ [K ] where X ⊆ Rd is the input space and [K ] =
{1, ..., K } is the corresponding label set. Let f : Rd → RK be a classiﬁer (a feed forward neu-
ral network in our paper) from an input space X returning the logits of each label in [K ] such
that the predicted label for a given input is arg maxi∈[K ] fi (x). The aim of ||.||∞ -bounded un-
targeted adversarial attacks is, for some input x with label y , to ﬁnd a perturbation τ such that
arg maxi∈[K ] fi (x) (cid:54)= y . Classically, ||.||∞ -bounded untargeted adversarial attacks aims at optimiz-
ing the following objective:

max

τ :||τ ||∞≤

L(f (x + τ ), y)

(1)

where L is a loss function (typically the cross entropy) and y the true label. For targeted attacks,
the attacker targets a label yt by maximizing −L(f (x + τ ), yt ). With access to the gradients of the
network, gradient descent methods have proved their efﬁciency (Kurakin et al., 2016; Madry et al.,
2018a). So far, the outline of most black box attacks was to estimate the gradient using either ﬁnite
differences or natural evolution strategies. Here using evolutionary strategies heuristics, we do not
want to take care of the gradient estimation problem.

3 .2 TWO O P T IM I ZAT ION PROB LEM S

In some DFO approaches, the default search space is Rd . In the (cid:96)∞ bounded adversarial attacks
setting, the search space is B∞ () = {τ : ||τ ||∞ ≤ }. It requires to adapt the problem in Eq 1. Two
variants are proposed in the sequel leading to continuous and discretized versions of the problem.

3

Paper under review

The continuous problem. As in Carlini & Wagner (2017), we use the hyperbolic tangent transfor-
mation to restate our problem since B∞ () =  tanh (Rd ). This leads to a continuous search space
on which evolutionary strategies apply. Hence our optimization problem writes:

max

τ ∈Rd

L(f (x +  tanh(τ )), y).

(2)
We will call this problem DFOc − optimizer where optimizer is the used black box derivative free
optimization strategy.
The discretized problem. Moon et al. (2019) pointed out that PGD attacks (Kurakin et al., 2016;
Madry et al., 2018b) are mainly located on the corners of the (cid:96)∞ -ball. They consider optimizing the
following

max

τ ∈{−,+}d

L(f (x + τ ), y).

(3)

The author in (Moon et al., 2019) proposed a purely discrete combinatorial optimization to solve
this problem (Eq. 3). As in Bello et al. (2017), we here consider how to automatically convert an
algorithm designed for continuous optimization to discrete optimization. To make the problem in
Eq. 3 compliant with our evolutionary strategies setting, we rewrite our problem by considering a
stochastic function f (x + τ ) where, for all i, τi ∈ {−1, +1} and P(τi = 1) = Softmax(ai , bi ) =
eai +ebi . Hence our problem amounts to ﬁnd the best parameters ai and bi that optimize:

eai

Eτ ∼Pa,b (L(f (x + τ ), y)

min

a,b

(4)

We then rely on evolutionary strategies to ﬁnd the parameters a and b. As the optima are determinis-
as will be discussed in the sequel. We will call this problem DFOd − optimizer where optimizer
tic, the optimal values for a and b are at inﬁnity. Some ES algorithms are well suited to such setting
is the used black box derivative free optimization strategy for a and b. In this case, one could re-
duce the problem to one variale ai with P(τi = 1) =
1+e−ai , but experimentally the results are
comparable, so we concentrate on Problem 4.

1

3 .3 D ER IVAT IV E - FR E E O P T IM I ZAT ION M E THOD S

Derivative-free optimization methods are aimed at optimizing an objective function without access
to the gradient. There exists a large and wide literature around derivative free optimisation. In this
setting, one algorithm aims to minimize some function f on some space X . The only thing that
could be done by this algorithm is to query for some points x the value of f (x). As evaluating f
can be computationally expensive, the purpose of DFO methods is to get a good approximation of
the optima using a moderate number of queries. We tested several evolution strategies (Rechen-
berg, 1973; Beyer, 2001): the simple (1 + 1)-algorithm (Matyas, 1965; Schumer & Steiglitz, 1968),
Covariance Matrix Adaptation (CMA (Hansen & Ostermeier, 2003)). For these methods, the un-
derlying algorithm is to iteratively update some distribution Pθ deﬁned on X . Roughly speaking,
the current distribution Pθ represents the current belief of the localization of the optimas of the goal
function. The parameters are updated using objective function values at different points. It turns out
that this family of algorithms, than can be reinterpreted as natural evolution strategies, perform best.
The two best performing methods will be detailed in Section 3.3.1; we refer to references above for
other tested methods.

3 .3 .1 OUR B E S T PER FORM ING M ETHOD S : EVO LU T ION STRATEG I E S

The (1 + 1)-ES algorithm. The (1 + 1)-evolution strategy with one-ﬁfth rule (Matyas, 1965;
Schumer & Steiglitz, 1968) is a simple but effective derivative-free optimization algorithm (in sup-
plementary material, Alg. 1). Compared to random search, this algorithm moves the center of the
Gaussian sampling according to the best candidate and adapts its scale by taking into account their
frequency. Yao & Liu (1996) proposed the use of Cauchy distributions instead of classical Gaussian
sampling. This favors large steps, and improves the results in case of (possibly partial) separabil-
ity of the problem, i.e. when it is meaningful to perform large steps in some directions and very
moderate ones in the other directions.

4

Paper under review

Figure 1: Illustration of the tiling trick: the same noise is applied on small tile squares.

CMA-ES algorithm. The Covariance Matrix Adaptation Evolution Strategy (Hansen & Oster-
meier, 2003) combines evolution strategies (Beyer, 2001), Cumulative Step-Size Adaptation (Arnold
& Beyer, 2004), and a speciﬁc method for adaptating the covariance matrix. An outline is provided
in supplementary material, Alg. 2. CMA-ES is an effective and robust algorithm, but it becomes
catastrophically slow in high dimension due to the expensive computation of the square root of the
matrix. As a workaround, Ros & Hansen (2008) propose to approximate the covariance matrix by
a diagonal one. This leads to a computational cost linear in the dimension, rather than the original
quadratic one.

Link with Natural Evolution Strategy (NES) attacks. Both (1+1)-ES and CMA-ES can be seen as

an instantiation of a natural evolution strategy (see for instance Ollivier et al. (2017); Wierstra et al.
(2014)). A natural evolution strategy consists in estimating iteratively the distribution of the optima.
For most NES approaches, a fortiori CMA-ES, the iterative estimation consists in a second-order
gradient descent (also known as natural gradient) in the space of distributions (e.g. Gaussians).
(1+1)-ES can also be seen as a NES, where the covariance matrix is restricted to be proportional
to the identity. Note however that from an algorithmic perspective, both CME-ES and (1+1)-ES
optimize the quantile of the objective function.

3 .3 .2 HY POTH E SE S FOR DFO M E THOD S IN TH E ADV ER SAR IA L AT TACK S CONT EX T

The state of the art in DFO and intuition suggest the followings. Using softmax for exploring only
points in the corner (Eq. 3) is better for moderate budget, as corners are known to be good adversarial
candidates; however, for high precision attacks (with small τ ) a smooth continuous precision (Eq 2)
is more relevant. With or without softmax, the optimum is at inﬁnity 1 , which is in favor of methods
having fast step-size adaptation or samplings with heavy-tail distributions. With an optimum at
inﬁnity, (Chotard et al., 2012) has shown how fast is the adaptation of the step-size when using
cumulative step-size adaptation (as in CMA-ES), as opposed to slower rates for most methods.
Cauchy sampling (Yao & Liu, 1996) in the (1 + 1)-ES is known for favoring fast changes; this is
consistent with the superiority of Cauchy sampling in our setting compared to Gaussian sampling.
Newuoa, Powell, SQP, Bayesian Optimization, Bayesian optimization are present in Nevergrad but
they have an expensive (budget consumption linear is linear w.r.t. the dimension) initial sampling
stage which is not possible in our high-dimensional / moderate budget context. The targeted case
needs more precision and favors algorithms such as Diagonal CMA-ES which adapt a step-size
per coordinate whereas the untargeted case is more in favor of fast random exploration such as the
(1 + 1)-ES. Compared to Diagonal-CMA, CMA with full covariance might be too slow; given a
number of queries (rather than a time budget) it is however optimal for high precision.

3 .4 TH E T I L ING TR ICK

Ilyas et al. (2018b) suggested to tile the attack to lower the number of queries necessary to fool the
network. Concretely, they observe that the gradient coordinates are correlated for close pixels in the
images, so they suggested to add the same noise for small square tiles in the image (see Fig. 1).

1 i.e. the optima of the ball constrained problem 1, would be close to the boundary or on the boundary of
the (cid:96)∞ ball. In that case, the optimum of the continuous problem 2 will be at ∞ or close to it. On the discrete
case 4 it is easy to see that the optimum is when ai or bi → ∞.

5

Tilesize: 6 pixels+ 0.03 x=Paper under review

Figure 2: Success rate of a single shot random attacks on ImageNet vs. the number of tiles used to
intensities ( ∈ {0.01, 0.03, 0.05, 0.1}). On the right,  is ﬁxed to 0.05 and the single shot attack is
craft the attack. On the left, attacks are plotted against InceptionV3 classiﬁer with different noise
evaluated on InceptionV3, ResNet50 and VGG16bn.

We exploit the same trick since it reduces the dimensionality of the search space, and makes hence
evolutionary strategies suited to the problem at hand. Besides breaking the curse of dimensionality,
tiling leads surprisingly to a new property that we discovered during our experiments. At a given
tiling scale, convolutional neural networks are not robust to random noise. Section 4.2 is devoted to
this intriguing property. Interestingly enough, initializing our optimization algorithms with a tiled
noise at the appropriate scale drastically speeds up the convergence, leading to a reduced number of
queries.

4 EX PER IM EN T S

4 .1 G EN ERA L S ET T ING AND IM PL EM EN TAT ION D ETA I L S

We compare our approach to the “bandits” method (Ilyas et al., 2018b) and the parsimonious at-
tack (Moon et al., 2019). The latter (parsimonious attack) is, to the best of our knowledge, the
state of the art in the black-box setting from the literature; bandits method is also considered in
our benchmark given its ties to our models. We reproduced the results from (Moon et al., 2019) in
our setting for fair comparison. As explained in section 3.2, our attacks can be interpreted as (cid:96)∞
ones. We use the large-scale ImageNet dataset (Deng et al., 2009). As usually done in most frame-
works, we quantify our success in terms of attack success rate, median queries and average queries.
Here, the number of queries refers to the number of requests to the output logits of a classiﬁer for
a given image. For the success rate, we only consider the images that were correctly classiﬁed by
our model. We use InceptionV3 (Szegedy et al., 2017) , VGG16 (Simonyan & Zisserman, 2014)
with batch normalization (VGG16bn) and ResNet50 (He et al., 2016) architectures to measure the
performance of our algorithm on the ImageNet dataset. These models reach accuracy close to the
the state of the art with around 75 − 80% for the Top-1 accuracy and 95% for the Top-5 accuracy.
We use pretrained models from PyTorch (Paszke et al., 2017). All images are normalized to [0, 1].
Results on VGG16bn and ResNet50 are deferred in supplementary material E. The images to be
attacked are selected at random.
We ﬁrst show that convolutional networks are not robust to tiled random noise, and more sur-
prisingly that there exists an optimal tile size that is the same for all architectures and noise in-
tensities. Then, we evaluate our methods on both targeted and untargeted objectives. We con-
sidered the following losses: the cross entropy L(f (x), y) = − log(P(y |x)) and a loss inspired
from the “Carlini&Wagner” attack: L(f (x), y) = −P(y |x) + maxy (cid:48) (cid:54)=y P(y (cid:48) |x) where P(y |x) =
[Softmax(f (x))]y , the probability for the classiﬁer to classify the input x to label y . The results
for the second loss are deferred in supplementary material C. For all our attacks, we use the Never-
grad (Rapin & Teytaud, 2018) implementation of evolution strategies. We did not change the default
parameters of the optimization strategies.

6

050100150200Number of tiles0.00.10.20.30.40.50.6Attack success rateAttack success for InceptionV3 architecture  with different noise intensities versus the number of tiles=0.01=0.03=0.05=0.1050100150200Number of tiles0.00.10.20.30.40.5Attack success rateAttack success for =0.05 with different  architetures versus the number of tilesResNet50InceptionV3VGG16bnPaper under review

Figure 3: The cumulative success rate in terms the number of queries for the number of queries
required for attacks on ImageNet with  = 0.05 in the untargeted (left) and targeted setting (right).
The number of queries (x-axis) is plotted with a logarithmic scale.

Table 1: Comparison of our method with the parsimonious and bandits attacks in the untargeted
setting on ImageNet on InceptionV3 pretrained network for  = 0.05 and 10, 000 as budget limit.

# of tiles Average queries Median queries

222
269
249
60
63
189
191
232
259

20

38

Success rate

98.4%
95.3%
95.1%
95.2%
97.3%
97.2%
98.7%
98.9%

99.2%

97.7%
97.4%

Method

Parsimonious
Bandits
Bandits

DFOc − Cauchy(1 + 1)-ES
DFOc − Cauchy(1 + 1)-ES
DFOc − DiagonalCMA
DFOc − DiagonalCMA
DFOc − CMA
DFOc − CMA
DFOd − DiagonalCMA
DFOd − DiagonalCMA

-
30
50
30
50
30
50
30
50
30
50

702
1007
995
466
510
533
623
589
630

424

485

4 .2 CONVO LU T IONA L N EURA L N E TWORK S AR E NOT ROBU ST TO T I LED RANDOM NO I SE

1

In this section, we highlight that neural neural networks are not robust to (cid:96)∞ tiled random noise. A
noise on an image is said to be tiled if the added noise on the image is the same on small squares
of pixels (see Figure 2). In practice, we divide our image in equally sized tiles. For each tile, we
2 and − with probability
add to the image a randomly chosen constant noise: + with probability 1
2 , uniformly on the tile. The tile trick has been introduced inIlyas et al. (2018a) for dimensionality
reduction. Here we exhibit a new behavior that we discovered during our experiments. As shown
in Fig. 1 for reasonable noise intensity ( = 0.05), the success rate of a one shot randomly tiled
attack is quite high. This fact is observed on many neural network architectures. We compared the
number of tiles since the images input size are not the same for all architectures (299 × 299 × 3
for InceptionV3 and 224 × 224 × 3 for VGG16bn and ResNet50). The optimal number of tiles (in
the sense of attack success rate) is, surprisingly, independent from the architecture and the noise
intensity. We also note that the InceptionV3 architecture is more robust to random tiled noise than
VGG16bn and ResNet50 architectures. InceptionV3 blocks are parallel convolutions with different
ﬁlter sizes that are concatenated. Using different ﬁlter sizes may attenuate the effect of the tiled noise
since some convolution sizes might be less sensitive. We test this with a single random attack with
various numbers of tiles (cf. Figure 1, 2). We plotted additional graphs in supplementary material B.

7

100101102103104# of queries (in logarithmic scale)0.00.20.40.60.81.0Attack success rateCumulative success rate in terms  of required budget for untargeted attacksContinuous CMA 30Continuous CMA 50Continuous Cauchy (1+1) 30Continuous Cauchy (1+1) 50Continuous DiagonalCMA 30Continuous  DiagonalCMA 50Discrete DiagonalCMA 30Discrete DiagonalCMA 50102103104105# of queries (in logarithmic scale)0.00.20.40.60.81.0Attack success rateCumulative success rate in terms of required budget for targeted attacksContinuous CMA 50Continuous Cauchy (1+1) 50Continuous  DiagonalCMA 50Discrete DiagonalCMA 50Paper under review

Table 2: Comparison of our method with the parsimonious and bandits attacks in the targeted setting
on ImageNet on InceptionV3 pretrained network for  = 0.05 and 100, 000 as budget limit.

Method

Parsimonious
Bandits

DFOc − Cauchy(1 + 1)-ES
DFOc − DiagonalCMA
DFOc − CMA
DFOd − DiagonalCMA

# of tiles

-
50
50
50
50
50

7184
25341
9789
6768

6662

8957

Average queries Median queries

5116
18053
6049

3797

4692
4619

Success rate

100%
92.5%
83.2%
94.0%

100%

64.2%

4 .3 UN TARG ET ED ADV ER SAR IA L AT TACK S

We ﬁrst evaluate our attacks in the untargeted setting. The aim is to change the predicted label of the
classiﬁer. Following (Moon et al., 2019; Ilyas et al., 2018b), we use 10, 000 images that are initially
correctly classiﬁed and we limit the budget to 10, 000 queries. We experimented with 30 and 50 tiles
on the images. Only the best performing methods are reported in Table 1. We compare our results
with (Moon et al., 2019) and (Ilyas et al., 2018b) on InceptionV3 (cf. Table 1). We also plotted the
cumulative success rate in terms of required budget in Figure 3. We also evaluated our attacks for
smaller noise in supplementary material D. We achieve results outperforming or at least equal to the
state of the art in all cases. More remarkably, We improve by far the number of necessary queries
to fool the classiﬁers. The tiling trick partially explains why the average and the median number of
queries are low. Indeed, the ﬁrst queries of our evolution strategies is in general close to random
search and hence, according to the observation of Figs 1-2, the ﬁrst steps are more likely to fool the
network, which explains why the queries budget remains low. This Discrete strategies reach better
median numbers of queries - which is consistent as we directly search on the limits of the (cid:96)∞ -ball;
however, given the restricted search space (only corners of the search space are considered), the
success rate is lower and on average the number of queries increases due to hard cases.

4 .4 TARG E TED ADV ER SAR IA L ATTACK S

We also evaluate our methods in the targeted case on ImageNet dataset. We selected 1, 000 images,
correctly classiﬁed. Since the targeted task is harder than the untargeted case, we set the maximum
budget to 100, 000 queries, and  = 0.05. We uniformly chose the target class among the incorrect
ones. We evaluated our attacks in comparison with the bandits methods (Ilyas et al., 2018b) and the
parsimonious attack (Moon et al., 2019) on InceptionV3 classiﬁer. We also plotted the cumulative
success rate in terms of required budget in Figure 3. CMA-ES beats the state of the art on all criteria.
DiagonalCMA-ES obtains acceptable results but is less powerful that CMA-ES in this speciﬁc case.
The classical CMA optimizer is more precise, even if the run time is much longer. Cauchy (1 + 1)-
ES and discretized optimization reach good results, but when the task is more complicated they do
not reach as good results as the state of the art in black box targeted attacks.

4 .5 UN TARG ET ED AT TACK S AGA IN ST AN ADVER SAR IAL LY TRA IN ED NE TWORK

In this section, we experiment our attacks against a defended network by adversarial training (Good-
fellow et al., 2015). Since adversarial training is computationally expensive, we restricted ourselves
to the CIFAR10 dataset (Krizhevsky et al., 2009) for this experiment. Image size is 32 × 32 × 3.
We adversarially trained a WideResNet28x10 (Zagoruyko & Komodakis, 2016) with PGD (cid:96)∞ at-
tacks (Kurakin et al., 2016; Madry et al., 2018a) of norm 8/256 and 10 steps of size 2/256. In this
setting, we randomly selected 1, 000 images, and limited the budget to 20, 000 queries. We ran PGD
(cid:96)∞ attacks (Kurakin et al., 2016; Madry et al., 2018a) of norm 8/256 and 20 steps of size 1/256
against our network, and achieved a success rate up to 36%, which is the the state of the art in the
white box setting. We also compared our method to the Parsimonious and bandit attacks. Results
are reported in Appendix 6. On this task, the parsimonious attack method is slightly better than our
best approach.

8

Paper under review

5 CONC LU S ION

In this paper, we proposed a new framework for crafting black box adversarial attacks based on
derivative free optimization. Because of the high dimensionality and the characteristics of the prob-
lem (see Section 3.3.2), not all optimization strategies give satisfying results. However, combined
with the tiling trick, evolutionary strategies such as CMA, DiagonalCMA and Cauchy (1+1)-ES
beats the current state of the art in both targeted and untargeted settings. In particular, DFOc − CMA
improves the state of the art in terms of success rate in almost all settings. We also validated the
robustness of our attack against an adversarially trained network. Future work will be devoted to
better understanding the intriguing property of the effect that a neural network is not robust to a one
shot randomly tiled attack.

R E F ER ENC E S
Abdullah Al-Dujaili and Una-May O’Reilly. There are no bit parts for sign bits in black-box attacks.
arXiv preprint arXiv:1902.06894, 2019.

Alexandre Araujo, Rafael Pinot, Benjamin Negrevergne, Laurent Meunier, Yann Chevaleyre, Florian
Yger, and Jamal Atif. Robust neural networks using randomized adversarial training. arXiv
preprint arXiv:1903.10219, 2019.

Dirk V. Arnold and Hans-Georg Beyer. Performance analysis of evolutionary optimization with
cumulative step length adaptation. IEEE Trans. Automat. Contr., 49(4):617–622, 2004.

Irwan Bello, Barret Zoph, Vijay Vasudevan, and Quoc V. Le. Neural optimizer search with reinforce-
ment learning. In Proc. of the 34th International Conference on Machine Learning, volume 70 of
ICML’17, pp. 459–468, 2017.

Hans-Georg Beyer. The Theory of Evolution Strategies. Natural Computing Series. Springer, Hei-
deberg, 2001.

Battista Biggio, Giorgio Fumera, and Fabio Roli. Evade hard multiple classiﬁer systems. In Appli-
cations of Supervised and Unsupervised Ensemble Methods, pp. 15–38. Springer, 2009.
Battista Biggio, Igino Corona, Davide Maiorca, Blaine Nelson, Nedim ˇSrndi ´c, Pavel Laskov, Gior-
gio Giacinto, and Fabio Roli. Evasion attacks against machine learning at test time.
In Joint
European conference on machine learning and knowledge discovery in databases, pp. 387–402.
Springer, 2013.

Nicholas Carlini and David Wagner. Towards evaluating the robustness of neural networks. In 2017
IEEE Symposium on Security and Privacy (SP), pp. 39–57. IEEE, 2017.

Pin-Yu Chen, Huan Zhang, Yash Sharma, Jinfeng Yi, and Cho-Jui Hsieh. Zoo: Zeroth order opti-
mization based black-box attacks to deep neural networks without training substitute models. In
Proceedings of the 10th ACM Workshop on Artiﬁcial Intelligence and Security, pp. 15–26. ACM,
2017.

Pin-Yu Chen, Yash Sharma, Huan Zhang, Jinfeng Yi, and Cho-Jui Hsieh. Ead: elastic-net attacks
to deep neural networks via adversarial examples. In Thirty-second AAAI conference on artiﬁcial
intelligence, 2018.

Alexandre Chotard, Anne Auger, and Nikolaus Hansen. Cumulative step-size adaptation on linear
functions.
In Carlos A. Coello Coello, Vincenzo Cutello, Kalyanmoy Deb, Stephanie Forrest,
Giuseppe Nicosia, and Mario Pavone (eds.), Parallel Problem Solving from Nature - PPSN XII,
pp. 72–81, Berlin, Heidelberg, 2012. Springer Berlin Heidelberg. ISBN 978-3-642-32937-1.

Jeremy M. Cohen, Elan Rosenfeld, and J. Zico Kolter. Certiﬁed adversarial robustness via ran-
domized smoothing. CoRR, abs/1902.02918, 2019. URL http://arxiv.org/abs/1902.

02918.

J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei. ImageNet: A Large-Scale Hierarchical
Image Database. In CVPR09, 2009.

9

Paper under review

Alhussein Fawzi, Seyed-Mohsen Moosavi-Dezfooli, and Pascal Frossard. Robustness of classiﬁers:
from adversarial to random noise. In Advances in Neural Information Processing Systems, pp.
1632–1640, 2016.

Ian Goodfellow, Jonathon Shlens, and Christian Szegedy. Explaining and harnessing adversarial
examples. In International Conference on Learning Representations, 2015.

N. Hansen and A. Ostermeier. Completely derandomized self-adaptation in evolution strategies.
Evolutionary Computation, 11(1), 2003.

Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recog-
nition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp.
770–778, 2016.

Andrew Ilyas, Logan Engstrom, Anish Athalye, and Jessy Lin. Black-box adversarial attacks with
limited queries and information. arXiv preprint arXiv:1804.08598, 2018a.

Andrew Ilyas, Logan Engstrom, and Aleksander Madry. Prior convictions: Black-box adversarial
attacks with bandits and priors. arXiv preprint arXiv:1807.07978, 2018b.

Alex Krizhevsky, Vinod Nair, and Geoffrey Hinton. Cifar-10 and cifar-100 datasets. URl:
https://www. cs. toronto. edu/kriz/cifar. html, 6, 2009.

Alexey Kurakin, Ian Goodfellow, and Samy Bengio. Adversarial examples in the physical world.
arXiv preprint arXiv:1607.02533, 2016.

M. Lecuyer, V. Atlidakis, R. Geambasu, D. Hsu, and S. Jana. Certiﬁed robustness to adversarial
examples with differential privacy. In 2019 IEEE Symposium on Security and Privacy (SP), pp.
727–743, 2018.

Sijia Liu, Pin-Yu Chen, Xiangyi Chen, and Mingyi Hong. signSGD via zeroth-order oracle.
In
International Conference on Learning Representations, 2019. URL https://openreview.

net/forum?id=BJe-DsC5Fm.

Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and Adrian Vladu.
Towards deep learning models resistant to adversarial attacks. In International Conference on
Learning Representations, 2018a.

Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and Adrian Vladu.
Towards deep learning models resistant to adversarial attacks. In International Conference on
Learning Representations, 2018b.

J. Matyas. Random optimization. Automation and Remote control, 26:246–253, 1965.

Dongyu Meng and Hao Chen. Magnet: a two-pronged defense against adversarial examples. In
Proceedings of the 2017 ACM SIGSAC Conference on Computer and Communications Security,
pp. 135–147. ACM, 2017.

Seungyong Moon, Gaon An, and Hyun Oh Song. Parsimonious black-box adversarial attacks via
efﬁcient combinatorial optimization. In Kamalika Chaudhuri and Ruslan Salakhutdinov (eds.),
Proceedings of the 36th International Conference on Machine Learning, volume 97 of Proceed-
ings of Machine Learning Research, pp. 4636–4645, Long Beach, California, USA, 09–15 Jun

2019. PMLR. URL http://proceedings.mlr.press/v97/moon19a.html.

Seyed-Mohsen Moosavi-Dezfooli, Alhussein Fawzi, and Pascal Frossard. Deepfool: a simple and
accurate method to fool deep neural networks. In Proceedings of the IEEE Conference on Com-
puter Vision and Pattern Recognition, pp. 2574–2582, 2016.

Seyed-Mohsen Moosavi-Dezfooli, Alhussein Fawzi, Omar Fawzi, and Pascal Frossard. Universal
adversarial perturbations. In 2017 IEEE Conference on Computer Vision and Pattern Recognition
(CVPR), pp. 86–94. Ieee, 2017.

Yann Ollivier, Ludovic Arnold, Anne Auger, and Nikolaus Hansen. Information-geometric opti-
mization algorithms: A unifying picture via invariance principles. J. Mach. Learn. Res., 18:

18:1–18:65, 2017. URL http://jmlr.org/papers/v18/14-467.html.

10

Paper under review

Nicolas Papernot, Patrick McDaniel, and Ian Goodfellow. Transferability in machine learning: from
phenomena to black-box attacks using adversarial samples. arXiv preprint arXiv:1605.07277,
2016a.

Nicolas Papernot, Patrick McDaniel, Somesh Jha, Matt Fredrikson, Z Berkay Celik, and Ananthram
Swami. The limitations of deep learning in adversarial settings. In Security and Privacy (Eu-
roS&P), 2016 IEEE European Symposium on, pp. 372–387. IEEE, 2016b.

Nicolas Papernot, Patrick McDaniel, Ian Goodfellow, Somesh Jha, Z. Berkay Celik, and Ananthram
Swami. Practical black-box attacks against machine learning. In Proceedings of the 2017 ACM
on Asia Conference on Computer and Communications Security, ASIA CCS ’17, pp. 506–519,
New York, NY, USA, 2017. ACM. ISBN 978-1-4503-4944-4. doi: 10.1145/3052973.3053009.

URL http://doi.acm.org/10.1145/3052973.3053009.

Adam Paszke, Sam Gross, Soumith Chintala, Gregory Chanan, Edward Yang, Zachary DeVito,
Zeming Lin, Alban Desmaison, Luca Antiga, and Adam Lerer. Automatic differentiation in
pytorch. 2017.

Rafael Pinot, Laurent Meunier, Alexandre Araujo, Hisashi Kashima, Florian Yger, C ´edric Gouy-
Pailler, and Jamal Atif. Theoretical evidence for adversarial robustness through randomization:
the case of the exponential family. arXiv preprint arXiv:1902.01148, 2019.

J. Rapin and O. Teytaud. Nevergrad - A gradient-free optimization platform. https://GitHub.

com/FacebookResearch/Nevergrad, 2018.

I. Rechenberg. Evolutionstrategie: Optimierung Technischer Systeme nach Prinzipien des Biologis-
chen Evolution. Fromman-Holzboog Verlag, Stuttgart, 1973.

Raymond Ros and Nikolaus Hansen. A simple modiﬁcation in cma-es achieving linear time and
space complexity. In G ¨unter Rudolph, Thomas Jansen, Nicola Beume, Simon Lucas, and Carlo
Poloni (eds.), Parallel Problem Solving from Nature – PPSN X, pp. 296–305, Berlin, Heidelberg,
2008. Springer Berlin Heidelberg. ISBN 978-3-540-87700-4.

Pouya Samangouei, Maya Kabkab, and Rama Chellappa. Defense-GAN: Protecting classiﬁers
against adversarial attacks using generative models.
In International Conference on Learning
Representations, 2018.

M. Schumer and K. Steiglitz. Adaptive step size random search. Automatic Control, IEEE Transac-
tions on, 13:270–276, 1968.

Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image
recognition. arXiv preprint arXiv:1409.1556, 2014.

Aman Sinha, Hongseok Namkoong, and John Duchi. Certifying some distributional robustness with
principled adversarial training. arXiv preprint arXiv:1710.10571, 2017.

Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan, Ian Goodfel-
low, and Rob Fergus. Intriguing properties of neural networks. In International Conference on
Learning Representations, 2014.

Christian Szegedy, Sergey Ioffe, Vincent Vanhoucke, and Alexander A Alemi.
Inception-v4,
inception-resnet and the impact of residual connections on learning. In Thirty-First AAAI Confer-
ence on Artiﬁcial Intelligence, 2017.

Chun-Chen Tu, Paishun Ting, Pin-Yu Chen, Sijia Liu, Huan Zhang, Jinfeng Yi, Cho-Jui Hsieh, and
Shin-Ming Cheng. Autozoom: Autoencoder-based zeroth order optimization method for attack-
ing black-box neural networks. In Proceedings of the AAAI Conference on Artiﬁcial Intelligence,
volume 33, pp. 742–749, 2019.

Daan Wierstra, Tom Schaul, Tobias Glasmachers, Yi Sun, Jan Peters, and J ¨urgen Schmidhuber.
Natural evolution strategies. The Journal of Machine Learning Research, 15(1):949–980, 2014.

Cihang Xie, Jianyu Wang, Zhishuai Zhang, Zhou Ren, and Alan Yuille. Mitigating adversarial
effects through randomization. In International Conference on Learning Representations, 2018.

11

Paper under review

Xin Yao and Yong Liu. Fast evolutionary programming. In Proceedings of the Fifth Annual Confer-
ence on Evolutionary Programming, pp. 451–460. MIT Press, 1996.

Sergey Zagoruyko and Nikos Komodakis. Wide residual networks. In Edwin R. Hancock Richard
C. Wilson and William A. P. Smith (eds.), Proceedings of the British Machine Vision Conference
(BMVC), pp. 87.1–87.12. BMVA Press, September 2016. ISBN 1-901725-59-6. doi: 10.5244/C.
30.87.

12

Paper under review

A A LGOR I THM S

A .1 TH E (1+1 ) -ES A LGOR I THM

Algorithm 1 The (1 + 1) Evolution Strategy.
Require: Function f : Rd → R to minimize

m ← 0, C ← Id , σ ← 1
for t = 1...n do

m ← m(cid:48) , σ ← 2σ
σ ← 2− 1
4 σ

else

end if
end for

A .2 CMA -ES A LGOR I THM

(Generate candidates)
Generate m(cid:48) ∼ m + σX where X is sampled from a Cauchy or Gaussian distribution.
if f (m(cid:48) ) ≤ f (m) then

Algorithm 2 CMA-ES algorithm. The T subscript denotes transposition.
Require: Function f : Rd → R to minimize, parameters b, c, w1 > . . . , wµ > 0, pc and others as
in e.g. (Hansen & Ostermeier, 2003).

m ← 0, C ← Id , σ ← 1
for t = 1...n do
Generate x1 , ..., xλ ∼ m + σN (0, C ).

Deﬁne x(cid:48)
Update the cumulation for C : pc ← cumulation of pc , overall direction of progress.
i the ith best of the xi .
Update the covariance matrix:

C ← (1 − c) C(cid:124)(cid:123)(cid:122)(cid:125)

inertia

+

c
b

(pc × pT
c )

(cid:123)(cid:122)

(cid:124)

(cid:125)

overall direction

+c(1 − 1
b

)

µ(cid:88)

i=1

wi

x(cid:48)

i − m
σ

(cid:124)

× (x(cid:48)

(cid:123)(cid:122)

i − m)T
σ

(cid:125)

“covariance” of the 1

σ x(cid:48)

i

Update mean:

m ← µ(cid:88)

i=1

wixi:λ

Update σ by cumulative step-size adaptation (Arnold & Beyer, 2004).

end for

13

Paper under review

B ADD I T IONAL P LOT S FOR THE T I L ING TR ICK

Figure 4: Random attack success rate against InceptionV3 (left), ResNet50 (center), VGG16bn
(right) for different noise intensities. We just randomly draw one tiled attack and check if it is
successful.

Figure 5: Random attack success rate for different noise intensities  ∈ {0.01, 0.03, 0.05, 0.1} (from
right to left) against different architectures. We just randomly draw one tiled attack and check if it
is successful.

C R E SU LT S W I TH “CAR L IN I&WAGN ER ” LO S S

In this section, we follow the same experimental setup as in Section 4.3, but we built our attacks
with the “Carlini&Wagner” loss instead of the cross entropy. We remark the results are comparable
and similar.

Table 3: Comparison of our method with“Carlini&Wagner” loss versus the parsimonious and bandits
attacks in the untargeted setting on InceptionV3 pretrained network for  = 0.05 and 10, 000 as
budget limit.

# of tiles Average queries Median queries

Method

DFOc − Cauchy(1 + 1)-ES
DFOc − Cauchy(1 + 1)-ES
DFOc − DiagonalCMA
DFOc − DiagonalCMA
DFOc − CMA
DFOc − CMA
DFOd − DiagonalCMA
DFOd − DiagonalCMA

30
50
30
50
30
50
30
50

353

347

483
528
475
491
482
510

57
63
167
181
225
246

27

37

Success rate

97.2%
98.8%
98.8%
99.2%
99.2%

99.4%

98.0%
98.0%

14

050100150200Number of tiles0.00.10.20.30.40.50.6Attack success rateAttack success for InceptionV3 architecture  with different noise intensities versus the number of tiles=0.01=0.03=0.05=0.1050100150200Number of tiles0.00.20.40.60.8Attack success rateAttack success for ResNet50 architecture  with different noise intensities versus the number of tiles=0.01=0.03=0.05=0.1050100150200Number of tiles0.00.20.40.60.8Attack success rateAttack success for VGG16bn architecture  with different noise intensities versus the number of tiles=0.01=0.03=0.05=0.1050100150200Number of tiles0.010.020.030.040.05Attack success rateAttack success for =0.01 with different  architetures versus the number of tilesResNet50InceptionV3VGG16bn050100150200Number of tiles0.0250.0500.0750.1000.1250.1500.1750.200Attack success rateAttack success for =0.03 with different  architetures versus the number of tilesResNet50InceptionV3VGG16bn050100150200Number of tiles0.00.10.20.30.40.5Attack success rateAttack success for =0.05 with different  architetures versus the number of tilesResNet50InceptionV3VGG16bn050100150200Number of tiles0.00.20.40.60.8Attack success rateAttack success for =0.1 with different  architetures versus the number of tilesResNet50InceptionV3VGG16bnPaper under review

D UN TARG ET ED AT TACK S W I TH SMAL LER NO I S E IN TEN S I T I E S
We evaluated our method on smaller noise intensities ( ∈ {0.01, 0.03, 0.05}) in the untargeted
setting on ImageNet dataset. In this framework, we also picked up randomly 10, 000 images and
limited our budget to 10, 000 queries. We compared to the bandits method (Ilyas et al., 2018b) and
to the parsimonious attack (Moon et al., 2019) on InceptionV3 network. We limited our experiments
to a number of tiles of 50. We report our results in Table 4. We remark our attacks reach state of the
art for  = 0.03 and  = 0.05 both in terms of success rate and queries budget. For  = 0.01, we
reach results comparable to the state of the art.

Table 4: Results of our method compared to the parsimonious and bandit attacks in the un-
targeted setting on InceptionV3 pretrained network for different values of noise intensities  ∈
{0.01, 0.03, 0.05} and a maximum of 10, 000 queries.

Method

Parsimonious
Bandits

# of tiles Avg. queries Med. queries

Success rate



0.05

0.03

0.01

DFOc − Cauchy(1 + 1)-ES
DFOc − DiagonalCMA
DFOc − CMA
DFOd − DiagonalCMA

Parsimonious
Bandits

DFOc − Cauchy(1 + 1)-ES
DFOc − DiagonalCMA
DFOc − CMA
DFOd − DiagonalCMA

Parsimonious
Bandits

DFOc − Cauchy(1 + 1)-ES
DFOc − DiagonalCMA
DFOc − CMA
DFOd − DiagonalCMA

-
50
50
50
50
50
-
50
50
50
50
50
-
50
50
50
50
50

722
995
510
623
630

485

1104
1376
846
971
911

799

2104
2018
1668
1958
1921

1188

237
249
63
191
259

38

392
466

203

429
404
293
1174
992

751

1175
1107
849

98.5%
95.1%
97.3%
98.7%

99.2%

97.4%
95.7%
92.7%
93,2%
96,5%

96.7%

94,1%
80.3%
72.9%
72,1%
79.2%

80.4%

71,3%

15

Paper under review

E UN TARG ET ED AT TACK S AGA IN ST OTHER ARCH I TEC TUR E S

We also evaluated our method on different neural networks architectures. For each network we
randomly selected 10, 000 images that were correctly classiﬁed. We limit our budget to 10, 000
queries and set the number of tiles to 50. We achieve a success attack rate up to 100% on every
classiﬁer with a budget as low as 8 median queries for the VGG16bn for instance (see Table 5). One
should notice that the performances are lower on InceptionV3 as it is also reported for the bandit
methods in (Ilyas et al., 2018b). This possibly due to the fact that the tiling trick is less relevant on
the Inception network than on the other networks (see Fig. 2).

Table 5: Comparison of our method on the ImageNet dataset with InceptionV3 (I), ResNet50 (R)
and VGG16bn (V) for  = 0.05 and 10, 000 as budget limit.

Avg queries

Med. queries

Method

Tile size

DFOc − Cauchy(1 + 1)-ES
DFOc − Cauchy(1 + 1)-ES
DFOc − DiagonalCMA
DFOc − DiagonalCMA
DFOc − CMA
DFOc − CMA
DFOd − DiagonalCMA
DFOd − DiagonalCMA

30
50
30
50
30
50
50
30

I
466
510
533
623
588
630
485

424

R

163

218
263
373
256
270
617
417

V
86

67

174
227
176
219
345
211

I
60
63
189
191
232
259
38

20

R

19

32
95
121
138
143
62
20

V
8
4
55
71
72
107
6

2

Succ. Rate

I
R
V
95.2% 99.6% 100%
97.3% 99.6% 99.7%
97.2% 99.0% 99.9%
98.7% 99.9% 100%
98.9% 99.9% 99.9%

99.2% 100% 99.9%

97.4% 99.2% 99.6%
97.7% 98.8% 99.5%

16

Paper under review

F TABL E FOR ATTACK S AGA IN ST ADVER SAR IAL LY TRAN IN ED N E TWORK

Table 6: Adversarial attacks against an adversarially trained WideResnet28x10 network on CI-
FAR10 dataset for  = 0.03125 and 20, 000 as budget limit.

Average queries Median queries

Method

PGD (not black-box)
Parsimonious
Bandits
Bandits
Bandits

DFOc − Cauchy(1 + 1)-ES
DFOc − Cauchy(1 + 1)-ES
DFOc − Cauchy(1 + 1)-ES
DFOc − DiagonalCMA
DFOc − DiagonalCMA
DFOc − DiagonalCMA
DFOc − CMA
DFOc − CMA
DFOc − CMA

# of tiles

-
-
10
20
32
10
20
32
10
20
32
10
20
32

20
1130
1429
1802
1993
429
902
1865
395
624
1379

363

1676
2311

20
450
530
798
812

60

93
764
85
151
860
156
740
1191

Success rate

36%

42%

29.1%
33.8%
34.8%
29.5%
30.5%
31.7%
30.5%
31.3%
34.7%
30.4%
40.2%
40.2%

17

Paper under review

G FA I L ING M E THOD S

In this section, we compare our attacks to other optimization strategies. We run our experiments
in the same setup as in Section 4.3. Results are reported in Table 7. DE and Normal (1+1)-ES
performs poorly, probably because these optimization strategies converge slower when the optima
are at “inﬁnity”. We reformulate this sentence accordingly in the updated version of the paper.
Finally, as the initialization of Powell is linear with the dimension and with less variance, it performs
poorer than simple random search. Newuoa, SQP and Cobyla algorithms have also been tried on
a smaller number images (we did not report the results), but their initialization is also linear in the
dimension, so they reach very poor results too.

Table 7: Comparison with other DFO optimization strategies in the untargeted setting on ImageNet
dataset InceptionV3 pretrained network for  = 0.05 and 10, 000 as budget limit.

# of tiles Average queries Median queries

60
63
189
191
232
259
159
149
45
66
6
5
5332
4076

Success rate

95.2%
97.3%
97.2%
98.7%
98.9%
99.2%
78.8%
76.0%
87.6%
92.8%
37.9%
38.2%
14.4%
7.3%

Method

DFOc − Cauchy(1 + 1)-ES
DFOc − Cauchy(1 + 1)-ES
DFOc − DiagonalCMA
DFOc − DiagonalCMA
DFOc − CMA
DFOc − CMA
DFOc − DE
DFOc − DE
DFOc − Normal(1 + 1)-ES
DFOc − Normal(1 + 1)-ES
DFOc − RandomSearch
DFOc − RandomSearch
DFOc − Powell
DFOc − Powell

30
50
30
50
30
50
30
50
30
50
30
50
30
50

466
510
533
623
589
630
756
699
581
661
568
527
4889
4578

18

