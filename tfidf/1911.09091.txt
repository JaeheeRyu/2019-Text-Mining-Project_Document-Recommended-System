Tool-Supported Experiments for Continuously Collecting Data of
Subjective Video Quality Assessments During Video Playback

Oliver Karras, Jil Kl¨under, and Kurt Schneider

Leibniz Universit¨at Hannover, Software Engineering Group, 30167 Hannover, Germany
Email: {oliver.karras, jil.kluender, kurt.schneider}@inf.uni-hannover.de

9
1
0
2

v
o

N

1
2

]

E

S

.

s

c

[

2
v
1
9
0
9
0

.

1
1
9
1

:

v

i

X

r

a

1 Introduction

The adequate use of documentation for communica-
tion is one challenge in requirements engineering (RE)
[4]. In recent years, several researchers [3, 5, 8] ad-
dressed this challenge by using videos as a communica-
tion mechanism. All of them concluded that this way
of using videos has the potential to facilitate require-
ments communication. Nevertheless, software profes-
sionals are not directors and thus do not necessarily
know what constitutes a good video [6]. This lack of
knowledge is one crucial reason why videos are still
not an established communication mechanism in RE
[6, 8]. When videos shall be established in the RE
activities, practices, and techniques, requirements en-
gineers have to acquire the necessary knowledge to
produce and use good videos on their own at moder-
ate costs, yet suﬃcient quality.
In our research project ViViReq (see Acknowledg-
ment), we aspire to bridge this knowledge gap about
what constitutes a good video. Whether a video is
good or not depends on its quality perceived by its
viewers [6]. However, video quality is a rather ill-
deﬁned concept due to numerous unspeciﬁed techni-
cal and subjective characteristics [9]. As part of our
research plan, we develop a quality model for videos
[7] inspired by the idea of Femmer and Vogelsang [2]
to deﬁne and evaluate the quality of videos as RE ar-
tifacts. In addition to evaluating videos, this quality
model can be used to identify the relevant character-
istics of videos for their speciﬁc purpose which can be
further used to specify requirements, their criteria for
satisfaction, and corresponding measures. Therefore,
software professionals may use the quality model as
guidance for producing and using videos.

2 Background and Problem

Based on our quality model [7], we want to analyze
how speciﬁc implementations of single video char-
acteristics, e.g., pleasure, exactly aﬀect the viewers’
quality assessment. Subjective video quality assess-
ment (VQA) is the most accurate and reliable practice
to determine video quality [1]. Experiments are an es-
tablished way for subjective VQAs since a controlled
environment is necessary to avoid potential disruptive
factors, e.g., varying viewing conditions [1]. In these

experiments, 15 to 30 subjects individually watch one
or more videos and assess the quality of each video di-
rectly after the video was completely watched. Each
subject assesses his perception of the video quality by
assigning a single value on a deﬁned 5- or 7-level scale
to each video. For each video, the average of the sub-
jects’ assessments is calculated. This value is deﬁned
as the Mean Opinion Score (MOS) which indicates the
perceived overall quality of the respective video from
the subjects’ point of view [9].
The established subjective VQA methods (cf. [1])
ultimately determine video quality based on the MOS.
Therefore, these methods only relate to an entire
video, so that it is not possible to determine exactly
which particular parts of a video had a particularly
positive or negative impact on a subject. Thus, the es-
tablished subjective VQA methods do not lend them-
selves to detailed analyses of videos to determine how
speciﬁc implementations of single video characteristics
exactly aﬀect the viewers’ quality assessment.

3 Solution: A Research Preview

Considering the established subjective VQA methods,
our intended detailed analysis of videos is not possible.
Thus, we propose to support the conduct of experi-
ments for subjective VQAs with a software tool which
allows continuously collecting the assessment data for
generic video characteristics during video playback.
The proposed solution consists of ﬁve key concepts
which are brieﬂy explained below. Figure 1 shows the
implementation of three concepts in our prototype.
Management of Experiments. The software tool
supports the management of several experiments and
their associated subjects to make it easier for the ex-
perimenter to handle the respective data sets.
Customizable Input Method. Figure 1a shows
the creation of an experiment. Each experiment in-
cludes a name, the video to be examined, and a cus-
tomizable input method for the continuous data col-
lection during video playback. At the moment, the
software tool supports a slider as well as two up to
ten radio buttons as an input method. These input
methods are completely customizable concerning the
labels, the scale, and the number of radio buttons.

 
 
 
 
 
 
(b) Continuous data collection

(a) Customizable input method

(c) Immediate data visualization

Figure 1: Screenshots of three concepts of the prototype

Continuous Data Collection. Figure 1b presents
the continuous data collection.
In this case, a sub-
ject can continuously assess his current perception of
the quality characteristic to be examined during video
playback by simply moving the slider.
Immediate Data Visualization. After data col-
lection, the experimenter can examine the collected
data of all subjects together and each subject individ-
ually (see Figure 1c).
In this way, an experimenter
can get an overview of the collected data and analyze
it immediately with the subject. Therefore, the exper-
imenter can gain deeper insights into what particular
parts of the video aﬀected the subject’s assessments.
Data Export. An experimenter can export all
collected data as CSV ﬁles for later analysis.

The results of a ﬁrst evaluation indicated that the
subjects did not feel distracted by the continuous data
collection and preferred the slider as input method.

4 Conclusion

Already a single very good or very bad implemented
characteristic of a RE artifact, e.g., a video, can signif-
icantly aﬀect its overall assessment by a stakeholder.
In this paper, we propose ﬁve key concepts for a soft-
ware tool that is intended to enable detailed analyses
of videos which are currently not supported by estab-
lished subjective VQA methods.
First, we assume that the software tool may be
beneﬁcial for other researchers and practitioners who
also deal with videos and their assessments due to
its ﬂexibility. Second, our intended use of such a
detailed analysis of videos is to provide ﬁne-grained
insights into the interrelationships of the speciﬁc im-
plementation of a video characteristic and its impact
on the viewers’ quality assessment. Based on these
insights, we expect to be able to derive recommen-

dations about what characteristics constitute a good
video and how these characteristics should be imple-
mented in a video. These recommendations are im-
portant to operationalize the quality model. They
enhance the practical utility of the quality model by
revealing how the single video characteristics can be
useful in practice. Based on our quality model and
the derived recommendations, we suppose to provide
software professionals with the knowledge necessary
to produce and use good videos in RE on their own
at moderate costs and with suﬃcient quality.

Acknowledgment

This work was supported by the Deutsche Forschungs-
gemeinschaft (DFG) under Grant No.: 289386339,
(2017 – 2019).

References

[1] Akramullah, S. : Digital Video Concepts, Methods, and Met-
rics: Quality, Compression, Performance, and Power Trade-
oﬀ Analysis. Apress, 2014
[2] Femmer, H. ; Vogelsang, A. : Requirements Quality is Quality
in Use. In: IEEE Software 36 (2018), Nr. 3, S. 83–91
[3] Fricker, S. A. ; Schneider, K. ; Fotrousi, F. ; Thuemmler,
C. : Workshop Videos for Requirements Communication. In:
Requirements Engineering 21 (2016), Nr. 4, S. 521–552
[4] Karras, O. : Software Professionals’ Attitudes Towards Video
as a Medium in Requirements Engineering. In: International
Conference on Product-Focused Software Process Improve-
ment Springer, 2018, S. 150–158
[5] Karras, O. ; Hamadeh, A. ; Schneider, K. : Enriching Require-
ments Speciﬁcations with Videos – The Use of Videos to Sup-
port Requirements Communication. In: GI Softwaretechnik-
Trends 38 (2017), Nr. 1, S. 51–52
[6] Karras, O. ; Schneider, K. : Software Professionals are Not
Directors: What Constitutes a Good Video?
In: 2018 1st
International Workshop on Learning from other Disciplines
for Requirements Engineering (D4RE) IEEE, 2018, S. 18–21
[7] Karras, O. ; Schneider, K. ; Fricker, S. A.: Represent-
ing Software Project Vision by Means of Video: A Quality
Model for Vision Videos.
In: Journal of Systems and Soft-

ware (2019). http://dx.doi.org/10.1016/j.jss.2019.110479. –
DOI 10.1016/j.jss.2019.110479
[8] Rupp, C. ; Dovica, J. ; Bezold, M. ; Wiener, M. ; Fryer, B. :
Pimp my Spec: Tuning von Speziﬁkationen durch Videos. In:
GI Softwaretechnik-Trends 39 (2018), Nr. 1, S. 15–16
[9] Winkler, S. ; Mohandas, P. : The Evolution of Video Qual-
ity Measurement: From PSNR to Hybrid Metrics. In: IEEE
Transactions on Broadcasting 54 (2008), Nr. 3, S. 660–668

