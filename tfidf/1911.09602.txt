L EARN ING H I ERARCH ICA L D I SCR ET E L INGU I S T IC
UN I T S FROM V I SUA LLY-GROUND ED S P EECH

David Harwath∗, Wei-Ning Hsu∗ , and James Glass

Computer Science and Artiﬁcial Intelligence Lab
Massachusetts Institute of Technology
Cambridge, MA 02139, USA

{dharwath,wnhsu,glass}@csail.mit.edu

9
1
0
2

v
o

N

1
2

]

L

C

.

s

c

[

1
v
2
0
6
9
0

.

1
1
9
1

:

v

i

X

r

a

AB STRAC T

In this paper, we present a method for learning discrete linguistic units by incorpo-
rating vector quantization layers into neural models of visually grounded speech.
We show that our method is capable of capturing both word-level and sub-word
units, depending on how it is conﬁgured. What differentiates this paper from prior
work on speech unit learning is the choice of training objective. Rather than us-
ing a reconstruction-based loss, we use a discriminative, multimodal grounding
objective which forces the learned units to be useful for semantic image retrieval.
We evaluate the sub-word units on the ZeroSpeech 2019 challenge, achieving a
27.3% reduction in ABX error rate over the top-performing submission, while
keeping the bitrate approximately the same. We also present experiments demon-
strating the noise robustness of these units. Finally, we show that a model with
multiple quantizers can simultaneously learn phone-like detectors at a lower layer
and word-like detectors at a higher layer. We show that these detectors are highly
accurate, discovering 279 words with an F1 score of greater than 0.5.

1

IN TRODUC T ION

By 8 months of age, human infants learn to recognize not only the names of their caregivers and
common objects, but also the contrast between the different vowels and consonants which comprise
these words (Dupoux, 2018). Nearly all toddlers learn to carry a conversation long before they can
read and write. Humans learn to model the discrete, hierarchical, and compositional nature of their
native language not from written text, but from speech audio - a continuous, time-varying waveform
which is the product not only of the underlying words which were spoken, but also the physical
properties of the speaker’s vocal tract, the speaker’s health and emotional state, and the noise and
reverberation present in the environment. The question of how such a complex symbolic system is
inferred from continuous and noisy sensory input data is of interest not only to the cognitive science
community, but also to machine learning researchers who aim to reproduce this ability with comput-
ers. A more comprehensive understanding of human language acquisition has practical signiﬁcance
in real-world applications, such as automatic speech recognition (ASR) and natural language under-
standing (NLU) systems. In the past several decades, enormous progress has been made in speech
recognition research, and nowadays ASR systems are able to achieve human-level accuracy in many
domains (Chiu et al., 2018). Unfortunately, the techniques that have been developed to achieve these
levels of performance are extremely data-hungry, requiring many thousands of hours of speech au-
dio recordings for training. Since supervised machine learning algorithms form the basis of ASR
training, the data also needs to be annotated by expert humans. Due to the immense cost of collect-
ing and annotating speech data, ASR technology currently exists for approximately 120 (Google,
2019) out of the nearly 7,000 (Lewis et al., 2016) human languages spoken worldwide. It is highly
unlikely that purely supervised machine learning techniques will be able to scale to include all hu-
man languages, necessitating the development of alternative methods by researchers which are able
to function with far fewer annotations, or even no annotations at all.

∗Equal contribution

1

 
 
 
 
 
 
Because human beings provide an existence proof of language acquisition from speech completely
without language supervision, it is plausible that this ability could be replicated by a machine learn-
ing algorithm.
In this paper, we present a method for discovering discrete and hierarchical representations of speech
units both at the sub-word level and the word level. The key difference between our work and
previously proposed linguistic unit discovery methods is the fact that we employ a discriminative,
visual-semantic grounding objective rather than a signal reconstruction objective. This forces our
models to learn representations which capture semantic information at the output layer of the net-
work. Because semantics are predominantly carried by words, and words are composed of sub-word
units (such as phones and syllables), the visual grounding objective indirectly forces the model to
learn speaker- and noise-invariant representations of speech units. By incorporating trainable quan-
tization layers into our networks, we are able to capture these units in discrete inventories. Whether
these units correspond to word-like or sub-word units depends on where the quantization layers are
inserted, and how they are trained.

2 R ELAT ED WORK

Prior work on unsupervised modeling of the speech signal has generally focused on learning repre-
sentations which either disentangle or isolate the latent factors that are of interest for downstream
tasks. In most cases the primary latent factor of interest is the phonetic or lexical identity of a given
segment of speech, but other factors, such as the identity of the speaker, are sometimes of interest
as well. Because the factors of interest are often inherently discrete (e.g. words and phones), many
of the proposed approaches attempt to perform segmentation and clustering of the surface features
in one way or another. One family of techniques is based upon Segmental Dynamic Time Warping
(S-DTW) (Park & Glass, 2005; 2008; Jansen et al., 2010; Jansen & Durme, 2011), which uses a self-
comparison algorithm to identify relatively long duration (on the order of a second) patterns which
frequently reoccur in a speech corpus; these patterns tend to capture words or short phrases. A dif-
ferent line of work employs probabilistic graphical models to jointly segment and cluster the speech
signal (Varadarajan et al., 2008; Zhang & Glass, 2009; Gish et al., 2009; Lee & Glass, 2012; Siu
et al., 2014; Lee et al., 2015; Ondel et al., 2016; Kamper et al., 2016; 2017a). With an appropriately
designed model, it is possible to learn multiple, hierarchical categories of speech units. However,
in order to enable efﬁcient inference, the conditional distributions of these models tend to be simple
and therefore have limited modeling power.
Deep neural network models have been successfully used to learn powerful speech representations
using weakly or unsupervised objectives (Thiolliere et al., 2015; Kamper et al., 2015; Hsu et al.,
2017a;b; Hsu & Glass, 2018; Holzenberger et al., 2018; Milde & Biemann, 2018; van den Oord
et al., 2018; Chung et al., 2019; Pascual et al., 2019). These representations have predominantly
been continuous in nature, as discrete latent variables are not trivially compatible with backpropaga-
tion. To obtain discrete representations, a post-hoc clustering step can be applied to the continuous
representations (Kamper et al., 2017b; Feng et al., 2019). More recently, several papers have pro-
posed ways of directly incorporating discrete variables into neural network models, including using
Gumbel-Softmax (Eloff et al., 2019b) or straight-through estimators (van den Oord et al., 2017;
Chorowski et al., 2019; Razavi et al., 2019).
A different method for learning meaningful representations of speech is via a multimodal ground-
ing objective, which encourages the learning of speech representations that are predictive of the
contextual information contained in a separate but accompanying modality, such as vision. Visual
grounding of speech is a form of self-supervised learning (Virginia de Sa, 1994), which is powerful
in part because it offers a way of training models with a discriminative objective that does not depend
on traditional transcriptions or annotations. The ﬁrst work in this direction relied on phone strings
to represent the speech (Roy & Pentland, 2002; Roy, 2003), but more recently this learning has been
shown to be possible directly on the speech signal (Synnaeve et al., 2014; Harwath & Glass, 2015;
Harwath et al., 2016). Subsequent work on visually-grounded models of speech has investigated
improvements and alternatives to the modeling or training algorithms (Leidal et al., 2017; Kamper
et al., 2017c; Havard et al., 2019a; Merkx et al., 2019; Chrupała et al., 2017; Scharenborg et al.,
2018; Kamper et al., 2019b;a; Sur´ıs et al., 2019; Ilharco et al., 2019; Eloff et al., 2019a), application
to multilingual settings (Harwath et al., 2018a; Kamper & Roth, 2017; Azuh et al., 2019; Havard

2

et al., 2019a), analysis of the linguistic abstractions, such as words and phones, which are learned by
the models (Harwath & Glass, 2017; Harwath et al., 2018b; Drexler & Glass, 2017; Alishahi et al.,
2017; Harwath et al., 2019; Harwath & Glass, 2019; Havard et al., 2019b), and the impact of jointly
training with textual input (Holzenberger et al., 2019; Chrupała, 2019; Pasad et al., 2019). Repre-
sentations learned by models of visually grounded speech are also well-suited for transfer learning
to supervised tasks, being highly robust to noise and domain shift (Hsu et al., 2019).

3 DATA AND MODE L S

3 .1 DATA S ET

For training our models, we utilize the MIT Places 205 dataset (Zhou et al., 2014) and their ac-
companying spoken audio captions (Harwath et al., 2016; 2018b). The caption dataset contains
approximately 400,000 spoken audio captions, each of which describes a different Places image.
These captions are free-form spontaneous speech, collected from over 2,500 different speakers and
covering a 40,000 word vocabulary. The average caption duration is approximately 10 seconds,
and each caption contains on average 20 words. For vetting our models during training, we use a
held-out validation set of 1,000 image-caption pairs.

3 .2 N EURA L MODE L S O F V I SUA LLY-GROUND ED S PE ECH

We base our model upon the Residual Deep Audio-Visual Embedding network (ResDAVEnet) ar-
chitecture (Harwath et al., 2019), which contains two branches of fully convolutional networks, one
for images and the other for audio. Each branch encodes samples of the corresponding modality into
a d-dimensional space, regardless of the original dimensionality of the samples. This is achieved by
applying global spatial mean pooling and global temporal mean pooling to the image branch output
and the audio branch output, respectively. The image branch is adapted from ResNet50 (He et al.,
2016), where the ﬁnal softmax layer and the preceding fully-connected layers are removed, replaced
with a 1x1 linear convolutional layer in order to project the feature map to the desired dimension.
To model the audio inputs, a 17-layer fully convolutional network with residual connections is used.
The input is a log Mel-frequency spectrogram with 40 frequency bins and 25 ms-wide, Hamming-
windowed frames with a shift of 10 ms. The ﬁrst layer of this network is a 1-D convolution that
spans the entire frequency axis of the spectrogram, while the remaining 16 convolutional layers are
1-D across the time axis. These 16 layers are divided into four residual blocks of 4 layers each, and
downsampling between these blocks is accomplished by applying the ﬁrst convolution of each block
with a stride of 2. For full details of the model, refer to Harwath et al. (2019).

3 .3 L EARN ING H I ERARCH ICA L D I SCR ET E UN I T S W I TH V EC TOR QUAN T I Z ING LAYER S

Previous analyses reveal that ResDAVEnet-like models learn linguistic abstractions at different lev-
els, including words (Harwath & Glass, 2017) and robust phonetic features (Harwath & Glass, 2019;
Hsu et al., 2019). To explicitly learn hierarchical discrete linguistic units within this framework, we
propose to incorporate multiple vector quantization (VQ) layers (van den Oord et al., 2017) into the
ResDAVEnet audio branch; we refer to this new architecture as ResDAVEnet-VQ.
VQ layers can be understood as a type of bottleneck, which constrain the amount of information
that can ﬂow through. While these layers have been used to learn discrete sub-word units (van den
Oord et al., 2017; Chorowski et al., 2019; Razavi et al., 2019), previous work injects VQ layers
into autoencoders that are trained with a reconstruction loss. As a result, the embedding dimen-
sion of each code and the number of codes need to be carefully tuned (Liu et al., 2019). When the
embedding dimension is too low or the codebook size too small, the model does not have enough
expressive power to capture linguistic variability. When it is too large, the model starts to encode
non-linguistic information in order to improve reconstruction. In contrast, the learning signal of
ResDAVEnet-VQ is provided by the visual-semantic grounding objective. Rather than encoding as
much information about input as possible, the learned codes in ResDAVEnet-VQ only need to cap-
ture semantic information. Since semantics in speech are predominantly transmitted by words, and
words are composed of sub-word units like phones, the grounding objective places pressure on the

3

model to robustly infer both from speech. Since words and phones are inherently discrete symbols,
representing them with learned discrete units may not even hurt the grounding performance.
Figure 1 illustrates the proposed ResDAVEnet-VQ model. We add a quantization layer after each of
the ﬁrst two residual blocks of the ResDAVEnet-VQ model, denoted as VQ2 and VQ3, respectively,
is deﬁned as E ∈ RK×D , where K represents the codebook size, and D represents the output
with the intention that they should capture discrete word-like and sub-word-like units. A VQ layer
dimensionality of the input features to the codebook. Denoting the tth temporal frame of the input
to the quantization layer as xt , quantization is performed according to qt = Ek,: , where k =
arg minj ||xt −Ej,: ||2 The quantized output is then fed as input to the subsequent residual block. As
in van den Oord et al. (2017), we use the straight-through estimator (Bengio et al., 2013) to compute
the gradient passed from qt to xt . We use the exponential moving average (EMA) codebook updates
proposed by van den Oord et al. (2017).

Figure 1: Diagram of the ResDAVEnet-VQ model. On the left, we show the placement of the
vector quantization blocks in the audio branch. The right half of the ﬁgure depicts the quantization
mechanism of each VQ block, as well as the bypass path when the block is disabled. For model
details not relating to the VQ blocks, refer to Harwath et al. (2019).

3 .4 CODEBOOK L EARN ING SCHEDUL E S

We include multiple VQ layers in the ResDAVEnet-VQ model, each of which can be independently
enabled or bypassed without changing the rest of the architecture conﬁguration. A model can be
warm-started by copying the weights from another trained model that has fewer VQ layers enabled,
and randomly initializing the codebook of the newly activated VQ layer(s). This gives rise to the
questions of how many quantizers should be used and in what order they should be enabled. It
is unclear whether models with same VQ layers activated would learn the same representation at
each layer regardless of the training curriculum. Let Am denote a subset of all VQ layers, and
Am−1 ⊂ Am . We use “A1 → ... → AM ” to denote a model that is obtained by sequentially
training models “A1 → ... → Am ” initialized from “A1 → ... → Am−1 ”, where the model A1 is
initialized from scratch, and the ﬁnal model would have VQ layers in AM activated. For instance, a
model initialized from scratch with no VQ layers enabled is denoted as “∅”, and a model initialized
with that and with both layers enabled is denoted as “∅ → {2, 3}”.

3 .5 TRA IN ING W I TH THE TR I PL ET LO S S

We train our models using the same loss function as Harwath et al. (2019). This loss function blends
two triplet loss terms (Weinberger & Saul, 2009), one based on random sampling of negative exam-
ples, and the other based on semi-hard negative mining (Jansen et al., 2018), in order to ﬁnd more
challenging negative samples. Speciﬁcally, let the sets of output embedding vectors for a minibatch
of B audio/image training pairs respectively be A = {a1 , . . . , aB } and I = {i1 , . . . , iB }. To com-
pute the randomly-sampled triplet loss term, we select impostor examples for the j th input according
to ¯aj ∼ UniformCategorical({a1 , . . . , aB } \ aj ) and ¯ij ∼ UniformCategorical({i1 , . . . , iB } \ ij ).
The randomly-sampled triplet loss is then computed as:

B(cid:88)

(cid:16)

Ls =

j ¯aj − iT
j aj − iT
max(0, iT
j aj + 1) + max(0, ¯iT
j aj + 1)

j=1

For the semi-hard negative triplet loss, we ﬁrst deﬁne the sets of impostor candidates for the j th
example as ˆAj = {a ∈ A|iT
j aj }. The semi-hard negative

j aj } and ˆIj = {i ∈ I|iT aj < iT
j a < iT

4

(cid:17)

(1)

PushPushPullImpostor imageImpostor captionImageCNNConv1Res2VQ2PoolPoolRes3VQ3Res4Res5xexee21x31eBypassVQ Blockloss is then computed as:

B(cid:88)

(cid:16)

Lh =

j=1

max(0, max

ˆa∈ ˆAj

j ˆa) − iT
(ˆiT aj ) − iT
(iT
j aj + 1) + max(0, max
j aj + 1)

ˆi∈ˆIj

(cid:17)

(2)

Finally, the overall loss function is computed as the sum of the two above losses, L = Ls + Lh .

3 .6

IM P LEM EN TAT ION D E TA I L S

All of our models were trained for 180 epochs using the Adam optimizer (Kingma & Ba, 2014)
with a batch size of 80. We used an exponentially decaying learning rate schedule, with an initial
value of 2e-4 that decayed by a factor of 0.95 every 3 epochs. Following van den Oord et al. (2017),
we use an EMA decay factor of γ = .99 for training each VQ codebook. Our core experimental
results all use a codebook size of 1024 vectors for all quantizers, but in the supplementary material
we include experiments with smaller and larger codebooks. Following Chorowski et al. (2019),
the jitter probability hyperparameter for each quantization layer was ﬁxed at 0.12. While we do
not apply data augmentation to the input spectrograms, during training we perform standard data
augmentation techniques to the images. We resize each raw image so that its smallest dimension
is 256 pixels, and then we apply an Inception-style random crop which is resized to 224 pixels
square. During training, we also ﬂip each image horizontally with a probability of 0.5. During
evaluation, the center 224 pixel square crop is always taken from the image. Finally, the RGB pixel
values are mean and variance normalized. We trained each model on the Places audio caption train
split, and computed the image and caption recall at 10 (R@10) scores on the validation split of
the Places audio captions after each training epoch. The model snapshot that achieved the highest
average R@10 score on the validation set from each training is used for all evaluation. To extract
embeddings and units from our models, we simply perform a forward pass through the speech branch
of the ResDAVEnet-VQ network and retain the outputs from the target layer at a uniform frame-rate.
The frame-rate is determined by the downsampling factor at the target layer relative to the input. For
non-quantized layers, these outputs will be continuous embeddings. For quantized layers, these will
be quantized embedding retrieved from the assigned entry in the codebook.

4 EX PER IM EN T S

4 .1 SUB -WORD UN I T L EARN ING ON TH E Z EROS P EECH 20 19 ABX TA SK

Evaluation metrics Learning unsupervised speech representations that are indicative of phonetic
content is of high interest to the speech community, and recently has been the focus of the Ze-
roSpeech Challenge (Versteegh et al., 2015; Dunbar et al., 2017; 2019). One of the core evaluations
is the model-free, minimal-pair ABX task (Schatz et al., 2013), which aims to benchmark repre-
sentations in terms of their discriminability between different sub-word speech units. In this task,
three speech waveform segments denoted by A, B , and X are presented to a model. A and B are
constrained to be a triphone minimal pair; that is, both segments capture three phones, but differ
only in the identity of their center phone. The third segment, X is chosen to contain the same un-
derlying triphone sequence as A. Supposing f (·) is a function to be evaluated that maps a waveform
segment to a feature segment, the ABX error rate under a given similarity metric S (·, ·) is deﬁned
as the fraction of ABX triples in which S (f (A), f (X )) > S (f (B ), f (X )). An ABX error rate of
50% indicates random assignment, while an ABX of 0% reﬂects perfect phone discriminability.
In the ZeroSpeech challenge, dynamic time warping (DTW) is used to measure similarity between a
pair of feature sequences for ABX evaluations, with several options available for computing the dis-
tance between a pair of feature frames. The ZeroSpeech 2019 challenge in particular emphasizes on
discovering an inventory of discrete sub-word units, rather than continuous representations. There-
fore, in addition to an ABX error rate, a bitrate is also computed for each model which reﬂects the
amount of information carried by the learned units. A lower bitrate can be achieved by having a more
compact inventory of learned units or having a smaller number of codes per second. The full details
of the evaluation can be found in Dunbar et al. (2019). To be clear, all of our ResDAVEnet-VQ
models were not trained on the ZeroSpeech training data, but instead on the Places audio captions,
thus there is a domain mismatch between training and testing these models.

5

Table 1: Comparison of R@10, ABX scores, and bit-rates between different conﬁgurations and
baseline models trained on ZeroSpeech 2019 data or Places Audio Caption. All quantizers reﬂected
in this table used a codebook size of 1,024 vectors. We do not compute RLE or segment scores for
the FHVAE-DPGMM model, since we did not re-implement that model.

Model ID

Layer R@10

Frame-Based
Segment-Based
ABX Bitrate
RLE Bitrate ABX Bitrate

FHVAE-DPGMM (ZS)
WaveNet-VQ (ZS)
WaveNet-VQ (PA)
“{2}”

“∅ → {2}”

“{3}”

“∅ → {3}”

“{2, 3}”
“∅ → {2, 3}”
“{2} → {2, 3}”
“{3} → {2, 3}”

-
-
-
VQ2
VQ2
VQ3
VQ3
VQ2
VQ3
VQ2
VQ3
VQ2
VQ3
VQ2
VQ3

N/A
N/A
N/A
.753
.760
.734
.794

.667

.787

.764

.760

21.67
19.98
24.87
12.33

11.79

38.21

15.04

25.62
32.23
13.15
14.95

12.51
14.52

13.55
33.70

413.23
151.55
149.00
433.30
390.61
213.92
182.93
408.75
218.76
405.43
199.91
415.13
167.84
421.23
208.63

-
136.74
136.27
361.09
317.66
129.65
140.04
258.37
156.69
334.39
172.05
341.85
136.11
271.91
117.37

-
20.48
25.23
12.78
12.66
38.68
16.53
26.32
32.49
13.30
15.60
13.06
15.68
14.38
33.58

-
126.17
126.22
332.86
289.11
108.84
121.26
217.58
136.90
303.03
159.07
311.82
121.17
232.87
98.29

Baseline models

In addition to the frame-based bitrate and ABX scores computed by the ZeroSpeech 2019 evaluation
toolkit, we implement our own extensions to these metrics. Because it is common for successive
frames to be assigned to the same codebook entry and phonetic information is not encoded at a ﬁxed
frame rate, lossless run length encoding (RLE) can be a more reasonable measure of the bitrate of
a frame-based model. RLE does not change the ABX score since it can be trivially inverted, but it
does change the bitrate. For computing the RLE bitrate, we modify the bitrate calculation speciﬁed
in Dunbar et al. (2019) so that a unique symbol is deﬁned as the tuple (unit, length) where length is
the number of frames assigned to a given unit with in a segment. We also consider segment-based
ABX and bitrate, which is similar to the RLE metrics except in this case we outright discard the
frame length information. This typically results in an even greater reduction in bitrate, but also an
accompanying deterioration in ABX score.
In Table 1, we compare our results to those derived from two of the top-
performing submissions to the ZeroSpeech 2019 challenge: a re-implementation of WaveNet-
VQ (Chorowski et al., 2019) provided by Cho et al. (2019) and FHVAE-DPGMM (Feng et al., 2019).
Using the code accompanied with the WaveNet-VQ submission, we were able to train their model
on the set of 400,000 Places audio captions to make a fairer comparison with our ResDAVEnet-VQ
models in terms of the amount of speech data used. In addition, when trying to reproduce the re-
ported WaveNet-VQ results, we obtain better performance than previously reported by training for
more steps.
Table 1 shows that WaveNet-VQ achieves similar bitrates regardless of the training data. However,
ABX deteriorates from 19.98 to 24.87, implying the model cannot utilize data of a larger scale but
out-of-domain relative to the test set. A similar degradation when testing on out-of-domain data
with FHVAE models was observed in Hsu et al. (2019). We did not re-train the model submitted
by Feng et al. (2019), and instead compare against the scores reported in Dunbar et al. (2019).

ABX discrimination without using quantization Our ﬁrst experiment investigates exactly which

layer in the ResDAVEnet-VQ model is most suited for ABX phone discrimination, and would thus
make a good candidate for learning of quantized sub-word units. The leftmost plot in Figure 2
shows that layers 2 and 3 of a ResDAVEnet-VQ model without any quantization enabled perform
the best in terms of ABX error rate on the ZeroSpeech 2019 English test set; the exact numbers for
this model are displayed in the caption of Figure 2. Because layers 2 and 3 achieve the lowest ABX
error rates without quantization, we focus our attention on the impact of quantization there.

6

Model “∅”

Model “{2}”

Model “{3}”

e

t

a

R

r

o

r
r

E

X
B
A

0
1

@

R

40

30

20

10

60
30
0

layer1
layer2
layer3
layer4

0

50

25

Epoch

40

30

20

10

60
30
0

0

50

25

Epoch

40

30

20

10

60
30
0

0

50

25

Epoch

Figure 2: R@10 and ABX tracked at various training epochs. The “∅” model achieves a ﬁnal R@10
of .735, with ABX scores of 19.77, 11.35, 10.86, and 14.05 for the conv1, res2, res3, and res4 layers.

Quantizing one layer When quantizing only one layer, we examine quantization of layer 2 vs.
layer 3, and using cold-start training vs. warm-start initialization from model “∅”. The ABX and
bitrate results for these models, as well as the R@10 scores on the Places validation set, are shown in
Table 1. In all cases, quantization applied at the output of layer 2 achieves a better ABX score than
quantization at layer 3, but VQ3 achieves a better bitrate. Quantization barely impacts the perfor-
mance of layer 2, whose ABX score very slightly rises from 11.35 to 11.79. Warm-start initialization
is beneﬁcial to R@10 and ABX score in both cases, but we notice an intriguing anomaly when ap-
plying cold-start quantization to layer 3: the ABX score deteriorates signiﬁcantly, rising from 10.86
in the case of the non-quantized model to 38.21. This indicates that while VQ2 is capable of learn-
ing a ﬁnite inventory of units that are highly predictive of phonetic identity from either a warm-start
or cold-start initialization, cold-start training of VQ3 results in very little phonetic information cap-
tured by the quantizer. Interestingly, this model is still learning to infer visual semantics from the
speech signal, as evidenced by a high R@10 score; we later show in Section 4.2 that the reason for
this anomaly is because cold-start training of VQ3 results in the learning of word detectors. In all
cases except for model “{3}”, we note that the ABX scores achieved by our models are signiﬁcantly
better than the baselines. Our best model in terms of ABX (“∅ → {2}”) achieves a 41.0% reduction
in ABX over the WaveNet-VQ baseline, at a cost of a 132.3% increase in RLE bitrate; however,
model “∅ → {3}” achieves a 24.7% reduction in ABX error rate with only a 2.4% increase in RLE
bitrate. These results do not constitute a fair comparison, however, because the WaveNet-VQ and
ResDAVEnet-VQ models were trained on different datasets; when training the WaveNet-VQ model
on the same set of audio captions used to train ResDAVEnet-VQ (but without the accompanying
images, since WaveNet-VQ is not a multimodal model), the ABX error rate increases to 24.87%,
tipping the results even more in favor of the ResDAVEnet-VQ models.
Quantizing two layers Quantizing multiple layers at once offers the possibility of learning a hi-
erarchy of units. Thus, we aim to capture phonetic information in a lower layer quantizer and
word-level information at a higher layer quantizer. Cold-start training of two quantizers (“{2, 3}”)
results in a signiﬁcant drop in ABX performance for both VQ2 and VQ3, but also a drop in R@10 on
the Places validation set. We see much better results in terms of R@10 and ABX for the remaining
3 models which were initialized from the “∅” model or a model with only one quantizer enabled;
for example, model “{3} → {2, 3}” achieves an ABX of 14.52 with an RLE bitrate of 136.11,
representing a 27.3% ABX improvement over the best baseline while keeping the bitrate approxi-
mately the same. We see in model “{3} → {2, 3}” that the same phenomenon observed with model
“{3}” persists: VQ3 achieves relatively poor ABX, despite a high overall R@10 and strong ABX
with VQ2 at 13.55%. We conﬁrm in Section 4.2 that the VQ3 layer of model “{3} → {2, 3}” does
indeed capture word-level information, indicating that this model has successfully localized pho-
netic unit identity in the second layer and lexical unit identity in the third layer. Overall, our results
suggest that when learning hierarchical quantized representations with a ResDAVEnet-VQ model,
the nature of the representations learned is highly dependent on the training curriculum.

7

Model

Layer

“∅ → {2}”

WaveNet-VQ (ZS)
WaveNet-VQ (PA)
“∅”
“∅”
“{2} → {2, 3}”
“{2} → {2, 3}”
“{3} → {2, 3}”
“{3} → {2, 3}”
“∅” (n)
“∅” (n)
“∅ → {2}” (n)
“{2} → {2, 3}” (n)
“{2} → {2, 3}” (n)
“{3} → {2, 3}” (n)
“{3} → {2, 3}” (n)

N/A
N/A
Res2
Res3
VQ2
VQ2
VQ3
VQ2
VQ3
Res2
Res3
VQ2
VQ2
VQ3
VQ2
VQ3

Clean
ABX
R-B
19.98
136.74
24.87
136.27
11.35
N/A
10.86
N/A
11.79
317.66
12.51
341.85
14.52
136.11
13.55
271.91
33.70
117.37
13.32
N/A
11.85
N/A
12.64
342.53
13.42
365.89
14.39
179.19
16.52
223.28
26.21
187.31

20-30 dB
ABX
R-B
21.22
141.07
27.18
137.70
11.63
N/A
11.16
N/A
12.15
325.40
12.56
350.28
14.73
137.68
13.65
272.46
32.56
118.22
12.30
N/A
11.90
N/A
12.20
348.57
13.71
359.14
14.92
180.36
16.47
223.61
25.88
187.92

10-20 dB
ABX
R-B
27.51
144.28
33.29
132.34
13.17
N/A
12.96
N/A
14.62
332.21
14.82
362.73
17.44
143.14
15.69
267.70
34.65
115.40
12.97
N/A
12.44
N/A
13.34
359.43
14.57
370.67
15.38
182.27
17.75
225.72
26.34
188.49

0-10 dB
ABX
R-B
42.55
126.96
42.67
110.50
19.44
N/A
19.43
N/A
23.96
327.15
25.02
330.54
27.68
133.13
24.06
244.52
39.82
102.48
16.91
N/A
16.09
N/A
18.82
373.60
18.78
392.10
19.58
188.32
22.68
230.01
31.26
191.28

Table 2: ABX scores and RLE bitrates for various SNRs on the noisy ZeroSpeech19 English test set.
“R-B” stands for “RLE-Bitrate,” and (n) denotes a model trained on the noisy Places Audio dataset.
For the WaveNet-VQ models, (ZS) and (PA) respectively denote training on the ZeroSpeech 19
English training set, and the clean Places Audio dataset.

Training and testing on noisy data In Hsu et al. (2019), it was shown that representations learned
by a ResDAVEnet model were far more robust to train/test domain mismatch in terms of background
noise, channel characteristics, and speaker identity than standard spectral features when training a
supervised speech recognizer. Here, we examine whether this robustness is also exempliﬁed by
the quantized versions of this model. We construct three additional test sets using the ZeroSpeech
2019 English testing data by adding noise sampled from the AudioSet (Jansen et al., 2018) dataset.
For each ZeroSpeech testing waveform, we randomly sampled an AudioSet waveform of the same
duration and performed linear mixing with a signal-to-noise ratio (SNR) selected randomly within
a speciﬁed range. We construct low, medium, and high noise testing sets, corresponding to SNRs
of 20-30 dB, 10-20 dB, and 0-10 dB. We then perform the ABX discrimination task on these noisy
waveforms, displaying the results in Table 2. We ﬁnd that for all models, a worsening SNR results in
a deterioration in ABX performance. However, the ResDAVEnet-VQ models prove to be far more
noise robust than the Wavenet-VQ model; even in the high noise testing set, the best ResDAVEnet-
VQ model achieves an ABX of 23.96%, while the WaveNet-VQ models degrade to nearly-random
ABX scores of 42.55% and 42.67%.
Given that a ResDAVEnet-VQ model trained on the “clean” Places Audio captions is highly robust
to additive noise on the ABX discrimination task, we investigated whether adding noise to the Places
Audio captions themselves would result in an even higher degree of noise robustness. To that end,
we followed a similar data augmentation approach to create a noisy version of the Places Audio
captions, where the SNR of each caption was randomly chosen to sit within the range of 0-30 dB.
The bottom half of Table 2 shows the results of training several ResDAVEnet-VQ models on the
noisy Places Audio captions and testing on the clean and noisy ZeroSpeech ABX tasks. In general,
we observe a degradation ABX score in the clean conditions, but with a signiﬁcantly higher degree
of noise robustness in the noisier conditions.
Visualization of learned units To better measure the correspondence between the VQ units and
English phones, we compute corpus-level co-occurrence statistics (at the frame-level) across the
TIMIT training set, excluding the sa dialect sentences. To facilitate visualization, we use the
“∅ → {2}” model with a codebook size of 128. We display the conditional probability matrix
P (phone|unit) in Figure 3, with the rows and columns ordered via spectral co-clustering with 10
clusters in order to group together phones that share similar sets of VQ codes. Visually, there is a
strong mapping between TIMIT phone labels and ResDAVEnet-VQ codes. In some cases, redun-
dant codes are used for the same phone label (this is especially the case for the silence label), and
in other cases we see that phones belonging to the same manner class often tend to share codebook

8

Figure 3: Conditional probability matrix displaying P (phone|unit) using the “∅ → {2}” model
with a VQ2 codebook size of 128. For visualization, we saturate the color scaling at probability 0.5.

units. We can numerically quantify the mapping between the phone and unit labels with the normal-
ized mutual information measure (NMI), which we found to be .378 in this case. We also include
several spectrograms with their time-aligned unit sequences in the supplementary material.

Table 3: Performance of the VQ3 layer from the “{3} → {2, 3}” model when codes are treated as
word detectors. Codes are ranked by the highest F1 score among the retrieved words for a given
code. Word hypotheses for a given code are ranked by the F1 score. P denotes precision, R recall,
and occ the number of co-occurrences of the code and word in the data.

rank

code

1
2
3
4
5

198
199
200

58
706
88
116
584

234
1000
842

word

baseball
background
classroom
construction
playground

alleyway
orange
pink

Top Hypotheses
F1
P

R

occ

word

Second Hypotheses
F1
P

90.09
89.26
88.78
88.16
87.84

67.26
67.25
67.19

89.45
92.36
87.39
90.19
85.68

56.62
89.09
78.12

90.75
86.36
90.21
86.22
90.12

82.81
54.00
58.94

· · ·

3266
17740
1512
2484
930

player
backgrounds
class
constructed
play

2.82
0.71
4.65
1.98
2.70

1.60
0.36
2.79
1.03
1.81

944
2987
2723

alley
oranges
paint

37.25
3.30
5.05

30.06
1.71
3.07

R

12.18
68.75
13.99
26.29
5.33

48.98
48.84
14.26

occ

139
66
82
56
51

863
63
140

Table 4: Performance of the VQ3 layer from the “{2} → {2, 3}” model when codes are treated as
word detectors.

rank

code

1
2
3
4
5

198
199
200

924
530
749
505
581

329
362
256

word

people
building
white
blue
snow

standing
white
trees

Top Hypotheses
F1
P

R

occ

word

Second Hypotheses
F1
P

R

occ

76.60
75.96
75.86
58.47
58.30

5.29
5.27
5.24

66.79
65.57
66.14
46.53
42.59

2.80
3.05
2.88

89.80
90.27
88.93
78.67
92.38

49.17
19.49
28.84

33662
35229
45097
21678
9746

· · ·

computer
buildings
one
pool
small

7769
9883
8963

woman
table
see

2.30
23.44
5.22
9.13
16.17

3.15
4.71
4.48

1.18
13.44
3.15
4.97
11.72

1.65
2.49
2.45

42.36
91.70
15.33
56.87
26.06

35.41
44.61
26.12

887
7260
3315
2674
5617

5704
7351
6346

9

ResDAVEnet-VQ Code IndicesjhchshuwyeyihiyrehuherdxahaefthmnnggtpkszsiloyawaaayaoowwlvhhbdhqdTIMIT Phone LabelsFigure 4: Visualization of the precision, recall, and F1 scores of individual VQ3 codes when treated
as word detectors on the Places Audio captions.

4 .2 FROM PHON E S TO WORD S : L EARN ING A H I ERARCHY O F UN I T S

As shown in Table 1, all of the ResDAVEnet-VQ models which underwent cold-start training of VQ3
exhibited a similar phenomenon in which the ABX error rate of that layer was particularly high,
despite the model performing well at the image-caption retrieval task. We hypothesized that this
could be due to VQ3 learning to recognize higher level linguistic units, such as words. To examine
this empirically, we inferred the VQ3 unit sequence for every audio caption in the Places Audio
training set according to several different models. Using the estimated word-level transcriptions of
the utterances (provided by the Google SpeechRecognition API), we computed precision, recall,
and F1 scores for every unique (word, VQ3 code) pair for a given model and quantization layer. We
then ranked the VQ codes in descending order according to their maximum F1 score for any word
in the vocabulary. Table 3 shows a sampling of these statistics for model “{3} → {2, 3}”, Table 4
for model “{2} → {2, 3}”. It should be emphasized that these models are exactly the same in all
respects, except for the order in which their quantizers were trained.
We examine the overall performance of VQ3 as a word detector for these models in Figure 4. The
right hand side of Figure 4 displays the number of VQ3 codes whose maximum F1 score is above
a given threshold, while the left hand side shows the distribution of precision and recall scores for
the top 250 words ranked by F1. This gives an approximate indication of how many VQ3 codes
have learned to specialize as detectors for a speciﬁc word. We see that the VQ3 layer of model
“{3} → {2, 3}” learns 279 codebook entries with an F1 score above 0.5. In contrast, the VQ3 layer
of model “{2} → {2, 3}” learns only a handful of word-detecting codebook entries with an F1 of
of the VQ3 layer in models “{3}” and “{3} → {2, 3}” is in fact due to its specialization for detecting
greater than 0.5. This experiment supports the notion that the reason for the poor ABX performances
speciﬁc words, and that this specialization only emerges when the VQ3 layer is learned before the
VQ2 layer. More extensive tables detailing the words learned by many more VQ3 codes for these
models can be found in the supplementary material.

5 CONCLU S ION S

In this paper, we demonstrated that the neural vector quantization layers proposed by van den Oord
et al. (2017) can be integrated into the visually-grounded speech models proposed by Harwath et al.
(2019). This resulted in the ability of the speech model to directly represent speech units, such as
phones and words, as discrete latent variables. We presented extensive experiments and analysis
of these learned representations, demonstrating signiﬁcant improvements in phone discrimination
ability over the current state-of-the-art models for sub-word speech unit discovery. We demonstrated
that these units are also far more robust to noise and domain shift than units derived from previously
proposed models. These results supported the notion that semantic supervision via a discriminative,
multimodal grounding objective has the potential to be more powerful than reconstruction-based
objectives typically used in unsupervised speech models.
We also showed how multiple vector quantizers could be employed simultaneously within a single
ResDAVEnet-VQ model, and that these quantizers could be made to specialize in learning a hier-

10

0.00.20.40.60.81.0Precision0.00.20.40.60.81.0RecallWords of Top 250 F1-Scores"{2}{2,3}""{3}{2,3}""{3}"0.00.20.40.60.81.0F1 Threshold100101102103104Number of WordsNumber of Words above F1 Threshold"{2}{2,3}""{3}{2,3}""{3}""{2,3}""{2,3}""{3}"archy of speech units: speciﬁcally, phones in the lower quantizer and words in the upper quantizer.
Our analysis showed that hundreds of codebooks in the upper quantizer learned to perform as word
detectors, and that these detectors were highly accurate. Our experiments also revealed that this
behavior only emerged when VQ3 was trained before VQ2. These results suggest the importance
of the learning curriculum, which should be more deeply investigated in future work. Future work
should attempt to make explicit what kind of compositional rules are implicitly encoded by these
models when mapping sequences of codes from the lower quantizer to word-level units in the up-
per quantizer; the automatic derivation of a sub-word unit inventory, vocabulary, and pronunciation
lexicon could serve as the starting point for a fully unsupervised speech recognition system. Future
work should also investigate whether layers above VQ3 could be made to learn even higher-level
linguistic abstractions, such as grammar, syntax, and compositional reasoning.

11

R E F ER ENC E S
Afra Alishahi, Marie Barking, and Grzegorz Chrupała. Encoding of phonology in a recurrent neural
model of grounded speech. In Proc. ACL Conference on Natural Language Learning (CoNLL),
2017.

Emmanuel Azuh, David Harwath, and James Glass. Towards bilingual lexicon discovery from visu-
ally grounded speech audio. In Proc. Annual Conference of International Speech Communication
Association (INTERSPEECH), 2019.

Yoshua Bengio, Nicholas L ´eonard, and Aaron Courville. Estimating or propagating gradients
through stochastic neurons for conditional computation. arXiv preprint arXiv:1308.3432, 2013.

Chung-Cheng Chiu, Tara N Sainath, Yonghui Wu, Rohit Prabhavalkar, Patrick Nguyen, Zhifeng
Chen, Anjuli Kannan, Ron J Weiss, Kanishka Rao, Ekaterina Gonina, et al. State-of-the-art
speech recognition with sequence-to-sequence models.
In Proc. International Conference on
Acoustics, Speech and Signal Processing (ICASSP), 2018.

Suhee Cho, Yeonjung Hong, Yookyunk Shin, and Youngsun Cho. VQVAE with speaker adversarial

training, 2019. URL https://github.com/Suhee05/Zerospeech2019.

Jan Chorowski, Ron J. Weiss, Samy Bengio, and A ¨aron van den Oord. Unsupervised speech rep-
resentation learning using wavenet autoencoders. IEEE Transactions on Audio, Speech and Lan-
guage Processing, 2019.

Grzegorz Chrupała. Symbolic inductive bias for visually grounded learning of spoken language. In
Proc. Annual Meeting of the Association for Computational Linguistics (ACL), 2019.

Grzegorz Chrupała, Lieke Gelderloos, and Afra Alishahi. Representations of language in a model of
visually grounded speech signal. In Proc. Annual Meeting of the Association for Computational
Linguistics (ACL), 2017.

Yu-An Chung, Wei-Ning Hsu, Hao Tang, and James R. Glass. An unsupervised autoregressive
model for speech representation learning. In Proc. Annual Conference of International Speech
Communication Association (INTERSPEECH), 2019.

Jennifer Drexler and James Glass. Analysis of audio-visual features for unsupervised speech recog-
nition. In Proc. Grounded Language Understanding Workshop, 2017.

Ewan Dunbar, Xuan Nga Cao, Juan Benjumea, Julien Karadayi, Mathieu Bernard, Laurent Besacier,
Xavier Anguera, and Emmanuel Dupoux. The zero resource speech challenge 2017. In Proc.
IEEE Workshop on Automatic Speech Recognition and Understanding (ASRU), 2017.

Ewan Dunbar, Robin Algayres, Julien Karadayi, Mathieu Bernard, Juan Benjumea, Xuan-Nga Cao,
Lucie Miskic, Charlotte Dugrain, Lucas Ondel, Alan W. Black, Laurent Besacier, Sakriani Sakti,
and Emmanuel Dupoux. The zero resource speech challenge 2019: TTS without T.
In Proc.
Annual Conference of International Speech Communication Association (INTERSPEECH), 2019.

Emmanuel Dupoux. Cognitive science in the era of artiﬁcial intelligence: A roadmap for reverse-
engineering the infant language-learner. In Cognition, 2018.

Ryan Eloff, Herman Engelbrecht, and Herman Kamper. Multimodal one-shot learning of speech and
images. In Proc. International Conference on Acoustics, Speech and Signal Processing (ICASSP),
2019a.

Ryan Eloff, Andr Nortje, Benjamin van Niekerk, Avashna Govender, Leanne Nortje, Arnu Pretorius,
Elan van Biljon, Ewald van der Westhuizen, Lisa van Staden, and Herman Kamper. Unsupervised
acoustic unit discovery for speech synthesis using discrete latent-variable neural networks.
In
Proc. Annual Conference of International Speech Communication Association (INTERSPEECH),
2019b.

Siyuan Feng, Tan Lee, and Zhiyuan Peng. Combining adversarial training and disentangled speech
representation for robust zero-resource subword modeling. In Proc. Annual Conference of Inter-
national Speech Communication Association (INTERSPEECH), 2019.

12

Herb Gish, Man-Hung Siu, Arthur Chan, and William Belﬁeld. Unsupervised training of an HMM-
based speech recognizer for topic classiﬁcation.
In Proc. Annual Conference of International
Speech Communication Association (INTERSPEECH), 2009.

Google.
Google
cloud
speech-to-text API.
speech-to-text/, 2019. Accessed: 2019-09-16.

https://cloud.google.com/

David Harwath and James Glass. Deep multimodal semantic embeddings for speech and images. In
Proc. IEEE Workshop on Automatic Speech Recognition and Understanding (ASRU), 2015.

David Harwath and James Glass. Learning word-like units from joint audio-visual analysis. In Proc.
Annual Meeting of the Association for Computational Linguistics (ACL), 2017.

David Harwath and James Glass. Towards visually grounded sub-word speech unit discovery. In
Proc. International Conference on Acoustics, Speech and Signal Processing (ICASSP), 2019.

David Harwath, Antonio Torralba, and James R. Glass. Unsupervised learning of spoken language
with visual context. In Proc. Neural Information Processing Systems (NeurIPS), 2016.

David Harwath, Galen Chuang, and James Glass. Vision as an interlingua: Learning multilingual
semantic embeddings of untranscribed speech. In Proc. International Conference on Acoustics,
Speech and Signal Processing (ICASSP), 2018a.

David Harwath, Adri `a Recasens, D´ıdac Sur´ıs, Galen Chuang, Antonio Torralba, and James Glass.
Jointly discovering visual objects and spoken words from raw sensory input.
In Proc. IEEE
European Conference on Computer Vision (ECCV), 2018b.

David Harwath, Adri `a Recasens, D´ıdac Sur´ıs, Galen Chuang, Antonio Torralba, and James Glass.
Jointly discovering visual objects and spoken words from raw sensory input. International Jour-
nal of Computer Vision, 2019.

William Havard, Jean-Pierre Chevrot, and Laurent Besacier. Models of visually grounded speech
signal pay attention to nouns: a bilingual experiment on English and Japanese. In Proc. Interna-
tional Conference on Acoustics, Speech and Signal Processing (ICASSP), 2019a.

William N. Havard, Jean-Pierre Chevrot, and Laurent Besacier. Word recognition, competition, and
activation in a model of visually grounded speech. In Proc. ACL Conference on Natural Language
Learning (CoNLL), 2019b.

Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recog-
nition. In Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2016.

Nils Holzenberger, Mingxing Du, Julien Karadayi, Rachid Riad, and Emmanuel Dupoux. Learning
word embeddings: Unsupervised methods for ﬁxed-size representations of variable-length speech
segments. In Proc. Annual Conference of International Speech Communication Association (IN-
TERSPEECH), 2018.

Nils Holzenberger, Shruti Palaskar, Pranava Madhyastha, Florian Metze, and Raman Arora. Learn-
ing from multiview correlations in open-domain videos. In Proc. International Conference on
Acoustics, Speech and Signal Processing (ICASSP), 2019.

Wei-Ning Hsu and James Glass. Scalable factorized hierarchical variational autoencoder training. In
Proc. Annual Conference of International Speech Communication Association (INTERSPEECH),
2018.

Wei-Ning Hsu, Yu Zhang, and James Glass. Learning latent representations for speech generation
and transformation. In Proc. Annual Conference of International Speech Communication Associ-
ation (INTERSPEECH), 2017a.

Wei-Ning Hsu, Yu Zhang, and James Glass. Unsupervised learning of disentangled and interpretable
representations from sequential data. In Proc. Neural Information Processing Systems (NeurIPS),
2017b.

13

Wei-Ning Hsu, David Harwath, and James Glass. Transfer learning from audio-visual grounding to
speech recognition. In Proc. Annual Conference of International Speech Communication Associ-
ation (INTERSPEECH), 2019.

Gabriel Ilharco, Yuan Zhang, and Jason Baldridge. Large-scale representation learning from visu-
ally grounded untranscribed speech. In Proc. ACL Conference on Natural Language Learning
(CoNLL), 2019.

Aren Jansen and Benjamin Van Durme. Efﬁcient spoken term discovery using randomized algo-
rithms. In Proc. IEEE Workshop on Automatic Speech Recognition and Understanding (ASRU),
2011.

Aren Jansen, Kenneth Church, and Hynek Hermansky. Toward spoken term discovery at scale with
zero resources. In Proc. Annual Conference of International Speech Communication Association
(INTERSPEECH), 2010.

Aren Jansen, Manoj Plakal, Ratheet Pandya, Daniel P.W. Ellis, Shawn Hershey, Jiayang Liu,
R. Channing Moore, and Rif A. Saurous. Unsupervised learning of semantic audio represen-
tations. In Proc. International Conference on Acoustics, Speech and Signal Processing (ICASSP),
2018.

Herman Kamper and Michael Roth. Visually grounded cross-lingual keyword spotting in speech.
In Proc. of the Workshop on Spoken Language Technologies for Under-Resourced Languages
(SLTU), 2017.

Herman Kamper, Aren Jansen, and Sharon Goldwater. Fully unsupervised small-vocabulary speech
recognition using a segmental Bayesian model.
In Proc. Annual Conference of International
Speech Communication Association (INTERSPEECH), 2015.

Herman Kamper, Aren Jansen, and Sharon Goldwater. Unsupervised word segmentation and lexicon
discovery using acoustic word embeddings. IEEE Transactions on Audio, Speech and Language
Processing, 2016.

Herman Kamper, Aren Jansen, and Sharon Goldwater. A segmental framework for fully-
unsupervised large-vocabulary speech recognition. Computer Speech and Language, 46(3):154–
174, 2017a.

Herman Kamper, Karen Livescu, and Sharon Goldwater. An embedded segmental k-means model
for unsupervised segmentation and clustering of speech. In Proc. IEEE Workshop on Automatic
Speech Recognition and Understanding (ASRU), 2017b.

Herman Kamper, Shane Settle, Gregory Shakhnarovich, and Karen Livescu. Visually grounded
learning of keyword prediction from untranscribed speech. In Proc. Annual Conference of Inter-
national Speech Communication Association (INTERSPEECH), 2017c.

Herman Kamper, Aristotelis Anastassiou, and Karen Livescu. Semantic query-by-example speech
search using visual grounding. In Proc. International Conference on Acoustics, Speech and Signal
Processing (ICASSP), 2019a.

Herman Kamper, Gregory Shakhnarovich, and Karen Livescu. Semantic speech retrieval with a
visually grounded model of untranscribed speech.
IEEE Transactions on Audio, Speech and
Language Processing, 2019b.

Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In Proc. Interna-
tional Conference on Learning Representations (ICLR), 2014.

Chia-Ying Lee and James Glass. A nonparametric Bayesian approach to acoustic model discovery.
In Proc. Annual Meeting of the Association for Computational Linguistics (ACL), 2012.

Chia-Ying Lee, Timothy J. O’Donnell, and James Glass. Unsupervised lexicon discovery from
acoustic input. In Proc. Annual Meeting of the Association for Computational Linguistics (ACL),
2015.

14

Kenneth Leidal, David Harwath, and James Glass. Learning modality-invariant representations for
speech and images. In Proc. IEEE Workshop on Automatic Speech Recognition and Understand-
ing (ASRU), 2017.

M. Paul Lewis, Gary F. Simon, and Charles D. Fennig. Ethnologue: Languages of the World,
Nineteenth edition. SIL International. Online version: http://www.ethnologue.com, 2016.

Andy T Liu, Po-chun Hsu, and Hung-yi Lee. Unsupervised end-to-end learning of discrete linguistic
units for voice conversion. In Proc. Annual Conference of International Speech Communication
Association (INTERSPEECH), 2019.

Danny Merkx, Stefan L. Frank, and Mirjam Ernestus. Language learning using speech to image
retrieval. In Proc. Annual Conference of International Speech Communication Association (IN-
TERSPEECH), 2019.

Benjamin Milde and Chris Biemann. Unspeech: Unsupervised speech context embeddings. In Proc.
Annual Conference of International Speech Communication Association (INTERSPEECH), 2018.
Lucas Ondel, Luk ´as Burget, and Jan ˇCernock ´y. Variational inference for acoustic unit discovery.
In Proc. of the Workshop on Spoken Language Technologies for Under-Resourced Languages
(SLTU), 2016.

Alex Park and James Glass. Towards unsupervised pattern discovery in speech.
Workshop on Automatic Speech Recognition and Understanding (ASRU), 2005.

In Proc. IEEE

Alex Park and James Glass. Unsupervised pattern discovery in speech. IEEE Transactions on Audio,
Speech and Language Processing, 2008.

Ankita Pasad, Bowen Shi, Herman Kamper, and Karen Livescu. On the contributions of visual
and textual supervision in low-resource semantic speech retrieval. In Proc. Annual Conference of
International Speech Communication Association (INTERSPEECH), 2019.

Santiago Pascual, Mirco Ravanelli, Joan Serr `a, Antonio Bonafonte, and Yoshua Bengio. Learning
problem-agnostic speech representations from multiple self-supervised tasks.
In Proc. Annual
Conference of International Speech Communication Association (INTERSPEECH), 2019.

Ali Razavi, Aaron van den Oord, and Oriol Vinyals. Generating diverse high-ﬁdelity images with
vq-vae-2. arXiv preprint arXiv:1906.00446, 2019.

Deb Roy. Grounded spoken language acquisition: Experiments in word learning. IEEE Transactions
on Multimedia, 5(2):197–209, 2003.

Deb Roy and Alex Pentland. Learning words from sights and sounds: a computational model.
Cognitive Science, 26:113–146, 2002.

Odette Scharenborg, Laurent Besacier, Alan W. Black, Mark Hasegawa-Johnson, Florian Metze,
Graham Neubig, Sebastian St ¨uker, Pierre Godard, Markus M ¨uller, Lucas Ondel, Shruti Palaskar,
Philip Arthur, Francesco Ciannella, Mingxing Du, Elin Larsen, Danny Merkx, Rachid Riad, Lim-
ing Wang, and Emmanuel Dupoux. Linguistic unit discovery from multi-modal inputs in unwrit-
ten languages: Summary of the ”Speaking Rosetta” JSALT 2017 workshop. In Proc. International
Conference on Acoustics, Speech and Signal Processing (ICASSP), 2018.

Thomas Schatz, Vijayaditya Peddinti, Francis Bach, Aren Jansen, Hynek Hermansky, and Em-
manuel Dupoux. Evaluating speech features with the minimal-pair ABX task: Analysis of the
classical MFC/PLP pipeline. In Proc. Annual Conference of International Speech Communica-
tion Association (INTERSPEECH), 2013.

Man-Hung. Siu, Herb Gish, Arthur Chan, William Belﬁeld, and Steve Lowe. Unsupervised training
of an HMM-based self-organizing unit recognizer with applications to topic classiﬁcation and
keyword discovery. Computer Speech and Language, 28(1):210–223, 2014.

D´ıdac Sur´ıs, Adri `a Recasens, David Bau, David Harwath, James Glass, and Antonio Torralba.
Learning words by drawing images.
In Proc. IEEE Conference on Computer Vision and Pat-
tern Recognition (CVPR), 2019.

15

Gabriel Synnaeve, Maarten Versteegh, and Emmanuel Dupoux. Learning words from images and
speech. In Proc. Neural Information Processing Systems (NeurIPS), 2014.

R. Thiolliere, E. Dunbar, G. Synnaeve, M. Versteegh, and E. Dupoux. A hybrid dynamic time
warping-deep neural network architecture for unsupervised acoustic modeling. In Proc. Annual
Conference of International Speech Communication Association (INTERSPEECH), 2015.

Aaron van den Oord, Oriol Vinyals, and Koray Kavukcuoglu. Neural discrete representation learn-
ing. In Proc. Neural Information Processing Systems (NeurIPS), 2017.

A ¨aron van den Oord, Yazhe Li, and Oriol Vinyals. Representation learning with contrastive predic-
tive coding. CoRR, abs/1807.03748, 2018. URL http://arxiv.org/abs/1807.03748.

Balakrishnan Varadarajan, Sanjeev Khudanpur, and Emmanuel Dupoux. Unsupervised learning of
acoustic sub-word units. In Proceedings of ACL-08: HLT, Short Papers, 2008.

Martin Versteegh, Roland Thiolliere, Thomas Schatz, Xuan Nga Cao, Xavier Anguera, Aren Jansen,
and Emmanuel Dupoux. The zero resource speech challenge 2015. In Proc. Annual Conference
of International Speech Communication Association (INTERSPEECH), 2015.

Virginia de Sa. Learning classiﬁcation with unlabeled data. In Proc. Neural Information Processing
Systems (NeurIPS), 1994.

Kilian Q. Weinberger and Lawrence K. Saul. Distance metric learning for large margin nearest
neighbor classiﬁcation. Journal of Machine Learning Research (JMLR), 2009.

Yaodong Zhang and James Glass. Unsupervised spoken keyword spotting via segmental dtw on
gaussian posteriorgrams. In Proc. IEEE Workshop on Automatic Speech Recognition and Under-
standing (ASRU), 2009.

Bolei Zhou, Agata Lapedriza, Jianxiong Xiao, Antonio Torralba, and Aude Oliva. Learning deep
features for scene recognition using places database.
In Proc. Neural Information Processing
Systems (NeurIPS), 2014.

16

A A P P END IX

A .1 VARY ING THE COD EBOOK S I Z E .

In Table 5, we examine the impact of varying the codebook size of model “∅ → {2}” from 128
through 2048. We ﬁnd that the ABX score is best for 1024 codebook vectors, although the perfor-
mance is quite good for all models. Unsurprisingly, models with smaller codebooks also achieve
lower bitrates.

Codebook size R@10 ABX Bitrate
128
.772
14.25
295.65
256
.756
12.95
341.18
512
.761
12.59
363.95
1024
.760
11.79
390.61
2048
.768
12.41
360.04

RLE-Bitrate
212.27
260.10
288.64
317.66
283.68

Segment-ABX Segment-Bitrate
15.42
179.38
14.21
228.07
13.10
259.94
12.66
289.11
13.15
254.23

Table 5: ABX scores and bitrates for various codebook sizes on the clean ZeroSpeech19 English
test set, using the “∅ → {2}” model.

A .2 UN I T V I SUA L I ZAT ION ON T IM IT S P EC TROGRAM S

In Figure 5, we visualize two unit sequences for two different TIMIT utterances from different
speakers containing the same underlying word sequence. Unit sequences are shown for the VQ2
layer for the “∅ → {2}” model with a codebook size of 128.

Figure 5: Two different instances of the same TIMIT sentence spoken by two different speakers.
Notice how despite the differences between the speakers, the same units are often assigned to the
same underlying phones.

17

1.01.21.41.61.82.0Time (seconds)02000400060008000Frequency (Hz)36651103049116389812342155810481132216113201098012563716101774113632263bclbrahdhaxrsixkclsehsbrother'ssuccess1.01.21.41.6Time (seconds)02000400060008000Frequency (Hz)36651103049533812347355104445922164256785109516322710817114201132263bclbrahdhaxrsax-hkclksehsbrother'ssuccessA .3 VQ3 WORD D E TECTOR TABL E S FOR VAR IOU S MOD EL S

In Table 6, we show a sampling of 50 word-detecting codebook entries from the VQ layer of the
“{3} → {2, 3}” model. Analagous results for the “{2} → {2, 3}” model are shown in Table 7.

Table 6: Performance of the VQ3 layer from the “{3} → {2, 3}” model when codes are treated as
word detectors. Codes are ranked by the highest F1 score among the retrieved words for a given
code. Word hypotheses for a given code are ranked by the F1 score.

rank

code

1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
186
187
188
189
190
191
192
193
194
195
196
197
198
199
200

58
706
88
116
584
596
48
625
557
5
534
274
44
310
18
560
598
769
892
124
802
162
918
388
641
85
1004
630
148
661
394
108
193
326
306
280
829
548
554
2
993
820
164
461
803
446
225
234
1000
842

word

baseball
background
classroom
construction
playground
kitchen
desert
background
concrete
airport
background
subway
patio
rocky
driveway
hospital
palm
bamboo
walking
stage
body
stadium
pantry
courtyard
volcano
yellow
standing
trees
course
distance
church
highway
small
shower
river
station
shirt
night
computer
empty
ruins
coffee
man
baby
train
lake
house
alleyway
orange
pink

Top Hypotheses
F1
P

R

occ

word

Second Hypotheses
F1
P

R

occ

90.09
89.26
88.78
88.16
87.84
87.26
87.17
86.79
86.61
86.46
86.09
86.01
85.93
85.90
85.68
84.35
83.68
83.68
83.56
83.43
83.39
83.08
83.00
82.88
82.81
82.78
82.78
82.75
82.74
82.47
82.42
82.36
82.32
81.88
81.83
68.23
68.21
68.15
68.14
67.96
67.93
67.88
67.80
67.71
67.62
67.56
67.40
67.26
67.25
67.19

89.45
92.36
87.39
90.19
85.68
86.42
87.85
94.12
91.17
89.04
80.24
86.62
89.94
86.19
91.60
90.16
82.26
85.57
85.02
84.65
84.24
80.35
83.46
88.76
79.72
78.72
85.87
79.45
86.86
77.00
74.32
82.27
89.55
88.68
83.88
54.23
70.83
64.95
71.66
76.93
59.53
63.95
73.79
64.00
85.63
78.98
58.83
56.62
89.09
78.12

90.75
86.36
90.21
86.22
90.12
88.11
86.50
80.52
82.49
84.02
92.86
85.41
82.27
85.62
80.49
79.24
85.16
81.88
82.15
82.24
82.56
86.00
82.54
77.74
86.15
87.28
79.90
86.35
79.00
88.77
92.51
82.45
76.17
76.06
79.89
91.95
65.78
71.69
64.95
60.87
79.08
72.32
62.71
71.88
55.87
59.02
78.89
82.81
54.00
58.94

3266
17740
1512
2484
930
5313
3319
16541
1917
962
19076
1264
2056
2375
1159
1191
2071
1265
7747
2103
6245
1327
558
1023
336
11412
12624
26838
1764
7779
4497
1038
16417
1312
4393
4366
6592
2277
1360
2805
586
1335
16622
1204
4720
2294
10776
944
2987
2723

player
backgrounds
class
constructed
play
kitchenette
doesn’t
back
country
escalator
back
station
patios
rock
sidewalk
hot
concrete
abandoned
walk
concert
large
boardwalk
country
graveyard
volcanoes
yellowish
stand
tree
golf
background
religious
highways
smaller
showers
rivers
gas
shirts
nighttime
computers
terminal
ruin
cream
men
baby’s
trains
late
houses
alley
oranges
paint

2.82
0.71
4.65
1.98
2.70
1.68
1.59
1.45
0.64
2.29
8.44
1.82
2.02
3.52
1.58
0.92
1.67
5.38
7.97
1.60
3.96
10.43
2.05
2.91
5.11
2.38
5.08
14.97
6.54
1.93
5.97
3.21
2.18
3.37
1.09
28.72
15.44
13.14
22.99
4.13
21.58
7.04
13.85
24.10
10.06
2.85
18.45
37.25
3.30
5.05

1.60
0.36
2.79
1.03
1.81
0.85
0.89
0.95
0.49
1.25
5.67
1.61
1.02
2.72
1.13
0.62
1.28
3.31
4.51
0.87
5.86
5.79
2.05
1.66
2.66
1.21
2.84
9.21
6.89
1.82
3.19
1.64
1.12
1.72
0.55
17.35
8.70
7.15
13.78
2.17
12.90
4.32
9.77
13.92
5.52
1.58
10.44
30.06
1.71
3.07

12.18
68.75
13.99
26.29
5.33
86.44
7.41
3.01
0.92
13.86
16.52
2.09
77.78
4.97
2.63
1.81
2.41
14.44
34.35
10.74
2.99
52.18
2.06
11.72
65.00
84.49
24.38
39.93
6.23
2.06
47.73
78.57
39.55
80.65
60.98
83.40
68.57
81.42
69.21
42.21
66.05
19.09
23.79
89.89
56.98
14.25
78.82
48.98
48.84
14.26

139
66
82
56
51
51
28
225
13
23
1233
99
28
266
47
17
56
109
519
51
1324
275
29
45
13
158
534
3436
195
423
263
22
263
25
25
1773
757
241
254
176
142
357
1390
249
306
53
1868
863
63
140

18

Table 7: Performance of the VQ3 layer from the “{2} → {2, 3}” model when codes are treated as
word detectors. Codes are ranked by the highest F1 score among the retrieved words for a given
code. Word hypotheses for a given code are ranked by the F1 score.

rank

code

1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
186
187
188
189
190
191
192
193
194
195
196
197
198
199
200

924
530
749
505
581
778
144
299
550
76
831
80
1015
719
816
614
457
536
985
0
870
243
480
968
245
815
153
526
293
538
395
971
133
740
715
374
869
982
746
791
522
181
790
42
801
975
432
329
362
256

word

people
building
white
blue
snow
building
with
small
large
trees
water
red
large
woman
water
people
sky
man
trees
black
trees
front
has
picture
yellow
with
yellow
small
large
black
picture
man
white
trees
white
there’s
old
structure
has
with
road
trees
photo
man
sitting
purple
parked
standing
white
trees

Top Hypotheses
F1
P

R

occ

word

Second Hypotheses
F1
P

R

occ

computer
buildings
one
pool
small
buildings
looking
snow
car
tree
wall
bed
car
women
river
table
skies
standing
tree
background
train
from
house
pictures
ﬂowers
white
area
large
bridge
glass
pictures
many
black
station
like
that
all
kids
that
there’s
ﬁeld
with
by
middle
with
parked
park
woman
table
see

2.30
23.44
5.22
9.13
16.17
14.46
5.94
30.58
8.35
15.18
18.10
9.46
6.50
7.01
5.32
22.26
10.67
8.90
11.13
21.75
13.53
14.71
9.95
15.88
14.94
7.90
14.93
5.41
18.38
8.57
13.13
17.62
16.84
3.72
9.73
4.92
5.19
2.62
5.34
3.15
5.93
5.40
5.51
3.70
4.97
5.18
5.11
3.15
4.71
4.48

1.18
13.44
3.15
4.97
11.72
7.85
3.21
18.38
4.41
8.31
10.18
5.08
3.40
3.70
2.76
12.94
5.68
4.77
6.04
12.64
7.34
8.72
5.56
8.83
8.17
4.48
9.18
3.64
10.39
4.52
7.28
10.08
9.60
1.91
5.75
2.80
2.80
1.98
3.01
1.76
3.11
3.39
2.99
1.91
3.66
2.69
2.64
1.65
2.49
2.45

42.36
91.70
15.33
56.87
26.06
91.28
39.15
90.95
79.56
87.91
81.56
68.27
74.64
65.24
74.00
79.67
86.90
66.21
71.19
77.69
87.36
47.02
47.15
78.99
87.48
33.60
40.10
10.55
79.22
81.80
66.27
69.85
68.17
66.22
31.49
20.53
36.29
3.88
23.72
14.96
63.57
13.33
35.62
60.91
7.77
67.98
82.26
35.41
44.61
26.12

887
7260
3315
2674
5617
7227
6194
9595
4446
7565
10428
3527
4171
2329
4069
13128
2580
10460
6126
15960
7380
7540
6441
13439
5623
17037
6956
4664
7066
4787
11276
10692
18818
3144
7730
11745
5490
55
13565
7790
7647
15456
4540
6059
9009
2567
2328
5704
7351
6346

76.60
75.96
75.86
58.47
58.30
53.38
49.44
48.88
46.00
45.43
42.01
39.67
39.36
38.39
38.00
37.71
36.42
34.14
34.12
33.50
33.41
33.31
33.15
32.94
32.91
32.65
32.25
32.21
31.95
31.32
30.79
29.89
29.80
29.39
29.09
6.42
6.41
6.39
6.25
6.15
6.11
6.11
6.04
6.01
5.82
5.76
5.67
5.29
5.27
5.24

66.79
65.57
66.14
46.53
42.59
37.93
41.91
33.61
31.22
30.22
27.59
26.38
25.85
24.68
24.37
25.01
23.76
21.12
22.22
20.83
21.18
21.81
23.13
21.29
20.16
22.54
19.76
19.65
22.88
19.53
20.31
18.58
18.86
18.56
19.41
3.62
3.39
11.15
3.44
3.78
3.18
3.28
3.25
3.23
3.10
3.02
2.94
2.80
3.05
2.88

89.80
90.27
88.93
78.67
92.38
90.05
60.29
89.58
87.40
91.45
88.06
79.94
82.37
86.37
86.24
76.59
77.99
89.09
73.46
85.51
79.07
70.48
58.46
72.74
89.55
59.20
87.63
89.32
52.92
78.98
63.58
76.36
71.02
70.52
58.11
28.70
58.22
4.48
33.94
16.43
77.10
45.19
41.89
42.78
48.26
63.14
78.60
49.17
19.49
28.84

33662
35229
45097
21678
9746
35144
69909
19308
38648
28423
24901
19978
36423
13913
24388
28709
10752
23614
22832
23603
24577
19397
23919
33055
11709
68651
11458
19251
23401
21801
28894
20239
36014
21918
29465
14948
6333
241
13888
19054
5614
14044
4958
11340
8202
2388
2968
7769
9883
8963

19

