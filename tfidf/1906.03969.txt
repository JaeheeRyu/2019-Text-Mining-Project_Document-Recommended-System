Datalog Disassembly

Antonio Flores-Montoya
GrammaTech, Inc.

Eric Schulte
GrammaTech, Inc.

afloresmontoya@grammatech.com

eschulte@grammatech.com

9
1
0
2

v
o

N

1
2

]

L

P

.

s

c

[

4
v
9
6
9
3
0

.

6
0
9
1

:

v

i

X

r

a

Abstract

Disassembly is fundamental to binary analysis and rewrit-
ing. We present a novel disassembly technique that takes a
stripped binary and produces reassembleable assembly code.
The resulting assembly code has accurate symbolic informa-
tion, providing cross-references for analysis and to enable ad-
justment of code and data pointers to accommodate rewriting.
Our technique features multiple static analyses and heuris-
tics in a combined Datalog implementation. We argue that
Datalog’s inference process is particularly well suited for dis-
assembly and the required analyses. Our implementation and
experiments supports this claim. We have implemented our ap-
proach into an open-source tool called Ddisasm. In extensive
experiments in which we rewrite thousands of x64 binaries
we ﬁnd Ddisasm is both faster and more accurate than the
current state-of-the-art binary reassembling tool, Ramblr.

1 Introduction

Software is increasingly ubiquitous and the identiﬁcation and
mitigation of software vulnerabilities is increasingly essential
to the functioning of modern society. In many cases—e.g.,
COTS or legacy binaries, libraries, and drivers—source code
is not available so identiﬁcation and mitigation requires binary
analysis and rewriting. Many disassemblers [8, 9, 20, 28, 33,
52, 53, 55], analysis frameworks [4, 6, 11, 18, 22–24, 26, 36, 44],
rewriting frameworks [8, 15, 16, 29, 30, 48, 51, 54, 59], and re-
assembling tools [33, 52, 53] have been developed to support
this need. Many applications depend on these tools includ-
ing binary hardening with control ﬂow protection [19, 34,
37, 50, 58, 60], memory protections [14, 38, 45], and memory
diversity [13, 27], binary refactoring [49], binary instrumenta-
tion [41], and binary optimization [41, 43, 51].
Modifying a binary is not easy. Machine code is not de-
signed to be modiﬁed and the compilation and assembly pro-
cess discards essential information. In general reversing as-
sembly is not decidable. The information required to produce
reassembleable disassembly includes:

Instruction boundaries Recovering where instructions start
and end can be challenging especially in architectures
such as x64 that have variable length instructions, dense
instruction sets1 , and sometimes interleave code and data.
This problem is also referred as content classiﬁcation.
Symbolization information In binaries, there is no distinc-
tion between a number that represents a literal and a
reference that points to a location in the code or data. If
we modify a binary—e.g., by moving a block of code—
all references pointing to that block, and to all of the
subsequently shifted blocks, have to be updated. On the
other hand, literals, even if they coincide with the address
of a block, have to remain unchanged. This problem is
also referred to as Literal Reference Disambiguation.
We have developed a disassembler that infers precise infor-
mation for both questions and thus generates reassembleable
assembly for a large variety of programs. These problems are
not solvable in general so our approach leverages a combi-
nation of static program analysis and heuristics derived from
empirical analysis of common compiler and assembler idioms.
The static analysis, heuristics, and their combination are im-
plemented in Datalog. Datalog is a declarative language that
can be used to express dataﬂow analyses very concisely [46]
and it has recently gained attention with the appearance of
engines such as Soufﬂe [25] that generate highly efﬁcient
parallel C++ code from a Datalog program. We argue that
Datalog is so well suited to the implementation of a disassem-
bler that it represents a qualitative change in what is possible
in terms of accuracy and efﬁciency.
We can conceptualize disassembly as taking a series of de-
cisions. Instruction boundary identiﬁcation (IBI) amounts to
deciding, for each address x in an executable section, whether
x represent the beginning of an instruction or not. Symbol-
ization amounts to deciding for each number that appears
inside an instruction operand or data section whether it corre-
sponds to a literal or to a symbolic expression and what kind
of symbolic expression it is.2

1Almost any combination of bytes corresponds to a valid instruction.

2 E.g., symbol, symbol+constant, or symbol−symbol.

1

 
 
 
 
 
 
The high level approach for each of these decisions is the
same. A variety of static analyses are performed that gather
evidence for possible interpretations. Then, Datalog rules as-
sign weights to the evidence and aggregate the results for
each interpretation. Finally, a decision is taken according to
the aggregate weight of each possible interpretation. Our im-
plementation infers instruction boundaries ﬁrst (described in
Sec. 4). Then it performs several static analyses to support the
symbolization procedure: the computation of def-use chains,
a novel register value analysis, and a data access pattern anal-
ysis described in Sec. 5.1, 5.2, and 5.3 respectively. Finally, it
combines the results of the static analyses with other heuris-
tics to inform symbolization. All these steps are implemented
in a single Datalog program. It is worth noting that—Datalog
being a purely declarative language—the sequence in which
each of the disassembly steps is computed stems solely from
the logical dependencies among the different Datalog rules.
We have tested Ddisasm and compared it to Ramblr [52]
(the current best published disassembler that produces re-
assembleable assembly) on 200 benchmark programs includ-
ing 106 Coreutils, 25 real world applications, and 69 bina-
ries from DARPA’s Cyber Grand Challenge (CGC) [1]. We
compile each benchmark using 5 compilers and 5 or 6 opti-
mization ﬂags (depending on the benchmark) yielding a total
of 5470 unique binaries (647 MB of binary data). We com-
pare the precision of the disassemblers by making semantics-
preserving modiﬁcations to the assembly code—we “stretch”
the program’s code address space by adding NOPs at regu-
lar intervals—reassembling the modiﬁed assembly code, and
then running the test suites distributed with the binaries to
check that they retain functionality. Additionally, we evaluate
the symbolization step by comparing the results of the disas-
sembler to the ground truth extracted from binaries generated
with all relocation information. Finally, we compare the dis-
assemblers in terms of the time taken by the disassembly
process. Ddisasm is faster and more accurate than Ramblr.
Our contributions are:
1. We present a new disassembly framework based on com-
bining static analysis and heuristics expressed in Datalog.
This framework enables much faster development and
empirical evaluation of new heuristics and analyses.
2. We present multiple static analyses implemented in this
framework to support building reassembleable assembly.
3. We present multiple empirically motivated heuristics
that are effective in inferring the necessary information
to produce reassembleable assembly.
4. Our implementation is called Ddisasm and it is open
source and publicly available3 . Ddisasm produces as-
sembly text as well as an intermediate representation
(IR) tailored for binary analysis and rewriting4 .
5. We demonstrate the effectiveness of our approach
through an extensive experimental evaluation of over

3 https://github.com/GrammaTech/ddisasm
4 https://github.com/grammatech/gtirb

5470 binaries in which we compare Ddisasm to the state-
of-the-art tool in reassembleable disassembly Ramblr.

2 Related Work

2.1 Disassemblers

Bin-CFI [60] is an early work in reassembleable disassem-
bly. This work requires relocation information (avoiding the
need for symbolization). With this information, disassembly
is reduced to the problem of IBI. Bin-CFI combines linear
disasssembly with the backward propagation of invalid op-
codes and invalid jumps. Our IBI also propagates invalid
opcodes and jumps backwards, but it couples it with a more
sophisticated forward traversal.
Many other works focus solely on IBI [9, 28, 33, 55]. None
of these address symbolization. In general they try to obtain
a superset of all possible instructions or basic blocks in the
binary and then determine which ones are real using heuris-
tics. This idea is also present in our approach. Both Miller et
al. [33] and Wartell et al. [55] use probabilistic methods to
determine which addresses contain instructions. In the former,
probabilistic techniques with weighted heuristics are used to
estimate the probability that each offset in the code section is
the start of an instruction. In the latter, a probabilistic ﬁnite
state machine is trained on a large corpus of disassembled
programs to learn common opcode operand pairs. These pairs
are used to select among possible assembly codes.
Despite all the work on disassembly, there are disagree-
ments on how often challenging features for IBI—e.g., over-
lapping instructions, data in code sections, and multi-entry
functions—are present in real code [5, 32, 33]. Our experience
matches [5] for GCC and Clang, in that we did not ﬁnd data
in executable sections nor overlapping instructions in ELF
binaries. However, this is not true for the Intel compiler (ICC)
which often allocates jump tables in executable sections.
There are only a few systems that address the symboliza-
tion problem directly. Uroboros [53] uses linear disassembly
as introduced by Bin-CFI [60] and adds heuristics for symbol-
ization. The authors distinguish four classes of symbolization
depending on if the source and target of the reference are
present in code or data. The difﬁculty of each class is as-
sessed and partial solutions are proposed for each class.
Ramblr [52] is the closest related work. It improves upon
Uroboros with increasingly sophisticated static analyses.
Ramblr is part of the Angr framework for binary analysis [44].
Our system also uses static analyses in combination with
heuristics. Our static analyses (Sec. 5) are specially tailored
to enable symbolization while remaining efﬁcient. Moreover,
our Datalog implementation allow us to easily combine anal-
ysis results and heuristics.
RetroWrite [17] also performs symbolization, but only for
position independent code (PIC) as it relies on relocations. In

2

Sec. 7.1, we argue why we believe that relocations are not
enough to perform symbolization even for PIC.

2.2 Rewriting Systems

REINS [54] rewrites binaries in such a way as to avoid mak-
ing difﬁcult decisions about symbolization. REINS partitions
the memory of rewritten programs into untrusted low-memory
which includes rewritten code and trusted high-memory (di-
vided at a power of two for efﬁcient guarding). They imple-
ment a lightweight binary lookup table to rewrite each old
jump targets with a tagged pointer to its new location in the
rewritten code. REINS targets Windows binaries and its main
goal is to rewrite untrusted code to execute it safely. REINS
uses IDA Pro [3] to perform IBI and to resolve indirect jumps.
SecondWrite [48] also avoids making symbolization deci-
sions by translating jump targets at their point of usage. They
do a conservative identiﬁcation of code and data by perform-
ing speculative disassembly and keeping the original code
section intact. Any data in the code section can still be ac-
cessed, but jumps and call targets are translated to a rewritten
code section. SecondWrite disassembles to LLVM IR.
MULTIVERSE [8] goes a step further than SecondWrite
and also avoids making code location determinations by treat-
ing every possible instruction offset as a valid instruction.
Similarly to SecondWrite, it avoids making symbolization
determinations by generating rewritten executables in which
every indirect control ﬂow is mediated by additional machin-
ery to determine where the control ﬂow would have gone
in the original program and redirecting it to the appropriate
portion of the rewritten program.
The approaches of REINS, SecondWrite and MULTI-
VERSE increasingly avoid making decisions about code lo-
cation and symbolization and thus offer more guarantees to
work for arbitrary binaries. However, these approaches also
have disadvantages. They introduce overhead in the rewritten
binaries both in terms of speed and size. Moreover, the addi-
tional translation process for indirect jumps or calls is likely
to hinder later analyses on the disassembled code. On the
other hand, our approach, although not guaranteed to work,
generates assembly code with symbolic references. This en-
ables performing advanced static analyses on the assembly
code that can be used to support more sophisticated rewriting
techniques. A binary can be rewritten multiple times without
introducing a new layer of indirection in every rewrite.

2.3 Static Analysis Using Datalog

Datalog has a long history of being used to specify and im-
plement static analyses. In 1995 Reps [40] presented an ap-
proach to obtain demand driven dataﬂow analyses from the
exhaustive counterparts speciﬁed in Datalog using the magic
sets transformation. Much of the subsequent effort has been
in scaling Datalog implementations. In that vein, Whaley et

al. [56, 57] achieved signiﬁcant pointer analysis scalability im-
provements using an implementation based on binary decision
diagrams. More recently, Datalog-based program analysis has
received new impetus with the development of Soufﬂe [25],
a highly efﬁcient Datalog engine. The most prominent ap-
plication of Datalog to program analysis to date has been
Doop [10, 46, 47], a context sensitive pointer analysis for Java
bytecode that scales to large applications. Doop is currently
one of the most comprehensive and efﬁcient pointer analysis
for Java.
In the context of binary analysis, we are only aware of the
work of Brumley et al. [12] which uses Datalog to specify an
alias analysis for assembly code. Schwartz et al. [42] present
a binary analysis to recover C++ classes from executables
written in Prolog. Prolog, being more expressive than Data-
log, is typically evaluated starting from a goal—in contrast to
Datalog which can be evaluated bottom-up—and using back-
tracking. Thus, in Prolog programs the order of the inference
rules is important and its evaluation is harder to parallelize.
Very recently, Grech et al. [21] have implemented a de-
compiler, named Gigahorse, for Etherium virtual machine
(EVM) byte code using Datalog. Gigahorse shares some high
level ideas with our approach, i.e. the inference of high level
information from low-level code using Datalog. However,
both the target and the inferred information differ consider-
ably. In EVM byte code, the main challenge is to obtain a
register based IR (EVM byte code is stack based), resolve
jump targets and identify function boundaries. On the other
hand, Ddisasm focuses on obtaining instruction boundaries
and symbolization information for x64 binaries. Additionally,
although Gigahorse also implements heuristics using Datalog
rules, it does not use our approach of assigning weights to
heuristics and aggregating them to make ﬁnal decisions.

3 Preliminaries

3.1

Introduction to Datalog

A Datalog program is a collection of Datalog rules. A Datalog
rule is a restricted kind of horn clause with the following
format: h : − t1 , t2 , . . . , tn where h, t1 , t2 , . . . , tn are predicates.
Rules represent a logical entailment: t1 ∧ t2 ∧ . . . ∧ tn → h.
Predicates in Datalog are limited to ﬂat terms of the form
t (s1 , s2 , . . . , sn ) where s1 , s2 . . . , sn are variables, integers or
strings. Given a Datalog rule h : − t1 , t2 , . . . , tn , we say h is the
head of the rule and t1 , t2 , . . . , tn is its body.
Datalog rules are often recursive, and they can contain
negated predicates, represented as !t . However, negated predi-
cates need to be stratiﬁed—there cannot be circular dependen-
cies that involve negated predicates e.g. p(X ): −!q(X ) and
q(A): −! p(A). This restriction guarantees that its semantics
are well deﬁned. Additionally, all variables in a Datalog rule
need to be grounded, i.e. they need to appear in at least one
non-negated predicate on the rule’s body. Datalog also admits

3

instruction(A:A,Size:Z64 ,Prefix:S,Opcode:S,Op1:O,Op2:O,Op3:O,Op4:O)
invalid(A:A)
op_regdirect(Op:O,Reg:R)
op_immediate(Op:O,Immediate:Z64 )
op_indirect(Op:O,Reg1:R,Reg2:R,Reg3:R,Mult:Z64 ,Disp:Z64 ,Size:Z64 )

data_byte(A:A,Val:Z64 )
address_in_data(A:A,Val:Z64 )

Figure 1: Initial facts. Facts generated for executable sections on the left and facts generated for all sections on the right.

disjunctive rules denoted with a semicolon e.g. h : − t1 ; t2 that
are equivalent to several regular rules h : − t1 and h : − t2 .
The Datalog dialect that we adopt (Soufﬂe’s dialect) sup-
ports additional constructs such as arithmetic operations,
string operations and aggregates. Aggregates compute op-
erations over a complete set of predicates such as summation,
maximums or minimums, and we use them to integrate the
results of our heuristics.
A Datalog engine takes as input a set of facts, which are
predicates known to be true, and a Datalog program (a set
of rules). The engine generates new predicates by repeatedly
applying the inference rules until a ﬁxpoint is reached. One of
the appeals of Datalog is that it is fully declarative. The result
of a computation does not depend on the order in which rules
are considered or the order in which predicates within a rule’s
body are evaluated. This makes it easy to deﬁne multiple
analyses that depend and collaborate with each other.
In our case, the initial set of facts encodes all the informa-
tion present in the binary, the disassembly procedure (with
all its auxiliary analyses) is speciﬁed as a set of Datalog rules.
The results of the disassembly are the new set of predicates.
These predicates are then used to build an IR for binaries that
can be reassembled.

3.2 Encoding Binaries in Datalog

The ﬁrst step in our analysis is to encode all the informa-
tion present in the binary into Datalog facts. We consider
two basic domains: strings, denoted as S, and 64 bit machine
numbers, denoted as Z64 . We consider also the following
sub-domains: addresses A ⊆ Z64 , register names R ⊆ S and
operand identiﬁers O ⊆ Z64 . We adopt the convention of hav-
ing Datalog variables start with a capital letter and predicates
with lower case. We represent addresses in hexadecimal and
all other numbers in decimal. We only use the preﬁx 0x for
hexadecimal numbers if there is ambiguity.
Fig. 1 declares the predicates used to represent the initial
set of raw instruction facts. Predicate ﬁelds are annotated
with their type. To generate these initial facts we apply a de-
coder (Capstone [39]) to attempt to decode every address x
in the executable sections of a binary5 . If the decoder suc-
ceeds, we generate an instruction fact with A= x. If the de-

5 This is different from linear disassembly which would try to decode an
instruction at address x + s after decoding an instruction of size s at address
x (skipping the addresses in between).

4 1 6 C 3 5 :
4 1 6 C 3 C :
4 1 6 C 4 0 :
4 1 6 C 4 7 :
4 1 6 C 4 E :
4 1 6 C 5 3 :
4 1 6 C 5 8 :
4 1 6 C 5 C :

mov RBX, - 6 2 4
nop
mov RDI, QWORD PTR [RIP + 0 x 2 5 D 2 3 9 ]
mov RSI, QWORD PTR [RBX + 0 x 4 5 D 3 2 8 ]
mov EDX, OFFSET 0 x 4 5 C B 2 3

call 0 x 4 1 3 0 5 0

add RBX, 2 4

jne 0 x 4 1 6 C 4 0

Figure 2: Assembly (before symbolization) extracted from
wget-1.19.1 compiled with Clang 3.8 and optimization -O2.
This code reads 8 byte data elements at address 416C47 within
the address range [45D0B8, 45D328] and spaced every 24
bytes.

coder fails, the fact invalid(x) is generated instead. In each
instruction predicate, the ﬁeld Size represents the size of
the instruction, Prefix is the instruction’s preﬁx, and Opcode
is the instruction code. Instruction operands are stored as inde-

pendent facts op_regdirect, op_immediate and op_indirect

, whose ﬁrst ﬁeld Op contains a unique identiﬁer. This identi-
ﬁer is used to match operands to their instructions. The ﬁelds
Op1 to Op4 in predicate instruction contain the operands’
unique identiﬁers or 0 if the instruction does not have as many
operands. We place source operands ﬁrst and the destination
operand last. The predicate op_regdirect contains a register
name Reg, op_immediate contains an immediate Immediate
and op_indirect represents an indirect operand of the form

Reg1:[Reg2+Reg3×Mult+Disp]. That is, Reg1 is the segment

register, Reg2 is the base register, Reg3 is the index register,
Mult represents the multiplier, and Disp represents the dis-
placement. Finally, the ﬁeld Size represents the size of the
data element being accessed in bytes.

Example 1. Consider the code in Fig. 2. The encoding of
the instructions at addresses 416C47 and 416C58 together with
their respective operands can be found below:

instruction(416C47,7,’’,’mov’,14806,538,0,0)
op_indirect(14806,’NONE’,’RBX’,’NONE’,1,45D328,8)
op_regdirect(538,’RSI’)

instruction(416C58,4,’’,’add’,188,519,0,0)
op_immediate(188,24)
op_regdirect(519,’RBX’)

Note that the operand identiﬁers have no particular mean-

4

ing. They are assigned to operands sequentially as these are
encountered during the decoding.

In addition to decoding every possible instruction, we en-
code every section (both data and executable sections) as
follows. For each address A in a section, a fact data_byte(A,
Val) is generated where Val is the value of the byte at address
A. We also generate the facts address_in_data(A,Addr) for
each address A in a section such that the values of the bytes
from A to A+7 (8 bytes)6 correspond to an address Addr that
falls in the address range of a section in the binary. These facts
will be our initial candidates for symbolization. Executable
sections are also encoded this way to support binaries that
interleave data with code.
Finally, additional facts are generated from the section, relo-
cation, and symbol tables of the executable as well as a special
fact entry_point(A:A) with the entry point of the executable.
Note that for libraries, function symbol predicates are gener-
ated for all exported functions and they will be considered as
entry points.

4 Instruction Boundary Identiﬁcation

The predicate instruction contains all the possible instruc-
tions that might be in the executable. IBI amounts to deciding
which of these are real instructions.
Our IBI is based on three steps:
1. A backward traversal starting from invalid addresses.
2. A forward traversal that combines elements of linear-
sweep and recursive-traversal.
3. A conﬂict resolution phase to discard spurious blocks.
Both the backward and forward traversals use the

auxiliary predicates may_fallthrough(From:A,To:A) and
must_fallthrough(From:A,To:A) to represent instructions

at address From that may fall through or must fall through
to an address To. Fig. 3 contains the rules that deﬁne both
predicates7 . An instruction at address From may fall through
to the next one at address From+Size as long as it is not a
return, a halt, or an unconditional jump instruction. Rule 1
depends in turn on other auxiliary predicates that abstract
away speciﬁc aspects of concrete assembler instructions e.g.

return_operation is simply deﬁned as return_operation

(’ret’) for x64. The predicate must_fallthrough restricts
may_fallthrough further by discarding instructions that
might not continue to the next instruction i.e. calls, jumps,
or interrupt operations (we consider instructions with a loop
preﬁx as having a jump to themselves).
The traversals also depend on other predicates whose deﬁ-

nitions we omit: direct_jump(From:A,To:A), direct_call
(From:A,To:A), pc_relative_jump(From:A,To:A), and
pc_relative_call(From:A,To:A) represent instructions at

6Our analysis considers x64 architecture.
7 Some of the rules have been slightly adapted for presentation purposes.

5

may_fallthrough(From,To):−
instruction(From,Size,_,OpCode,_,_,_,_),
To=From+Size,
!return_operation(OpCode),
!unconditional_jump_operation(OpCode),
!halt_operation(OpCode).
must_fallthrough(From,To):−
may_fallthrough(From,To),
instruction(From,_,_,OpCode,_,_,_,_),
!call_operation(OpCode),
!interrupt_operation(OpCode),
!jump_operation(OpCode),
!instruction_has_loop_prefix(From).

(1)

(2)

Figure 3: Auxiliary Datalog predicates used for traversal.

address From that have a direct or RIP-relative jump or call to
an address To.

Example 2. Consider the code in Fig. 2. The mov
instruction at address 416C4E generates the predicates

must_fallthrough(416C4E,416C53) and may_fallthrough

(416C4E,416C53) whereas the call instruction only generates

may_fallthrough(416C53,416C58). This is because the func-

tion at address 413050 (the target of the call) might not return.
The call instruction also generates the predicate direct_call

(416C53,413050).

4.1 Backward Traversal

Our backward traversal simply expands the amount of
invalid predicates through the implication that any instruc-
tion unconditionally leading to an invalid instruction must
itself be invalid.

invalid(From):−
(must_fallthrough(From,To) ;
direct_jump(From,To) ;
direct_call(From,To) ;
pc_relative_jump(From,To) ;
pc_relative_call(From,To)),
(invalid(To) ;
!instruction(To,_,_,_,_,_,_,_)).
possible_effective_address(A):−
instruction(A,_,_,_,_,_,_,_), !invalid(A).

(3)

(4)

Rule 3 speciﬁes that an instruction at address From that
jumps, calls or must fall through to an address To that does
not contain an potential instruction or to an address To
that contains an invalid instruction is also invalid. The

predicate possible_effective_address(A:A) contains the

addresses of the remaining instructions not discarded by
invalid (Rule 4).

code_in_block_candidate(A,A):−
possible_target(A),
possible_effective_address(A).
code_in_block_candidate(A,Block):−
code_in_block_candidate(Aprev,Block),
must_fallthrough(Aprev,A),
!block_limit(A).
code_in_block_candidate(A,A):−
code_in_block_candidate(Aprev,Block),
may_fallthrough(Aprev,A),
(!must_fallthrough(Aprev,A) ;
block_limit(A)),
possible_effective_address(A).
possible_target(A):−
initial_target(A).
possible_target(Dest):−
code_in_block_candidate(Src,_),
(may_have_symbolic_immediate(Src,Dest) ;
pc_relative_jump(Src,Dest) ;
pc_relative_call(Src,Dest)).
possible_target(A):−
after_block_end(_,A).

(5)

(6)

(7)

(8)

(9)

(10)

Figure 4: Block forward traversal rules.

4.2 Forward Traversal

The forward traversal follows an approach that falls between
the two classical approaches linear-sweep and recursive-
traversal. It traverses the code recursively but is much more
aggressive than typical traversals in terms of the targets that
it considers. Instead of starting the traversal only on the tar-
gets of direct jumps or calls, every address that appears in
one of the operands of the already traversed code is consid-
ered a possible target. For example, in Fig. 2, as soon as the
analysis traverses instruction mov EDX, OFFSET 0x45CB23,
it will consider the address 45CB23 as a potential target that it
needs to explore. Additionally, potential addresses appearing
in the data (instances of predicate address_in_data) are also
considered potential targets.
The traversal is deﬁned with two mutually recursive predi-
cates: possible_target(A:A) speciﬁes addresses where we
start traversing the code and code_in_block_candidate(A:A
,Block:A) takes care of the traversing and assigning instruc-
tions to basic blocks. A predicate code_in_block_candidate
(A:A,Block:A) denotes that the instruction address A be-
longs to the candidate code block that starts at address Block.
The deﬁnition of these predicates can be found in Fig. 4.
The traversal starts with the initial_target (Rule 8) that
contains the addresses of: entry points, any existing function
symbols, landing pad addresses (deﬁned in the exception in-
formation sections), the start addresses of executable sections,
and all addresses in address_in_data. This last component
implies that all the targets of jump tables or function pointers

present in the data sections will be traversed.
However, not all jump tables are lists of absolute addresses
(captured by address_in_data). Sometimes jump tables are
stored as differences between two symbols i.e. Symbol1−
Symbol2. In these tables, the jump target Symbol1 is computed
by loading Symbol2 ﬁrst and then adding the content of the
jump table entry. We found this pattern in PIC code and in
position dependent code compiled with ICC (see App. A). An
approximation of these jump tables is detected with ad-hoc
rules and their targets are included in initial_target.
A possible target, marks the beginning of a new basic block
candidate (Rule 5). The candidate block is then extended
as long as the instructions are guaranteed to fall through
and we do not reach a block_limit (Rule 6). The predi-
cate block_limit over-approximates possible_target (it is
computed the same way but without requiring the predicate
code_in_block_candidate in Rule 9). Rule 7 starts a new
block if the instruction is not guaranteed to fall through or
if there is a block limit. That is where the previous block
ends. Any addresses or jump/call targets that appear in a
block candidate are considered new possible targets (Rule 9).

may_have_symbolic_immediate includes direct jumps and

calls but also other immediates. E.g. instruction mov EDX,

OFFSET 45CB23 generates may_have_symbolic_immediate

(416C4E,45CB23). Note that this is much more aggressive
than a typical recursive traversal that would only consider the
targets of jumps or calls. Finally, Rule 10 adds a linear-sweep
component to the traversal. after_block_end(End:A,A:A)
contains addresses A after blocks that end with an instruction
that cannot fall through at End (e.g. an unconditional jump or
a return). This predicate skips any padding (e.g., contiguous
NOPs) that might be found after the end of the previous block.
It is worth noting that in our Datalog speciﬁcation we do
not have to worry about many issues that would be important
in lower level implementations of equivalent binary traver-
sals. For instance, we do not need to keep track of which
instructions and blocks have already been traversed nor do
we specify the order in which different paths are explored.

4.3 Solving Block Conﬂicts

Once the second traversal is over, we have a set of candi-
date blocks, each one with a set of instructions (encoded in
the predicate code_in_block_candidate). These blocks rep-
resent our best effort to obtain an over-approximation of the
basic blocks in the original program. In principle, it is pos-
sible to miss code blocks. However, such code block would
have to be reachable only through a computed jump/call and
be preceded by data that derails the linear-sweep component
of the traversal (Rule 10). We have not found any instance of
this situation. We remark that if the address of a block appears
anywhere in the code or in the data, it will be considered. For
instance, ICC puts some jump tables in executable sections.
By detecting these jump tables, we consider their jump tar-

6

gets (which are typically the blocks after the jump table) as
possible targets in our traversal.
The next step in our IBI is to decide which candidate blocks
are real. For that, we detect the blocks that overlap with each
other or with a potential data segment (e.g. a jump table in
the executable section). Overlapping blocks are extremely
uncommon in compiled code. The situations in which they
appear tend to respond to very speciﬁc patterns such as a block
starting with or without a lock preﬁx [32]. We recognize those
patterns with ad-hoc rules and consider that the remaining
blocks should not overlap. Thus, if two blocks overlap, we
assume one of them is spurious and needs to be discarded.
This assumption could be relaxed if we wanted to disassemble
malware but it is generally useful for compiled binaries.
We decide which blocks to discard using heuristics.
Each heuristic is implemented as a Datalog rule that pro-
duces a predicate of the form block_points(Block:A,Src:A
,Points:Z64 ,Why:S). Such a predicate assigns Points points
to the block starting at address Block. The ﬁeld Src is an
optional reference to another block that is the cause of the
points or zero for heuristics that are not based on other blocks.
The ﬁeld Why is a string that describes the heuristic for debug-
ging purposes and to distinguish the predicate from others
generated from different heuristics.
We compute the total number of points for each block using
Soufﬂe’s aggregates [25]. Then, given two overlapping blocks,
we discard the one with the least points. In case of a tie, we
keep the ﬁrst block and emit a warning. We also discard blocks
if their total points is below a threshold. This is useful for
blocks whose heuristics indicate overlap with data elements.
Our heuristics are mainly based on how blocks are inter-
connected, how they ﬁt together spatially, and whether they
are referenced by potential pointers or overlap with jump ta-
bles. Some of the heuristics used are described below (+ for
positive points and − for negative points):
+ The block is called, jumped to, or there is a fallthrough
from a non-overlapping block.
+ The block’s initial address appears somewhere in the
code or data sections. If the appearance is at an aligned
address, it receives more points.
+ The block calls/jumps other non-overlapping blocks.
− A potential jump table overlaps with the block.
All memory not covered by a block is considered data.

5 Auxiliary Analyses

The next step in our disassembly procedure is symbolization.
However, we ﬁrst perform several static analyses to infer how
data is accessed and used, and thus deduce its layout.

5.1 Register Def-Use Analysis

First, we compute register deﬁnition-uses chains. The analysis
produces predicates of the form:

def_used(Adef:A,Reg:R,Aused:A,Index:Z64 )

The register Reg is deﬁned at address Adef and used at address
Aused in the operand with index Index.
The analysis ﬁrst infers deﬁnitions def(Adef:A,Reg:R)

and uses use(Aused:A,Reg:R,Index:Z64 ). Then, it propa-

gates deﬁnitions through the code and matches them to uses.
The analysis is intra-procedural in that it does not traverse
calls but only direct jumps. This makes the analysis incom-
plete but improves scalability. During the propagation of
deﬁnitions, the analysis assumes that certain registers keep
their values through calls following Linux x64 calling con-
vention [31].
Example 3. Consider the code fragment in Fig. 2. The Def-
Use analysis produces the following predicates:

def_used(416C35,’RBX’,416C47,1)
def_used(416C35,’RBX’,416C58,2)
def_used(416C58,’RBX’,416C58,2)
def_used(416C58,’RBX’,416C47,1)

One important detail is that the analysis considers the 32
bits and 64 bits registers as one given that the x64 architec-
ture zeroes the upper part of 64 bits registers whenever the
corresponding 32 bits register is written. That means that for

instruction mov EDX, OFFSET 0x45CB23 at address 416C4E,

the analysis generates a deﬁnition def(416C4E,RDX).
Once we have def-use chains, we want to know which reg-
ister deﬁnitions are potentially used to compute addresses to
access memory. For that purpose, the disassembler computes
a new predicate:

def_used_for_address(Adef:A,Reg:R)

that denotes that the register Reg deﬁned at address Adef might
be used to compute a memory access. This predicate is com-
puted by traversing def-use chains backwards starting from
instructions that access memory. This traversal is transitive,
if a register R is used in an instruction that deﬁnes another
register R(cid:48) and that register is used to compute an address,
then R is also used to compute an address. This is captured in
the following Datalog rule:

def_used_for_address(Adef,Reg):−
def_used_for_address(Aused,_),
def_used(Adef,Reg,Aused,_).

(11)

5.2 Register Value Analysis

In contrast to instructions that refer to code, where direct ref-
erences (direct jumps or calls) predominate, memory accesses
are usually computed. Rather than accessing a ﬁxed address,
instructions typically access addresses computed with a com-
bination of register values and constants. This address com-
putation is often done over several instructions. Such is the
case in the example code in Fig. 2.
In order to approximate this behavior, we developed an
analysis that computes the value held in a register at an

7

address. There are many ways of approximating register
values ranging from simple constant propagation to complex
abstract domains that take memory locations into account
e.g. [7]. Generally, the more complex the analysis domain,
the more expensive it is. Therefore, we have chosen a
minimal representation that captures the kind of register
values that are typically used for accessing memory. Our
value analysis representation is based on the idea that typical
memory accesses follow a particular pattern where the
memory address that is accessed is computed using a base
address, plus an index multiplied by a multiplier. Conse-
quently, the value analysis produces predicates of the form:

reg_val(A:A,Reg:R,A2:A,Reg2:R,Mult:Z64 ,Disp:Z64 )

which represents that the value of a register Reg at address
A is equal to the value of another register Reg2 at address A2
multiplied by Mult plus an displacement Disp (or offset).
The analysis proceeds in two phases. The ﬁrst phase pro-
duces predicates of the form reg_val_edge which share the
signature with reg_val. We generate one reg_val_edge per
instruction and def-use predicate for the instructions whose be-
havior can be modeled in this domain and are used to compute
an address (def_used_for_address). For example, Rule 12
below generates reg_val_edge predicates for add instructions
that add a constant to a register:

reg_val_edge(A,Reg,Aprev,Reg,1,Imm):−
def_used_for_address(Aprev,Reg),
def_used(Aprev,Reg,A,_),
instruction(A,_,_,’add’,Op1,Op2,0,0),
op_immediate(Op1,Imm),
op_regdirect(Op2,Reg).

(12)

Example 4. Continuing with Example 3, the predicates
reg_val_edge generated for the code in Fig. 2 are:

P1 val_reg_edge(416C35,’RBX’,416C35,’NONE’,0,−624)
P2 val_reg_edge(416C58,’RBX’,416C35,’RBX’,1,24)
P3 val_reg_edge(416C58,’RBX’,416C58,’RBX’,1,24)

Predicate P1 captures that RBX has a constant value after
executing the instruction in address 416C35 (note that the
multiplier is 0 and the register has a special value ’NONE’).
Predicate P2, generated from Rule 12, speciﬁes that the value
of RBX deﬁned at address 416C58 corresponds to the value of
RBX deﬁned at 416C35 plus 24. Finally, P3 denotes that the
value of RBX at 416C58 can be the result of incrementing the
value of RBX deﬁned at the same address by 24.

The set of predicates reg_val_edge can be seen as directed
relational graph. The nodes in the graph are pairs of address
and register (A, Reg) and the edges express relations between
their values i.e. they are labeled with a multiplier and offset.
Once this graph is computed, we perform a propagation
phase akin to a transitive closure. This propagation phase
chains together reg_val_edge predicates. The chaining starts
from the leafs of the graph (nodes with no incoming edges).
Leafs in the reg_val_edge graph can be instructions that load

8

a constant into a register such as mov RBX, -624 in Fig. 2
or instructions where a register is assigned the result of an
operation not supported by the domain. For example, loading
a value from memory mov RDI, [RIP+0x25D239] in Fig. 2.
In that case, the generated predicate would be the tautological

predicate reg_val(416C40,RBX,416C40,RBX,1,0).

In order to ensure termination and for efﬁciency reasons
we limit the number of propagation steps by a constant
step_limit with an additional ﬁeld S:Z64 in the reg_val
predicates. The main rule for combining reg_val_edge predi-
cates is the following:

reg_val(A1,R1,A3,R3,M1∗M2,(D2∗M1)+D1,S+1):−
reg_val(A2,R2,A3,R3,M2,D2,S),
reg_val_edge(A1,R1,A2,R2,M1,D1), A1 != A2,
step_limit(Limit), S+1 < Limit.

(13)

This rule chains edges linearly by combining their multipliers
and displacements. It keeps track of operations that involve
one source register and one destination register. However, we
also want to detect situations where multiple edges converge
into one instruction. Speciﬁcally, we want to detect loops and
operations that involve multiple registers.
Detecting Simple Loops. The following rule (Rule 14)
detects situations where a register R is initialized to a constant
D1, then incremented/decremented in a loop by a constant D2.

reg_val(A,Reg,A2,’Unknown’,D2,D1,S+1):−
reg_val(A,R,A2,’NONE’,0,D1,S),
reg_val_edge(A,R,A,R,0,D2),
step_limit(Limit), S+1 < Limit.

(14)

This pattern can be interpreted as D1 being the base for a
memory address and D2 being the multiplier used to access
different elements of a data structure. Our new multiplier D2
does not actually multiply any real register, so we set the
register ﬁeld to a special value ’Unknown’.

Example 5. Consider the propagation of the predicates in
Example 4. The generated predicates are:

P4 val_reg(416C35,’RBX’,416C35,’NONE’,0,−624)
P5 val_reg(416C58,’RBX’,416C35,’NONE’,0,−600)
P6 val_reg(416C58,’RBX’,416C35,’Unknown’,24,−600)

First, predicate P4 is generated from P1 which is a leaf. Then,
P4 is combined with P2 using Rule 13 into predicate P5.
Finally, Rule 14 is applied to P5 and P3 to generate P6 which
denotes that the register RBX takes values that start at −600
and are incremented in steps of 24 bytes.

Multiple Register Operations. In general, operations over

two source registers cannot be expressed with reg_val predi-
cates. However, if one of the registers has a constant value or
both registers can be expressed in terms of a third common
register (a diamond pattern), we can propagate their value.

Example 6. The following assembly code contains a simple
diamond pattern:

0 : mov RBX, [RCX]
1 : mov RAX, RBX
2 : add RAX, RAX
3 : add RAX, RBX

The last instruction adds the registers RAX and RBX. However,
the value of RAX is two times the value of RBX. This is reﬂected

in the predicates reg_val(2,RAX,0,RBX,2,0) and reg_val

(0,RBX,0,RBX,1,0). Therefore, we can generate a predicate

reg_val(3,RAX,0,RBX,3,0).

Note that the register value analysis intends to capture
some of the relations between register values but it makes no
attempt capture all of them. The goal of this analysis is not
to obtain a sound over-approximation of the register values
but to provide as much information as possible about how
memory is accessed. The analysis is also not strictly an under-
approximation as it is based on def-use chains which are
over-approximating.

5.3 Data Access Pattern Analysis

The data access pattern (DAP) analysis takes the results of the
register value analysis and the results of the def-use analysis
to infer the register values at each of the data accesses and
thus compute which addresses are accessed and which
pattern is used to access them. The DAP analysis generates
predicates of the form:

data_access_pattern(A:A,Size:Z64 ,Mult:Z64 ,From:A)

which speciﬁes that address A is accessed from an instruction
at address From and Size bytes are read or written. Moreover,
the access uses a multiplier Mult.
Example 7. The code in Fig. 2 generates several DAPs:

P7 data_access_pattern(673E80,8,0,416C40)
P8 data_access_pattern(45D0B8,8,0,416C47)
P9 data_access_pattern(45D0D0,8,24,416C47)

The instruction at address 416C40 produces P7 which repre-
sents an access to a ﬁxed address that reads 8 bytes. Con-
versely, the instruction at address 416C47 yields two predi-
cates: P8 and P9. This is because register RBX can have multi-
ple values at address 416C47. If there are multiple DAPs to the
same address, we choose the one with the highest multiplier.

These DAPs provide very sparse information, but if an ad-
dress x is accessed with a multiplier m, it is likely that x + m,
x + 2m, etc., are also accessed the same way. Thus, we ex-
tend DAPs based on their multiplier. The analysis produces
a predicate propagated_data_access with the same format
as data_access_pattern. Our auxiliary analyses provide no
information on what is the upper limit of an index in a data
access. Thus, we simply propagate a DAP until it reaches the
next DAP that coincides on the same address or that has a
different multiplier. The idea behind this criterion is that the
next data structure in the data section is probably accessed

9

from somewhere in the code. So rather than trying to deter-
mine the size of the data structure being accessed, we assume
that such data structure ends where the next one starts. These
propagated DAPs will inform our symbolization heuristics.
Example 8. In our running example (Fig. 2) the DAP

data_access_pattern(45D0D0,8,24,416C40) is propagated

from address 45D0D0 up to address 45D310 in 24 byte
intervals. The generated predicates are:

propagated_data_access(45D0D0,8,24,416C40)
propagated_data_access(45D0E8,8,24,416C40)

· · ·

· · ·

propagated_data_access(45D310,8,24,416C40)

The DAP is not propagated to the next address 45D328
because that address contains another DAP generated at a
different part of the code.

5.4 Discussion

There are two important aspects that set our register value
analysis and DAP analysis apart from previous approaches
like Ramblr [52].
First, the register value analysis is relational—it represents
the value of one register at some location in terms of the
value of another register at another location—in contrast to
traditional value set analyses (VSA) [7]. This is also different
from the afﬁne-relations analysis [35] used in VSA analyses
which computes relations between register values at the same
location. A reg_val predicate between two registers also
implies a data dependency i.e. a register is deﬁned in terms
of the other.
As a consequence, register value analysis can provide use-
ful information (for our use-case) in many cases where ob-
taining a concrete value for a register would be challenging.
Consider the code in Example 6. Our analysis concludes that
at address 3 RAX is 3 times the value of RBX at address 0 re-
gardless of what that value might be. In contrast, a traditional
VSA analysis will only provide useful information for the
value of RAX as long as it can precisely approximate the value
of RCX and the values of all the possible memory locations
pointed by RCX. If any of those locations has an imprecise
abstract value e.g. (cid:62), so will RAX.
Example 9. Let us consider a continuation of Example 6:

4 : mov R8, QWORD PTR [RAX * 8 + 0 x 1 0 0 0 ]
5 : mov R9, WORD PTR [RAX * 8 + 0 x 1 0 0 8 ]
6 : mov R10, BYTE PTR [RAX * 8 + 0 x 1 0 1 0 ]

There will be DAPs for addresses 0x1000, 0x1008 and 0x1010
with sizes 8, 2, and 1 and a multiplier of 24 each. This infor-
mation, though unsound in the general case (we are assuming
RAX can take the value 0), is useful in practice.
These DAPs are the second distinguishing aspect of our
analyses. Ramblr recognizes primitives and arrays of prim-
itives. However, these DAPs indicate that address 0x1000
likely contains a struct with (at least) three ﬁelds of

different sizes. Moreover, thanks to the multiplier and the
propagated_access_pattern predicate we can conclude that
address 0x1000 holds in fact an array of structs where the ﬁrst
ﬁeld (at addresses 0x1000, 0x1018, 0x1030. . .) has size 8 and
might contain a pointer whereas the second and third ﬁelds (at

addresses 0x1008, 0x1020, 0x1038. . . and 0x1010, 0x1028, 0

x1040. . . respectively) have size 2 and 1 and thus are unlikely
to hold a pointer.

6 Symbolization

The next step to obtain assembleable code is to perform sym-
bolization. It consists of deciding for each constant in the
code or in the data whether it is a literal or a symbol. A ﬁrst
approximation can be achieved by considering as symbols all
numbers that fall within the range of the address space. How-
ever, as reported by Wang et al. [52], this leads to both false
positives and false negatives. Next, we explain our approach
to reduce the presence of false positives and negatives.

6.1 False Positives: Value Collisions

False positives are due to value collisions, literals that happen
to coincide with range of possible addresses. In order to re-
duce the false positive rate, we require additional evidence in
order to classify a number as a symbol.

6.1.1 Numbers in Data

For numbers in data, similarly to the approach used for blocks,
we start by deﬁning a set of “data object” candidates. Each
candidate has an address, a type, and a size. We deﬁne data
object candidates for the following types:
Symbol Whenever the number falls into the right range

(address_in_data).

String A sequence of printable characters ended in 0.
Symbol-Symbol We detect jump tables using ad-hoc rules
based on def-use chains, register values, and the DAPs
computed in Sec. 5 (see App. A).
Other An address is accessed with a different size than the
pointer size (8 bytes in x64 architecture) using the predi-
cate propagated_data_access computed in Sec. 5.3.
We assign points to each of the candidates using heuristics
based on the analyses results and detect if they are overlapping.
If they are, we discard the candidate with fewer points. Note
that detecting objects of type “String” and “Other” helps to
discard false positives (i.e. symbol candidates) that overlap
with them. As with blocks, we also discard candidates if their
total points fall below a threshold.
The main heuristics for data objects are (+ positive points
and − for negative points):

+ Pointer to instruction beginning: A symbol candidate

points to the beginning of an instruction. This heuristic
relies on the results of the already computed IBI.

+ Data access match: The data object candidate is ac-
cessed from the code with the right size. This heuristic
checks the existence of a propagated_data_access that
matches the data object candidate’s address and size.
+ Symbol arrays: There are several (at least 3) contiguous
or evenly spaced symbol candidates. This indicates that
they belong to the same data structure. Also, it is less
likely to have several consecutive value collisions.
+ Pointed by symbol array: Multiple candidates of the
same type pointed by a single symbol array.
+ Aligned symbols: A symbol candidate is located at an
address with 8 bytes alignment.
+ Long strings: A string candidate is longer that 5 bytes.
− Access conﬂict: There is some data access in the middle
of a symbol candidate.

6.1.2 Numbers in Code

We follow the same approach to disambiguate numbers in in-
struction operands. However, only the ﬁrst heuristic, “Pointer
to instruction beginning” of the ones listed above is applica-
ble to numbers in code. We distinguish two cases: numbers
that represent immediate operands and numbers that repre-
sent a displacement in an indirect operand. Once taking into
account the “Pointer to instruction beginning” heuristic, we
have not found false positives in displacements. For immedi-
ate operands we consider the following additional heuristics:
+ Used for address: The immediate is stored in a register
used to compute an address (detected using predicate

def_used_for_address from Sec. 5).

− Uncommon pointer operation: The immediate or the

register where it is loaded is used in an operation uncom-
mon for pointers such as MUL or XOR.

− Compared to non-address: The immediate is com-

pared or moved to a register that in turn is compared
to another immediate that cannot be an address.
These heuristics are tailored to the inference of how the imme-
diate is used, and they rely on def-use chains and the results
of the register value analysis.

6.2 False Negatives: Symbol+Constant

False negatives can occur in situations where the original
code contains an expression of the form symbol+constant. In
such cases, the binary under analysis contains the result of
computing that expression.
There is no general procedure to recover the original ex-
pression in the code as that information is simply not present
in the binary. Having a new symbol pointing to the result
of the symbol+constant expression instead of the original
expression is not a problem for rewrites which leave the
data sections unmodiﬁed (even if the sections are moved)
or rewrites that only add data to the beginning or the end of
data sections. However, sometimes the resulting address of

10

a symbol+constant expression falls outside the data section
ranges or falls into the wrong data section. In such cases, a
naive symbolization approach can result in false negatives.
We detect and correct these cases by detecting common
patterns where compilers generate symbol+constant using
the results of our def-use analysis and the register value anal-
ysis. We distinguish two cases: displacements in an indirect
operands and immediate operands.

6.2.1 Displacements in Indirect Operands

For displacements in indirect operands, we know that the
address that results from the indirect operand should be valid.
Consider a generic data access [R1+R2×M+D] where R1 and
R2 are registers, M is the multiplier and D the displacement.
The displacement D might not fall onto a data section, but the
expression R1+R2×M+D should.
Typically, in a data access as the one above, one of the
addends represents a valid base address that points to the
beginning of a data structure and the rest of the addends
represent an offset into the data structure. In our generic
access, D might be the base address, in which case it should be
symbolic, or the base address might be in one of the registers,
in which case D should not be symbolic.
We detect cases in which D should be symbolic even if it
does not fall in the range of a data section. For example if the
data access is of the form [R2×M+D] with M > 1, it is likely that
D represents the base address and should be symbolic. We can
detect less obvious cases with the help of the register value
analysis (see Sec. 5.2). If we have a data access of the form
[R1+D] but the value of R1 can be expressed as the value of
some other register Ro multiplied by a multiplier M > 1 (there
is a predicate of the form reg_val(_,R1,_,Ro,M,0)) , then D
is also likely to be the base address and thus symbolic. On the
other hand, if R1 has a value that is a valid data address (there

is a predicate reg_val(_,R1,_,’NONE’,0,A) where A falls in

a data section), then D is probably not a base address.
Knowing that a displacement should be symbolic is not
enough, we need to infer the right data section to which the
symbolic expression should refer. If the data access generates
a DAP, we use the destination address of the DAP as a ref-
erence for creating the symbolic expression. Otherwise, we
choose the closest boundary of a data section as a reference.

6.2.2 Immediate Operands

Having a symbolic immediate that falls outside the data sec-
tions is uncommon. The main pattern that we have identiﬁed
is when the immediate is used as an initial value for a loop
counter or as a loop bound to which the counter is compared.

Example 10. Consider the following code fragment taken
from the program conﬂict-6.0 compiled with GCC 5.5 and
optimization -O1. It presents an immediate of the form symbol
+constant landing in a different section.

11

4 0 1 0 9 D :
4 0 1 0 A 2 :
4 0 1 0 A 7 :
. . .
4 0 1 0 C 5 :
4 0 1 0 C 9 :
4 0 1 0 C C :

mov EBX, 4 0 2 D 4 0
mov EBP, 4 0 2 D E 8

mov RCX,QWORD PTR [RBX]

. . .

add RBX , 8
cmp RBX,RBP

jne 4 0 1 0 A 7

The number 402DE8 loaded at 4010A2 represents a loop bound
and it is used in instruction 4010C9 to check if the end of the
data structure has been reached. Address 402d40 is in section

.rodata but address 402DE8 is in section .eh_frame_hdr.

We detect this and similar patterns by combining the in-
formation of the def-use analysis and the value analysis. We
note that in these situations, the address that falls outside
the section or on a different section and the address range
of the correct section are within the distance of one multi-
plier. That is, let x be a candidate address that might represent
the result of a symbol+constant expression, and let [si , s f )
be the address range of the original symbol’s section. Then
x ∈ [si − M , s f + M ] where M is the increment of the loop
counter. Therefore, our detection mechanism generates an
extended section range as above for every register that we
identify as loop counter. Then, it checks if there is some im-
mediate compared to the loop counter that falls within this
extended range. If that happens, the immediate is rewritten
using the base of the loop counter as a symbol.

Example 11. Example 10 continued. The register value anal-
ysis detects that RBX is a loop counter with a base address of
402D40 and a step size of 8. Thus, we consider an extension of
section .rodata to the range [402718, 402DF0] (the original
address range is [402720, 402DE8)). Finally, using def-use
chains we detect that the loop counter is compared to the im-
mediate 402DE8 which falls within the extended section range.
Consequently, we generate the following statement:

4010A2: mov EBP,OFFSET .L_402D40+168

where .L_402D40 is a new symbol pointing to address 402D40.

7 Experimental Evaluation

We implemented our disassembly technique in a tool called
Ddisasm. Ddisasm takes a binary and produces an IR
called GrammaTech Intermediate Representation for Bina-
ries (GTIRB). This representation can be printed to assembly
code that can be directly reassembled. Currently Ddisasm
only supports x64 Linux ELF binaries but we plan to extend
it to support other architectures and binary formats. Ddisasm
is predominantly implemented in Datalog (4336 non-empty
LOC) which is compiled into highly efﬁcient parallel C++
code using Soufﬂe [25].
Benchmarks. We performed several experiments against
a variety of benchmarks, compilers, and optimization ﬂags.
We selected 3 benchmarks. The ﬁrst one is Coreutils 8.25
which is composed of 106 binaries and has been used in the

Benchmark Binaries

Refs

Ddisasm

Ramblr

FP
FN WS Broken
FP
FN Broken Broken w/o ICC
Real world
750
4521315
0
20
20
3
49739
61883
322
187
Coreutils
2650
3204872
3
0
0
3
8244
136994
435
6
CGC
2070
5829586
0
15
2
10
10889
43683
388
28
Table 1: Symbolization evaluation of Ddisasm and Ramblr. “Refs” represents the total number of references in these binaries;
“FP” and “FN” list the number of false positives and false negatives respectively for each tool; “WS” lists the number of references
pointing to the wrong section (only shown for Ddisasm); “Broken” lists the number of binaries that are broken (have at least one
“FP,” “FN” or “WS”). “Broken w/o ICC” lists broken binaries without counting the ones compiled with ICC.

experimental evaluations of Ramblr [52] and Uroboros [53].
The second benchmark is a subset of the programs from the
DARPA Cyber Grand Challenge (CGC). We adopt a modiﬁed
version of these binaries that can be compiled for Linux sys-
tems in x64 [2]. We exclude programs that fail to compile or
fail all their tests. That leaves 69 CGC programs. Finally, the
third benchmark is a collection of 25 real world open source
applications whose binary size ranges from 28 KB to 2.5 MB.
Some of the original binaries in all benchmarks fail some tests.
We take the results of the original binary as a baseline which
rewritten binaries must match exactly—including failures.
Compilation Settings. For each of those programs we
compile the binaries with 5 compilers: GCC 5.5.0, GCC 7.1.0,
Clang 3.8.0, Clang 6.0 and ICC 19.0.5. For each compiler we
use the following 6 compiler ﬂags: -O0, -O1, -O2, -O3, -Os
and -Ofast. All programs are compiled as position dependent
code8 .That means that for each original program we test 30
versions except for Coreutils where -Ofast generates original
binaries that fail many of the tests and thus we skip it. In
summary, we test 2650 different binaries for Coreutils 2070
binaries for the CGC benchmark and 750 binaries from our
real world selection. All benchmarks together represent a to-
tal of 647 MB of binaries. Note that the real world examples
represent a signiﬁcant portion of the binary data (232 MB).

7.1 Symbolization Experiments

We disassemble all the benchmarks and collect the num-
ber of false positives (FP) and false negatives (FN) in the
symbolization procedure. We obtain ground truth by generat-
ing binaries with complete relocation information using the
-emit-relocs ld linker option. We also detect an additional
kind of error WS—i.e. when we create a symbolic expression,
but the symbol points to the wrong section (see Sec. 6.2).
For comparison, we run the same experiments using
Ramblr, the tool with the best published symbolization re-
sults. Table 1 contains the results of this experiment. Ddisasm
presents a very low error rate. This shows the effectiveness
of the approach. Ddisasm builds on many of the ideas imple-
mented in Ramblr, but makes signiﬁcant improvements (see

8 This is harder to disassemble than position independent code (PIC),
which is though to be easier because it contains relocation information for
absolute addresses [17]. Nonetheless, this does not make symbolization of
PIC trivial as we argue in Sec. 7.1.

Heuristics
No Strings
No DAP
No DAP & Strings

FP
45
36
87

FN WS Broken
20
20
38
43
20
37
0
20
71

Table 2: Symbolization evaluation of Ddisasm on the real
world benchmarks deactivating groups of heuristics.

Sec.5.4). App. B contains detailed tables with results broken
down by compiler and optimization ﬂag and a discussion of
Ddisasm’s failures. Ramblr performs well on Coreutils com-
piled with GCC and Clang (in line with their experiments)
with 6 broken binaries out of 424, but its precision drops
greatly against the real world examples (42% of broken exam-
ples) and binaries compiled with ICC (where all optimized
binaries are broken). Additionally, we do not detect WS in
Ramblr, as this information is not readily available. Thus, the
numbers in the ’Broken’ column are biased against Ddisasm
as there might be binaries broken by Ramblr that are not
counted.
It is worth pointing out that the ground truth extracted from
relocations is incomplete for binaries compiled with ICC. This
compiler generates jump tables with Symbol−Symbol entries.
These jump tables do not need nor have relocations associated
to them—even in PIC. We believe that this directly contradicts
the claim made by Dinesh et al. [17] that x64 PIC code can
be symbolized without heuristics—only using relocations.
The heuristics’ weights for both IBI and symbolization
have been manually set and work well generically across
compilers and ﬂags. When ground truth can be obtained,
these weights could be automatically learned and adjusted
based on a program corpus, we leave that for future work.
Finally, we are interested in knowing the importance of
different heuristics. Thus, we repeat the symbolization exper-
iments for the real world benchmarks deactivating different
kinds of heuristics. We deactivate heuristics that 1) detect
strings, 2) heuristics that use DAPs, and 3) both kinds at the
same time. The results are in Table 2. Without both kinds of
heuristics (row 3), we have a high number of FPs. Detecting
strings (row 2) brings this number down, but we miss symbols
that look like strings (FNs). DAPs give us additional evidence
for those symbols. With DAPs but no strings (row 1), we also
discard some FPs (by detecting objects of type “Other”) but
not all. The heuristics complement each other. Note that the

12

Ddisasm

Ramblr

Benchmark Binaries

Disasm Reassemble
Test
Disasm Reassemble
Test Test w/o ICC
Real world
750
100.00%
100.00%
99.86%
99.87%
70.13% 38.67%
46.83%
Coreutils
2650
100.00%
100.00% 100.00%
98.94%
83.81% 68.60%
81.36%
CGC
2048
100.00%
99.95%
99.41% 100.00%
68.75% 47.80%
57.39%
Table 3: The functionality of binaries reassembled using Ddisasm and Ramblr as measured using the test suites distributed
with the binaries. The “Disasm”, “Reassemble” and “Test” (w/o ICC) columns list the percentage of binaries successfully
disassembled, reassembled into a new binary, and that pass their original test suite (without counting binaries compiled with
ICC) respectively.

20 FNs produced by DAPs correspond to an array of structs
that is correctly detected, but its pointer ﬁelds are accessed
with size 4 instead of 8 which derails the analysis.

7.2 Functionality Experiments

Using the same benchmarks we check how many of the disas-
sembled binaries can be reassembled and how many of those
pass their original test suites without errors.
For Ddisasm, we perform the experiment on the stripped
versions of the binaries. Additionally, in order to increase our
conﬁdence that both IBI and symbolization are correct, we
modify the locations (and relative locations) of all the instruc-
tions by adding NOPs at regular intervals before reassembling.
We add 8 NOPs every 8 instructions to maintain the original
instructions’ alignment throughout the executable section9 .
We also add 64 zero bytes at the beginning of each data sec-
tion. This demonstrates that our symbolization is robust to
signiﬁcant modiﬁcation of code (by adding or removing code)
and data (by adding content at the beginning of sections).
For Ramblr, we use unstripped binaries because Ramblr
fails to produce reassembleable assembly for the stripped
versions of most binaries. Many of the failures are because
Ramblr generates assembly with undeﬁned labels or with la-
bels deﬁned twice. Additionally, we do not perform any mod-
iﬁcation of assembly generated by Ramblr—this ensures that
we do not report an overly pessimistic result for Ramblr by
accidentally breaking the code generated by Ramblr. So we
compare Ddisasm at a signiﬁcant handicap against Ramblr.
The results of this experiment are in Table 3. For CGC, we
discarded 22 binaries that fail their tests non-deterministically
leaving 2048 binaries. Ddisasm produces reassembleable as-
sembly code for all the binaries but one. One binary in the real
world benchmarks and 11 binaries in the CGC benchmark fail
their tests. This is close to the results of our previous experi-
ment (Table 1). The FNs in real world examples and 3 of the
15 FNs in CGC cause test failures. The remaining FPs, FNs,
and WS symbols do not cause test failures. Additionally, there
are 7 other test failures in CGC not caused by symbolization
errors.

9We skip regions in between jump table entries of the form .byte
Symbol−Symbol. Adding NOPs to these regions can easily make the
result of Symbol−Symbol fall out of the range expressible with one byte.

Figure 5: Disassembly time. The two graphs show the disas-
sembly times (in seconds) for all the binaries at two different
scales (the bottom graph displays smaller binaries in detail).
Ddisasm’s disassembly time is plotted (vertically) against
Ramblr’s (horizontally). In all graphs, points below the di-
agonal represent binaries for which Ddisasm is faster than

Ramblr.

7.3 Performance Evaluation

Finally, we measure and compare the performance of both
Ramblr and Ddisasm. We measure the time that it takes to
disassemble each of the binaries in the three benchmarks. The
results can be found in Fig. 5. Ddisasm is faster than Ramblr
in all but 39 of 5470 total binaries. In particular, Ddisasm is
on average 5.15 times faster than Ramblr.

8 Conclusion

We have developed a new reassembleable disassembler called
Ddisasm. Ddisasm in implemented in Datalog and combines
novel static analyses and heuristics to determine how data
is accessed and used. We show that Datalog is well suited
to this task as it enables the compositional and declarative
speciﬁcation of static analyses and heuristics, and it compiles
them into a uniﬁed, parallel, and efﬁcient executable.
Ddisasm is, to the best of our knowledge, the ﬁrst disas-
sembler for machine code implemented in Datalog. Our ex-

13

050100150200250300350050100150RamblrDdisasm01020304050607080901000102030RamblrDdisasmperiments show that Ddisasm is both more precise and faster
than the state-of-the-art tools for reassembleable disassem-
bly, and better handles large complex real-world programs.
Ddisasm makes binary rewriting practical by enabling binary
rewriting of real world programs compiled with a range of
compilers and optimization levels with unprecedented speed
and accuracy.

9 Acknowledgments

This material is based upon work supported by the Ofﬁce
of Naval Research under contract No. N68335-17-C-0700.
Any opinions, ﬁndings and conclusions or recommendations
expressed in this material are those of the authors and do not
necessarily reﬂect the views of the Ofﬁce of Naval Research.

References

[1] Cyber grand challenge (CGC). https://www.darpa.

mil/program/cyber-grand-challenge.

[2] GrammaTech’s CGC benchmarks. https://github.

com/grammatech/cgc-cbs.

[3] Hex-rays: The IDA Pro disassembler and debugger.

https://www.hex-rays.com/products/ida.

[4] National Security Agency. Ghidra, 2019. https://

www.nsa.gov/resources/everyone/ghidra/.

[5] Dennis Andriesse, Xi Chen, Victor van der Veen, Asia
Slowinska, and Herbert Bos. An in-depth analysis of
disassembly on full-scale x86/x64 binaries.
In 25th
USENIX Security Symposium (USENIX Security 16),
pages 583–600, Austin, TX, 2016. USENIX Associa-
tion.

[6] Cryptic Apps. Hopper. https://www.hopperapp.

com/.

[7] Gogul Balakrishnan and Thomas Reps. Analyzing mem-
ory accesses in x86 executables. In Evelyn Duesterwald,
editor, Compiler Construction, pages 5–23, Berlin, Hei-
delberg, 2004. Springer Berlin Heidelberg.

[8] Erick Bauman, Zhiqiang Lin, and Kevin W. Hamlen.
Superset disassembly: Statically rewriting x86 binaries
without heuristics. In NDSS, 01 2018.

[9] M. Ammar Ben Khadra, Dominik Stoffel, and Wolf-
gang Kunz. Speculative disassembly of binary code. In
Proceedings of the International Conference on Compil-
ers, Architectures and Synthesis for Embedded Systems,
CASES ’16, pages 16:1–16:10, New York, NY, USA,
2016. ACM.

[10] Martin Bravenboer and Yannis Smaragdakis. Strictly
declarative speciﬁcation of sophisticated points-to anal-
yses. In Proceedings of the 24th ACM SIGPLAN Con-
ference on Object Oriented Programming Systems Lan-
guages and Applications, OOPSLA ’09, pages 243–262,
New York, NY, USA, 2009. ACM.

[11] David Brumley, Ivan Jager, Thanassis Avgerinos, and
Edward J. Schwartz. BAP: A binary analysis platform.
In Ganesh Gopalakrishnan and Shaz Qadeer, editors,
Computer Aided Veriﬁcation, pages 463–469, Berlin,
Heidelberg, 2011. Springer Berlin Heidelberg.

[12] David Brumley and James Newsome. Alias analysis
for assembly. Technical report, Technical Report CMU-
CS-06-180, Carnegie Mellon University School of Com-
puter Science, 2006.

[13] Xi Chen, Herbert Bos, and Cristiano Giuffrida. Codear-
mor: Virtualizing the code space to counter disclosure
attacks. In 2017 IEEE European Symposium on Security
and Privacy (EuroS&P), pages 514–529. IEEE, 2017.

[14] Xi Chen, Asia Slowinska, Dennis Andriesse, Herbert
Bos, and Cristiano Giuffrida. Stackarmor: Comprehen-
sive protection from stack-based memory error vulnera-
bilities for binaries. In Proceedings of the 2015 Annual
Network and Distributed System Security Symposium,
2015.

[15] Zhui Deng, Xiangyu Zhang, and Dongyan Xu. Bistro:
Binary component extraction and embedding for soft-
ware security applications.
In European Symposium
on Research in Computer Security, pages 200–218.
Springer, 2013.

[16] ARTEM DINABURG and ANDREW RUEF. Mcsema:
Static translation of x86 instructions to LLVM. In Re-
Con 2014 Conference, Montreal, Canada, 2014.

[17] Sushant Dinesh, Nathan Burow, Dongyan Xu, and Math-
ias Payer. Retrowrite: Statically instrumenting cots bina-
ries for fuzzing and sanitization. In Proceedings of the
41st Symposium on Security and Privacy. IEEE, 2020.
To Appear.

[18] Chris Eagle. The IDA Pro Book: The Unofﬁcial Guide
to the World’s Most Popular Disassembler. No Starch
Press, 2011.

[19] Mohamed Elsabagh, Dan Fleck, and Angelos Stavrou.
Strict virtual call integrity checking for C++ binaries.
In Proceedings of the 2017 ACM on Asia Conference
on Computer and Communications Security, pages 140–
154. ACM, 2017.

[20] Free Software Foundation. GNU Binary Utilities. Free
Software Foundation, May 2002.

14

[21] Neville Grech, Lexi Brent, Bernhard Scholz, and Yannis
Smaragdakis. Gigahorse: Thorough, declarative decom-
pilation of smart contracts. In ICSE, 2019. To appear.

[22] Galois Inc. Open source binary analysis tools. https:

//github.com/GaloisInc/macaw.

[23] Vector 35 Inc. Binary ninja: a new kind of reversing

platform. https://binary.ninja/.

[24] Software Engineering Institute. Automated static analy-
sis tools for binary programs. https://github.com/

cmu-sei/pharos.

[25] Herbert Jordan, Bernhard Scholz, and Pavle Suboti ´c.
Soufﬂé: On synthesis of program analyzers. In Swarat
Chaudhuri and Azadeh Farzan, editors, Computer Aided
Veriﬁcation, pages 422–430, Cham, 2016. Springer In-
ternational Publishing.

[26] Minkyu Jung, Soomin Kim, HyungSeok Han, Jaeseung
Choi, and Sang Kil Cha. B2r2: Building an efﬁcient
front-endfor binary analysis.
In Binary Analysis Re-
search (BAR), 2019, 2019.

[27] Koen Koning, Herbert Bos, and Cristiano Giuffrida.
Secure and efﬁcient multi-variant execution using
hardware-assisted process virtualization. In 2016 46th
Annual IEEE/IFIP International Conference on De-
pendable Systems and Networks (DSN), pages 431–442.
IEEE, 2016.

[28] Christopher Kruegel, William Robertson, Fredrik Valeur,
and Giovanni Vigna. Static disassembly of obfuscated
binaries.
In Proceedings of the 13th Conference on
USENIX Security Symposium - Volume 13, SSYM’04,
pages 18–18, Berkeley, CA, USA, 2004. USENIX As-
sociation.

[29] James R Larus and Eric Schnarr.
Eel: Machine-
independent executable editing. In ACM Sigplan No-
tices, volume 30, pages 291–300. ACM, 1995.

[30] Zephyr Software LLC.

Irdb cookbook examples.

https://git.zephyr-software.com/opensrc/
irdb-cookbook-examples.

[31] Michael Matz, Jan Hubicka, Andreas Jaeger, Mark
Mitchell, Milind Girkar, Hongjiu Lu, David Kreitzer,
and Vyacheslav Zakharin. System V Application Binary
Interface: AMD64 Architecture Processor Supplement
(With LP64 and ILP32 Programming Models), 2013.

[32] Xiaozhu Meng and Barton P. Miller. Binary code is
not easy. In Proceedings of the 25th International Sym-
posium on Software Testing and Analysis, ISSTA 2016,
pages 24–35, New York, NY, USA, 2016. ACM.

[33] Kenneth Miller, Yonghwi Kwon, Yi Sun, Zhuo Zhang,
Xiangyu Zhang, and Zhiqiang Lin. Probabilistic dis-
assembly.
In International Conference on Software
Engineering (ICSE). ACM, 2019.

[34] Vishwath Mohan, Per Larsen, Stefan Brunthaler,
K Hamlen, and Michael Franz. Opaque control-ﬂow
integrity. In Symposium on Network and Distributed
System Security (NDSS), 2015.

[35] Markus Müller-Olm and Helmut Seidl. Precise inter-
procedural analysis through linear algebra. In Proceed-
ings of the 31st ACM SIGPLAN-SIGACT Symposium
on Principles of Programming Languages, POPL ’04,
pages 330–341, New York, NY, USA, 2004. ACM.

[36] pancake. radare. https://www.radare.org/r/.

[37] Vasilis Pappas, Michalis Polychronakis, and Angelos D
Keromytis. Smashing the gadgets: Hindering return-
oriented programming using in-place code randomiza-
tion. In 2012 IEEE Symposium on Security and Privacy,
pages 601–615. IEEE, 2012.

[38] Manish Prasad and Tzi-cker Chiueh. A binary rewriting
defense against stack based buffer overﬂow attacks. In
USENIX Annual Technical Conference, General Track,
pages 211–224, 2003.

[39] Nguyen Anh Quynh. Capstone: Next-gen disassembly
framework. Black Hat USA, 2014.

[40] Thomas W. Reps. Demand interprocedural program
analysis using logic databases. In Raghu Ramakrishnan,
editor, Applications of Logic Databases, pages 163–196,
Boston, MA, 1995. Springer US.

[41] Ted Romer, Geoff Voelker, Dennis Lee, Alec Wolman,
Wayne Wong, Hank Levy, Brian Bershad, and Brad
Chen. Instrumentation and optimization of Win32/Intel
executables using Etch. In Proceedings of the USENIX
Windows NT Workshop, volume 1997, pages 1–8, 1997.

[42] Edward J. Schwartz, Cory F. Cohen, Michael Duggan,
Jeffrey Gennari, Jeffrey S. Havrilla, and Charles Hines.
Using logic programming to recover C++ classes and
methods from compiled executables. In Proceedings
of the 2018 ACM SIGSAC Conference on Computer
and Communications Security, CCS ’18, pages 426–441,
New York, NY, USA, 2018. ACM.

[43] Benjamin Schwarz, Saumya Debray, Gregory Andrews,
and Matthew Legendre. Plto: A link-time optimizer for
the Intel IA-32 architecture. In Proc. 2001 Workshop on
Binary Translation (WBT-2001), 2001.

[44] Y. Shoshitaishvili, R. Wang, C. Salls, N. Stephens,
M. Polino, A. Dutcher, J. Grosen, S. Feng, C. Hauser,

15

[54] Richard Wartell, Vishwath Mohan, Kevin W Hamlen,
and Zhiqiang Lin. Securing untrusted code via compiler-
agnostic binary rewriting. In Proceedings of the 28th An-
nual Computer Security Applications Conference, pages
299–308. ACM, 2012.

[55] Richard Wartell, Yan Zhou, Kevin W Hamlen, and Murat
Kantarcioglu. Shingled graph disassembly: Finding
the undecideable path. In Paciﬁc-Asia Conference on
Knowledge Discovery and Data Mining, pages 273–285.
Springer, 2014.

[56] John Whaley, Dzintars Avots, Michael Carbin, and Mon-
ica S. Lam. Using Datalog with binary decision dia-
grams for program analysis. In Kwangkeun Yi, editor,
Programming Languages and Systems, pages 97–118,
Berlin, Heidelberg, 2005. Springer Berlin Heidelberg.

[57] John Whaley and Monica S. Lam. Cloning-based
context-sensitive pointer alias analysis using binary de-
cision diagrams. In Proceedings of the ACM SIGPLAN
2004 Conference on Programming Language Design
and Implementation, PLDI ’04, pages 131–144, New
York, NY, USA, 2004. ACM.

[58] Chao Zhang, Tao Wei, Zhaofeng Chen, Lei Duan, László
Szekeres, Stephen McCamant, Dawn Song, and Wei Zou.
Practical control ﬂow integrity and randomization for
binary executables. In Security and Privacy (SP), 2013
IEEE Symposium on, pages 559–573. IEEE, 2013.

[59] Mingwei Zhang, Rui Qiao, Niranjan Hasabnis, and
R Sekar. A platform for secure static binary instru-
mentation.
In Proceedings of the 10th ACM SIG-
PLAN/SIGOPS international conference on Virtual exe-
cution environments, pages 129–140. ACM, 2014.

[60] Mingwei Zhang and R Sekar. Control ﬂow integrity for
COTS binaries. In USENIX Security, pages 337–352,
2013.

C. Kruegel, and G. Vigna. Sok: (state of) the art of
war: Offensive techniques in binary analysis. In 2016
IEEE Symposium on Security and Privacy (SP), pages
138–157, May 2016.

[45] Asia Slowinska, Traian Stancescu, and Herbert Bos.
Body armor for binaries: Preventing buffer overﬂows
without recompilation. In USENIX Annual Technical
Conference, pages 125–137, 2012.

[46] Yannis Smaragdakis and Martin Bravenboer. Using
Datalog for fast and easy program analysis. In Oege
de Moor, Georg Gottlob, Tim Furche, and Andrew Sell-
ers, editors, Datalog Reloaded, pages 245–251, Berlin,
Heidelberg, 2011. Springer Berlin Heidelberg.

[47] Yannis Smaragdakis, George Kastrinis, and George Bal-
atsouras.
Introspective analysis: Context-sensitivity,
across the board. In Proceedings of the 35th ACM SIG-
PLAN Conference on Programming Language Design
and Implementation, PLDI ’14, pages 485–495, New
York, NY, USA, 2014. ACM.

[48] Matthew Smithson, Khaled ElWazeer, Kapil Anand,
Aparna Kotha, and Rajeev Barua. Static binary rewrit-
ing without supplemental information: Overcoming the
tradeoff between coverage and correctness. In Reverse
Engineering (WCRE), 2013 20th Working Conference
on, pages 52–61. IEEE, 2013.

[49] Eli Tilevich and Yannis Smaragdakis. Binary refactor-
ing: Improving code behind the scenes. In Proceedings
of the 27th international conference on Software engi-
neering, pages 264–273. ACM, 2005.

[50] Victor Van Der Veen, Enes Göktas, Moritz Contag, An-
dre Pawoloski, Xi Chen, Sanjay Rawat, Herbert Bos,
Thorsten Holz, Elias Athanasopoulos, and Cristiano
Giuffrida. A tough call: Mitigating advanced code-reuse
attacks at the binary level. In 2016 IEEE Symposium on
Security and Privacy (SP), pages 934–953. IEEE, 2016.

[51] Ludo Van Put, Dominique Chanet, Bruno De Bus, Bjorn
De Sutter, and Koen De Bosschere. Diablo: a reliable, re-
targetable and extensible link-time rewriting framework.
In Proceedings of the Fifth IEEE International Sympo-
sium on Signal Processing and Information Technology,
2005., pages 7–12. IEEE, 2005.

[52] Ruoyu Wang, Yan Shoshitaishvili, Antonio Bianchi, Ar-
avind Machiry, John Grosen, Paul Grosen, Christopher
Kruegel, and Giovanni Vigna. Ramblr: Making reassem-
bly great again. In NDSS, 2017.

[53] Shuai Wang, Pei Wang, and Dinghao Wu. Reassem-
bleable disassembling. In 24th USENIX Security Sym-
posium (USENIX Security 15), pages 627–642, Wash-
ington, D.C., 2015. USENIX Association.

16

A Symbol-Symbol Jump Tables

This appendix describes jump tables with relative offsets and
how they are detected by our disassembler.
Most jump tables in programs compiled with GCC and
Clang (position dependent code) are lists of absolute ad-
dresses that can be detected like any other symbolic value.
This is not the case for jump tables generated by ICC and
jump tables generated by PIC code. These jump tables are
often expressed as lists of Symbol−Symbol expressions.
In this kind of jump tables, one of the symbols represents
a reference point, and the other symbol represents the jump
target. The reference point is the same for all the jump table
entries and the actual value stored at each the jump table entry
is the distance between the jump target and the reference point.
The size of the jump table entries can vary i.e. 1, 2, 4 or 8
bytes.

4 7 D A 7 b :
4 7 D A 8 2 :
4 7 D A 8 6 :
4 7 D A 8 d :
4 7 D A 9 0 :

4 a 0 9 f 0 :

lea RDX, QWORD PTR [RIP+ . L _ 4 A 0 9 F 0 ]
movzx EDX, BYTE PTR [RDX+RCX * 1 ]
lea RAX, QWORD PTR [RIP+ . L _ 4 7 D A 9 3 ]
add RAX,RDX
jmp RAX

. b y t e . L _ 4 7 D B 3 F - . L _ 4 7 D A 9 3
. b y t e . L _ 4 7 D B 3 6 - . L _ 4 7 D A 9 3
. b y t e . L _ 4 7 D B 2 B - . L _ 4 7 D A 9 3
. b y t e . L _ 4 7 D B 2 0 - . L _ 4 7 D A 9 3

Figure 6: Assembly (after symbolization) extracted from tar-
1.29 compiled with ICC -O2. This code implements a jump
table of Symbol-Symbol entries of size 1 byte.

Example 12. Consider the example code in Fig. 6. The ﬁrst
instruction loads the start address of the jump table onto RDX;
the second instruction reads the jump table entry and stores it
in RDX; the third instruction loads address 47DA93 that acts as
a reference for the jump table onto RAX; the fourth instruction
computes the jump target by adding RAX and RDX and the last
instruction executes the jump.
In order to ﬁnd a jump table, we need to determine: the
jump table starting point, the jump table reference point, and
the size of each jump table entry. Fortunately, the code pat-
terns used to implement this kind of jump tables are relatively
regular. We have specialized Datalog rules to detect them.

jump_table_start(AJump,Size,Start,Reference):−
reg_jump(AJump,_),
def_used(ASum,Reg,AJump,_),
reg_reg_op(ASum,Reg,RegEntry,RegRef,1,0),

def_used(AEntry,RegEntry,ASum,_),
data_access_pattern(Start,Size,Size,AEntry),

(15)

def_used(ARef,RegRef,ASum,_),
reg_val(ARef,RegRef,_,’NONE’,0,Reference).

Rule 15 is simpliﬁed version of the rule that detect the
pattern in Fig. 6. The rule ﬁnds a jump that uses a register and
“walks back” the code using def-use chains to the instruction
where the jump target is computed (at address Asum). At that
location, reg_reg_op represents an abstraction of an assem-
bly instruction on two registers Reg=RegEntry+RegRef×1+0.
Then, the rule examines the deﬁnition of RegEntry to ﬁnd
where the jump table entry is read (at address AEntry) and
thanks to its data_access_pattern, it determines the jump
table starting address Start and the size of each entry Size.
The other register RegRef should contain the jump table ref-
erence point. So its value is obtained using reg_val which
should contain a constant value (not expressed in terms of
another register).
By relying on the analyses presented in Sec. 5, i.e def-use
chains, DAPs and the register value analysis; the Datalog rule
is more robust than exact pattern matching. The instructions
involved in the jump table do not necessarily appear all to-
gether or in a ﬁxed order, and the rule does not rely on speciﬁc
instructions being used. E.g. the jump target computation is
sometimes done using LEA instead of ADD.
Once we have found the jump table beginning and
its corresponding data_access_pattern, we can use the

propagated_data_access (see Sec. 5.3) to create symbol−

symbol candidates for each of the jump table entries. That
means that we will consider that the jump table extends until
there is another data access from a different part of the code.
The detection of these jump tables has been the main ad-
dition required to support the ICC compiler. Other analyses
and heuristics have remained largely the same. We expect that
supporting additional compilers will require similar additions
as each compiler has its own particular code patterns. How-
ever, the analyses described in Sec. 5 remain useful building
blocks that facilitate supporting these special constructs in a
robust manner.

B Detailed Experimental Results

This appendix contains additional details on the experiments.
Table 4 contains the names, version, and sizes (in KB) of the
applications in the real world benchmark.
Tables 5, 6, and 7 contain the results of the symbolization
broken down by compiler and optimization ﬂag. Tables 8, 9,
and 10 contain the results of the functionality experiments
broken down by compiler and optimization ﬂag. These tables
present absolute numbers rather than percentages to facilitate
a more detailed analysis.
Additionally, we manually examined and diagnosed
Ddisasm’s symbolization failures to determine what are the
causes that lead to the remaining false positives, false nega-
tives or symbols pointing to the wrong section.

17

Program
bar-1.11.0
ed-0.9
grep-2.5.4
marst-2.4
tar-1.29

Size
91
63
181
104
547

Program
bison-2.1
enscript-1.6.1
gzip-1.2.4
patch-2.6.1
tnef-1.4.7

Size
359
253
81
155
74

Program
bool-0.2
ﬂex-2.5.4
lighttpd-1.4.18
re2c-0.13.5
units-1.85

Size
48
196
255
2554
65

Program
conﬂict-6.0
gawk-3.1.5
m4-1.4.4
rsync-3.0.7
wget-1.19.1

Size
28
485
154
1685
620

Program
doschk-1.1
gperf-3.0.3
make-3.80
sed-4.2
yasm-1.2.0

Size
18
409
202
201
899

Table 4: Real world example benchmarks. Each program is annotated with its size in KB when compiled with GCC 7.1.0 and
optimization ﬂag -O0.

Real world Benchmarks

In the real world benchmarks, the
20 false negatives corresponds to a single array of structs that
contains pointers. Our analysis obtains the right DAP with
the right multipliers but only 4 bytes of each of the pointers
are read instead of 8. This leads Ddisasm to conclude that
those locations contains data objects of type “Other” of size 4
instead of symbols. These FNs cause the corresponding tests
to fail.
The 20 symbols pointing to the wrong section are displace-
ments in indirect operands and happen in two variants of
the same program compiled with clang-6.0. These particular
cases are not currently detected by our heuristics but they also
do not cause test failures in our functionality experiments.

Coreutils Benchmarks

In Coreutils, there are 3 false posi-
tives, all in binaries compiled with -O0. They correspond to
immediate operands that are moved or compared to registers.
Those registers are loaded from the stack immediately before
the location of the immediate and they are stored in the stack
again immediately after. Therefore, our analyses do not obtain
any evidence on the type of those immediates. These FPs do
not cause tests failures, probably because the Coreutils test
suites are not exhaustive.

CGC Benchmarks

In the CGC benchmarks, 5 of the “Bro-
ken” binaries have false negatives where the corresponding
relocations refer to the symbols __init_array_start and
__init_array_end. These binaries, compiled with ICC, do
not have an .init_array section and in fact the symbols’
addresses are the same and fall outside all data sections.
Nonetheless, the code uses the difference between the two
symbols (which is zero) and thus it has the same behavior
even though these references have not been made symbolic.
In fact, we do not observe test failures in these binaries.

There are 2 other binaries, variants of the same program
compiled with ICC, that have displacements in an indirect
operand pointing to the wrong section. These particular cases
are not currently detected by our heuristics. They also do not
cause test failures.

The 3 remaining failures are due to false negatives in vari-
ants of the same program compiled with GCC 7.1. They cor-
respond to an immediate that should be a Symbol+Constant.
The immediate is a loop bound but it corresponds to a triple
nested loop that our heuristics do not detect well. The ex-
tended section considered is not large enough for the constant
required by the immediate. These FPs cause the tests to fail.

18

Real World Benchmarks

Compiler Optimization Binaries References

GCC 5.5

GCC 7.1

Clang 3.8

Clang 6.0

ICC

-O0
-O1
-O2
-O3
-Ofast
-Os
-O0
-O1
-O2
-O3
-Ofast
-Os
-O0
-O1
-O2
-O3
-Ofast
-Os
-O0
-O1
-O2
-O3
-Ofast
-Os
-O0
-O1
-O2
-O3
-Ofast
-Os

25
25
25
25
25
25
25
25
25
25
25
25
25
25
25
25
25
25
25
25
25
25
25
25
25
25
25
25
25
25

146169
139982
136633
162674
162664
118192
146825
139653
136776
165484
165477
117889
132401
125023
142910
149566
149539
126359
142370
134185
157599
164270
164277
124989
152957
121901
219130
227835
227832
119754

Ddisasm

FN WS Broken
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
10
1
0
10
1
0
0
0
0
0
0
20
0
1
0
0
0
0
0
0
0
0
0
0
0
0

FP
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0

Ramblr

FN Broken
8
7
8
8
8
8
9
9
9
8
9
7
8
7
9
8
8
8
48
8
43
8
9
7
9
6
12
8
20
9
21
9
20
8
11
8
8
6
12
8
25
8
20
8
22
8
12
8
2403
10
3832
25
16529
25
17460
25
17462
25
3829
25

FP
50
50
49
53
49
42
50
50
49
55
52
42
39
39
41
43
34
38
38
42
41
41
38
41
3044
8762
9133
9491
9489
8754

Table 5: Real world benchmark symbolization evaluation. Results broken down by compiler and optimization ﬂag.

19

Coreutils Benchmarks

Compiler Optimization Binaries References

GCC 5.5

GCC 7.1

Clang 3.8

Clang 6.0

ICC

-O0
-O1
-O2
-O3
-Os
-O0
-O1
-O2
-O3
-Os
-O0
-O1
-O2
-O3
-Os
-O0
-O1
-O2
-O3
-Os
-O0
-O1
-O2
-O3
-Os

106
106
106
106
106
106
106
106
106
106
106
106
106
106
106
106
106
106
106
106
106
106
106
106
106

124991
123239
113264
171177
81661
124919
123205
122142
188624
81319
98876
97662
107051
108836
98669
116987
118466
118200
123213
98565
112111
104858
270000
272180
104657

Ddisasm

FN WS Broken
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
1
0
0
0
0
0
0
0
0
0
0
0
0
0
0
2
0
0
0
0
0
0
0
0
0
0
0
0

FP
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
1
0
0
0
0
2
0
0
0
0

Ramblr

FN Broken
0
0
0
0
0
0
0
2
0
0
0
0
0
0
0
1
0
2
0
0
0
0
0
0
0
0
0
0
0
0
0
1
0
0
0
0
0
0
0
0
0
3
6572
106
61709
106
62141
106
6572
106

FP
0
0
0
3
0
0
0
1
2
0
0
0
0
0
0
1
0
0
0
0
3
455
3662
3662
455

Table 6: Coreutils benchmark symbolization evaluation. Results broken down by compiler and optimization ﬂag.

20

CGC Benchmarks

Compiler Optimization Binaries References

GCC 5.5

GCC 7.1

Clang 3.8

Clang 6.0

ICC

-O0
-O1
-O2
-O3
-Ofast
-Os
-O0
-O1
-O2
-O3
-Ofast
-Os
-O0
-O1
-O2
-O3
-Ofast
-Os
-O0
-O1
-O2
-O3
-Ofast
-Os
-O0
-O1
-O2
-O3
-Ofast
-Os

69
69
69
69
69
69
69
69
69
69
69
69
69
69
69
69
69
69
69
69
69
69
69
69
69
69
69
69
69
69

281509
278277
130326
135812
135759
127048
281452
278275
130253
136674
136658
127088
297970
231882
235458
236308
236319
128936
298708
135007
140984
141717
141744
128649
287675
150848
234370
236434
236442
151004

Ddisasm

FN WS Broken
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
1
0
1
1
0
1
1
0
1
0
0
0
0
0
0
0
1
1
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
1
1
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
4
0
2
0
0
0
3
0
1
3
0
1
2
0
1

FP
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0

Ramblr

FN Broken
0
0
1
1
1
2
1
4
1
4
0
0
0
0
1
1
1
2
1
3
1
3
0
0
0
0
0
1
0
1
0
1
0
1
0
0
0
0
0
1
0
1
0
1
0
1
0
0
2160
15
6406
69
9561
69
9572
69
9572
69
6404
69

FP
0
0
1
3
3
0
0
0
1
2
2
0
0
1
1
1
1
0
0
1
1
1
1
0
150
2123
2157
2158
2158
2123

Table 7: CGC benchmark symbolization evaluation. Results broken down by compiler and optimization ﬂag.

21

Compiler Optimization Binaries

GCC 5.5

GCC 7.1

Clang 3.8

Clang 6.0

ICC

-O0
-O1
-O2
-O3
-Ofast
-Os
-O0
-O1
-O2
-O3
-Ofast
-Os
-O0
-O1
-O2
-O3
-Ofast
-Os
-O0
-O1
-O2
-O3
-Ofast
-Os
-O0
-O1
-O2
-O3
-Ofast
-Os

25
25
25
25
25
25
25
25
25
25
25
25
25
25
25
25
25
25
25
25
25
25
25
25
25
25
25
25
25
25

Real World Benchmarks

Ddisasm

Ramblr

Disasm Reassemble Test Disasm Reassemble Test
25
25
25
25
21
19
25
25
25
25
21
16
25
25
25
25
21
16
25
25
25
25
20
9
25
25
25
25
21
11
25
25
25
25
21
14
25
25
25
25
21
14
25
25
25
25
21
15
25
25
25
25
21
11
25
25
25
25
21
6
25
25
25
25
21
6
25
25
25
25
21
11
25
25
25
25
22
17
25
25
25
25
21
11
25
25
25
25
21
10
25
25
25
25
21
10
25
25
25
25
23
12
25
25
25
25
21
9
25
25
25
25
22
17
25
25
25
25
21
10
25
25
25
25
21
8
25
25
25
25
21
9
25
25
25
25
23
11
25
25
25
25
21
9
25
25
25
25
17
9
25
25
24
25
0
0
25
25
25
24
0
0
25
25
25
25
0
0
25
25
25
25
0
0
25
25
25
25
0
0

Table 8: Real World benchmarks functionality evaluation. Results broken down by compiler and optimization ﬂag.

22

Compiler Optimization Binaries

GCC 5.5

GCC 7.1

Clang 3.8

Clang 6.0

ICC

-O0
-O1
-O2
-O3
-Os
-O0
-O1
-O2
-O3
-Os
-O0
-O1
-O2
-O3
-Os
-O0
-O1
-O2
-O3
-Os
-O0
-O1
-O2
-O3
-Os

106
106
106
106
106
106
106
106
106
106
106
106
106
106
106
106
106
106
106
106
106
106
106
106
106

Coretuils Benchmarks

Ddisasm

Ramblr

Disasm Reassemble Test Disasm Reassemble Test
106
106
106
106
106
106
106
106
106
106
106
93
106
106
106
106
105
92
106
106
106
105
104
87
106
106
106
106
106
78
106
106
106
106
106
90
106
106
106
106
106
92
106
106
106
106
105
76
106
106
106
106
105
67
106
106
106
106
106
78
106
106
106
106
106
93
106
106
106
106
106
89
106
106
106
106
106
84
106
106
106
106
106
84
106
106
106
106
106
82
106
106
106
106
106
93
106
106
106
106
106
89
106
106
106
106
106
84
106
106
106
106
106
84
106
106
106
106
106
84
106
106
106
106
106
93
106
106
106
106
0
0
106
106
106
93
0
0
106
106
106
92
0
0
106
106
106
106
0
0

Table 9: Coreutils benchmarks functionality evaluation. Results broken down by compiler and optimization ﬂag.

23

Compiler Optimization Binaries

gcc

gcc-7

clang

clang-6.0

icc

-O0
-O1
-O2
-O3
-Ofast
-Os
-O0
-O1
-O2
-O3
-Ofast
-Os
-O0
-O1
-O2
-O3
-Ofast
-Os
-O0
-O1
-O2
-O3
-Ofast
-Os
-O0
-O1
-Os
-O2
-O3
-Ofast

69
68
69
68
68
68
69
69
69
69
69
68
69
69
68
68
68
69
69
69
69
68
68
68
68
69
68
66
66
66

CGC Benchmarks

Ddisasm

Ramblr

Disasm Reassemble Test Disasm Reassemble Test
69
69
69
69
66
53
68
68
68
68
64
46
69
69
69
69
55
39
68
68
68
68
49
35
68
68
68
68
49
35
68
68
68
68
64
45
69
68
68
69
66
53
69
69
69
69
65
47
69
69
68
69
57
41
69
69
68
69
52
38
69
69
68
69
52
38
68
68
68
68
64
45
69
69
69
69
64
47
69
69
68
69
52
31
68
68
68
68
47
29
68
68
68
68
47
29
68
68
68
68
47
29
69
69
69
69
65
45
69
69
69
69
64
47
69
69
69
69
53
33
69
69
69
69
50
32
68
68
68
68
49
31
68
68
68
68
49
31
68
68
68
68
64
45
68
68
68
68
54
35
69
69
68
69
0
0
68
68
67
68
0
0
66
66
65
66
0
0
66
66
64
66
0
0
66
66
64
66
0
0

Table 10: CGC benchmarks functionality evaluation. Results broken down by compiler and optimization ﬂag.

24

