Make Skeleton-based Action Recognition Model
Smaller, Faster and Better

Fan Yang1,2 , Sakriani Sakti1,2 , Yang Wu3 , Satoshi Nakamura1,2
1 Nara Institute of Science and Technology, Japan 2 RIKEN, AIP, Japan 3 Kyoto University, Japan

1

9
1
0
2

v
o

N

1
2

]

V

C

.

s

c

[

6
v
8
5
6
9
0

.

7
0
9
1

:

v

i

X

r

a

Abstract—Although skeleton-based action recognition has
achieved great success in recent years, most of the existing meth-
ods may suffer from a large model size and slow execution speed.
To alleviate this issue, we analyze skeleton sequence properties to
propose a Double-feature Double-motion Network (DD-Net) for
skeleton-based action recognition. By using a lightweight network
structure (i.e., 0.15 million parameters), DD-Net can reach a
super fast speed, as 3,500 FPS on one GPU, or, 2,000 FPS on
one CPU. By employing robust features, DD-Net achieves the
state-of-the-art performance on our experiment datasets: SHREC
(i.e., hand actions) and JHMDB (i.e., body actions). Our code is
on https://github.com/fandulu/DD-Net.
Index Terms—Skeleton-based Action Recognition, Body Ac-
tions, Hand Gestures.

I . IN TRODUC T ION

Skeleton-based action recognition has been widely used
in multimedia applications, such as human-computer inter-
action [1], human behavior understanding [2] and medical
assistive applications [3]. However, most of the existing meth-
ods may suffer from a large model size and slow execution
speed [4], [5], [6], [7], [8].
In real applications, a desirable skeleton-based action recog-
nition model should run efﬁciently by using a few param-
eters, and, also be adaptable to various application sce-
narios (e.g.,
hand/body, 2D/3D skeleton, and actions re-
lated/unrelated to global trajectories). To achieve this goal,
we investigate skeleton sequence properties to propose a
lightweight Double-feature Double-motion Network (DD-
Net), which is equipped with a Joint Collection Distances
(JCD) feature and a two-scale global motion feature.
More speciﬁcally, we conduct
research on four
types
of skeleton sequence properties (see Fig. 1): (a) location-
viewpoint variation,
(b) motion scale variation,
(c)
re-
lated/unrelated to global trajectories, (4) uncorrelated joint
indices. To address challenges caused by these properties,
previous works may prone to propose complicated neural
network models, which end up with large model size.
In contrast, we address these challenges by simplifying
both the input feature and the network structure. Our JCD
feature contains the location-viewpoint invariant information
of skeleton sequences. Compared with other similar features,
it can be easily computed and includes fewer elements.
Since global motions cannot be incorporated into a location-
viewpoint invariant feature, we introduce a two-scale global
motion feature to improve the generalization of DD-Net.
Besides, its two-scale structure makes it robust to the motion
scale variance. Through an embedding process, DD-Net can

(a) Location-viewpoint variation

(b) Motion scale variation

(c) Related/unrelated to global trajectories

(d) Uncorrelated joint indices (PuppetModel [9])

Fig. 1: Examples of skeleton sequence properties.

Slow MotionFast MotionActionsrelatedtoglobaltrajectories,e.g.,swipeV.Actionsunrelatedtoglobaltrajectories,e.g.,pinch.Adjacent indices but notlocally correlated joints:[9, 10],[11, 12],[13, 14], etc.Locally correlated but not adjacent indices joints:[4, 8, 12],[5, 9, 13],[2, 6, 7], etc.134567891011121314152 
 
 
 
 
 
2

Fig. 2: The network architecture of DD-Net. “2×CNN(3,
2*f ilters), /2” denotes two 1D ConvNet layers (kernel size
= 3, channels = 2*f ilters) and a Maxpooling (strides = 2).
Other ConvNet layers are deﬁned in the same format. GAP
denotes Global Average Pooling. FC denotes Fully Connected
Layers (or Dense Layers). We can change the model size by
modifying f ilters.

The Cartesian coordinate feature is variant to locations and
viewpoints. As Fig. 1 (a) shows, when skeletons are rotated or
shifted, the Cartesian coordinate feature can be signiﬁcantly
changed. The geometric feature (e.g.,
angles/distances), on
the other hand, is location-viewpoint invariant, and thereby it
has been utilized for skeleton-based action recognition for a
while. However, existing geometric features may need to be
heavily redesigned from one dataset to another [18], [22], or,
contain redundant elements [33]. To alleviate these issues, we
introduce a Joint Collection Distances (JCD) feature.
We calculate the Euclidean distances between a pair of
collective joints to obtain a symmetric matrix. To reduce
the redundancy, only the lower triangular matrix without the
diagonal part is used as the JCD feature (see Fig. 3). Hence,
the JCD feature is less than half the size of [33].

automatically learn the proper correlation of joints, which is
hard to be predeﬁned by joint indices.
Compared to methods relying on complicated model struc-
tures, DD-Net provides higher action recognition accuracy and
demonstrates its generalization on our experiential datasets.
With its efﬁciency both in terms of computational complexity
and the number of parameters, DD-Net is sufﬁcient to be
applied in real applications.

I I . R ELAT ED WORK S

Nowadays, with the fast advancement of deep learning,
skeleton acquisition is not
limited to use motion capture
systems [10] and depth cameras [11]. The RGB data, for
instance, can be used to infer 2D skeletons [12], [13] or
3D skeletons [14], [15] in real time. Moreover, even WiFi
signals can be used to estimate skeleton data [16], [17]. Those
achievements have made skeleton-based action recognition
available on a huge amount of multimedia resources and
therefore have stimulated the model’s development.
In general, in order to achieve a better performance for
skeleton-based action recognition, previous studies attempt to
work on two aspects: introduce new features for skeleton
sequences [18], [19], [20], [21], [22], [8], [23], and, propose
novel neural network architectures [24], [25], [26], [27], [5],
[6], [28].
A good skeleton-sequence representation should contain
global motion information and be location-viewpoint invariant.
However, it is challenging to satisfy both requirements in
one feature. The studies [19], [21], [8], [23] focused on
global motions without considering the location-viewpoint
variation in their features. Other studies [18], [20], [22], on
the contrary, introduced location-viewpoint invariant features
without considering global motions. Our work bridges their
gaps by seamlessly integrating a location-viewpoint invariant
feature and a two-scale global motion feature together.
Although Recurrent Neural Networks (RNNs) are com-
monly used in skeleton-based action recognition [29], [30],
[31], [32], [33], [22], we argue that it is relatively slow and
difﬁcult for parallel computing, compared with methods [24],
[5], [23] that use Convolutional Neural Networks (CNNs).
Since we take the model speed as one of our priorities, we
utilize 1D CNNs to construct the backbone network of DD-
Net.

I I I . M ETHODO LOGY

The network architecture of Double-feature Double-motion
Network (DD-Net) is shown in Fig. 2. In the following,
we explain our motivation for designing input features and
network structures of DD-Net.

A. Modeling Location-viewpoint Invariant Feature by Joint
Collection Distances (JCD)
For skeleton-based action recognition, two types of input
features are commonly used: the geometric feature [18], [22]
and the Cartesian coordinate feature [31], [32], [34], [6], [7].

Fig. 3: An example of Joint Collection Distances (JCD) feature
at frame k , where the number of joints is N .

In more detail, we assume the total frame number is K
(K = 32 as the default setting) and there are totally N joints

ConcatenateCNN(1,2*filters)CNN(1,filters), /2Temporal difference(stride=2)CNN(1,2*filters)CNN(1,filters), /2CNN(1,2*filters)CNN(1,filters)Temporal difference(stride=1)Cartesian CoordinatesJoints Collection Distances2×CNN(3,2*filters), /22×CNN(3,4*filters), /22×CNN(3,8*filters)CNN(3,filters)CNN(3,filters)CNN(3,filters)Fast motionSlowmotionEmbeddingGAPFC(num_classes)FC(128)Joints 1:N-1Joints 2:Nfor one subject. At frame k , the 3D Cartesian coordinates of
joint n is represented as J k
i = (x, y , z ), while the 2D Cartesian
coordinates is represented as J k
i = (x, y). Put all of joints
together, we have a joint collection S k = {J k
N }.
The formula for calculating the JCD feature of S k is:

1 , J k
2 , ..., J k



J k

2 J k
1

(cid:13)(cid:13)(cid:13)−−−→
(cid:13)(cid:13)(cid:13)−−−→

N J k
1

(cid:13)(cid:13)(cid:13)
(cid:13)(cid:13)(cid:13) · · ·

· · ·

 ;
(cid:13)(cid:13)(cid:13)

2

J k

J k

J k

J k

i J k
j

(cid:1).

N J k
N −1

J CDk =

(cid:13)(cid:13)(cid:13)−−−→

· · · (cid:13)(cid:13)(cid:13)−−−−−→

...
. . .
(1)
...
. . .
(cid:13)(cid:13)(cid:13)(i (cid:54)= j ) denotes the Euclidean distance between
where
i and J k
j .
ﬂattened JCD is (cid:0)N
In our processing, the JCD feature is ﬂattened to be a one-
dimensional vector as our model’s input. The dimension of
B. Modeling Global Scale-invariant Motions by a Two-scale
Motion Feature
Although the JCD feature is location-viewpoint invariant,
the same as other geometric features, it does not contain global
motion information. When actions are associated with global
trajectories (see Fig. 1 (c)), solely using the JCD feature is
insufﬁcient. Unlike previous works that only utilize either
the geometric feature [18], [22] or the Cartesian coordinate
feature [24], [25], [26], [27], our DD-Net seamlessly integrates
both of them.
We calculate the temporal differences (i.e., the speed) of the
Cartesian coordinate feature to obtain global motions, which
is location-invariant. For the same action, however, the scale
of global motions may not be exactly identical. Some might
be faster, and others might be slower (see Fig. 1 (b)). To learn
a robust global motion feature, both fast and slow motions
should be considered. Conferring this intuition to DD-Net, we
employ a fast global motion and a slow global motion to form
a two-scale global motion feature. This idea is inspired by
the two-scale optical ﬂows proposed for RGB-based action
recognition [35].
Technically, the two-scale motions can be generated by the
following equation:

slow = S k+1 − S k , k ∈ {1, 2, 3, ..., K − 1};
M k
f ast = S k+2 − S k , k ∈ {1, 3, ..., K − 2};
M k

(2)

slow and M k

to S [1,...,K ] , we have M [1,...,K−1]
and M [1,...,K/2−1]
slow
f ast
slow ∈ RDmotion and M k
f ast ∈
slow and M k
f ast as M k

where M k
f ast denote the slow motion and the fast
motion at frame k , respectively. S k+1 and S k+2 are behind the
S k of one frame and two frames, respectively. Corresponding
.
To generate an one-dimensional input at each frame, we
reshape M k
RDmotion , respectively, where Dmotion is the dimension of
ﬂattened vector. To match the frame number of the JCD
feature, we perform linear interpolation to resize M [1,...,K−1]
, respectively.
Consequently, two-scale global motion feature is composed of

and M [1,...,K/2−1]
as M [1,...,K ]
and M [1,...,K/2]
f ast
slow
f ast
∈ RK×Dmotion and M [1,...,K/2]
∈ R(K/2)×Dmotion .
f ast

M [1,...,K ]
slow

slow

Such a process can be done in our DD-Net, and only the
Cartesian coordinate feature is needed as the input.

3

C. Modeling Joint Correlations by an Embedding
Fig. 1 (d) shows that the joint indices (i.e., the IDs of the
head, left and right hands, etc.) are not locally correlated.
Moreover, in different actions, the correlation of joints could
be dynamically changed. Hence, the difﬁculty arises when we
try to pre-deﬁne the correlation of joints by manually ordering
their indices.
Since most of neural networks inherently assume that inputs
are locally correlated, directly processing the locally uncorre-
lated joint feature is inappropriate. To tackle this problem, our
DD-Net embeds the JCD feature and the two-scale motion
feature into latent vectors at each frame. The correlation of
joints is automatically learned through the embedding. As
another beneﬁt, the embedding process also reduces the effect
of skeleton noise.
More formally, let embedding representations of J CDk ,
and εk
, respec-
tively, the embedding operation is as follows,

f ast to be εk

slow and M k

J CD , εk

M k

Mslow

Mf ast

(3)

Mf ast

Mslow

slow );
f ast ).

εk
J CD = Embed1 (J CDk );
εk
= Embed1 (M k
εk
= Embed2 (M k
where the Embed1 is deﬁned as C onv1D(1, 2 ∗ f ilters) →
C onv1D(3,
f ilters) → C onv1D(1, f ilters), and the Embed2 is deﬁned
C onv1D(1, 2 ∗ f ilters) → C onv1D(3, f ilters) →
C onv1D(1, f ilters)
→ M axpooling(2), because J CDk and M k

slow have double

the temporal length of M k
DD-Net futher concatenates embedding features to a repre-
sentation εk by

f ast .

as

εk = εk

J CD ⊕ εk
w .r .t . εk ∈ R(K/2)×f ilters ;

⊕ εk

Mslow

Mf ast

,

(4)

where ⊕ is the concatenation operation.
After the embedding process, subsequent processes are not
affected by the joint indices, and therefore DD-Net can use
the 1D ConvNet to learn the temporal information as Fig. 2
shows.

IV. EX PER IM EN T S

A. Experimental Datasets
We select two skeleton-based action recognition datasets, as
SHREC dataset [4] and JHMDB dataset [9], to evaluate our
DD-Net from different perspectives (see Table I).
Although other information (e.g., RGB data) is available,
only the skeleton information is used in our experiments. 3D
skeletons are given by SHREC dataset, which are derived from
RGB-D data and contain more spatial information. In JHMDB
dataset, 2D skeletons are interpreted from RGB videos, which

TABLE I: Properties of experimental datasets

TABLE II: Results on SHREC (Using 3D skeletons only) [4]

4

SHREC
Dataset

2,800
1 Training Set
1 Testing Set
3
hand
14 and 28

JHMDB
Dataset

928
3 Split Training/
Testing Sets
2
body
21





Number of samples
Training/
Testing Setup
Dimension of skeletons
subject
Number of actions
Actions are strongly
correlated to
global trajectories

can be applied in more general cases where inferring the depth
information may be hard or impossible. Besides, actions in
SHREC dataset are strongly correlated to the subject’s global
trajectories (e.g., a hand swipes a ‘V’ shape), while JHMDB
dataset may have a weak connection with global trajectories.
We show how these properties affect the performance and
demonstrate the generalization of DD-Net
in our ablation
studies.

B. Evaluation Setup
The SHREC dataset is evaluated in two cases: 14 gestures
and 28 gestures. The JHMDB dataset is evaluated by using
the manually annotated skeletons, and we average the results
from three split training/testing sets.
In ablation studies, we explore how each DD-Net com-
ponent contributes to the action recognition performance by
removing one component while remaining others unchanged.
Furthermore, we also explore how the performance varies with
different model size by adjusting the value of f ilters in Fig. 2.

C. Implementation Details
Since the DD-Net is small, it is feasible to put all of the
training sets into one batch on a single GTX 1080Ti GPU.
We choose Adam (β1 = 0.9, β2 = 0.999) [36] as the
optimizer, with an annealing learning rate that drops from 1−3
to 1−5 . During the training, DD-Net only takes a temporal
augmentation, which randomly selects 0.9 of entire frames.
To demonstrate the superiority of DD-Net, we do not apply
any ensemble strategy or pre-trained weights to boost the
performance. To make DD-Net can be easily deployed to
real applications, we implement it by Keras [37] backend in
Tensorﬂow, which is “notorious” for its slow execution speed.
Using other neural network frameworks may make DD-Net
faster.

D. Result Analysis and Discussion
The action recognition results of SHREC dataset are pre-
sented in Table II and more details are listed in their confusion
matrix. The confusion matrix of 14 actions and 28 actions are
Fig. 4 and Fig. 5, respectively. The action recognition results
of JHMDB dataset are presented in Table III.
Overall, although DD-Net takes fewer parameters, it can
achieve superior results on SHREC dataset and JHMDB

Methods

Parameters

14
Gestures

28
Gestures

-
7.92 M
-
8-9 M
13.83 M
5-6 M
-

88.2%
82.9%
77.9%
89.8%
91.3%
93.6%
91.3%

81.9%
71.9%
-
86.3%
84.4%
90.7%
86.6%

1.70 M

55.2%

41.6%

1.76 M

92.7%

90.2%

Speed
on GPU

-
-
-
238 FPS
-
303 FPS
361 FPS

-

-

Dynamic hand [19] (CVPRW16)
Key-frame CNN [4] (3DOR17)
3 Cent [21] (STAG17)
CNN+LSTM[38] (PR18)
Parallel CNN [5] (RFIAP18)
STA-Res-TCN [6] (Gesture18)
MFA-Net [23] (Sensor19)
DD-Net (ﬁlters=64, w/o
global fast&slow motion)
DD-Net (ﬁlters=64,
w/o global slow motion)
DD-Net (ﬁlters=64,
w/o global fast motion)
DD-Net (ﬁlters=64)
DD-Net (ﬁlters=32)
DD-Net (ﬁlters=16)

1.76 M
1.82 M
0.50 M

0.15 M

93.3%

94.6%

93.5%
91.8%

90.5%
-
91.9% 2,200 FPS
90.4%
3,100 FPS
90.0%
3,500 FPS

TABLE III: Results on JHMDB (Using 2D skeletons only) [9]

Methods

Chained Net [7] (ICCV17)
EHPI [?] (ITSC19)
PoTion [8] (CVPR18)
DD-Net (ﬁlters=64, w/o
global fast&slow motion)
DD-Net (ﬁlters=64,
w/o global slow motion)
DD-Net (ﬁlters=64,
w/o global fast motion)
DD-Net (ﬁlters=64)
DD-Net (ﬁlters=32)
DD-Net (ﬁlters=16)

Parameters Manually
annotated
skeletons

17.50 M
1.22 M
4.87 M

56.8%
65.5%
67.9%

Speed
on GPU

33 FPS
29 FPS
100 FPS

0.46 M

70.3%

0.48 M

72.5%

-

-

0.48 M
1.82 M
0.50 M

0.15 M

73.1%

77.2%

73.7%
65.7%

-
2,200 FPS
3,100 FPS
3,500 FPS

Fig. 4: Confusion matrix of SHREC dataset (14 hand actions)
obtained by DD-Net.

dataset. The confusion matrix also shows that DD-Net is robust
to each action class. Despite the data property divergence
existing, DD-Net demonstrates its generalization ability, which

5

Fig. 5: Confusion matrix of SHREC dataset (28 hand actions) obtained by DD-Net.

suggests it can accommodate a wide range of skeleton-based
action recognition scenarios.
From ablation studies, we can inspect that when actions
are strongly correlated to global trajectories (e.g., SHREC
dataset), just using the JCD feature cannot produce a satisfac-
tory performance. When actions are not strongly correlated to
global trajectories (e.g., JHMDB dataset), the global motion
feature still helps to improve the performance, but not as
signiﬁcant as the previous case. Such results agree with our
assumptions: although the JCD feature is location-viewpoint
invariant, it is isolated from global motions. The results also
show that using the two-scale motion feature generates higher
classiﬁcation accuracy than only using a one-scale motion
feature, which suggests that our proposed two-scale global
motion feature is more robust to scale variation of motions.
With the same components, DD-Net can adjust its model size
by modifying the value of f ilters in CNN layers. We select
64, 32 and 16 as the values of f ilters to perform experiments.
When DD-Net reaches the best performance on SHREC and
JHMDB datasets, the values of f ilters are 64. It is worth
noting that DD-Net can generate comparable results by only
using 0.15 million parameters.
In addition, since DD-net employs one-dimensional CNNs
to extract the feature, it is much faster than other models
that use RNNs [31], [22], [32], [25] or 2D/3D CNNs [5],
[39], [7], [8], [28]. During its inferences, DD-Net’s speed can
reach around 3,500 FPS on one GPU (i.e., GTX 1080Ti), or,

2,000 FPS on one CPU (i.e., Intel E5-2620). While RNN-
based models face great challenges for parallel processing
(due to sequential dependency), our DD-Net does not have
this issue because CNNs are used. Therefore, whether low-
computational (e.g., on small devices) or high-computational
applications (e.g., on parallel computing stations) are con-
cerned, our DD-Net enjoys signiﬁcant superiority.

V. CONC LU S ION

By analyzing the basic properties of skeleton sequence, we
propose two features and a DD-Net for efﬁcient skeleton-based
action recognition. Although DD-Net only contains a few
parameters, it can achieve state-of-the-art performance on our
experimental datasets. Due to the simplicity of DD-Net, many
possibilities exist to enhance/extend it for broader studies.
For instance, online action recognition can be approached
by modifying the frame sampling strategies; RGB data or
depth data could be used with it to further improve the action
recognition performance; it is also possible to extend it for
temporal action detection by adding temporal segmentation
related modules.

R E FER ENC E S

[1] Z. Ren, J. Meng, J. Yuan, and Z. Zhang, “Robust hand gesture recogni-
tion with kinect sensor,” in Proceedings of the 19th ACM international
conference on Multimedia. ACM, 2011, pp. 759–760.

6

[24] C. Li, Y. Hou, P. Wang, and W. Li, “Joint distance maps based action
recognition with convolutional neural network,” IEEE Signal Processing
Letters, vol. 24, no. 5, pp. 624–628, 2017.
[25] I. Lee, D. Kim, S. Kang, and S. Lee, “Ensemble deep learning for
skeleton-based action recognition using temporal sliding lstm networks,”
in Proceedings of
the IEEE International Conference on Computer
Vision, 2017, pp. 1012–1020.
[26] Y. Tang, Y. Tian, J. Lu, P. Li, and J. Zhou, “Deep progressive reinforce-
ment learning for skeleton-based action recognition,” in Proceedings
of the IEEE Conference on Computer Vision and Pattern Recognition,
2018, pp. 5323–5332.
[27] Z. Yang, Y. Li, J. Yang, and J. Luo, “Action recognition with spatio-
temporal visual attention on skeleton image sequences,” IEEE Transac-
tions on Circuits and Systems for Video Technology, 2018.
[28] D. Ludl, T. Gulde, and C. Curio, “Simple yet efﬁcient real-time pose-
based action recognition,” arXiv preprint arXiv:1904.09140, 2019.
[29] Y. Du, W. Wang, and L. Wang, “Hierarchical recurrent neural network
for skeleton based action recognition,” in Proceedings of the IEEE
conference on computer vision and pattern recognition, 2015, pp. 1110–
1118.
[30] J. Liu, A. Shahroudy, D. Xu, and G. Wang, “Spatio-temporal lstm with
trust gates for 3d human action recognition,” in European Conference
on Computer Vision. Springer, 2016, pp. 816–833.
[31] H. Wang and L. Wang, “Modeling temporal dynamics and spatial
conﬁgurations of actions using two-stream recurrent neural networks,”
in e Conference on Computer Vision and Pa ern Recognition (CVPR),
2017.
[32] S. Song, C. Lan, J. Xing, W. Zeng, and J. Liu, “An end-to-end spatio-
temporal attention model for human action recognition from skeleton
data.” in AAAI, vol. 1, no. 2, 2017, pp. 4263–4270.
[33] C. Li, P. Wang, S. Wang, Y. Hou, and W. Li, “Skeleton-based action
recognition using lstm and cnn,” in Multimedia & Expo Workshops
(ICMEW), 2017 IEEE International Conference on.
IEEE, 2017, pp.
585–590.
[34] S. Yan, Y. Xiong, and D. Lin, “Spatial
temporal graph convolu-
tional networks for skeleton-based action recognition,” arXiv preprint
arXiv:1801.07455, 2018.
[35] C. Feichtenhofer, H. Fan, J. Malik, and K. He, “Slowfast networks for
video recognition,” arXiv preprint arXiv:1812.03982, 2018.
[36] D. P. Kingma and J. Ba, “Adam: A method for stochastic optimization,”
arXiv preprint arXiv:1412.6980, 2014.
[37] F. Chollet et al., “Keras,” 2015.
[38] J. C. Nunez, R. Cabido, J. J. Pantrigo, A. S. Montemayor, and J. F.
Velez, “Convolutional neural networks and long short-term memory for
skeleton-based human activity and hand gesture recognition,” Pattern
Recognition, vol. 76, pp. 80–94, 2018.
[39] M. Liu and J. Yuan, “Recognizing human actions as the evolution of pose
estimation maps,” in Proceedings of the IEEE Conference on Computer
Vision and Pattern Recognition, 2018, pp. 1159–1168.

[2] S.-E. Wei, N. C. Tang, Y.-Y. Lin, M.-F. Weng, and H.-Y. M. Liao,
“Skeleton-augmented human action understanding by learning with
progressively reﬁned data,” in Proceedings of the 1st ACM International
Workshop on Human Centered Event Understanding from Multimedia.
ACM, 2014, pp. 7–10.
[3] Y.-J. Chang, S.-F. Chen, and J.-D. Huang, “A kinect-based system
for physical rehabilitation: A pilot study for young adults with motor
disabilities,” Research in developmental disabilities, vol. 32, no. 6, pp.
2566–2570, 2011.
[4] Q. De Smedt, H. Wannous, J.-P. Vandeborre, J. Guerry, B. Le Saux, and
D. Filliat, “Shrec’17 track: 3d hand gesture recognition using a depth
and skeletal dataset,” in 10th Eurographics Workshop on 3D Object
Retrieval, 2017.
[5] G. Devineau, W. Xi, F. Moutarde, and J. Yang, “Convolutional neural
networks for multivariate time series classiﬁcation using both inter-
and intra-channel parallel convolutions,” in Reconnaissance des Formes,
Image, Apprentissage et Perception (RFIAP’2018), 2018.
[6] J. Hou, G. Wang, X. Chen, J.-H. Xue, R. Zhu, and H. Yang, “Spatial-
temporal attention res-tcn for skeleton-based dynamic hand gesture
recognition,” gesture, vol. 30, no. 5, p. 3, 2018.
[7] M. Zolfaghari, G. L. Oliveira, N. Sedaghat, and T. Brox, “Chained multi-
stream networks exploiting pose, motion, and appearance for action
classiﬁcation and detection,” in Computer Vision (ICCV), 2017 IEEE
International Conference on Computer Vision.
IEEE, 2017, pp. 2923–
2932.
[8] V. Choutas, P. Weinzaepfel, J. Revaud, and C. Schmid, “Potion: Pose
motion representation for action recognition,” in CVPR 2018, 2018.
[9] H. Jhuang, J. Gall, S. Zufﬁ, C. Schmid, and M. J. Black, “Towards
understanding action recognition,” in International Conf. on Computer
Vision (ICCV), Dec. 2013, pp. 3192–3199.
[10] T. B. Moeslund and E. Granum, “A survey of computer vision-based
human motion capture,” Computer vision and image understanding,
vol. 81, no. 3, pp. 231–268, 2001.
[11] Z. Zhang, “Microsoft kinect sensor and its effect,” IEEE multimedia,
vol. 19, no. 2, pp. 4–10, 2012.
[12] Z. Cao, T. Simon, S.-E. Wei, and Y. Sheikh, “Realtime multi-person
2d pose estimation using part afﬁnity ﬁelds,” in Computer Vision and
Pattern Recognition (CVPR), 2017 IEEE Conference on.
IEEE, 2017,
pp. 1302–1310.
[13] B. Xiao, H. Wu, and Y. Wei, “Simple baselines for human pose
estimation and tracking,” in Proceedings of the European Conference
on Computer Vision (ECCV), 2018, pp. 466–481.
[14] D. Mehta, S. Sridhar, O. Sotnychenko, H. Rhodin, M. Shaﬁei, H.-P.
Seidel, W. Xu, D. Casas, and C. Theobalt, “Vnect: Real-time 3d
human pose estimation with a single rgb camera,” vol. 36, no. 4, 2017.
[Online]. Available: http://gvv.mpi- inf.mpg.de/projects/VNect/
[15] L. Ge, H. Liang, J. Yuan, and D. Thalmann, “Real-time 3d hand pose
estimation with 3d convolutional neural networks,” IEEE Transactions
on Pattern Analysis and Machine Intelligence, 2018.
[16] M. Zhao, T. Li, M. Abu Alsheikh, Y. Tian, H. Zhao, A. Torralba, and
D. Katabi, “Through-wall human pose estimation using radio signals,”
in Proceedings of the IEEE Conference on Computer Vision and Pattern
Recognition, 2018, pp. 7356–7365.
[17] F. Wang, S. Panev, Z. Dai, J. Han, and D. Huang, “Can wiﬁ estimate
person pose?” arXiv preprint arXiv:1904.00277, 2019.
[18] C. Chen, Y. Zhuang, F. Nie, Y. Yang, F. Wu, and J. Xiao, “Learning a
3d human pose distance metric from geometric pose descriptor,” IEEE
Transactions on Visualization and Computer Graphics, vol. 17, no. 11,
pp. 1676–1689, 2011.
[19] Q. De Smedt, H. Wannous, and J.-P. Vandeborre, “Skeleton-based dy-
namic hand gesture recognition,” in Proceedings of the IEEE Conference
on Computer Vision and Pattern Recognition Workshops, 2016, pp. 1–9.
[20] M. Liu, H. Liu, and C. Chen, “Enhanced skeleton visualization for view
invariant human action recognition,” Pattern Recognition, vol. 68, pp.
346–362, 2017.
[21] F. M. Caputo, P. Prebianca, A. Carcangiu, L. D. Spano, and A. Giachetti,
“A 3 cent recognizer: Simple and effective retrieval and classiﬁcation
of mid-air gestures from single 3d traces,” Smart Tools and Apps for
Graphics. Eurographics Association, 2017.
[22] S. Zhang, Y. Yang, J. Xiao, X. Liu, Y. Yang, D. Xie, and Y. Zhuang,
“Fusing geometric features for skeleton-based action recognition using
multilayer lstm networks,” IEEE Transactions on Multimedia, vol. 20,
no. 9, pp. 2330–2343, 2018.
[23] X. Chen, G. Wang, H. Guo, C. Zhang, H. Wang, and L. Zhang,
“Mfa-net: Motion feature augmented network for dynamic hand gesture
recognition from skeletal data,” Sensors, vol. 19, no. 2, p. 239, 2019.

