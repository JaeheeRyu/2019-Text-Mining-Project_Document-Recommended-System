9
1
0
2

v
o

N

1
2

]

L

M

.

t

a

t

s

[

1
v
1
1
4
9
0

.

1
1
9
1

:

v

i

X

r

a

Random Machines: A bagged-weighted support vector model with free
kernel choice

Anderson Ara

Department of Statistics
Federal University of Bahia
Salvador, Bahia, Brazil

Mateus Maia

Department of Statistics
Federal University of Bahia
Salvador, Bahia, Brazil

Samuel Macêdo

Department of Natural Sciences and Mathemathics
Federal Institute of Pernambuco
Recife, Pernambuco, Brazil

Francisco Louzada

Institute of Mathematical and Computer Sciences
University of São Paulo
São Carlos, São Paulo, Brazil

Abstract

Improvement of statistical learning models in order to increase eﬃciency in solving classiﬁcation or
regression problems is still a goal pursued by the scientiﬁc community. In this way, the support vector
machine model is one of the most successful and powerful algorithms for those tasks. However, its
performance depends directly from the choice of the kernel function and their hyperparameters. The
traditional choice of them, actually, can be computationally expensive to do the kernel choice and the
tuning processes. In this article, it is proposed a novel framework to deal with the kernel function selection
called Random Machines. The results improved accuracy and reduced computational time. The data
study was performed in simulated data and over 27 real benchmarking datasets.
Keywords: Support Vector Machines, Bagging, Kernel Functions

1. Introduction

The application and development of statistical learning methods is currently an important and
signiﬁcant research ﬁeld in science. The supervised machine learning techniques have numerous applications
in classiﬁcation tasks ranging from cancer diagnostics and prediction [1], speech recognition [2], text

Preprint submitted to Journal of LATEX Templates

November 22, 2019

 
 
 
 
 
 
classiﬁcation [3, 4] and ﬁnancial fraud detection [5]. The variety of methods has been used in the ﬁeld is
huge, but one of them emerges, the Support Vector Machine (SVM). The SVM [6] is the youngest well
established and successful in traditional learning methods. Smola [7] presented some good proprieties
of this learning algorithm, including good generalization capacity, high eﬃciency in prediction tasks,
beyond the convexity of the objective function which guarantees a global minimum. Some works present
the superiority of the SVM when compared with other supervised learning benchmarking techniques,
highlighting good accuracy results [8, 9, 10].
At the same time, the ensemble methods have been gaining more strength as a tool to improve the
accuracy in classiﬁcation models. The combination of singular models can enhance predictive power and
increase its generalization capacity [11]. There are two main classes of ensemble algorithms: bagging [12]
that uses independent bootstrap samples to create multiple models and built a ﬁnal classiﬁer combining
them, and boosting algorithms [13] that built sequential models in order to assign diﬀerent weights relying
on their performance.
The literature already proposed bagging methods jointly with the support vector machine algorithm
[14] as a methodology of increasing its accuracy. Wang et al. [15] realized an empirical study of Bagged
SVM and showed that the technique performs as well or better than other methods with a relatively higher
generality. Moreover, diﬀerent applications of bagged SVM are reported, e.g breast cancer prediction
[16, 17], credit score modelling [18], gene detection [19], spatial prediction of landslides [20], bacterial
transcription start sites prediction [21] , text speech recognition [22] and membership authentication [23].
Despite the diverse number of works that present the bagging based on support vector machine
classiﬁers, none of them presents an optimal framework to choose which kernel function will be used in
the ensemble classiﬁer. The choice of the kernel function, as their hyperparameters, has a crucial impact
on the accuracy of the technique [24]. Generally, this selection is supported by a grid-search that runs all
functions and their parameters combinations in order to select which one has the lowest generalization
error rate. Random Search [25] is another approach to tuning the hyperparameters, where the parameters
conﬁgurations are randomly chosen until a particular budget B is exhausted. Beside these, Tree-Structured
Parzen Estimator [26], and Simulated Annealing [27] are optimization structures used in tuning workﬂow
too. However, all of them can be computationally expensive and slow, making it infeasible to use.
The kernel methods, e.g: Kernelized Support Vector Machines, could be considered as non parametric
machine learning models which are useful to capture the non-linear behaviour, beyond their strong
theoretical proprieties. However, they have some problems to be applied to large scale datasets since their
time and memory demand, that is at least n2 , where n is the the number of observations. Currently works,
solve the problem of computational limitations through the use of Nyström method [28, 29] or random

2

features. Both of them have their speciﬁc versions for support vector machine [30, 31], and represents a
solid advance in those techniques.
This work introduces a novel method that presents a solution for the choice of kernel function to
be used in the bagged supported vector machine, in order to give an alternative to the open problem
of hyperparameters’ selection, with adequate computational time and robust accuracy power, hereafter,
the Random Machines (RM). The method received this name because it uses random kernel choice for
each model that composes the bagged support vector machine method, besides proposing weights to
these classiﬁers, increasing the accuracy and lowering the correlation of the ﬁnal model. The result was
validated over simulation studies, and on 27 diﬀerent benchmarking datasets.
The following paper is organized on the ensuing outline. The Section 2 presents a theoretical
description about the support vector machine method, proposed by [6],the challenges on the selection
of hyperparameters and some traditional kernel functions; Section 3 presents a general description of
the bagging algorithm and bagged SVM; Section 4 presents how the proposed Random Machines (RM)
approach works in detail, followed by the simulations studies in Section 5, as well as the applications in
real data in Section 6. Section 7 shows an empirical justiﬁcation of how the method works that proves the
consistency of the technique. Finally, in Section 8, ﬁnal considerations, regarding the improvements and
limitations that could be explored in this novel approach.

2. Support Vector Machine

The support vector machines [6], have been introduced for solving classiﬁcation problems. The overall
idea of the technique is to calculate a hyperplane which separates observations between two classes,
maximizing the distance between the support vectors.
Supposing a database given by {xi , yi }, yi = {−1, 1}, i=1,. . . , n, where n is the number of observations.
The yi = 1 represents that the observation belongs to a positive class, while yi = −1 the negative one.
Therefore, the hyperplane that accurately separate these two classes is given by

w · x + b = 0

(1)

In order to ﬁnd such hyperplane the estimation of w and b is made in order to maximize the distance
between the support vectors [32, 6], following the restrictions of yi (w · xi + b) ≥ 1, if yi belongs to the
positive class yi = 1, and yi (w · xi + b) ≥ −1, otherwise yi = −1. These equations are expressed by

yi (w · xi + b) − 1 ≥ 0

(2)

3

The distance is given by 2
||w|| , to maximize it is necessary to solve a convex problem given by
min 1

(3)

following the constraints given by the Equation (2). The cost function which will be minimized is deﬁned
by the Lagrangian Multipliers, in Equation (4).
L(w, b,α) = 1

αi [yi (w · xi + b) − 1]

(4)

2 ||w||2
2 ||w||2 − nX

i=1

where αi is the Lagrangian Multiplier.
There are cases where the training data cannot be separated without error, as pointed out by [6]. In
such a case, it is needed to construct a soft margin separator by inputting slack variables (εi ). Therefore,
a transformation in the Equation (3) was needed [6], and then, it becomes
min 1
2 ||w||2 + C
where C ≥ 0 is a regularization parameter. The constraints become yi (w · xi + b) − (1 − εi ) ≥ 0 and εi ≥ 0
for i = 1, . . . , n. And the cost function, which will be minimized, becomes
αi [yi (w · xi + b) − 1 + εi ] − nX
L(w, b, α, r) = 1
2 ||w||2 + C
The solution, considering the Lagrangian Dual Optimization for the soft margin problem [33], is given

εi − nX
i=1

nX

nX

ri εi .

(5)

(6)

i=1

i=1

εi

i=1

by

max

α

 nX

i



αiαj yi yj xi · xj

nX

j

nX
Pn

i

αi − 1
2



s.t =

i αi yi = 0,
C ≥ αi ≥ 0,

(7)

with i = 1, . . . , n.
This approach of SVM works well to linearly classiﬁcation groups and problems. In the presence of
non-linearity, it may be used trick kernels, based in Mercer’s Theorem. Instead of considering the input
space, it’s considered higher feature spaces, where the observations could be linearly separable through
the following function K (xi , xj ) = φ(xi ) · φ(xj ) that replaces the inner product in Equation (7).
The functions K (x, y) = φ(x) · φ(y) are deﬁned as the semideﬁnite kernel functions [34]. Several types
of kernel functions are employed in diﬀerent classiﬁcation tasks. The choice of distinct kernels functions
provide diﬀerent nonlinear mappings, and the performance of the resulting SVM often depends on the
appropriate choice of the kernel [24]. Some works that compare the eﬃciency for each kernel function,

4

which is used for each classiﬁcation model [35, 36], demonstrating that select the kernel function is an
important aspect of obtaining the best model. There are kernel functions in the general framework for
SVM, which were used in this paper, that are considered the most common. They are presented in Table
1.

Table 1: Kernel Functions.

Kernel
Linear Kernel
Polynomial Kernel
Gaussian Kernel
Laplacian Kernel

K(x,y)
γ (x · y)
(γ (x · y))d

e−γ ||x−y ||2
e−γ ||x−y ||

Parameters

γ

γ , d

γ

γ

in which γ ∈ R+ , d ∈ N.

Nevertheless, ﬁnd out which is the best kernel by grid search, or other exhaustive methods, can
be an expensive and appalling computational problem [37].
In order to deal with this issue, many
works have tried to develop a methodology which can improve the selection of the best kernel function
[24, 38, 39, 40, 41]. In this work we propose a novel approach which makes unnecessary to perform a grid
search, or other tuning algorithm, to choose a single speciﬁc kernel function when using the trick kernel.

3. Bagging

Bagging is an abbreviation of Bootstrapping Aggregation, which was ﬁrstly proposed by Breiman
[12]. Bagging is an ensemble method that can be used for diﬀerent prediction tasks. In general, the
Bootstrapping Aggregating generates datasets by random sampling with replacement from the training set
with the same size n, also known as bootstrap samples. Then, each model hj (xi ) is trained independently
for each bootstrapping sample j , ∀j ∈ {1, . . . , B }. The ﬁnal bagging model, for binary classiﬁcation tasks,
is given by the following equation,

 BX

!

H (x) = sign

hi (x)

,

(8)

i=1

where hi (x) is the model generated to each bootstrap sample from i = 1, , . . . , B , and B is the number of
bootstrap samples.
Another critical feature of Bagging classiﬁer is the out of bag samples [12]. For each bootstrap sample,
almost 1
3 of observations are not included. Therefore, those observations can be used as a test sample
since they were not used to train the bootstrap models.

5

3.1. Bagging SVM
In the bagging classiﬁer, the function hi (x) from (8) can be any model. One possibility is to use the
SVM as the base classiﬁer [14] in order to improve it is accuracy. As we already have seen, the applications
of the bagged SVM for predictive tasks are wide, and empirical studies [15] demonstrated that the bagged
version of the support vector machine algorithm increased the accuracy and it is generalization capacity.
Moreover, some of them already presented some modiﬁcations using the SVM in bagging context as [42],
and others implemented some libraries as EnsembleSVM, that make it shorten to use simple ensemble
methods with SVM [43].
Despite the numerous works using bagged SVM, none of them present a general framework to deal
with the choice of the best kernel function, choosing it by trial evaluation or by a grid search. As this
proceeding is computationally expensive [37], this paper proposed a novel bagging approach that can
overcome the diﬃcult to choose the best kernel function, besides showing an improvement in the accuracy
of classiﬁcation models by combining several diﬀerent SVM models by varying the kernel functions: the
Random Machines, exposed in next section.

4. Random Machines

Given a training set {(xi , yi )}N
i=1 with xi ∈ Rp and yi ∈ {−1, 1}, ∀i = 1, . . . , n. The kernel bagging
method initialize by training single models hr (x), where r = 1, . . . , R, where R is the total number of
diﬀerent kernel functions that could be used in support vector machine models. For example, if R = 4 a
possible choice is deﬁne h1 as SVM with Linear kernel , h2 as SVM with Polinomial kernel, h3 as SVM
with Gaussian kernel and h4 as SVM with Laplacian kernel.
Each model is validated for the test set {(xk , yk )}L
k=1 , and the accuracy AC Cr is calculated for each
model, ∀r = 1, . . . , R, in which R means the numbers of kernel functions that will be used. Therefore, the
probabilities, λr , is given by the Equation (9) for each kernel function

(cid:17) ,

(9)

with ∀r = 1, . . . , R.
Afterwards, is sampled B bootstrap samples from the training set. A support vector machine model
gk is trained for each bootstrap sample, k = i, . . . , B and the kernel function that will be used for gk will
be determined by a random choice with probability λr , ∀r = 1, . . . , R. The probabilities λr are higher if
determined kernel function used in hr (x) predicted correctly observations from test set. Consequently,
the kernel functions with higher accuracy will appear often when the random kernel selection for each

(cid:17)

(cid:16) ACCr
(cid:16) ACCi
1−ACCr
1−ACCi

i=1 ln

ln

PR

λr =

6

bootstrap model is made. If any kernel function applied in hr (x) does not do better than a random choice,
then AC Cr is closer to 0.5 and the probability of select that kernel function is next to zero.
Subsequently, a weight wi is assigned to each bootstrap model calculated for gi ∀i = 1, . . . , B . The
weight is given by the Equation (10).

1
(1 − Ωi )2 ,
where Ωi is the accuracy of model’s prediction gi calculated on Out of Bag Sample (OOBGi ) obtained
from i bootstrap sample ∀i = 1, . . . , B as test sample.
The ﬁnal classiﬁcation is held in Equation (11).

i = 1, . . . , B ,

wi =

(10)

 BX

 ,

G(xi ) = sign

wj gj (xi )

i = 1, . . . , N .

(11)

All the modeling process is summed up in the pseudo-code exposed in Algorithm 1.

j

Algorithm 1 Random Machines
Input: Training Data, Test Data, B, Kernel Functions

for each K ernelF unctionr do

Calculate the model hr
Calculate the accuracy αr
Calculate the probabilities λr
Generate B bootstrap samples
for b in B do
Model gb (xi ) by sampling a kernel function with probability λr
Assign a weight Ωb using OOBGb samples.
Calculate G(x)

The entire Random Machines is schematically presented in Figure 1, where it is designed all the steps
used in all cases presented in this article.

7

Figure 1: The graphical sketch represents all the workﬂow that’s followed by the Random Machines.

5. Artiﬁcial Data Application

In this section simulations studies were conducted in order to evaluate the eﬃciency of the RM applied
to binary classiﬁcation tasks. The other methods compared were:
linear, polynomial, Gaussian and
Laplacian SVM, beyond their bagged versions, respectively. A good variety between the simulated datasets
is observed with three diﬀerent scenarios. The dimensionality (p) ranges from {2, 10, 50}, the number of
observations (n) ranges from {10, 1000}, and the proportion’s ratio between the two classes assume two
values {0.1, 0.5}.
The generation from the Dataset 1 and Dataset 2 consider continuous explanatory variables [44],
were the observations belonging to each class follow a multivariate distribution with their respective mean
vector and covariate matrix. The Dataset 1 follows the conﬁguration that instances from Class A are
sampled from a normal multivariate which has mean vector µA = ~0p and covariate matrix ΣA = 4I p

8

and the Class B instances are sampled from a normal multivariate that has mean vector µB = ~4p and
covariate matrix ΣA = I p . The Dataset 2, has the same distribution with the exception that the mean
vector for Class B is given by µB = ~2p . The diﬀerence between those two datasets relies on the diﬃcult
to create the hyperplane that separate the two classes, since the Dataset 1 has observations from each
group that are further away when compared with Dataset 2.
The Dataset 3 considers a classiﬁcation problem in which is generated a circle uniformly distributed
inside in the middle a p-dimensional cube. This dataset is fundamentally more complex to realize a
classiﬁcation, since it’s has a non-linear behavior.The performance of each model was appraised using the
following metrics.
Accuracy (ACC): it measures the ratio of correctly classiﬁed observations to total observations from
the sample. It is calculated by

AC C =

T P + T N
T P + T N + F P + F N

(12)

Matthew’s Correlation Coeﬃcient (MCC): introduced by Matthews, 1975 [45], is usually used
to evaluate the predictions made from the model [46] and it is deﬁned by,

M C C =

p(T P + F P )(T P + F N )(T N + F P )(T N + F N ) .
T P × T N − F P × F N

(13)

It can be considered an accurate coeﬃcient, since it penalizes the False Positive and False Negative
predictions, besides being considered a better evaluator if the classes are of very diﬀerent sizes [47]. It is
range varies from [−1, 1], in which 1 represents a perfect prediction, 0 no better than a random choice,
and −1 a complete reverse classiﬁcation.
In order to compare directly with the accuracy, as the scales between the metrics are diﬀerent, we
proposed a modiﬁcation to MCC. The transformation is given by uM C C = M CC+1
and results in a new
evaluation metric: Uniform MCC (uMCC). The uMCC lies in the interval [0, 1], where 1 represents a
perfect prediction, 0 no better than a random prediction.
The validation technique used was the repeated holdout with 30 repetitions with a split ratio of
training-test of 70% − 30%. The result is summarized in Table 2 where all possible combination of kernel
functions and datasets setups are presented. It is possible to see that in most cases, the RM surpasses
or equals the other methods. For instance, in Dataset 3, where the nonlinear behavior is an essential
characteristic from the data, we can observe the RM overcomes the other classiﬁers as the dimensionality
of the data increases.

2

9

Table 2: Summary of the simulation’s results for the diﬀerent databases.

Setup
p Ratio

SVMlin
SVMlap
BSVMlin BSVMpoly BSVMgau BSVMlap
RM
ACC
uMCC ACC uMCC ACC uMCC ACC uMCC ACC uMCC ACC uMCC ACC uMCC ACC uMCC ACC uMCC

SVMpoly

SVMgaus

Dataset 1 (n=100)
0.1
1.00
0.5
0.96
0.1
1.00
0.5
1.00
0.1
1.00
0.5
1.00

2

0.98
0.96
0.97
1.00
0.97
1.00

1.00
0.96
1.00
1.00
1.00
1.00

0.98
0.96
0.95
1.00
0.97
1.00

0.99
0.96
0.91
0.95
0.91
0.49

0.96
0.96
0.50
0.95
0.50
0.56

0.99
0.96
0.91
1.00
0.91
1.00

0.97
0.96
0.50
1.00
0.50
1.00

1.00
0.96
1.00
1.00
1.00
1.00

0.98
0.96
0.97
1.00
0.97
1.00

1.00
0.96
0.99
1.00
1.00
1.00

0.98
0.96
0.95
1.00
0.97
1.00

0.98
0.96
0.91
0.95
0.91
0.54

0.92
0.96
0.50
0.95
0.50
0.54

0.99
0.96
0.91
1.00
0.91
1.00

0.95
0.96
0.50
1.00
0.50
1.00

1.00
0.96
0.99
1.00
1.00
1.00

0.97
0.96
0.95
1.00
0.97
1.00

10

50

Dataset 1 (n=1000)
0.1
0.99
0.5
0.97
0.1
1.00
0.5
1.00
0.1
1.00
0.5
1.00

2

0.96
0.97
1.00
1.00
1.00
1.00

0.99
0.97
1.00
1.00
1.00
1.00

0.97
0.97
1.00
1.00
1.00
1.00

0.99
0.97
0.90
0.99
0.90
0.48

0.97
0.97
0.50
0.99
0.50
0.51

0.99
0.97
1.00
1.00
0.90
1.00

0.97
0.97
1.00
1.00
0.50
1.00

0.99
0.97
1.00
1.00
1.00
1.00

0.96
0.97
1.00
1.00
1.00
1.00

0.99
0.97
1.00
1.00
1.00
1.00

0.97
0.97
1.00
1.00
1.00
1.00

0.99
0.97
0.90
0.98
0.90
0.51

0.97
0.97
0.50
0.98
0.50
0.51

0.99
0.97
1.00
1.00
0.90
1.00

0.97
0.97
1.00
1.00
0.50
1.00

0.99
0.97
1.00
1.00
1.00
1.00

0.97
0.97
1.00
1.00
1.00
1.00

10

50

Dataset 2 (n=100)
0.1
0.96
0.5
0.87
0.1
0.99
0.5
0.93
0.1
1.00
0.5
1.00

2

0.86
0.88
0.93
0.94
0.97
1.00

0.96
0.88
0.95
0.94
0.96
0.88

0.86
0.88
0.78
0.94
0.81
0.90

0.95
0.89
0.91
0.85
0.91
0.47

0.80
0.89
0.50
0.87
0.50
0.54

0.96
0.89
0.91
0.98
0.91
0.83

0.86
0.89
0.50
0.98
0.50
0.88

0.96
0.87
0.98
0.94
1.00
1.00

0.88
0.88
0.90
0.94
0.97
1.00

0.96
0.88
0.96
0.94
0.94
0.81

0.87
0.88
0.79
0.95
0.69
0.84

0.94
0.89
0.91
0.81
0.91
0.53

0.76
0.89
0.50
0.85
0.50
0.53

0.95
0.89
0.91
0.98
0.91
0.76

0.82
0.89
0.50
0.98
0.50
0.77

0.96
0.89
0.96
0.96
0.99
1.00

0.87
0.89
0.83
0.96
0.92
1.00

10

50

Dataset 2 (n=1000)
0.1
0.94
0.5
0.83
0.1
1.00
0.5
0.98
0.1
1.00
0.5
1.00

2

0.79
0.83
0.99
0.98
1.00
1.00

0.94
0.86
0.99
0.99
1.00
1.00

0.82
0.86
0.96
0.99
1.00
1.00

0.94
0.86
0.90
0.92
0.90
0.48

0.81
0.87
0.50
0.93
0.50
0.50

0.94
0.86
1.00
1.00
0.90
1.00

0.81
0.86
0.99
1.00
0.50
1.00

0.94
0.83
1.00
0.98
1.00
1.00

0.79
0.83
0.99
0.98
1.00
1.00

0.94
0.86
0.99
0.99
1.00
1.00

0.82
0.86
0.97
0.99
0.99
1.00

0.94
0.86
0.90
0.91
0.90
0.52

0.81
0.86
0.50
0.92
0.50
0.52

0.94
0.86
0.98
1.00
0.90
1.00

0.82
0.86
0.93
1.00
0.50
1.00

0.94
0.86
1.00
1.00
1.00
1.00

0.82
0.86
0.99
1.00
1.00
1.00

10

50

Dataset 3 (n=100)
0.1
0.52
0.5
0.56
0.1
0.76
0.5
0.49
0.1
0.52
0.5
0.58

2

0.61
0.58
0.78
0.50
0.55
0.59

0.86
0.97
0.58
0.68
0.46
0.58

0.86
0.97
0.62
0.69
0.50
0.61

0.66
0.92
0.46
0.46
0.41
0.45

0.70
0.93
0.57
0.54
0.53
0.52

0.64
0.92
0.47
0.64
0.53
0.54

0.72
0.93
0.57
0.70
0.65
0.59

0.59
0.58
0.71
0.52
0.53
0.63

0.63
0.60
0.74
0.53
0.58
0.63

0.78
0.96
0.53
0.68
0.46
0.55

0.80
0.96
0.57
0.68
0.50
0.58

0.64
0.92
0.57
0.52
0.47
0.52

0.69
0.92
0.58
0.53
0.51
0.51

0.63
0.92
0.58
0.61
0.58
0.57

0.70
0.93
0.59
0.68
0.63
0.57

0.78
0.95
0.69
0.68
0.57
0.69

0.81
0.95
0.71
0.70
0.62
0.70

10

50

Dataset 3 (n=1000)
0.1
0.48
0.5
0.49
0.1
0.54
0.5
0.50
0.1
0.46
0.5
0.49

2

0.51
0.51
0.55
0.51
0.47
0.49

0.99
0.99
0.78
0.95
0.55
0.72

0.99
0.99
0.78
0.95
0.57
0.72

0.94
0.98
0.48
0.76
0.46
0.49

0.94
0.98
0.52
0.78
0.50
0.50

0.95
0.98
0.79
0.92
0.49
0.67

0.95
0.98
0.80
0.92
0.53
0.70

0.52
0.52
0.54
0.51
0.47
0.48

0.53
0.53
0.54
0.51
0.47
0.49

0.99
0.99
0.77
0.95
0.54
0.70

0.99
0.99
0.77
0.95
0.58
0.72

0.95
0.98
0.50
0.75
0.50
0.50

0.95
0.98
0.53
0.77
0.50
0.50

0.95
0.98
0.78
0.92
0.52
0.61

0.95
0.98
0.79
0.92
0.53
0.63

0.98
0.99
0.81
0.96
0.59
0.83

0.98
0.99
0.82
0.96
0.61
0.84

10

50

10

6. Real Data Application

Our methodology was applied on 27 real-world datasets from the UCI Repository [48] to evaluate its
performance. The datasets present a wide variety in the number of observations, dimensionality, and type
of data. Although, all of them represent a binary classiﬁcation task. Table 3 summarizes all datasets
considered. The continuous features scaled to zero mean and unit variance, in the exception of the discrete
features. The validation technique is also used was the repeated holdout with 30 repetitions with a split
ratio of training-test of 70% − 30%.

ID Data Set
1
haberman
2
heart statlog
3
hungarian
4
hepatitis
5
liver disorders
6
parkinsons
7
sonar
8
column 2C
9
ionosphere
10 spam
11 dataR2
12 kidney disease
13 clean
14 whosale

Table 3: Description of the twenty seven binary data sets.
#Instance #Features Class Proportion ID Data Set #Instance #Feature Class Proportion
306
3
81/225
15 audit risk
775
26
305/470
270
14
120/150
16 adult autism
609
20
180/429
261
10
98/163
17 banknote
1372
4
610/762
80
19
33/47
18 transfusion
748
4
178/570
345
6
145/200
19 caesarian
80
4
34/46
195
22
48/147
20 thoraric
470
16
70/400
208
60
97/111
21 circles
100
2
50/50
310
6
110/210
22 spirals
500
2
250/250
351
33
126/225
23 australian
690
14
307/383
4601
57
1813/2788
24 tic tac toe
958
3
332/626
116
9
52/64
25 german
100
24
300/700
155
24
41/114
26 sick
2643
31
212/2431
476
168
207/269
27 vehicle
846
18
218/628
440
7
142/298

The Random Machines was compared with the bagged SVM using each single kernel function presented
in Table 1, and with the standard SVM with the same kernel functions. Without losing generality, the
chosen parameters were: the cost parameter C = 1, the number of bootstrap samples B = 100, the degree
of polynomial kernel d = 2, and the hyperparameter γ from the Laplacian and Gaussian kernel γ = 1. The
result is summarized in the Figure 2 considering the accuracy and in the Figure 3 considering the uMCC.

11

Figure 2: The chart presents the proportion of the number of times which a method have greater accuracy than the others.
The proportion summarizes the applications overall 27 datasets and 30 holdout values. It is clear the superiority of the
Random Machines when it is compared with the other models.

As shown in Figure 2, the RM demonstrates higher accuracy than the other bagged support vectors
using unique kernel functions. Comparing the RM with the traditional bagged SVM, it is beaten almost
80% of times considering the Kernel Linear Bagging, 81% for the Kernel Polynomial Bagging, 94% for the
Gaussian Bagging, and 87% for the Laplacian Kernel Bagging. This outcome shows oﬀ that the random
weighted choice of the kernels functions improved, generally, the accuracy of the predictions from the
model. The diﬀerence is even more signiﬁcant when the Random Machines is compared with the singular
SVM, where the RM is more accurate 82% of times considering the Kernel Linear, 81% for the Kernel
Polynomial, 94% for the Gaussian Bagging, and 84% for the Laplacian Kernel.

12

Figure 3: The chart presents the proportion of the number of times which a method have greater uMCC than the others.
The proportion summarizes the applications overall 27 datasets and 30 holdout values. It is clear the superiority of the
Random Machines when it is compared with the other models.

The same behavior is also observed when it is considered the Uniform Matthew’s Correlation Coeﬃcient,
in which the RM present a robust superiority when compared to other methods. Analyzing the RM with
the traditional bagged SVM is beaten almost 74% of times considering the Kernel Linear Bagging, 71% for
the Kernel Polynomial Bagging, 92% for the Gaussian Bagging, and 84% for the Laplacian Kernel Bagging.
It also happens when the RM is compared with the singular SVM, where the RM is more accurate 82% of
times considering the Kernel Linear, 81% for the Kernel Polynomial, 94% for the Gaussian Bagging, and
84% for the Laplacian Kernel.
The scheme also solves the problem of the selection of best kernel function, since is not necessary to
perform a grid-search among all the diﬀerent functions and deﬁne which is one has lower test error, which

13

is general framework adopted. Therefore, it is appealing that the eﬃciency increasing and computational
cost reduction given by the technique.
As the hyperparameter tuning is a remarkable question in the proceeding of the support vector
machine vector, the value of γ was changed in order to study the variation and the behavior or Random
Machines when this change exists. The variation experiment relies on the interval of values γ =
{2−3 , 2−2 , 2−1 , 20 , 21 , 22 , 23 }. The result is showed in Figure 4 and 5 in which it is possible to see that
the RM surpassed the other bagging kernels all the times. As mentioned before the choice of these
hyparameters, as the kernel function, has a direct impact on the model performance, and the results
reinforce that RM gives a good and consistent result independent for all γ values.

Figure 4: Summary of the ACC applied over 27 real datasets with the variation of kernel function’s parameter γ .

14

Figure 5: Summary of the uMCC applied over 27 real datasets with the variation of kernel function’s parameter γ .

7. Performance and agreement evaluation

In this section, we justify the reason that the Random Machines is an ensemble method that can
improve the predictive power for classiﬁcation tasks. The main idea of the random selection of the kernel
function is to select diﬀerent functions that belong to a Reproducing Kernel Hilbert Space (RHKS). The
reason for this choice is to aim a lower correlation between classiﬁers that compose the RM, and a high
strength of them since these characteristics result in better results as is shown in [49].
The idea of the strength of a model relies on how well a model correctly predicts an new observation,
while the correlation between model consists of how much they are similar. A method to estimate the
correlation between classiﬁcation models is to calculate the area from decision boundaries that overlaps
among them [50].Ho, [51] deﬁnes the similarity, also called agreement, of two models as the number of
observations that are equally labeled with the same class, and proposes that it can be estimated through

15

a random sample with n observations, by the Equation (14).

where

f (xk )

(14)

nX

k=1

ˆsi,j = 1
n

1,

f (xk ) =

if gi (xk ) = gj (xk )

0, otherwise
This measure called, similarity or agreement can be used as a correlation metric between models.
In order to evaluate the correlation and strength of the RM in comparison with the traditional bagged
version of SVM, the method was applied over the Circles database that was generated under the same
conﬁguration of Dataset 3 presented in Section 5. The similarity of each method was estimated using
the average of the similarity ˆsi,j , ∀i 6= j and i, j = 1, . . . , B , over ﬁxed k points generated by a Monte
Carlo’s simulation. The accuracy was used in order to measure the strength of the model.
The dataset was modiﬁed in three conﬁgurations, changing the dimension p in a range corresponding
to p={2, 10, 30, 50}. The average similarity, that can also be called as agreeement of the model [51], was
calculated using k observations, where k=1000 × p. Both accuracy and agreement were calculated using
a 30 Repeated Holdout validation set with split ratio of 70-30% training-test. The parameters of the
methods were: B=100, γ = 1, C = 1.
One of the main results can be represented in the Figure 6 where the circles database with p=2 was
used as example. In the Figure 6 (a) the plot of observations, which in each color represents a class. The
panel (b) represents the ﬁnal decision boundary of the RM, showing that the model captures the behavior
from the observations. The panel (c) shows examples of the decision region generated by a bootstrap
model gi for each kernel.
It is clear that diﬀerent kernel functions used in each SVM model produce diverse decision boundaries,
and that diﬀerence implies in a reduction of the correlation, resulting at the decreasing of generalization
error.
All the results are summarized in Table 4 where it is presented the mean accuracy and agreement for
each dataset for all conﬁgurations of the circles.
In general, it is remarkable that the higher predictive capacity of the RM when compared to the
other methods in all cases. Moreover, beyond the great accuracy, it is possible to see that the RM
shows simultaneously a lower agreement when compared with the other methods that have an excellent
performance. Although sometimes the BSVM.Lin and BSVM.Gau produce a desirable low agreement, they
are considered weaker, since they have a lower accuracy when compared with the others. As [51] discuss,

16

Table 4: Summary of Accuracy and Agreement measure to each method
Method

Circles Dataset

BSVM.Lin

BSVM.Pol

BSVM.Gau

BSVM.Lap

RM

ACC AGR ACC AGR ACC AGR ACC AGR ACC AGR

0.54
0.49
0.49
0.55

0.59
0.64
0.49
0.67

0.98
0.95
0.78
0.71

0.98
0.92
0.78
0.71

0.97
0.74
0.51
0.49

0.97
0.72
0.59
0.61

0.97
0.91
0.87
0.57

0.97
0.91
0.87
0.62

0.99
0.96
0.94
0.79

0.96
0.84
0.67
0.62

p

2
10
30
50

the accuracy of the models aﬀects the agreement and vice-versa, and optimize both values simultaneously
can be a challenging task. Generally, models with high accuracy also result in large agreement values, as
we can see in the results exhibited in Table 4. On another hand, small values of accuracy produce lower
agreement measures among models. However, it is clear to notice that the Random Machines it is capable
create a better classiﬁer (low generalization error) with both characteristics: low correlation and reliable
strength.

Figure 6: The ﬁgure shows the circles database where p=2. The panel (a) show all the observations with the class associated
with each color. The panel (b) present the decision region given by the RM. The panel (c) reveals the diversity of decision
regions produced by each kernel function of bootstrap models that composes the RM.

17

These proprieties become better with higher dimensions as they can be observed in Table 4. This
diﬀerence is showed in Figure 7 that display the boxplots of the accuracy and agreement for each method,
reinforcing even more than the RM has both better proprieties than the other ones.

Figure 7: Boxplots of the accuracy and agreement for each method.

8. Final Considerations

The main contribution of this paper is to propose a novel learning method to do ensemble using
Support Vector Machine models that can enhance the accuracy from the conventional BSVM, and solve
the problem choosing the best kernel function that should be used. Through the Random Machines, the
combination of diﬀerent SVM using the diﬀerent kernel functions states an approach that avoids the
expensive computational cost of doing a grid search between the kernel functions, besides improve the
accuracy. Furthermore, our results show a good behavior with diﬀerent kernel hyperparameters in RM,
that provides a bagged-weighted support vector model with free kernel choice. In this way, as SVM is
one of the most important and an essential method in machine learning with high-performance capacity
and power of generalization, the RM method can be viewed as an extension of traditional SVM, giving
an alternative solution to the hyperparameters choice problem. This methodology can be explored in
many other contexts, as well as be applied to any practical machine learning problem. Future theoretical
studies may be done regarding computational cost, comparison with other traditional machine learning
methods, and the use of other and more kernel functions as well as other weights in the bagging phase.

18

References

[1] M. Sato, K. Morimoto, S. Kajihara, R. Tateishi, S. Shiina, K. Koike, Y. Yatomi, Machine-learning
approach for the development of a novel predictive model for the diagnosis of hepatocellular carcinoma.,
Scientiﬁc reports 9 (1) (2019) 7704–7704.

[2] T. B. Mokgonyane, T. J. Sefara, T. I. Modipa, M. M. Mogale, M. J. Manamela, P. J. Manamela,
Automatic speaker recognition system based on machine learning algorithms, in: 2019 Southern
African Universities Power Engineering Conference/Robotics and Mechatronics/Pattern Recognition
Association of South Africa (SAUPEC/RobMech/PRASA), IEEE, 2019, pp. 141–146.

[3] S. G. Burdisso, M. Errecalde, M. Montes-y Gómez, A text classiﬁcation framework for simple and
eﬀective early depression detection over social media streams, Expert Systems with Applications.

[4] H. Kim, P. Howland, H. Park, Dimension reduction in text classiﬁcation with support vector machines,
Journal of Machine Learning Research 6 (Jan) (2005) 37–53.

[5] D. Dighe, S. Patil, S. Kokate, Detection of credit card fraud transactions using machine learning
algorithms and neural networks: A comparative study, in: 2018 Fourth International Conference on
Computing Communication Control and Automation (ICCUBEA), IEEE, 2018, pp. 1–6.

[6] C. Cortes, V. Vapnik, Support-vector networks, Machine learning 20 (3) (1995) 273–297.

[7] A. J. Smola, P. J. Bartlett, D. Schuurmans, B. Schölkopf, M. I. Jordan, et al., Advances in large
margin classiﬁers, MIT press, 2000.

[8] N. Cueto-López, M. T. García-Ordás, V. Dávila-Batista, V. Moreno, N. Aragonés, R. Alaiz-Rodríguez,
A comparative study on feature selection for a risk prediction model for colorectal cancer, Computer
Methods and Programs in Biomedicine.

[9] P. Thanh Noi, M. Kappas, Comparison of random forest, k-nearest neighbor, and support vector
machine classiﬁers for land cover classiﬁcation using sentinel-2 imagery, Sensors 18 (1) (2018) 18.

[10] S. A. R. Shah, B. Issac, Performance comparison of intrusion detection systems and application of
machine learning to snort system, Future Generation Computer Systems 80 (2018) 157–170.

[11] M. Van Wezel, R. Potharst, Improved customer choice predictions using ensemble methods, European
Journal of Operational Research 181 (1) (2007) 436–452.

[12] L. Breiman, Bagging predictors, Machine learning 24 (2) (1996) 123–140.

19

[13] Y. Freund, R. Schapire, N. Abe, A short introduction to boosting, Journal-Japanese Society For
Artiﬁcial Intelligence 14 (771-780) (1999) 1612.

[14] H.-C. Kim, S. Pang, H.-M. Je, D. Kim, S.-Y. Bang, Support vector machine ensemble with bagging,
in: International Workshop on Support Vector Machines, Springer, 2002, pp. 397–408.

[15] S.-j. Wang, A. Mathew, Y. Chen, L.-f. Xi, L. Ma, J. Lee, Empirical analysis of support vector machine
ensemble classiﬁers, Expert Systems with applications 36 (3) (2009) 6466–6476.

[16] M.-W. Huang, C.-W. Chen, W.-C. Lin, S.-W. Ke, C.-F. Tsai, Svm and svm ensembles in breast
cancer prediction, PloS one 12 (1) (2017) e0161501.

[17] H. Wang, B. Zheng, S. W. Yoon, H. S. Ko, A support vector machine-based ensemble algorithm for
breast cancer diagnosis, European Journal of Operational Research 267 (2) (2018) 687–699.

[18] L. Zhou, K. K. Lai, L. Yu, Least squares support vector machines ensemble models for credit scoring,
Expert Systems with Applications 37 (1) (2010) 127–133.

[19] M. Tong, K.-H. Liu, C. Xu, W. Ju, An ensemble of svm classiﬁers based on gene pairs, Computers in
biology and medicine 43 (6) (2013) 729–737.

[20] B. T. Pham, D. T. Bui, I. Prakash, Bagging based support vector machines for spatial prediction of
landslides, Environmental earth sciences 77 (4) (2018) 146.

[21] J. J. Gordon, M. W. Towsey, J. M. Hogan, S. A. Mathews, P. Timms, Improved prediction of bacterial
transcription start sites, Bioinformatics 22 (2) (2005) 142–148.

[22] Z. Lei, Y. Yang, Z. Wu, Ensemble of support vector machine for text-independent speaker recognition,
Int. J. Comput. Sci. Networks Secur 6 (5) (2006) 163–167.

[23] S. Pang, D. Kim, S. Y. Bang, Membership authentication in the dynamic group by face classiﬁcation
using svm ensemble, Pattern Recognition Letters 24 (1-3) (2003) 215–225.

[24] T. Jebara, Multi-task feature and kernel selection for svms, in: Proceedings of the twenty-ﬁrst
international conference on Machine learning, ACM, 2004, p. 55.

[25] J. Bergstra, Y. Bengio, Random search for hyper-parameter optimization, Journal of Machine Learning
Research 13 (Feb) (2012) 281–305.

[26] J. S. Bergstra, R. Bardenet, Y. Bengio, B. Kégl, Algorithms for hyper-parameter optimization, in:
Advances in neural information processing systems, 2011, pp. 2546–2554.

20

[27] S. Kirkpatrick, C. D. Gelatt, M. P. Vecchi, Optimization by simulated annealing, science 220 (4598)
(1983) 671–680.

[28] C. K. Williams, M. Seeger, Using the nyström method to speed up kernel machines, in: Advances in
neural information processing systems, 2001, pp. 682–688.

[29] A. J. Smola, B. Schölkopf, Sparse greedy matrix approximation for machine learning.

[30] Y. Sun, A. Gilbert, A. Tewari, But how does it work in theory? linear svm with random features, in:
Advances in Neural Information Processing Systems, 2018, pp. 3379–3388.

[31] Z. Li, T. Yang, L. Zhang, R. Jin, Fast and accurate reﬁned nyström-based kernel svm, in: Thirtieth
AAAI Conference on Artiﬁcial Intelligence, 2016.

[32] B. E. Boser, I. M. Guyon, V. N. Vapnik, A training algorithm for optimal margin classiﬁers, in:
Proceedings of the ﬁfth annual workshop on Computational learning theory, ACM, 1992, pp. 144–152.

[33] R. Fletcher, Practical methods of optimization john wiley & sons, New York 80.

[34] R. Courant, D. Hilbert, Methods of mathematical physics, vol. i, interscience publ, Inc., New York
(1953) 106.

[35] M. Hussain, S. K. Wajid, A. Elzaart, M. Berbar, A comparison of svm kernel functions for breast cancer
detection, in: 2011 Eighth International Conference Computer Graphics, Imaging and Visualization,
IEEE, 2011, pp. 145–150.

[36] J. H. Min, Y.-C. Lee, Bankruptcy prediction using support vector machine with optimal choice of
kernel function parameters, Expert systems with applications 28 (4) (2005) 603–614.

[37] O. Chapelle, V. Vapnik, Model selection for support vector machines, in: Advances in neural
information processing systems, 2000, pp. 230–236.

[38] N.-E. Ayat, M. Cheriet, C. Y. Suen, Automatic model selection for the optimization of svm kernels,
Pattern Recognition 38 (10) (2005) 1733–1745.

[39] C.-H. Wu, G.-H. Tzeng, R.-H. Lin, A novel hybrid genetic algorithm for kernel function and parameter
optimization in support vector regression, Expert Systems with Applications 36 (3) (2009) 4725–4735.

[40] F. Friedrichs, C. Igel, Evolutionary tuning of multiple svm parameters, Neurocomputing 64 (2005)
107–117.

21

[41] V. Cherkassky, Y. Ma, Practical selection of svm parameters and noise estimation for svm regression,
Neural networks 17 (1) (2004) 113–126.

[42] H.-T. Lin, L. Li, Support vector machinery for inﬁnite ensemble learning, Journal of Machine Learning
Research 9 (Feb) (2008) 285–312.

[43] M. Claesen, F. De Smet, J. A. Suykens, B. De Moor, Ensemblesvm: a library for ensemble learning
using support vector machines, The Journal of Machine Learning Research 15 (1) (2014) 141–145.

[44] L. Breiman, et al., Arcing classiﬁer (with discussion and a rejoinder by the author), The annals of
statistics 26 (3) (1998) 801–849.

[45] B. W. Matthews, Comparison of the predicted and observed secondary structure of t4 phage lysozyme,
Biochimica et Biophysica Acta (BBA)-Protein Structure 405 (2) (1975) 442–451.

[46] P. Baldi, S. Brunak, Y. Chauvin, C. A. Andersen, H. Nielsen, Assessing the accuracy of prediction
algorithms for classiﬁcation: an overview, Bioinformatics 16 (5) (2000) 412–424.

[47] S. Boughorbel, F. Jarray, M. El-Anbari, Optimal classiﬁer for imbalanced data using matthews
correlation coeﬃcient metric, PloS one 12 (6) (2017) e0177678.

[48] D. Dua, C. Graﬀ, UCI machine learning repository (2017).

URL http://archive.ics.uci.edu/ml

[49] L. Breiman, Random forests, Machine learning 45 (1) (2001) 5–32.

[50] P. Turney, Bias and the quantiﬁcation of stability, Machine Learning 20 (1-2) (1995) 23–33.

[51] T. K. Ho, The random subspace method for constructing decision forests, IEEE Trans. Pattern Anal.
Mach. Intell 20 (8) (1998) 1–22.

22

