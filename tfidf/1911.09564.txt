9
1
0
2

v
o

N

1
2

]

G

L

.

s

c

[

1
v
4
6
5
9
0

.

1
1
9
1

:

v

i

X

r

a

Parameter-Free Locally Differentially Private
Stochastic Subgradient Descent

Kwang-Sung Jun

University of Arizona

kjun@cs.arizona.edu

Francesco Orabona

Boston University

francesco@orabona.com

Abstract

We consider the problem of minimizing a convex risk with stochastic subgradients
guaranteeing -locally differentially private (-LDP). While it has been shown that
stochastic optimization is possible with -LDP via the standard SGD (Song et al.,
2013), its convergence rate largely depends on the learning rate, which must be
tuned via repeated runs. Further, tuning is detrimental to privacy loss since it
signiÔ¨Åcantly increases the number of gradient requests. In this work, we propose
BANCO (Betting Algorithm for Noisy COins), the Ô¨Årst -LDP SGD algorithm that
essentially matches the convergence rate of the tuned SGD without any learning
rate parameter, reducing privacy loss and saving privacy budget.

1

Introduction

In this paper, we consider the problem of minimizing the convex risk of a machine learning predic-
tor, guaranteeing local differential privacy. Instead of going through the empirical risk minimization
route, we directly optimize the stochastic objective of the risk via stochastic subgradients appropri-
ately sanitized to guarantee the local differential privacy. Our proposed algorithm is a variant of
stochastic subgradient descent, with the important feature of not requiring the tuning of learning
rates. Yet, it guarantees the same convergence rate and local differential privacy of a stochastic
More in details, consider the risk R(w) ‚à∂ Rd ‚Üí R deÔ¨Åned as R(w) ‚à∂= Ex‚àºœÅX [(cid:96)(w , x)], where
subgradient descent procedure with the oracle tuning of its learning rate.
(cid:96)(w, x) is convex in the Ô¨Årst argument and x represents sensitive data about an individual drawn
i.i.d. from œÅX . In machine learning terms, R(w) represents the test loss that we are interested
in directly minimizing, while (cid:96)(w , x) is the loss on a single sample x. It is well-known that, by
linearity of the expectation, we can get an unbiased estimate of a subgradient of R(w) just using
a subgradient w.r.t. the Ô¨Årst argument of (cid:96)(‚ãÖ, x) where x is drawn i.i.d. from œÅX . Hence, we can
construct sanitized subgradients simply returning a noisy version G (w) ‚àà ‚àÇ (cid:96)(w , x) + Œæ where the
noise Œæ guarantees the -Local Differential Privacy (-LDP), which prevents the private information
(SGD) algorithm together with the popular choice of density p(Œæ) ‚àù exp(‚àí 
from being identiÔ¨Åed. We elaborate more on the problem deÔ¨Ånition and the assumptions in Section 2.
2 Œæ) (Song et al., 2013;
One immediately obvious algorithm to try for -LDP is the standard stochastic subgradient descent
convergence rate of SGD w.r.t. the optimum w‚àó = minw R(w) after one pass over T samples is
2015; Wu et al., 2017). Let us consider the SGD with a constant learning rate Œ∑ for simplicity.1 The
2 Œ∑). Notice that the convergence rate largely depends on the
learning rate, and setting Œ∑ = ~d‚àö
results in the rate of O( d
T ). While the optimal tuning
would result in a much better rate of O( d
T ), the value w‚àó  cannot be known
beforehand. Therefore, practitioners need to tune the learning rate by trying out various values over
multiple runs. However, this signiÔ¨Åcantly increases the number of subgradient requests, which can

E[R(wT )] ‚àí R(w‚àó ) = O( w
w‚àó  ~d‚àö

 w‚àó 2 ~‚àö
 w‚àó ~‚àö

‚àó 2

Œ∑T + d2
T

T

1The same argument holds for adaptive learning rates such as Duchi et al. (2010).

33rd Conference on Neural Information Processing Systems (NeurIPS 2019), Vancouver, Canada.

 
 
 
 
 
 
T )

 w‚àó ~‚àö

be interpreted as the privacy cost. This also implies that, when there is a Ô¨Åxed budget of privacy loss,
one would have to use a smaller  to allow multiple runs, which slows down the convergence.
BANCO (Betting Algorithm for Noisy COins) that enjoys the convergence rate ÀúO( d
Departing from the cumbersome and privacy-costing parameter tuning, we propose a new algorithm
algorithm is a decomposition that isolates the work of adapting to the norm of the optimum w‚àó 
without having to tune any parameters, where ÀúO hides logarithmic factors. At the heart of the
and delegates it to a one-dimensional stochastic optimization algorithm. We describe our algorithm
and highlight the proof sketch in Section 3 and conclude with future directions in Section 4.
Related work. While there is a large body of work on differentially private (DP) empirical risk
minimization, there is less research on studying the generalization properties of the obtained solu-
tion. Moreover, most of the work is based on the notion of differential privacy rather than on the
stronger notion of local differential privacy (LDP). We remind the readers that the DP is useful when
the data provider trusts the learning agents to be DP and allows them to access sensitive data (or data
provider may possess the learners internally) whereas LDP is useful when the data provider does not
trust the learning agents and thus have to release sanitized version of individual data. Due to space
constraints, we focus on discussing the most relevant works. The notion of LDP was introduced in
Duchi et al. (2014) and Song et al. (2013; 2015). Duchi et al. (2014) study the generalization of lo-
cally differentially private SGD, focusing on the optimal dependency on the number of dimensions,
but assuming bounded domains to optimally tune the learning rates. Song et al. (2013; 2015) use lo-
cally differentially private SGD to minimize the regularized ERM problems. Wu et al. (2017) study
the convergence of SGD to minimize the ERM problem and guarantee differential privacy through
the output perturbation method, and they also require the knowledge of the norm of the optimal so-
lution to optimally tune the learning rates. Bassily et al. (2014) studies the generalization properties
of locally differentially private SGD, but it requires a bounded domain to set the learning rates or
the presence of a regularizer, that in turn must be tuned. Instead, our work gives a generalization
guarantee for a locally differentially private SGD with one pass over the data and no learning rates
to tune.
The problem of parameter-free -LDP SGD can be seen as a noisy version of the parameter-free
SGD problem (Streeter and McMahan, 2012; Orabona and Pal, 2016; Orabona and Tommasi, 2017;
Cutkosky and Orabona, 2018). However, the extension of existing (near-optimal) parameter-free
SGDs to noisy gradients is not immediately clear since the noise makes the gradients unbounded
while the parameter-free SGDs require bounded gradients. SpeciÔ¨Åcally, naively feeding in the un-
bounded gradients can even make the algorithm undeÔ¨Åned, let alone the lack of convergence guaran-
tees; see (Jun and Orabona, 2019, Section 3) for a detailed argument. Our solution in this paper goes
back to the more general version of Online Convex Optimization (OCO) (Zinkevich, 2003) and re-
consider its formulation to allow unbounded noise, which is more general than the -LDP setting. In
this paper, however, we focus on the LDP setting; we refer the interested readers to Jun and Orabona
(2019).

2 Problem DeÔ¨Ånition and Preliminaries

In this section, we describe our notations and provide relevant background.
Locally Differentially Private (LDP) SGD. An -differentially private algorithm must guarantee
that the log-likelihood ratio of the outputs of the algorithm under two databases differing in a single
individual‚Äôs data is smaller than  (Dwork et al., 2006). In the stricter deÔ¨Ånition of local differen-
tial privacy (LDP) (Wasserman and Zhou, 2010; Kasiviswanathan et al., 2011; Duchi et al., 2014;
Song et al., 2013; 2015) an untrusted algorithm is allowed to access a perturbed version of a sensi-
guarantee that the log-likelihood ratio of the data of two individuals x and x‚Ä≤ is smaller than .
tive data point only through a sanitization interface. In particular, the sanitization mechanism must
DeÔ¨Ånition 1 (Local Differential Privacy). Let D = (X1 , . . . , Xn ) be a sensitive dataset where each
Xi ‚àº œÅX corresponds to data about individual i. A randomized sanitization mechanism M which
outputs a disguised version (U1 , . . . Un ) of D is said to provide -local differential privacy to indi-
vidual i, if

x,x‚Ä≤ ‚ààD P[Ui ‚àà S  Xi = x]

P[Ui ‚àà S  Xi = x‚Ä≤ ] ‚â§ exp(),

sup

sup

S

where the probability is w.r.t. the randomization in the sanitization mechanism.

2

5:

6:

7:

8: end for

2

G , 1

2 ‚àí1

2 

ÀÜgt‚àët
s=1 ÀÜgs 2

T ‚àëT
t=1 wt

2: for t = 1 to T do

1: Set w1 = q1 = 0 ‚àà Rd
3:
4:

2a ‚à´ a‚àía Œ≤ exp Œ≤ ‚àët
s=1 ÀÜgs , qs  ‚àí Œ≤ 2 t  œÉ2
2 + G2  dŒ≤
b , and k1 = 0.6838
2 = q t ‚àí
2 ‚ãÖ min 1, q t+ 1

Algorithm 1 Betting Algorithm for Noisy COins (BANCO) for Differentially Private SGD
Receive a noisy negative subgradient ÀÜg t such that E[ÀÜg t ] ‚àà ‚àí‚àÇ (cid:96)(wt , x) where x ‚àº œÅX
Update magnitude: mt+1 = 1
where a = min  k1
Update direction: q t+ 1
Project direction onto L2 ball: q t+1 = q t+ 1
Update the weight vector: wt+1 = mt+1q t+1 ‚àà Rd
Denote by ‚àÇ (cid:96)(w, x) the subdifferential set of (cid:96) w.r.t. its Ô¨Årst argument evaluated at (w , x). The
9: Return 1
that, upon receiving a subgradient in ‚àÇ (cid:96)(w, x) with x i.i.d.‚àº œÅX , returns a noisy version G (w) ‚àà
‚àÇ (cid:96)(w, x) + Œæ , where the noise Œæ guarantees the -local differential privacy.
local differential setting can be specialized to SGD, as described in the introduction, by a mechanism
be the negative subgradient received at time step t. Let g t = Et [ÀÜg t ] be the true negative subgradient
Assumptions. Throughout, we work with negative subgradients for notational simplicity. Let ÀÜg t
and Œæ t = ÀÜg t ‚àí Et [ÀÜg t ] where we use the notation Et to denote E[‚ãÖ   Œæ1 , . . . , Œæ t‚àí1 ].
We assume that the true subgradients are bounded by G: g t 2 ‚â§ G. Furthermore, the noise Œæ t is
conditionally zero-mean and has conditional Ô¨Ånite variance:
for some œÉ > 0. We also assume a tail condition such that Œæ t is conditionally sub-exponential with
parameters (œÉ2
(1)
1D ‚â§ œÉ2 . The intuition of the
(2)
covers a wide range of distributions, including Gaussian and Laplace. If d = 1, we have œÉ2 = œÉ2
One can show that, when (2) is achieved with equality, we have œÉ2
This is not true in general and the relationship depends on the noise distribution. If Œæ t ‚àº N (0, s2 I),
condition above is that the tail of the noise Œæ t behaves well in any direction. This noise deÔ¨Ånition
1D = s2 and œÉ2 = ds2 . As another example, the Laplace mechanism noise
then one can see that œÉ2
used in differentially-private learning satisÔ¨Åes the tail condition above; see Jun and Orabona (2019,
Lemma 7).

2 (cid:6) ‚â§ œÉ2 , ‚àÄt,
Et [exp(Œ≤ Œæ t , a)] ‚â§ exp Œ≤ 2œÉ2
1D ~2 , ‚àÄ Œ≤   ‚â§ 1~b .

Et Œæ t 2

1D , b):

a‚à∂a2 ‚â§1

max

1D .

3 Parameter-free Stochastic Optimization with Noise

x = ‚àët

s=1 ÀÜgs , qs  and y = t(œÉ2 ~2 + G2 ),
mt+1 = e‚àía(ay+x) ‚àöœÄx exp  (2ay+x)2

We now describe BANCO that achieves essentially the convergence rate of optimally-tuned SGD
without repeated runs for tuning the learning rate, thus being truly one-pass. BANCO decomposes
the optimization process into two parts: one for optimizing the direction and the other for the mag-
nitude of the solution vector. For the direction, we use projected stochastic gradient descent with
adaptive learning rates (Orabona and P ¬¥al, 2018). While, for the magnitude we use a parameter-
free algorithm based on coin-betting (Orabona and Pal, 2016; Jun and Orabona, 2019). We then
construct the solution vector as the multiplication of the two solutions. The full pseudocode can
be found in Algorithm 1. Note that mt+1 has a closed form solution as follows: with shorthands
 erf( 2ay+x
2‚àöy ) + erf( 2ay‚àíx
For improving numerical precision for computing mt+1 , we refer to (Koolen, 2015).
zation mechanism that adds noise with probability density function œÅŒæ (z) ‚àù exp(‚àí 
2 z2 ), which
We now show that BANCO is a parameter-free -LDP SGD algorithm. Consider the Laplace saniti-
3

2‚àöy ) + 2‚àöy(1 ‚àí e2ax )

8ay3~2

4y

.

nism is -locally differentially private and ensures E Œæ t 2
makes the subgradients very similar to one another. Song et al. (2015) proved that this mecha-
b = ~4.
, satisfying (1). Further,
Theorem 2. Assume that (cid:96)(w, x) is convex in the Ô¨Årst argument w ‚àà Rd and has its subgradients‚Äô
Jun and Orabona (2019, Lemma 7) show that this mechanism satisÔ¨Åes (2) with œÉ2
ÀÜg t ‚àí E[ÀÜg t ] follow the density œÅ(z) ‚àù exp(‚àí 
2 z2 ). Then, for any w ‚àà Rd , after one pass over T
L2-norm bounded by 1, where the subgradient is with respect to the Ô¨Årst argument. Let the noise
samples Algorithm 1 guarantees

1D = 18d2 ~2 and

2 (cid:6) ‚â§ 4(d2 +d)

2



ln 1 + d2 w

2

 2 T

 + 1

T  .

E R  1

T

TQ

t=1

wt ‚àí R(w ) ‚â§ O  dw 2

‚àö



T

This convergence rate matches the private SGD of Wu et al. (2017) up to polylogarithmic terms,
optimal solution w to tune the learning rates. Furthermore, the rate in Theorem 2 is unimprovable
with the important difference that we do not need to assume the knowledge of the norm of the
well, resulting a better dependency in the dimension of the space, but in the weaker (, Œ¥)-LDP.
up to logarithmic factors as shown in Jun and Orabona (2019). Finally, we remark that Algorithm 1
is in fact an instance of Jun and Orabona (2019), and more generic algorithms are available via a
reduction therein, which can be of independent interest. Also, the Gaussian noise can be used as

3.1 Proof Sketch

T (v) ‚à∂= ‚àëT

T (u) ‚à∂= ‚àëT

The proof is mainly concerned with regret, a standard quantity in online learning that measures the
suboptimality with an arbitrary comparator u. Theorem 2 is a direct consequence of the expected
regret bound we describe below using so-called the online-to-batch conversion (Cesa-Bianchi et al.,
2004). Theorem 3 below shows that the expected regret of Algorithm 1 is decomposed into two
expected regrets, each for the magnitude learner mt and the direction learner q t . The fact that
we require the expected regret of the direction learner w.r.t.
the unit norm comparator frees us
from tuning its learning rate, delegating the burden of adaptation to the magnitude learner. In turn,
we consider linear functions that can be written as (cid:96)(w, x) = ‚àíg t , w for some g t without loss of
the magnitude algorithm is parameter-free, giving the best guarantee without having to tune any
learning rates. The proof is simple and immediate from Cutkosky and Orabona (2018). Hereafter,
generality (due to the convexity).
for any competitor u in the unit ball S ‚äÇ Rd and the magnitude learner obtains expected regret
Theorem 3. Suppose the direction learner obtains expected regret RegretD
t=1 st ‚ãÖ (v ‚àí mt ) for any competitor v ‚àà R where st = ÀÜg t , q t . Then, the iterates
mtq t guarantee
where we deÔ¨Åne u~u = 0 when u = 0.
For the direction learner, one can invoke any algorithm that guarantees the optimal regret with re-
spect to competitors in the unit ball. Here, we have used projected online gradient descent with the
scale-free learning rates in Orabona and P ¬¥al (2018). One can then immediately obtain the expected
regret bound with noisy subgradients:

t=1 ÀÜg t , u ‚àí q t 
ÀÜg t , u ‚àí mtq t  ‚â§ RegretM
T (u) + u RegretD

RegretM
E RegretT (u) ‚à∂= E TQ

g t , u ‚àí mtq t  = E TQ

√Ø√Ø√ØE

¬ø``(cid:192) TQ

√Ø√Ø (a)= O
√Ø√Ø√Ø
¬ø``(cid:192) TQ
T (u)] = O√Ø√Ø u  max (1 + b) ln ( u (1 + b)) ,
(1 + œÉ2

where (a) uses Jensen‚Äôs inequality and the fact that E[ÀÜg t 2 ] = E[g t 2
2 ] + œÉ2 .
Finally,
for
the magnitude learner, we have instantiated the coin betting algorithm of
Jun and Orabona (2019) that has the following regret bound:

2 + œÉ2 )√Ø√Ø ,
1D )T + 1 + 1√Ø .

1D )T ln  u (1 + œÉ2

(E g t 2

E RD

T  uu2

 = O

E[RD

T ( uu ),

ÀÜg t 2

2

t=1

t=1

t=1

t=1

4

4 Future Work

dependent regret bound; we have (G2 + œÉ2 )T rather than E[‚àëT
Our study opens up numerous research directions. First, one immediate difference in our upper
t=1 ÀÜg t 2 ]. It would be interesting
bound from the standard SGD algorithm with adaptive step size is that we do not have a data-
require the knowledge of the noise through (œÉ2 , b) so we can have a single algorithm that can adapt
to investigate whether data-dependent bounds are possible. Second, it would be desirable not to
to various types of noise. Finally, high probability regret bounds are a straightforward research
direction.

Acknowledgments

This material is based upon work supported by the National Science Foundation under grant no.
1740762 ‚ÄúCollaborative Research: TRIPODS Institute for Optimization and Learning.‚Äù We would
like to thank Adam Smith for his valuable feedback on differentially-private SGDs.

References

R. Bassily, A. Smith, and A. Thakurta. Differentially private empirical risk minimization: EfÔ¨Åcient
algorithms and tight error bounds. arXiv preprint arXiv:1405.7085, 2014.

N. Cesa-Bianchi, A. Conconi, and C. Gentile. On the generalization ability of on-line learning
algorithms. IEEE Trans. Inf. Theory, 50(9):2050‚Äì2057, 2004.

A. Cutkosky and F. Orabona. Black-box reductions for parameter-free online learning in Banach
spaces. In Proc. of the Conference on Learning Theory (COLT), 2018.

J. Duchi, E. Hazan, and Y. Singer. Adaptive subgradient methods for online learning and stochastic
optimization. In COLT, 2010.

J. C. Duchi, M. I. Jordan, and M. J. Wainwright. Privacy aware learning. Journal of the ACM, 61
(6):38, 2014.

C. Dwork, F. McSherry, K. Nissim, and A. Smith. Calibrating noise to sensitivity in private data
analysis. In Theory of cryptography conference, pages 265‚Äì284. Springer, 2006.

K.-S. Jun and F. Orabona. Parameter-free online convex optimization with sub-exponential noise.
In Proc. of the Conference on Learning Theory (COLT), 2019.

S. P. Kasiviswanathan, H. K. Lee, K. Nissim, S. Raskhodnikova, and A. Smith. What can we learn
privately? SIAM Journal on Computing, 40(3):793‚Äì826, 2011.

W. M. Koolen. Implementing squint, 2015. http://blog.wouterkoolen.info/Squint_implementation/post.html.

F. Orabona and D. Pal. Coin betting and parameter-free online learning. In D. D. Lee, M. Sugiyama,
U. V. Luxburg, I. Guyon, and R. Garnett, editors, Advances in Neural Information Processing
Systems 29, pages 577‚Äì585. Curran Associates, Inc., 2016.

F. Orabona and D. P ¬¥al. Scale-free online learning. Theoretical Computer Science, 716:50‚Äì69, 2018.
Special Issue on ALT 2015.

F. Orabona and T. Tommasi. Training deep networks without learning rates through coin betting. In
Advances in Neural Information Processing Systems, pages 2160‚Äì2170, 2017.

S. Song, K. Chaudhuri, and A. D. Sarwate. Stochastic gradient descent with differentially private
updates. In Global Conference on Signal and Information Processing (GlobalSIP), 2013 IEEE,
pages 245‚Äì248. IEEE, 2013.

S. Song, K. Chaudhuri, and A. Sarwate. Learning from data with heterogeneous noise using SGD.
In Proc. of International Conference on ArtiÔ¨Åcial Intelligence and Statistics (AISTATS), pages
894‚Äì902, 2015.

M. Streeter and B. McMahan. No-regret algorithms for unconstrained online convex optimization.
In Advances in Neural Information Processing Systems 25, pages 2402‚Äì2410. Curran Associates,
Inc., 2012.

5

L. Wasserman and S. Zhou. A statistical framework for differential privacy. Journal of the American
Statistical Association, 105(489):375‚Äì389, 2010.

X. Wu, F. Li, A. Kumar, K. Chaudhuri, S. Jha, and J. Naughton. Bolt-on differential privacy for
scalable stochastic gradient descent-based analytics.
In Proc. of the 2017 ACM International
Conference on Management of Data, pages 1307‚Äì1322. ACM, 2017.

M. Zinkevich. Online convex programming and generalized inÔ¨Ånitesimal gradient ascent. In Proc.
of ICML, pages 928‚Äì936, 2003.

6

