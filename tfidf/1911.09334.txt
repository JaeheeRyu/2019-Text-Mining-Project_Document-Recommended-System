Incorporating Textual Evidence in Visual Storytelling

Tianyi Li

Sujian Li

MOE Key Lab of Computational Linguistics, School of EECS, Peking University
Peng Cheng Laboratory, Shenzhen, China

{litianyi01,lisujian}@pku.edu.cn

9
1
0
2

v
o

N

1
2

]

L

C

.

s

c

[

1
v
4
3
3
9
0

.

1
1
9
1

:

v

i

X

r

a

Abstract

Previous work on visual storytelling mainly
focused on exploring image sequence as evi-
dence for storytelling and neglected textual ev-
idence for guiding story generation. Motivated
by human storytelling process which recalls
stories for familiar images, we exploit textual
evidence from similar images to help gener-
ate coherent and meaningful stories. To pick
the images which may provide textual experi-
ence, we propose a two-step ranking method
based on image object recognition techniques.
To utilize textual information, we design an
extended Seq2Seq model with two-channel
encoder and attention. Experiments on the
VIST dataset show that our method outper-
forms state-of-the-art baseline models without
heavy engineering.

1

Introduction

Multi-image visual storytelling is extended from a
long trend of research in image captioning and has
attracted considerable attention in recent years.
To generate the stories, previous work em-
ployed a Seq2Seq framework, using image en-
coder to encode the image sequences and sentence
decoder to generate stories from encoded image
sequences. Most of the researches (Smilevski
et al., 2018; Kim et al., 2018; Gonzalez-Rico and
Pineda, 2018; Wang et al., 2018b; Huang et al.,
2018; Yu et al., 2017) focused on improving the
decoder, and took simple concatenation or an
LSTM as encoder. With such design, only images
are utilized as input in generating the stories.
However, through our observations, the images
alone are inadequate for visual storytelling. Sto-
rytelling is creative and diversiﬁed, so background
knowledge is often required to convert a few im-
ages to a complete story. However, extracting such
background knowledge is very difﬁcult, especially
with limited data.

To alleviate such drawback, it is important to
take previous experience of story-writing into ac-
count. Imagining when a person starts to tell sto-
ries from images, he/she may not understand the
implications in those images and fail to write a
proper story. However, if he/she had heard others
telling stories, he/she may be able to tell a story
from the stories of similar image sequences he/she
previously heard. Motivated by such process, we
propose to utilize the large corpus as an inventory
and improve the visual storytelling model by in-
cluding stories from similar image sequences in
corpus as input to strengthen the encoder design.
On building such models, two major problems
need to be solved: (1) how to measure the related-
ness of stories from the image sequence pair; (2)
how to incorporate the textual information into the
model so as to fully exploit it for storytelling.
To handle the ﬁrst problem of picking the most
relevant stories, we propose a two-step ranking
method for their image sequences. We ﬁrst ﬁl-
ter out the ’dissimilar’ images with object co-
occurrence, and then sort the remaining candidates
with feature vectors. For the second problem of in-
corporating textual information, we design an en-
hanced Seq2Seq model with two-channel encoder,
one for visual input and the other for textual input.
We conduct experiments on the VIST dataset
(Huang et al., 2016), a widely used multi-image
visual storytelling dataset. We show that with tex-
tual evidence, our model outperforms our base-
lines and state-of-the-art models.

2 Method

Our method is based on the Seq2Seq framework,
composed of a two-channel encoder and a RNN-
based decoder. The whole architecture of our
method is shown in Figure 1.
In the two-channel encoder, one channel en-

 
 
 
 
 
 
Joint Decoder Different from previous meth-
ods, our decoder depend on both image and text
encoder. The incorporation of the two encoders is
the key problem. Here we adopt two approaches
to solve this problem. First, we use the concatena-
tion of the image encoder output, the embedding
of last word and the last hidden states of sentence
encoder as the input of the decoder. Second, we
design a Luong attention layer in decoder to at-
tend to sentence encoder outputs. Formally, the
concatenation decoder can be denoted as:

si
t = DEC (si
t−1 , [embi
t−1 , senti

lensenti

, img i ])

(1)

and the downstream attention mechanism can be
denoted as:

t · senti
weightst = si
Ct = S of tmax(weightt ) · senti
t |w i
1:t−1 ) = sof tmax(Wc · [Ct , si
πβ (w i
t ] + bc )

(2)
(3)

(4)

where DEC is decoder RNN, si
t is RNN output
for image i at step t, emb is word embedding, img
and sent are image and sentence encoder output,
Wc and bc are appended linear matrix and bias.
To be noticed, in our model, both decoder RNN
and image encoder are generic and not limited to
one particular design. The image encoder can be
of arbitrary architecture as long as it generates a
vector for each image, and the decoder RNN can
also be designed ﬂexibly as long as it takes a vec-
tor as input and outputs another vector at each step.
Speciﬁcally, we implemented these modiﬁca-
tions on two popular systems: GLACNet (2018),
the group with best human evaluation scores in Vi-
sual Storytelling Challenge NAACL 2018, wwho
use residual encoder to generate GLOCAL vec-
tors; XE-ss, a baseline model of Wang et al.
(2018b), who proposed to improve performance
with reinforcement model (AREL). We call our
two models GLAC-TG and XE-TG. (see section
3.1 for details).

2.2 Textual Evidence Selection

Figure 1: Overall architecture of our proposed method.

codes visual evidence from the image sequence
and the other encodes textual evidence from rel-
evant stories.
In the decoder, we adopt another
RNN model to generate stories from the two en-
coder outputs. To integrate the two types of infor-
mation, we use Luong attention (2015) to dynam-
ically attend to the stories. There are also other
modiﬁcations, as further explained in 2.1.
To collect the textual evidence for encoder in-
put, we design a selection method described in
Section 2.2 to get stories from the most similar
images.

2.1 Visual Storytelling Framework

Most previous works on visual storytelling fol-
lowed the Seq2Seq framework,
taking image
recognition models such as ResNet (He et al.,
2015) or Inception (Szegedy et al., 2016) to ex-
tract image features, feeding them into a story-
level RNN encoder, bringing encoder output to the
sentence-level decoder throughout the generation
of the corresponding sentence.
We base our model on this framework with two
key modiﬁcations: ﬁrst, we design a text encoder
to model the most similar stories which may pro-
vide evidence for story generation; second, we
adopt the Luong attention Luong et al. (2015)
mechanism on the textual side of encoded input
to better utilize its information.

Text Encoder We use an RNN encoder to model
the textual inputs. For each story, we feed its 5
sentences into the RNN one by one, retaining the
hidden state across sentences. We take the RNN
output of every step through the fully connected
layers as encoder output.

To provide strong textual evidence for story gener-
ation, we aim to select stories which are most sim-
ilar to the expected story for the given sequence of
images.
With the assumption that similar images usually
have similar stories, we take stories of similar im-

REFCORPUSImg1Img2Img5OBJRECOGNITION500Obj1Obj2Obj3Obj4Obj5Obj1Obj2Obj3Obj4Obj5Obj1&Img1Obj2&Img2Obj5&Img5NResNet152RNN Image EncoderSent2Sent1Sent3Sent4Sent5SentenceEncoder...   ImFEmbAttnMLPObj1Obj2Obj3Obj4Obj5Obj1Obj2Obj3Obj4Obj5REFOBJECTS45145RNN DecoderEncImFEmbEncImFEmbEncImFEmbEncSORTObj1Obj2Obj5FILTER.....................ImF1ImF2ImF5...EncAttnAttnAttnSELECTIONStories of Top K Similar Albumsages as similar stories. While it’s most straight-
forward to choose the image with the most similar
feature vector, it’s shown through experiments 2
that comparing each pair of feature vectors for a
large image corpus would be computationally ex-
pensive and suffer severely from false positives.
Therefore, we propose to employ a two-step ﬁlter-
and-sort method to pick out the most similar sto-
ries.

2.2.1 Filter

In the ﬁlter step, we use object co-occurrence
to discriminate ’roughly similar’ image sequences
from ’dissimilar’ ones. Here we ﬁlter by image ob-
ject information because it conforms with the intu-
ition that images with similar objects describe rel-
evant events. It is also because object information
has been widely used in image captioning as help-
ful information on images. (Mishra and Liwicki,
2019; Liu et al., 2018; Jiang et al., 2018; Anderson
et al., 2017; Yin and Ordonez, 2017; Wang et al.,
2018a).
We ﬁrst get the types and numbers of objects
in each image using an object recognition model,
and then we measure image similarity with a cat-
egorical criterion and a numerical criterion. For-
mally, Oa and Ob are the set of objects present in
image a and b respectively, ck
x is the count of oc-
currence for object k in image x. The categorical
criterion concerns the types of common objects,

namely scorecat =

|Oa

same topic of wilderness adventure with similar
story-ﬂows. The generated story also catches the
essence of the image sequence, with basic details
closely relevant. It shows that our textual evidence
selection method is capable of selecting proper
textual evidence, and our storytelling framework
is capable of capturing the provided information
and telling ﬂuent and coherent stories.

3.3 Analysis on Textual Evidence Selection

In this section, we further explore the effective-
ness of similar stories. We experimented on ﬁlter-
ing candidate size 50, 100 and 500 with both cate-
gorical and numerical criteria, using sorting on the
entire reference corpus for comparison and ME-
TEOR score as a metric of actual story similar-
ity. In Table 2, we show that for all methods, the
selected stories are signiﬁcantly more similar to
gold stories than randomly selected ones, and sto-
ries with higher rankings are generally better than
those with lower rankings. Moreover, for both cri-
teria, candidate size poses negligible effect.
On the other hand, neither sorting on full corpus
nor sorting by bi-linear model shows competitive
results compared to our approach.

50
24.8
24.9
24.6
24.5
24.8

M

1
2
3
4
5
rand
full
B-L

numerical
100
24.7
24.5
24.6
24.5
24.5

categorical
100
500
50
24.8
25.0
24.9
24.8
24.7
24.4
24.5
24.6
24.6
24.9
24.8
24.5
24.6
24.6
24.5
23.8
23.28 (average on top 5)
23.62 (average on top 5)

500
24.5
24.6
24.5
24.3
24.5

Table 2: METEOR scores for top 1 to 5 similar stories
regarding two criteria, B-L refers to Bi-Linear

4 Conclusion

In this paper, we show that textual evidence from
similar image sequences contains rich informa-
tion for visual storytelling, therefore it’s capable
of boosting storytelling performance. We propose
a feasible two-step approach to extract textual ev-
idence from a large corpus. We also design a two-
channel encoder to incorporate textual and visual
evidence into the Seq2Seq visual storytelling mod-
els and achieve state-of-the-art performance with-

Figure 2: An example sequence of visual storytelling.

In Table 1, we compare our models with sev-
eral strong baselines on three automatic evalua-
tion metrics, ROUGE-L, CIDEr and METEOR. In
the top block of Table 1, we present 4 previous
baselines: 1) a standard Seq2Seq baseline model
developed by Huang et al. (2016); 2) a hierarchi-
cally attentive model designed by Yu et al. (2017);
3) the Seq2Seq model with sentence-wise separate
decoders by Gonzalez-Rico and Pineda (2018); 4)
reinforcement learning with topic guided decoders
by Huang et al. (2018). In the middle block, we
present the GLACNet model Kim et al. (2018)
and our improved GLAC-TG model. In the bot-
tom block, we present our XE-TG models which
are improved based on the XE-ss model in AREL
framework (Wang et al., 2018b). For fair compar-
ison, we evaluate all models with the open source
evaluation code1 (Yu et al., 2017).
Result shows that both our models outper-
form their corresponding baselines. Even us-
ing textual evidence only, our XE-TG-only model
shows competitive performance compared to the
baselines. Moreover, our XE-TG models using
cross entropy loss outperformed state-of-the-art
baselines with reinforcement learning techniques
(Wang et al., 2018b; Huang et al., 2018). By using
simple cross entropy loss, our models are also less
costly to train, easier to tune and more stable when
re-trained.
We conduct a qualitative analysis on XE-TG-
top1 model
in Figure
2 as an example.
It
shows that the selected similar story shares the

1https://github.com/lichengunc/vist_
eval

GOLD1)2)3)4)5)Brothers bike riding in the mountains.Exploring an abandon house.And discovering a hidden chamber.Secret passage way to the beach.A beautiful view of the beach at the end of the tunnel.SEL(TOP1)It was a perfect day for a hikeThe setting was beautiful and the weather just perfectWe came across several over passes that were picturesqueI loved how the foliage of this one made us feel like we were in a magical placeAs the day came to close the sun began to set and we knew that all was right for that momentGENWe took a trip to the beachThere were many old buildingsThere were a lot of people thereWe saw a lot of rocksThe view from the top was amazingIMGAlex Krizhevsky, Ilya Sutskever, and Geoffrey E Hin-
ton. 2012.
Imagenet classiﬁcation with deep con-
volutional neural networks.
In F. Pereira, C. J. C.
Burges, L. Bottou, and K. Q. Weinberger, editors,
Advances in Neural Information Processing Systems
25, pages 1097–1105. Curran Associates, Inc.

Tsung-Yi Lin, Michael Maire, Serge J. Belongie,
Lubomir D. Bourdev, Ross B. Girshick, James Hays,
Pietro Perona, Deva Ramanan, Piotr Doll ´ar, and
C. Lawrence Zitnick. 2014. Microsoft COCO: com-
mon objects in context. CoRR, abs/1405.0312.

Fenglin Liu, Xuancheng Ren, Yuanxin Liu, Houfeng
Wang, and Xu Sun. 2018.
simnet: Stepwise
image-topic merging network for generating de-
tailed and comprehensive image captions. CoRR,
abs/1808.08732.

Minh-Thang Luong, Hieu Pham,
and Christo-
pher D. Manning. 2015. Effective approaches to
attention-based neural machine translation. CoRR,
abs/1508.04025.

Ashutosh Mishra and Marcus Liwicki. 2019. Using
deep object features for image descriptions. CoRR,
abs/1902.09969.

Marko Smilevski, Ilija Lalkovski, and Gjorgji Mad-
jarov. 2018.
Stories for images-in-sequence by
using visual and narrative components.
CoRR,
abs/1805.05622.

Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe,
Jon Shlens, and Zbigniew Wojna. 2016. Rethinking
the inception architecture for computer vision.
In
The IEEE Conference on Computer Vision and Pat-
tern Recognition (CVPR).

Josiah Wang, Pranava Swaroop Madhyastha, and Lu-
cia Specia. 2018a. Object counts! bringing ex-
plicit detections back into image captioning. CoRR,
abs/1805.00314.

Xin Wang, Wenhu Chen, Yuan-Fang Wang, and
William Yang Wang. 2018b. No metrics are perfect:
Adversarial reward learning for visual storytelling.
CoRR, abs/1804.09160.

Xuwang Yin and Vicente Ordonez. 2017. OBJ2TEXT:
generating visually descriptive language from object
layouts. CoRR, abs/1707.07102.

Licheng Yu, Mohit Bansal, and Tamara L. Berg. 2017.
Hierarchically-attentive RNN for album summariza-
tion and storytelling. CoRR, abs/1708.02977.

out heavy engineering.

Acknowledgments

We thank the anonymous reviewers for their help-
ful comments on this paper.
This work was
partially supported by National Natural Science
Foundation of China (61572049 and 61876009).

References

Waleed Abdulla. 2017. Mask r-cnn for object detec-
tion and instance segmentation on keras and tensor-

ﬂow. https://github.com/matterport/
Mask_RCNN.

Peter Anderson, Xiaodong He, Chris Buehler, Damien
Teney, Mark Johnson, Stephen Gould, and Lei
Zhang. 2017.
Bottom-up and top-down atten-
tion for image captioning and VQA.
CoRR,
abs/1707.07998.

Ziqiang Cao, Wenjie Li, Sujian Li, and Furu Wei.
2018. Retrieve, rerank and rewrite: Soft template
based neural summarization. In Proceedings of the
56th Annual Meeting of the Association for Com-
putational Linguistics (Volume 1: Long Papers),
pages 152–161, Melbourne, Australia. Association
for Computational Linguistics.

Diana Gonzalez-Rico and Gibran Fuentes Pineda.
2018. Contextualize, show and tell: A neural visual
storyteller. CoRR, abs/1806.00738.

Kaiming He, Georgia Gkioxari, Piotr Doll ´ar, and
Ross B. Girshick. 2017. Mask R-CNN. CoRR,
abs/1703.06870.

Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian
Sun. 2015. Deep residual learning for image recog-
nition. CoRR, abs/1512.03385.

Qiuyuan Huang, Zhe Gan, Asli C¸ elikyilmaz,
Dapeng Oliver Wu,
Jianfeng Wang,
and Xi-
aodong He. 2018.
Hierarchically structured
reinforcement learning for topically coherent visual
story generation. CoRR, abs/1805.08191.

Ting-Hao K. Huang,
Francis Ferraro, Nasrin
Mostafazadeh, Ishan Misra, Jacob Devlin, Aish-
warya Agrawal, Ross Girshick, Xiaodong He,
Pushmeet Kohli, Dhruv Batra, et al. 2016. Visual
storytelling.
In 15th Annual Conference of the
North American Chapter of
the Association for
Computational Linguistics (NAACL 2016).

Wenhao Jiang, Lin Ma, Xinpeng Chen, Hanwang
Zhang, and Wei Liu. 2018. Learning to guide decod-
ing for image captioning. CoRR, abs/1804.00887.

Taehyeong Kim, Min-Oh Heo, Seonil Son, Kyoung-
Wha Park, and Byoung-Tak Zhang. 2018. GLAC
net: Glocal attention cascading networks for
multi-image cued story generation.
CoRR,
abs/1805.10973.

