9
1
0
2

v
o

N

1
2

]

A

M

.

s

c

[

3
v
9
4
8
9
0

.

9
0
9
1

:

v

i

X

r

a

Multiagent Evaluation under Incomplete Information

Mark Rowland1,∗

Shayegan Omidshaﬁei2,∗

Karl Tuyls2

markrowland@google.com

somidshafiei@google.com

karltuyls@google.com

Julien Pérolat1

Michal Valko2

Georgios Piliouras3

perolat@google.com

valkom@deepmind.com

georgios@sutd.edu.sg

Rémi Munos2

munos@google.com

1DeepMind London

2DeepMind Paris

3 Singapore University of Technology and Design

∗Equal contributors

Abstract

This paper investigates the evaluation of learned multiagent strategies in the in-
complete information setting, which plays a critical role in ranking and training of
agents. Traditionally, researchers have relied on Elo ratings for this purpose, with
recent works also using methods based on Nash equilibria. Unfortunately, Elo is
unable to handle intransitive agent interactions, and other techniques are restricted
to zero-sum, two-player settings or are limited by the fact that the Nash equilibrium
is intractable to compute. Recently, a ranking method called α-Rank, relying on a
new graph-based game-theoretic solution concept, was shown to tractably apply
to general games. However, evaluations based on Elo or α-Rank typically assume
noise-free game outcomes, despite the data often being collected from noisy sim-
ulations, making this assumption unrealistic in practice. This paper investigates
multiagent evaluation in the incomplete information regime, involving general-sum
many-player games with noisy outcomes. We derive sample complexity guarantees
required to conﬁdently rank agents in this setting. We propose adaptive algorithms
for accurate ranking, provide correctness and sample complexity guarantees, then
introduce a means of connecting uncertainties in noisy match outcomes to uncer-
tainties in rankings. We evaluate the performance of these approaches in several
domains, including Bernoulli games, a soccer meta-game, and Kuhn poker.

1

Introduction

This paper investigates evaluation of learned multiagent strategies given noisy game outcomes. The
Elo rating system is the predominant approach used to evaluate and rank agents that learn through,
e.g., reinforcement learning [12, 35, 42, 43]. Unfortunately, the main caveat with Elo is that it
cannot handle intransitive relations between interacting agents, and as such its predictive power is too
restrictive to be useful in non-transitive situations (a simple example being the game of Rock-Paper-
Scissors). Two recent empirical game-theoretic approaches are Nash Averaging [3] and α-Rank [36].
Empirical Game Theory Analysis (EGTA) can be used to evaluate learning agents that interact in
large-scale multiagent systems, as it remains largely an open question as to how such agents can be
evaluated in a principled manner [36, 48, 49]. EGTA has been used to investigate this evaluation
problem by deploying empirical or meta-games [37, 38, 47, 51–54]. Meta-games abstract away the

33rd Conference on Neural Information Processing Systems (NeurIPS 2019), Vancouver, Canada.

 
 
 
 
 
 
(a)

(b)

Figure 1.1: (a) Illustration of converting plausible payoff matrices consistent with an empirical
estimate ˆM to empirical rankings ˆπ . The set of plausible payoff matrices and plausible rankings
are shown, respectively, in grey and blue. (b) Ranking uncertainty vs. payoff uncertainty for a
soccer meta-game involving 10 agents. Each cluster of bars shows conﬁdence intervals over ranking
weights given an observed payoff matrix with a particular uncertainty level; payoff uncertainty here
corresponds to the mean conﬁdence interval size of payoff matrix entries. This example illustrates
the need for careful consideration of payoff uncertainties when computing agent rankings.

atomic decisions made in the game and instead focus on interactions of high-level agent strategies,
enabling the analysis of large-scale games using game-theoretic techniques. Such games are typically
constructed from large amounts of data or simulations. An evaluation of the meta-game then gives
a means of comparing the strengths of the various agents interacting in the original game (which
might, e.g., form an important part of a training pipeline [25, 26, 42]) or of selecting a ﬁnal agent
after training has taken place (see Fig. 1.1a).
Both Nash Averaging and α-Rank assume noise-free (i.e., complete) information, and while α-Rank
applies to general games, Nash Averaging is restricted to 2-player zero-sum settings. Unfortunately,
we can seldom expect to observe a noise-free speciﬁcation of a meta-game in practice, as in large
multiagent systems it is unrealistic to expect that the various agents under study will be pitted against
all other agents a sufﬁcient number of times to obtain reliable statistics about the meta-payoffs in
the empirical game. While there have been prior inquiries into approximation of equilibria (e.g.,
Nash) using noisy observations [15, 28], few have considered evaluation or ranking of agents in
meta-games with incomplete information [40, 53]. Consider, for instance, a meta-game based on
various versions of AlphaGo and prior state-of-the-art agents (e.g., Zen) [41, 48]; the game outcomes
are noisy, and due to computational budget not all agents might play against each other. These issues
are compounded when the simulations required to construct the empirical meta-game are inherently
expensive.
Motivated by the above issues, this paper contributes to multiagent evaluation under incomplete
information. As we are interested in general games that go beyond dyadic interactions, we focus on
α-Rank. Our contributions are as follows: ﬁrst, we provide sample complexity guarantees describing
the number of interactions needed to conﬁdently rank the agents in question; second, we introduce
adaptive sampling algorithms for selecting agent interactions for the purposes of accurate evaluation;
third, we develop means of propagating uncertainty in payoffs to uncertainty in agent rankings. These
contributions enable the principled evaluation of agents in the incomplete information regime.

2 Preliminaries

We review here preliminaries in game theory and evaluation. See Appendix A for related work.
S k of pure strategies. Denote by S = (cid:81)
Games and meta-games. Consider a K -player game, where each player k ∈ [K ] has a ﬁnite set
(s1 , . . . , sK ) ∈ S of pure strategies, the game speciﬁes a joint probability distribution ν (s) of payoffs
k S k the space of pure strategy proﬁles. For each tuple s =
to each player. The vector of expected payoffs is denoted M(s) = (M1 (s), . . . , MK (s)) ∈ RK .
In empirical game theory, we are often interested in analyzing interactions at a higher meta-level,
wherein a strategy proﬁle s corresponds to a tuple of machine learning agents and the matrix M
captures their expected payoffs when played against one another in some domain. Given this, the
notions of ‘agents’ and ‘strategies’ are considered synonymous in this paper.
Evaluation. Given payoff matrix M ∈ (RK )S , a key task is to evaluate the strategies in the game.
This is sometimes done in terms of a game-theoretic solution concept (e.g., Nash equilibria), but may

2

Agent Ranking AlgorithmEvaluatingTrainingPlayingMeta-gameSynthesisGame simulationPayoff uncertainty0.00.20.40.60.81.0Agent ranking  confidence interval00.010.030.050.0750.10.2Agent 0Agent 1Agent 2Agent 3Agent 4Agent 5Agent 6Agent 7Agent 8Agent 9also consist of rankings or numerical scores for strategies. We focus particularly on the evolutionary
dynamics based α-Rank method [36], which applies to general many-player games, but also provide
supplementary results for the Elo ranking system [12]. There also exist Nash-based evaluation
methods, such as Nash Averaging in two-player, zero-sum settings [3, 48], but these are not more
generally applicable as the Nash equilibrium is intractable to compute and select [11, 20].
The exact payoff table M is rarely known; instead, an empirical payoff table ˆM is typically con-
structed from observed agent interactions (i.e., samples from the distributions ν (s)). Based on
collected data, practitioners may associate a set of plausible payoff tables with this point estimate,
either using a frequentist conﬁdence set, or a Bayesian high posterior density region. Figure 1.1a
illustrates the application of a ranking algorithm to a set of plausible payoff matrices, where rankings
can then be used for evaluating, training, or prescribing strategies to play. Figure 1.1b visualizes an
example demonstrating the sensitivity of computed rankings to estimated payoff uncertainties (with
ranking uncertainty computed as discussed in Section 5). This example highlights the importance of
propagating payoff uncertainties through to uncertainty in rankings, which can play a critical role,
e.g., when allocating training resources to agents based on their respective rankings during learning.
α-Rank. The Elo ranking system (reviewed in Appendix C) is designed to estimate win-loss
probabilities in two-player, symmetric, constant-sum games [12]. Yet despite its widespread use for
ranking [2, 19, 31, 42], Elo has no predictive power in intransitive games (e.g., Rock-Paper-Scissors)
[3]. By contrast, α-Rank is a ranking algorithm inspired by evolutionary game theory models, and
applies to K -player, general-sum games [36]. At a high level, α-Rank deﬁnes an irreducible Markov
chain over strategy set S , called the response graph of the game [32]. The ordered masses of this
Markov chain’s unique invariant distribution π yield the strategy proﬁle rankings. The Markov
transition matrix, C, is deﬁned in a manner that establishes a link to a solution concept called
Markov-Conley chains (MCCs). MCCs are critical for the rankings computed, as they capture agent
interactions even under intransitivities and are tractably computed in general games, unlike Nash
equilibria [11].
s = (s1 , . . . , sK ) ∈ S be a pure strategy proﬁle, and let σ = (σk , s−k ) be the pure strategy proﬁle
In more detail, the underlying transition matrix over S is deﬁned by α-Rank as follows. Let
which is equal to s, except for player k , which uses strategy σk ∈ S k instead of sk . Denote by η the
only a single player deviates in her strategy), so that η = ((cid:80)K
reciprocal of the total number of valid proﬁle transitions from a given strategy proﬁle (i.e., where
l=1 (|S l | − 1))−1 . Let Cs,σ denote the
transition probability from s to σ , and Cs,s the self-transition probability of s, with each deﬁned as:

(cid:40)

Cs,σ =

η

1−exp(−α(Mk (σ)−Mk (s)))
1−exp(−αm(Mk (σ)−Mk (s)))
η
m

if Mk (σ) (cid:54)= Mk (s)

otherwise ,

and Cs,s = 1 − (cid:88)

Cs,σ ,

(1)

k∈[K ]
σ |σk ∈S k \{sk }

where if two strategy proﬁles s and s(cid:48) differ in more than one player’s strategy, then Cs,s(cid:48) = 0.
Here α ≥ 0 and m ∈ N are parameters to be speciﬁed; the form of this transition probability is
informed by particular models in evolutionary dynamics and is explained in detail by Omidshaﬁei
et al. [36], with large values of α corresponding to higher selection pressure in the evolutionary
model considered. A key remark is that the correspondence of α-Rank to the MCC solution concept
occurs in the limit of inﬁnite α. In practice, to ensure the irreducibility of C and the existence of a
unique invariant distribution π , α is either set to a large but ﬁnite value, or a perturbed version of
C under the inﬁnite-α limit is used. We theoretically and numerically analyze both the ﬁnite- and
inﬁnite-α regimes in this paper, and provide more details on α-Rank, response graphs, and MCCs in
Appendix B.

3 Sample complexity guarantees

This section provides sample complexity bounds, stating the number of strategy proﬁle observations
needed to obtain accurate α-Rank rankings with high probability. We give two sample complexity
results, the ﬁrst for rankings in the ﬁnite-α regime, and the second an instance-dependent guarantee
on the reconstruction of the transition matrix in the inﬁnite-α regime. All proofs are in Appendix D.
Theorem 3.1 (Finite-α). Suppose payoffs are bounded in the interval [−Mmax , Mmax ], and deﬁne

L(α, Mmax ) = 2α exp(2αMmax ) and g(α, η , m, Mmax ) = η exp(2αMmax )−1
exp(2αmMmax )−1 . Let ε ∈ (0, 18 ×

3

n=1

n

2−|S | (cid:80)|S |−1
(cid:0)|S |
(cid:1)n|S | ), δ ∈ (0, 1). Let ˆM be an empirical payoff table constructed by taking Ns
i.i.d. interactions of each strategy proﬁle s ∈ S . Then the invariant distribution ˆπ derived from the
empirical payoff matrix ˆM satisﬁes maxs∈(cid:81)
k S k |π(s) − ˆπ(s)| ≤ ε with probability at least 1 − δ , if
(cid:0)|S |
(cid:1)n|S |(cid:17)2

max log(2|S |K/δ)L(α, Mmax )2 (cid:16)(cid:80)|S |−1
648M 2
ε2 g(α, η , m, Mmax )2

∀s ∈ S .

Ns >

n=1

n

The dependence on δ and ε are as expected from typical Chernoff-style bounds, though Markov chain
perturbation theory introduces a dependence on the α-Rank parameters as well, most notably α.
Theorem 3.2 (Inﬁnite-α). Suppose all payoffs are bounded in [−Mmax , Mmax ], and that ∀k ∈ [K ]
and ∀s−k ∈ S−k , we have |Mk (σ, s−k ) − Mk (τ , s−k )| ≥ ∆ for all distinct σ, τ ∈ S k , for some
∆ > 0. Let δ > 0. Suppose we construct an empirical payoff table ( ˆMk (s) | k ∈ [K ], s ∈ S )
through Ns i.i.d games for each strategy proﬁle s ∈ S . Then the transition matrix ˆC computed from
payoff table ˆM is exact (and hence all MCCs are exactly recovered) with probability at least 1 − δ , if

Ns > 8∆−2M 2
max log(2|S |K/δ)

∀s ∈ S .

A consequence of the theorem is that exact inﬁnite-α rankings are recovered with probability at least
1 − δ . We also provide theoretical guarantees for Elo ratings in Appendix C for completeness.

4 Adaptive sampling-based ranking

Whilst instructive, the bounds above have limited utility as the payoff gaps that appear in them are
rarely known in practice. We next introduce algorithms that compute accurate rankings with high
conﬁdence without knowledge of payoff gaps, focusing on α-Rank due to its generality.
Problem statement. Fix an error tolerance δ > 0. We seek an algorithm which speciﬁes (i) a
sampling scheme S that selects the next strategy proﬁle s ∈ S for which a noisy game outcome is
observed, and (ii) a criterion C (δ) that stops the procedure and outputs the estimated payoff table
used for the inﬁnite-α α-Rank rankings, which is exactly correct with probability at least 1 − δ .
The assumption of inﬁnite-α simpliﬁes this task; it is sufﬁcient for the algorithm to determine, for
each k ∈ [K ] and pair of strategy proﬁles (σ, s−k ), (τ , s−k ), whether Mk (σ, s−k ) > Mk (τ , s−k )
or Mk (σ, s−k ) < Mk (τ , s−k ) holds. If all such pairwise comparisons are correctly made with
probability at least 1 − δ , the correct rankings can be computed. Note that we consider only instances
for which the third possibility, Mk (σ, s−k ) = Mk (τ , s−k ), does not hold; in such cases, it is
well-known that it is impossible to design an adaptive strategy that always stops in ﬁnite time [13].
This problem can be described as a related collection of pure exploration bandit problems [4]; each
such problem is speciﬁed by a player index k ∈ [K ] and set of two strategy proﬁles {s, (σk , s−k )}
(where s ∈ S , σk ∈ S k ) that differ only in player k ; the aim is to determine whether player k receives
a greater payoff under strategy proﬁle s or (σk , s−k ). Each individual best-arm identiﬁcation problem
can be solved to the required conﬁdence level by maintaining empirical means and a conﬁdence
bound for the payoffs concerned. Upon termination, an evaluation technique such as α-Rank can
then be run on the resulting response graph to compute the strategy proﬁle (or agent) rankings.

4.1 Algorithm: ResponseGraphUCB

We introduce a high-level adaptive sampling algorithm, called ResponseGraphUCB, for computing
on the choice of sampling scheme S and stopping criterion C (δ), which we detail next.
accurate rankings in Algorithm 1. Several variants of ResponseGraphUCB are possible, depending
Sampling scheme S . Algorithm 1 keeps track of a list of pairwise strategy proﬁle comparisons that
α-Rank requires, removing pairs of proﬁles for which we have high conﬁdence that the empirical table
is correct (according to C (δ)), and selecting a next strategy proﬁle for simulation. There are several
ways in which strategy proﬁle sampling can be conducted in Algorithm 1. Uniform (U): A strategy
proﬁle is drawn uniformly from all those involved in an unresolved pair. Uniform-exhaustive (UE):

4

Algorithm 1 ResponseGraphUCB(δ, S , C (δ))
1: Construct list L of pairs of strategy proﬁles to compare
2: Initialize tables ˆM, N to store empirical means and interaction counts
3: while L is not empty do
Select a strategy proﬁle s appearing in an edge in L using sampling scheme S
Simulate one interaction for s and update ˆM, N accordingly
Check whether any edges are resolved according to C (δ), remove them from L if so
7: return empirical table ˆM

4:
5:
6:

II

0

1

0.50, 0.50 0.85, 0.15
0.15, 0.85 0.50, 0.50

I 0
1

(a) Players I and II payoffs.

(c) Strategy-wise sample counts.

(b) Reconstructed response graph.

Figure 4.1: ResponseGraphUCB(δ : 0.1, S : UE, C : UCB) run on a two-player game. (a) The payoff
tables for both players. (b) Reconstructed response graph, together with ﬁnal empirical payoffs and
conﬁdence intervals (in blue) and true payoffs (in red). (c) Strategy-wise sample proportions.

A pair of strategy proﬁles is selected uniformly from the set of unresolved pairs, and both strategy
proﬁles are queried until the pair is resolved. Valence-weighted (VW): As each query of a proﬁle
informs multiple payoffs and has impacts on even greater numbers of pairwise comparisons, there
may be value in ﬁrst querying proﬁles that may resolve a large number of comparisons. Here we set
the probability of sampling s proportional to the squared valence of node s in the graph of unresolved
comparisons. Count-weighted (CW): The marginal impact on the width of a conﬁdence interval for
a strategy proﬁle with relatively few queries is greater than for one with many queries, motivating
preferential sampling of strategy proﬁles with low query count. Here, we preferentially sample the
strategy proﬁle with lowest count among all strategy proﬁles with unresolved comparisons.
Stopping condition C (δ). The stopping criteria we consider are based on conﬁdence-bound methods,
with the intuition that the algorithm stops only when it has high conﬁdence in all pairwise comparisons
made. To this end, the algorithm maintains a conﬁdence interval for each of the estimates, and judges
a pairwise comparison to be resolved when the two conﬁdence intervals concerned become disjoint.
There are a variety of conﬁdence bounds that can be maintained, depending on the speciﬁcs of the
game; we consider Hoeffding (UCB) and Clopper-Pearson (CP-UCB) bounds, along with relaxed
variants of each (respectively, R-UCB and R-CP-UCB); full descriptions are given in Appendix F.
We build intuition by evaluating ResponseGraphUCB(δ : 0.1, S : UE, C : UCB), i.e., with a 90%
conﬁdence level, on a two-player game with payoffs shown in Fig. 4.1a; noisy payoffs are simulated
as detailed in Section 6. The output is given in Fig. 4.1b; the center of this ﬁgure shows the estimated
response graph, which matches the ground truth in this example. Around the response graph, mean
payoff estimates and conﬁdence bounds are shown for each player-strategy proﬁle combination in
blue; in each of the surrounding four plots, ResponseGraphUCB aims to establish which of the true
payoffs (shown as red dots) is greater for the deviating player, with directed edges pointing towards
estimated higher-payoff deviations. Figure 4.1b reveals that strategy proﬁle (0, 0) is the sole sink of
the response graph, thus would be ranked ﬁrst by α-Rank. Each proﬁle has been sampled a different
number of times, with running averages of sampling proportions shown in Fig. 4.1c. Exploiting
knowledge of game symmetry (e.g., as in Fig. 4.1a) can reduce sample complexity; see Appendix H.3.

We now show the correctness of ResponseGraphUCB and bound the number samples required for it
to terminate. Our analysis depends on the choice of conﬁdence bounds used in stopping condition

5

C (δ); we describe the correctness proof in a manner agnostic to these details, and give a sample
complexity result for the case of Hoeffding conﬁdence bounds. See Appendix E for proofs.
Theorem 4.1. The ResponseGraphUCB algorithm is correct with high probability: Given δ ∈ (0, 1),
for any particular sampling scheme there is a choice of conﬁdence levels such that ResponseGra-
phUCB outputs the correct response graph with probability at least 1 − δ .

Theorem 4.2. The ResponseGraphUCB algorithm, using conﬁdence parameter δ and Hoeffding
conﬁdence bounds, run on an evaluation instance with ∆ = min(sk ,s−k ),(σk ,s−k ) |Mk (sk , s−k ) −
Mk (σk , s−k )| requires at most O(∆−2 log(1/(δ∆))) samples with probability at least 1 − 2δ .

5 Ranking uncertainty propagation

This section considers the remaining key issue of efﬁciently computing uncertainty in the ranking
weights, given remaining uncertainty in estimated payoffs. We assume known element-wise upper-
and lower-conﬁdence bounds U and L on the unknown true payoff table M, e.g., as provided by
ResponseGraphUCB. The task we seek to solve is, given a particular strategy proﬁle s ∈ S and these
payoff bounds, to output the conﬁdence interval for π(s), the ranking weight for s under the true
payoff table M; i.e., we seek [inf L≤ ˆM≤U π ˆM (s), supL≤ ˆM≤U π ˆM (s)], where π ˆM denotes the output
of inﬁnite-α α-Rank under payoffs ˆM. This section proposes an efﬁcient means of solving this task.
At the very highest level, this essentially involves ﬁnding plausible response graphs (that is, response
graphs that are compatible with a payoff matrix ˆM within the conﬁdence bounds L and U) that
minimize or maximize the probability π(s) given to particular strategy proﬁles s ∈ S under inﬁnite-α
α-Rank. Considering the maximization case, intuitively this may involve directing as many edges
adjacent to s towards s as possible, so as to maximize the amount of time the corresponding Markov
chain spends at s. It is less clear intuitively what the optimal way to set the directions of edges not
adjacent to s should be, and how to enforce consistency with the constraints L ≤ ˆM ≤ U. In fact,
similar problems have been studied before in the PageRank literature for search engine optimization
[7, 9, 10, 16, 24], and have been shown to be reducible to constrained dynamic programming
problems.
More formally, the main idea is to convert the problem of obtaining bounds on π to a constrained
stochastic shortest path (CSSP) policy optimization problem which optimizes mean return time for
the strategy proﬁle s in the corresponding . In full generality, such constrained policy optimization
problems are known to be NP-hard [10]. Here, we show that it is sufﬁcient to optimize an uncon-
strained version of the α-Rank CSSP, hence yielding a tractable problem that can be solved with
standard SSP optimization routines. Details of the algorithm are provided in Appendix G; here, we
provide a high-level overview of its structure, and state the main theoretical result underlying the
correctness of the approach.
The ﬁrst step is to convert the element-wise conﬁdence bounds L ≤ ˆM ≤ U into a valid set of
constraints on the form of the underlying response graph. Next, a reduction is used to encode the
problem as policy optimization in a constrained shortest path problem (CSSP), as in the PageRank
literature [10]; we denote the corresponding problem instance by CSSP(S, L, U, s). Whilst solution
of CSSPs is in general hard, we note here that it is possible to remove the constraints on the problem,
yielding a stochastic shortest path problem that can be solved by standard means.

Theorem 5.1. The unconstrained SSP problem given by removing the action consistency constraints
of CSSP(S, L, U, s) has the same optimal value as CSSP(S, L, U, s).

See Appendix G for the proof. Thus, the general approach for ﬁnding worst-case upper and lower
bounds on inﬁnite-α α-Rank ranking weights π(s) for a given strategy proﬁle s ∈ S is to formulate
the unconstrained SSP described above, ﬁnd the optimal policy (using, e.g., linear programming,
policy or value iteration), and then use the inverse relationship between mean return times and
stationary distribution probabilities in recurrent Markov chains to obtain the bound on the ranking
weight π(s) as required; full details are given in Appendix G. This approach, when applied to the
soccer domain described in the sequel, yields Fig. 1.1b.

6

(a) Bernoulli games.

(b) Soccer meta-game.

(c) Poker meta-game.

Figure 6.1: Samples needed per strategy proﬁle (Ns ) for ﬁnite-α α-Rank, without adaptive sampling.

6 Experiments

We consider three domains of increasing complexity, with experimental procedures detailed in
Appendix H.1. First, we consider randomly-generated two-player zero-sum Bernoulli games, with
the constraint that payoffs Mk (s, σ) ∈ [0, 1] cannot be too close to 0.5 for all pairs of distinct
strategies s, σ ∈ S where σ = (σk , s−k ) (i.e., a single-player deviation from s). This constraint
implies that we avoid games that require an exceedingly large number of interactions for the sampler
to compute a reasonable estimate of the payoff table. Second, we analyze a Soccer meta-game with
the payoffs in Liu et al. [33, Figure 2], wherein agents learn to play soccer in the MuJoCo simulation
environment [46] and are evaluated against one another. This corresponds to a two-player symmetric
zero-sum game with 10 agents, but with empirical (rather than randomly-generated) payoffs. Finally,
we consider a Kuhn poker meta-game with asymmetric payoffs and 3 players with access to 3
agents each, similar to the domain analyzed in [36]; here, only α-Rank (and not Elo) applies for
evaluation due to more than two players being involved. In all domains, noisy outcomes are simulated
by drawing the winning player i.i.d. from a Bernoulli(Mk (s)) distribution over payoff tables M.
We ﬁrst consider the empirical sample complexity of α-Rank in the ﬁnite-α regime. Figure 6.1
invariant distribution error , where maxs∈(cid:81)
visualizes the number of samples needed per strategy proﬁle to obtain rankings given a desired
k S k |π(s) − ˆπ(s)| ≤ ε. As noted in Theorem 3.1, the
sample complexity increases with respect to α, with the larger soccer and poker domains requiring on
the order of 1e3 samples per strategy proﬁle to compute reasonably accurate rankings. These results
are also intuitive given the evolutionary model underlying α-Rank, where lower α induces lower
selection pressure, such that strategies perform almost equally well and are, thus, easier to rank.
As noted in Section 4, sample complexity and ranking error under adaptive sampling are of particular
interest. To evaluate this, we consider variants of ResponseGraphUCB in Fig. 6.2, with particular
focus on the UE sampler (S : UE) for visual clarity; complete results for all combinations of S
and C (δ) are presented in Appendix Section H.2. Consider ﬁrst the results for the Bernoulli games,
shown in Fig. 6.2a; the top row plots the number of interactions required by ResponseGraphUCB to
accurately compute the response graph given a desired error tolerance δ , while the bottom row plots
the number of response graph edge errors (i.e., the number of directed edges in the estimated response
graph that point in the opposite direction of the ground truth graph). Notably, the CP-UCB conﬁdence
bound is guaranteed to be tighter than the Hoeffding bounds used in standard UCB, thus the former
requires fewer interactions to arrive at a reasonable response graph estimate with the same conﬁdence
as the latter; this is particularly evident for the relaxed variants R-CP-UCB, which require roughly
an order of magnitude fewer samples compared to the other sampling schemes, despite achieving a
reasonably low response graph error rate.
Consider next the ResponseGraphUCB results given noisy outcomes for the soccer and poker meta-
games, respectively in Figs. 6.2b and 6.2c. Due to the much larger strategy spaces of these games, we
cap the number of samples available at 1e5. While the results for poker are qualitatively similar to the
Bernoulli games, the soccer results are notably different; in Fig. 6.2b (top), the non-relaxed samplers
use the entire budget of 1e5 interactions, which occurs due to the large strategy space cardinality.
Speciﬁcally, the player-wise strategy size of 10 in the soccer dataset yields a total of 900 two-arm
bandit problems to be solved by ResponseGraphUCB. We note also an interesting trend in Fig. 6.2b
(bottom) for the three ResponseGraphUCB variants (S : UE, C (δ): UCB), (S : UE, C (δ): R-UCB),
and (S : UE, C (δ): CP-UCB). In the low error tolerance (δ ) regime, the uniform-exhaustive strategy
used by these three variants implies that ResponseGraphUCB spends the majority of its sampling
budget observing interactions of an extremely small set of strategy proﬁles, and thus cannot resolve
the remaining response graph edges accurately, resulting in high error. As error tolerance δ increases,

7

(a) Bernoulli games.

(b) Soccer meta-game.

(c) Poker meta-game.

Figure 6.2: ResponseGraphUCB performance metrics versus error tolerance δ for all games. First
and second rows, respectively, show the # of interactions required and response graph edge errors.

(a) Soccer meta-game.

(b) Poker meta-game.

Figure 6.3: Payoff table Frobenius error and ranking errors for various ResponseGraphUCB conﬁ-
dence levels δ . Number of samples is normalized to [0, 1] on the x-axis.

while the probability of correct resolution of individual edges decreases by deﬁnition, the earlier
stopping time implies that the ResponseGraphUCB allocates its budget over a larger set of strategies
to observe, which subsequently lowers the total number of response graph errors.
Figure 6.3a visualizes the ranking errors for Elo and inﬁnite-α α-Rank given various ResponseG-
raphUCB error tolerances δ in the soccer domain. Ranking errors are computed using the Kendall
partial metric (see Appendix H.4). Intuitively, as the estimated payoff table error decreases due to
added samples, so does the ranking error for both algorithms. Figure 6.3b similarly considers the
α-Rank ranking error in the poker domain. Ranking errors again decrease gracefully as the number

(a) Soccer meta-game.

(b) Poker meta-game.

Figure 6.4: The ground truth distribution of payoff gaps for all response graph edges in the soccer and
poker meta-games. We conjecture that the higher ranking variance may be explained by these gaps
tending to be more heavily distributed near 0 for poker, making it difﬁcult for ResponseGraphUCB to
sufﬁciently capture the response graph topology given a high error tolerance δ .

8

of samples increases. Interestingly, while errors are positively correlated with respect to the error
tolerances δ for the poker meta-game, this tolerance parameter seems to have no perceivable effect on
the soccer meta-game. Moreover, the poker domain results appear to be much higher variance than the
soccer counterparts. To explore this further, we consider the distribution of payoff gaps, which play a
key role in determining the response graph reconstruction errors. Let ∆(s, σ) = |Mk (s) − Mk (σ)|,
the payoff difference corresponding to the edge of the response graph where player k deviates, causing
a transition between strategy proﬁles s, σ ∈ S . Figure 6.4 plots the ground truth distribution of these
gaps for all response graph edges in soccer and poker. We conjecture that the higher ranking variance
may be explained by these gaps tending to be more heavily distributed near 0 for poker, making it
difﬁcult for ResponseGraphUCB to distinguish the ‘winning’ proﬁle and thereby sufﬁciently capture
the response graph topology given a high error tolerance δ .
Overall, these results indicate a need for careful consideration of payoff uncertainties when ranking
agents, and quantify the effectiveness of the algorithms proposed for multiagent evaluation under
incomplete information. We conclude by remarking that the pairing of bandit algorithms and α-Rank
seems a natural means of computing rankings in settings where, e.g., one has a limited budget for
adaptively sampling match outcomes. Our use of bandit algorithms also leads to analysis which is
ﬂexible enough to be able to deal with K -player general-sum games. However, approaches such
as collaborative ﬁltering may also fare well in their own right. We conduct a preliminary analysis
of this in Appendix H.5, speciﬁcally for the case of two-player win-loss games, leaving extensive
investigation for follow-up work.

7 Conclusions

This paper conducted a rigorous investigation of multiagent evaluation under incomplete information.
We focused particularly on α-Rank due to its applicability to general-sum, many-player games.
We provided static sample complexity bounds quantifying the number of interactions needed to
conﬁdently rank agents, then introduced several sampling algorithms that adaptively allocate samples
to the agent match-ups most informative for ranking. We then analyzed the propagation of game
outcome uncertainty to the ﬁnal rankings computed, providing sample complexity guarantees as
well as an efﬁcient algorithm for bounding rankings given payoff table uncertainty. Evaluations
were conducted on domains ranging from randomly-generated two-player games to many-player
meta-games constructed from real datasets. The key insight gained by this analysis is that noise in
match outcomes plays a prevalent role in determination of agent rankings. Given the recent emergence
of training pipelines that rely on the evaluation of hundreds of agents pitted against each other in
noisy games (e.g., Population-Based Training [25, 26]), we strongly believe that consideration of
these uncertainty sources will play an increasingly important role in multiagent learning.

Acknowledgements

We thank Daniel Hennes and Thore Graepel for extensive feedback on an earlier version of this paper,
and the anonymous reviewers for their comments and suggestions to improve the paper. Georgios
Piliouras acknowledges MOE AcRF Tier 2 Grant 2016-T2-1-170, grant PIE-SGP-AI-2018-01 and
NRF 2018 Fellowship NRF-NRFF2018-07.

9

References

[1] Michele Aghassi and Dimitris Bertsimas. Robust game theory. Mathematical Programming,
107(1):231–273, Jun 2006.

[2] Broderick Arneson, Ryan B Hayward, and Philip Henderson. Monte Carlo tree search in Hex.
IEEE Transactions on Computational Intelligence and AI in Games, 2(4):251–258, 2010.

[3] David Balduzzi, Karl Tuyls, Julien Perolat, and Thore Graepel. Re-evaluating evaluation. In
Advances in Neural Information Processing Systems (NeurIPS), 2018.

[4] Sébastien Bubeck, Rémi Munos, and Gilles Stoltz. Pure exploration in ﬁnitely-armed and
continuous-armed bandits. Theoretical Computer Science, 412(19):1832–1852, 2011.

[5] Xiangrui Chao, Gang Kou, Tie Li, and Yi Peng. Jie Ke versus AlphaGo: A ranking approach
using decision making method for large-scale data with incomplete information. European
Journal of Operational Research, 265(1):239–247, 2018.

[6] Charles Clopper and Egon Pearson. The use of conﬁdence or ﬁducial limits illustrated in the
case of the binomial. Biometrika, 26(4):404–413, 1934.

[7] Giacomo Como and Fabio Fagnani. Robustness of large-scale stochastic matrices to localized
perturbations. IEEE Transactions on Network Science and Engineering, 2(2):53–64, 2015.

[8] Rémi Coulom. Whole-history rating: A Bayesian rating system for players of time-varying
strength. In Computers and Games, 6th International Conference, CG 2008, Beijing, China,
September 29 - October 1, 2008. Proceedings, pages 113–124, 2008.

[9] Balázs Csanád Csáji, Raphaël M Jungers, and Vincent D Blondel. PageRank optimization in
polynomial time by stochastic shortest path reformulation. In International Conference on
Algorithmic Learning Theory. Springer, 2010.

[10] Balázs Csanád Csáji, Raphaël M Jungers, and Vincent D Blondel. PageRank optimization by
edge selection. Discrete Applied Mathematics, 169:73–87, 2014.

[11] Constantinos Daskalakis, Paul W. Goldberg, and Christos H. Papadimitriou. The complexity of
computing a Nash equilibrium. In Proceedings of the 38th Annual ACM Symposium on Theory
of Computing, Seattle, WA, USA, May 21-23, 2006, pages 71–78, 2006.

[12] Arpad E Elo. The Rating of Chessplayers, Past and Present. Arco Pub., 1978.

[13] Eyal Even-Dar, Shie Mannor, and Yishay Mansour. Action elimination and stopping conditions
for the multi-armed bandit and reinforcement learning problems. Journal of Machine Learning
Research, 7(Jun):1079–1105, 2006.

[14] Ronald Fagin, Ravi Kumar, Mohammad Mahdian, D Sivakumar, and Erik Vee. Comparing
partial rankings. SIAM Journal on Discrete Mathematics, 20(3):628–648, 2006.

[15] John Fearnley, Martin Gairing, Paul W Goldberg, and Rahul Savani. Learning equilibria of
games via payoff queries. Journal of Machine Learning Research, 16(1):1305–1344, 2015.

[16] Olivier Fercoq, Marianne Akian, Mustapha Bouhtou, and Stéphane Gaubert. Ergodic control
and polyhedral approaches to PageRank optimization. IEEE Transactions on Automatic Control,
58(1):134–148, 2013.

[17] Victor Gabillon, Mohammad Ghavamzadeh, and Alessandro Lazaric. Best arm identiﬁcation:
A uniﬁed approach to ﬁxed budget and ﬁxed conﬁdence. In Advances in Neural Information
Processing Systems (NeurIPS). 2012.

[18] Aurélien Garivier and Olivier Cappé. The KL-UCB algorithm for bounded stochastic bandits
and beyond. In Proceedings of the Conference on Learning Theory (COLT), 2011.

[19] Audrunas Gruslys, Will Dabney, Mohammad Gheshlaghi Azar, Bilal Piot, Marc Bellemare,
and Rémi Munos. The reactor: A sample-efﬁcient actor-critic architecture. Proceedings of the
International Conference on Learning Representations (ICLR), 2018.

10

[20] John Harsanyi and Reinhard Selten. A General Theory of Equilibrium Selection in Games,
volume 1. The MIT Press, 1 edition, 1988.

[21] Johannes Heinrich, Marc Lanctot, and David Silver. Fictitious self-play in extensive-form
games. In International Conference on Machine Learning, 2015.
[22] Daniel Hennes, Daniel Claes, and Karl Tuyls. Evolutionary advantage of reciprocity in collision
avoidance. In AAMAS Workshop on Autonomous Robots and Multirobot Systems, 2013.
[23] Ralf Herbrich, Tom Minka, and Thore Graepel. TrueSkill: a Bayesian skill rating system. In
Advances in Neural Information Processing Systems (NIPS), 2007.
[24] Romain Hollanders, Giacomo Como, Jean-Charles Delvenne, and Raphaël M Jungers. Tight
bounds on sparse perturbations of Markov chains. In International Symposium on Mathematical
Theory of Networks and Systems, 2014.
[25] Max Jaderberg, Valentin Dalibard, Simon Osindero, Wojciech M Czarnecki, Jeff Donahue, Ali
Razavi, Oriol Vinyals, Tim Green, Iain Dunning, Karen Simonyan, et al. Population based
training of neural networks. arXiv preprint arXiv:1711.09846, 2017.
[26] Max Jaderberg, Wojciech M. Czarnecki, Iain Dunning, Luke Marris, Guy Lever, Antonio Garcia
Castañeda, Charles Beattie, Neil C. Rabinowitz, Ari S. Morcos, Avraham Ruderman, Nicolas
Sonnerat, Tim Green, Louise Deason, Joel Z. Leibo, David Silver, Demis Hassabis, Koray
Kavukcuoglu, and Thore Graepel. Human-level performance in 3d multiplayer games with
population-based reinforcement learning. Science, 364(6443):859–865, 2019.
[27] Prateek Jain, Praneeth Netrapalli, and Sujay Sanghavi. Low-rank matrix completion using
alternating minimization. In Proceedings of the ACM Symposium on Theory of Computing
(STOC), 2013.
[28] Patrick R Jordan, Yevgeniy Vorobeychik, and Michael P Wellman. Searching for approximate
equilibria in empirical games. In Proceedings of the International Conference on Autonomous
Agents and Multiagent Systems (AAMAS), 2008.
[29] Shivaram Kalyanakrishnan, Ambuj Tewari, Peter Auer, and Peter Stone. PAC subset selection
in stochastic multi-armed bandits. In Proceedings of the International Conference on Machine
Learning (ICML), 2012.
[30] Zohar Karnin, Tomer Koren, and Oren Somekh. Almost optimal exploration in multi-armed
bandits. In Proceedings of the International Conference on Machine Learning (ICML), 2013.
[31] Matthew Lai. Giraffe: Using deep reinforcement learning to play chess. arXiv preprint
arXiv:1509.01549, 2015.
[32] Marc Lanctot, Vinicius Zambaldi, Audrunas Gruslys, Angeliki Lazaridou, Karl Tuyls, Julien
Pérolat, David Silver, and Thore Graepel. A uniﬁed game-theoretic approach to multiagent
reinforcement learning. In Advances in Neural Information Processing Systems (NIPS), 2017.
[33] Siqi Liu, Guy Lever, Nicholas Heess, Josh Merel, Saran Tunyasuvunakool, and Thore Graepel.
Emergent coordination through competition. In Proceedings of the International Conference on
Learning Representations (ICLR), 2019.
[34] H. Brendan McMahan, Geoffrey J. Gordon, and Avrim Blum. Planning in the presence of
cost functions controlled by an adversary. In Proceedings of the International Conference on
Machine Learning (ICML), 2003.
[35] Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A. Rusu, Joel Veness, Marc G.
Bellemare, Alex Graves, Martin Riedmiller, Andreas K. Fidjeland, Georg Ostrovski, Stig Pe-
tersen, Charles Beattie, Amir Sadik, Ioannis Antonoglou, Helen King, Dharshan Kumaran, Daan
Wierstra, Shane Legg, and Demis Hassabis. Human-level control through deep reinforcement
learning. Nature, 518(7540):529–533, 02 2015.
[36] Shayegan Omidshaﬁei, Christos Papadimitriou, Georgios Piliouras, Karl Tuyls, Mark Rowland,
Jean-Baptiste Lespiau, Wojciech M Czarnecki, Marc Lanctot, Julien Perolat, and Rémi Munos.
α-Rank: Multi-agent evaluation by evolution. Scientiﬁc Reports, 9, 2019.

11

[37] Steve Phelps, Simon Parsons, and Peter McBurney. An evolutionary game-theoretic comparison
of two double-auction market designs. In Agent-Mediated Electronic Commerce VI, Theories
for and Engineering of Distributed Mechanisms and Systems, AAMAS 2004 Workshop, 2004.

[38] Steve Phelps, Kai Cai, Peter McBurney, Jinzhong Niu, Simon Parsons, and Elizabeth Sklar.
Auctions, evolution, and multi-agent learning. In Adaptive Agents and Multi-Agent Systems
III. Adaptation and Multi-Agent Learning, 5th, 6th, and 7th European Symposium, ALAMAS
2005-2007 on Adaptive and Learning Agents and Multi-Agent Systems, 2007.

[39] Marc J. V. Ponsen, Karl Tuyls, Michael Kaisers, and Jan Ramon. An evolutionary game-theoretic
analysis of poker strategies. Entertainment Computing, 1(1):39–45, 2009.

[40] Achintya Prakash and Michael P. Wellman. Empirical game-theoretic analysis for moving target
defense. In Proceedings of the Second ACM Workshop on Moving Target Defense, 2015.

[41] David Silver, Aja Huang, Chris J. Maddison, Arthur Guez, Laurent Sifre, George van den
Driessche, Julian Schrittwieser, Ioannis Antonoglou, Vedavyas Panneershelvam, Marc Lanctot,
Sander Dieleman, Dominik Grewe, John Nham, Nal Kalchbrenner, Ilya Sutskever, Timothy P.
Lillicrap, Madeleine Leach, Koray Kavukcuoglu, Thore Graepel, and Demis Hassabis. Master-
ing the game of Go with deep neural networks and tree search. Nature, 529(7587):484–489,
2016.

[42] David Silver, Julian Schrittwieser, Karen Simonyan, Ioannis Antonoglou, Aja Huang, Arthur
Guez, Thomas Hubert, Lucas Baker, Matthew Lai, Adrian Bolton, et al. Mastering the game of
go without human knowledge. Nature, 550(7676):354, 2017.

[43] David Silver, Thomas Hubert, Julian Schrittwieser, Ioannis Antonoglou, Matthew Lai, Arthur
Guez, Marc Lanctot, Laurent Sifre, Dharshan Kumaran, Thore Graepel, Timothy Lillicrap,
Karen Simonyan, and Demis Hassabis. A general reinforcement learning algorithm that masters
chess, shogi, and go through self-play. Science, 362(6419):1140–1144, 2018.

[44] Samuel Sokota, Caleb Ho, and Bryce Wiedenbeck. Learning deviation payoffs in simulation
based games. In AAAI Conference on Artiﬁcial Intelligence, 2019.

[45] Eilon Solan and Nicolas Vieille. Perturbed Markov chains. Journal of Applied Probability, 40
(1):107–122, 2003.

[46] Emanuel Todorov, Tom Erez, and Yuval Tassa. MuJoCo: A physics engine for model-based
control. In Proceedings of the International Conference on Intelligent Robots and Systems
(IROS), 2012.

[47] Karl Tuyls and Simon Parsons. What evolutionary game theory tells us about multiagent
learning. Artif. Intell., 171(7):406–416, 2007.

[48] Karl Tuyls, Julien Perolat, Marc Lanctot, Joel Z Leibo, and Thore Graepel. A generalised
method for empirical game theoretic analysis. In Proceedings of the International Conference
on Autonomous Agents and Multiagent Systems (AAMAS), 2018.

[49] Karl Tuyls, Julien Perolat, Marc Lanctot, Rahul Savani, Joel Leibo, Toby Ord, Thore Graepel,
and Shane Legg. Symmetric decomposition of asymmetric games. Scientiﬁc Reports, 8(1):
1015, 2018.

[50] Yevgeniy Vorobeychik. Probabilistic analysis of simulation-based games. ACM Transactions
on Modeling and Computer Simulation, 20(3), 2010.

[51] William E. Walsh, Rajarshi Das, Gerald Tesauro, and Jeffrey O. Kephart. Analyzing complex
strategic interactions in multi-agent games. In AAAI Workshop on Game Theoretic and Decision
Theoretic Agents, 2002.

[52] William E. Walsh, David C. Parkes, and Rjarshi Das. Choosing samples to compute heuristic-
strategy Nash equilibrium. In Proceedings of the Fifth Workshop on Agent-Mediated Electronic
Commerce, 2003.

12

[53] Michael P. Wellman. Methods for empirical game-theoretic analysis. In Proceedings of The
National Conference on Artiﬁcial Intelligence and the Innovative Applications of Artiﬁcial
Intelligence Conference, 2006.

[54] Michael P. Wellman, Tae Hyung Kim, and Quang Duong. Analyzing incentives for protocol
compliance in complex domains: A case study of introduction-based routing. In Proceedings of
the 12th Workshop on the Economics of Information Security, 2013.

[55] Bryce Wiedenbeck and Michael P. Wellman. Scaling simulation-based game analysis through
deviation-preserving reduction. In Proceedings of the International Conference on Autonomous
Agents and Multiagent Systems (AAMAS), 2012.

[56] Yichi Zhou, Jialian Li, and Jun Zhu. Identify the Nash equilibrium in static games with random
payoffs. In Proceedings of the International Conference on Machine Learning (ICML), 2017.

13

Appendices:
Multiagent Evaluation under Incomplete Information

We provide here supplementary material that may be of interest to the reader. Note that section and
ﬁgures in the main text that are referenced here are clearly indicated via numerical counters (e.g.,
Fig. 1.1), whereas those in the appendix itself are indicated by alphabetical counters (e.g., Fig. H.2).

A Related Work

Originally, Empirical Game Theory was introduced to reduce and study the complexity of large
economic problems in electronic commerce, e.g., continuous double auctions [51–53], and later it has
been also applied in various other domains and settings [22, 37–39, 48]. Empirical game theoretic
analysis and the effects of uncertainty in payoff tables (in the form of noisy payoff estimates and/or
missing table elements) on the computation of Nash equilibria have been studied for some time [1, 15,
28, 44, 50, 55, 56], with contributions including sample complexity bounds for accurate equilibrium
estimation [50], adaptive sampling algorithms [56], payoff query complexity results of computing
approximate Nash equilibria in various types of games [15], and the formulation of particular
varieties of equilibria robust to noisy payoffs [1]. These earlier methods are mainly based on the
Nash equilibrium concept and use, amongst others, information-theoretic ideas (value of information)
and regression techniques to generalize payoffs of strategy proﬁles. By contrast, in this paper we
focus on both Elo ratings and an approach inspired by response graphs, evolutionary dynamics
and Markov-Conley Chains, capturing the underlying dynamics of the multiagent interactions and
providing a rating of players on their long-term behavior [36].
The Elo rating system was originally introduced to rate chess players and named after Arpad Elo [12].
It deﬁnes a measure to express the relative strength of a player, and as such has also been widely
adopted in machine learning to evaluate the strength of agents or strategies [41, 42, 48]. Unfortunately,
when applying Elo rating in machine learning, and multiagent learning particular, Elo is problematic:
it is restricted to 2-player interactions, it is unable to capture intransitive behaviors and an Elo score
can potentially be artiﬁcially inﬂated [3]. A Bayesian skill rating system called TrueSkill, which
handles player skill uncertainties and generalized Elo rating, was introduced in Herbrich et al. [23].
For an introduction and discussion of extensions to Elo rating see, e.g., Coulom [8]. Other researchers
have also introduced a method based on a fuzzy pair-wise comparison matrix that uses a cosine
similarity measure for ratings, but this approach is also limited to two-player interactions [5].
Another recent work that inherently uses response graphs as its underlying dynamical model is the
PSRO algorithm (Policy-Space Response Oracles) [32]. The Deep Cognitive Hierarchies model
relates PSRO to cognitive hierarchies, and is equivalent to a response graph. The algorithm is essen-
tially a generalization of the Double Oracle algorithm [34] and Fictitious Self-Play [21], iteratively
computing approximate best responses to the meta-strategies of other agents.

B α-Rank: Additional Background

This section provides additional background on the α-Rank ranking algorithm.
Given match outcomes for a K -player game, α-Rank computes rankings as follows:
1. Construct meta-payoff tables Mk for each player k ∈ {1, . . . , K } (e.g., by using the
win/loss ratios for the different strategy/agent match-ups as payoffs)
2. Compute the transition matrix C, as detailed in Section 2
3. Compute the stationary distribution, π , of C
In the transition structure outlined in Section 2 Eq. (1), the factor ((cid:80)K
4. Compute the agent rankings by ordering the masses of π
l=1 (|S l | − 1))−1 normalizes
across the different strategy proﬁles that s may transition to, whilst the second factor represents the
relative ﬁtness of the two proﬁles s and σ In practice, m ∈ N is typically ﬁxed and one considers
the invariant distribution πα as a function of the parameter α. Figure B.1 illustrates the ﬁxation
probabilities in the α-Rank model, for various values of m and α.

Finite-α limit.

In general, the invariant distribution tends to converge as α → ∞, and we take α to
be sufﬁciently large such that πα has effectively converged and corresponds to the MCC solution
concept.

Inﬁnite-α limit. An alternative approach is to set α inﬁnitely large, then introduce a small per-
turbation along every edge of the response graph, such that transitions can occur from dominated
strategies to dominant ones. This perturbation enforces irreducibility of the Markov transition matrix
C, yielding a unique stationary distribution and corresponding ranking.

(a) α-Rank population size m = 2.

(b) α-Rank population size m = 50.

Figure B.1: Illustrations of ﬁxation probabilities in the α-Rank model.

II

C

1, 2
2, 1
0, 1

R

0, 0
1, 0
2, 2

I

L

U 2, 1
M 1, 2
D 0, 0

(a)

(b)

(c)

Figure B.2: The response graph associated to payoffs shown in (a) is visualized in (b). (c) MCCs
associated with the response graph highlighted in blue.

The response graph of a game is a directed graph where nodes correspond to pure strategy proﬁles,
and directed edges if the deviating player’s new strategy is a better-response. The response graph for
the game speciﬁed in Fig. B.2a is illustrated in Fig. B.2b.
Markov-Conley Chains (MCCs) are deﬁned as the sink strongly connected components of the
response graph. The MCCs associated with the payoffs speciﬁed in Fig. B.2a are illustrated in
Fig. B.2c. The stationary distribution computed by α-Rank corresponds to a ranking of strategy
proﬁles in the MCCs of the game response graph, indicating the average amount of time individuals
in the underlying evolutionary model spend playing each strategy proﬁle.

C Elo Rating System: Overview and Theoretical Results

This section provides an overview of the Elo rating system, along with theoretical guarantees on the
number of samples needed to construct accurate payoff matrices using Elo.

C.1 Elo Evaluation

Consider games involving two players with shared strategy set S 1 . Elo computes a vector r ∈ RS 1
quantifying the strategy ratings. Let φ(x) = (1 + exp(−x))−1 , then the probability of s1 ∈ S 1
beating s2 ∈ S 1 predicted by Elo is qs1 ,s2 (r) = φ(rs1 −rs2 ). Consider a batch of N two-player game
n=1 , where {s1
n} ∈ S 1 are the player strategies and un is the observed
(noisy) payoff to player 1 in game n. Let u ∈ RN denote the vector of all observed payoffs, and
outcomes (s1

n , s2
n , un )N

n , s2

(U,R)(D,C)(D,L)(M,L)(D,R)(M,C)(U,L)(U,C)(M,R)(U,R)(D,C)(D,L)(M,L)(D,R)(M,C)(U,L)(U,C)(M,R)denote by BatchElo the algorithm applied to the batch of outcomes. BatchElo ﬁts ability parameters
r by minimizing the following objective with respect to r:

N(cid:88)

−un log (cid:0)φ(rs1

n

− rs2

n

LElo (r; u) =

)(cid:1) − (1 − un ) log (cid:0)1 − φ(rs1

− rs2

n

n

)(cid:1) .

(2)

n=1

Ordering the elements of r gives the strategy rankings. Yet despite its widespread use for ranking
[2, 19, 31, 42], Elo has no predictive power in intransitive games (e.g., Rock-Paper-Scissors) [3].

C.2 Theoretical Results

In analogy with the sample complexity results for α-Rank presented in Section 3, we give the
following result on the sample complexity of Elo ranking, building on the work of Balduzzi et al. [3].

Theorem C.1. Consider a symmetric, two-player win-loss game with ﬁnite strategy set S 1 and payoff
matrix M. Let q be the ﬁtted payoffs obtained from the BatchElo model on the payoff matrix M,
and let ˆq be the ﬁtted payoffs obtained from the BatchElo model on an empirical payoff table ˆM,
based on Ns,s(cid:48) interactions between each pair of strategies s, s(cid:48) . If we take, for each pair of strategy
proﬁles s, s(cid:48) ∈ S 1 , a number of interactions Ns,s(cid:48) satisfying

Then it follows that with probability at least 1 − δ ,

Ns,s(cid:48) > 0.5|S 1 |2 ε−2 log(|S 1 |2/δ) .

(qs,s(cid:48) − ˆqs,s(cid:48) )

∀s ∈ S 1 .

(3)

(4)

Proof. As in Balduzzi et al. [3, Proposition 1], we have that the row and column sums of ˆq, q match
those of ˆp, p, respectively. Thus, as a ﬁrst result we obtain

(qs,s(cid:48) − ˆqs,s(cid:48) ) =

(qs,s(cid:48) − ps,s(cid:48) ) +

(ps,s(cid:48) − ˆps,s(cid:48) ) +

( ˆps,s(cid:48) − ˆqs,s(cid:48) )

(cid:88)

s(cid:48)

(cid:88)

s(cid:48)

(ps,s(cid:48) − ˆps,s(cid:48) ) ∀s ,

By analogous calculation, we obtain the following result for column sums:

(qs,s(cid:48) − ˆqs,s(cid:48) ) =

(ps,s(cid:48) − ˆps,s(cid:48) ) ∀s .

We may now apply Hoeffding’s inequality to each ˆps,s(cid:48) with at least Ns,s(cid:48) samples as in the statement
of the theorem, and applying a union bound yields the required inequality.

D Proofs of results from Section 3

D.1 Proof of Theorem 3.1

n

n=1

Theorem 3.1 (Finite-α). Suppose payoffs are bounded in the interval [−Mmax , Mmax ], and deﬁne
2−|S | (cid:80)|S |−1
(cid:0)|S |
(cid:1)n|S | ), δ ∈ (0, 1). Let ˆM be an empirical payoff table constructed by taking Ns
empirical payoff matrix ˆM satisﬁes maxs∈(cid:81)
i.i.d. interactions of each strategy proﬁle s ∈ S . Then the invariant distribution ˆπ derived from the
k S k |π(s) − ˆπ(s)| ≤ ε with probability at least 1 − δ , if
(cid:0)|S |
(cid:1)n|S |(cid:17)2

L(α, Mmax ) = 2α exp(2αMmax ) and g(α, η , m, Mmax ) = η exp(2αMmax )−1
exp(2αmMmax )−1 . Let ε ∈ (0, 18 ×
max log(2|S |K/δ)L(α, Mmax )2 (cid:16)(cid:80)|S |−1
648M 2
ε2 g(α, η , m, Mmax )2

∀s ∈ S .

Ns >

n=1

n

We begin by stating and proving several preliminary results.

s(cid:48)

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:88)
(cid:88)
(cid:88)
(cid:88)

=

s(cid:48)

s(cid:48)

s(cid:48)

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) < ε
(cid:88)

s(cid:48)

(cid:88)

s(cid:48)

Theorem D.1 (Finite-α conﬁdence bounds). Suppose all payoffs are bounded in the interval
2|S |L(α,Mmax ) , and let ˆM be an empirical payoff table, such

[−Mmax , Mmax ]. Let 0 < ε < g(α,η ,m,Mmax )

that

|Mk (sk , s−k ) − ˆMk (sk , s−k )| ≤ ε .

sup

k∈[K ]

s∈(cid:81)
Then, denoting the invariant distribution of the Markov chain associated with ˆM by ˆπ , we have
|S |−1(cid:88)
s∈(cid:81)

|π(s) − ˆπ(s)| ≤ 18ε

L(α, Mmax )
g(α, η , m, Mmax )

(cid:18)|S |

n|S | .

(cid:19)

max

l S l

n

k S k

(5)

(6)

n=1

We base our proof of Theorem D.1 on the following corollary of [45, Theorem 1].

Theorem D.2. Let q be an irreducible transition kernel on a ﬁnite state space S , with invariant
distribution π . Let β ∈ (0, 1/2|S | ), and let ˆq be another transition kernel on S . Suppose that
|q(t|s) − ˆq(t|s)| ≤ β q(t|s) for all states s, t ∈ S . If q is irreducible, then ˆq is irreducible, and the
invariant distributions π , ˆπ of q , ˆq satisfy
|S |−1(cid:88)

(cid:18)|S |

(cid:19)

|π(s) − ˆπ(s)| ≤ 18π(s)β

n|S | ,

for all s ∈ S .

n

n=1

We next derive several technical bounds on properties of the α-Rank transition matrix C, deﬁned in
(1).
Lemma D.3. Suppose all payoffs are bounded in the interval [−Mmax , Mmax ]. Then all non-zero
elements of the transition matrix C are lower-bounded by g(α, η , m, Mmax ).

Proof. Consider ﬁrst an off-diagonal, non-zero element of the matrix. The transition probability is
given by

(cid:18) 1 − exp(−αx)
1 − exp(−αmx)

(cid:19)

,

η

for some x ∈ [−2Mmax , 2Mmax ], by assumption of boundedness of payoffs. This quantity
is minimal for x = −2Mmax , and we hence obtain the required lower-bound. We note also
that these transition probabilities are upper-bounded by taking x = 2Mmax , yielding an upper
1−exp(−2αmMmax ) . The transition probability of a diagonal element Cii takes the form
j (cid:54)=i Cij . There are η−1 non-zero terms in the sum, each of which is upper-bounded by
1−exp(−2αmMmax ) . Hence, we obtain the following lower bound on the diagonal entries:

bound of η 1−exp(−2αMmax )

η 1−exp(−2αMmax )

1 − (cid:80)

1 − η−1 η

1 − exp(−2αMmax )
1 − exp(−2αmMmax )

=

=1 − 1 − exp(−2αMmax )
1 − exp(−2αmMmax )
1 − exp(−2αmMmax ) − 1 + exp(−2αMmax )
1 − exp(−2αmMmax )
exp(2α(m − 1)Mmax ) − 1
exp(2αmMmax ) − 1
exp(2αMmax ) − 1
exp(2αmMmax ) − 1

≥η

=

,

as required.
Lemma D.4. Suppose all payoffs are bounded in the interval [−Mmax , Mmax ]. All transition
probabilities are Lipschitz continuous as a function of the collection of payoffs ( ˆMk (sk , s−k )|k ∈
l S l ) under the inﬁnity norm, with Lipschitz constant upper-bounded by L(α, Mmax ).

[K ], s ∈ (cid:81)

(cid:17)

(cid:16) 1−exp(−αx)
1−exp(−αmx)
(cid:16) 1−exp(−αx)
1−exp(−αmx)

Proof. We begin by considering off-diagonal, non-zero elements. The transition probability takes the
, for some x ∈ [−2Mmax , 2Mmax ], representing the payoff difference for the
form η
pair of strategy proﬁles concerned. First, the Lipschitz constant of x (cid:55)→ exp(−αx) on the domain
x ∈ [−2Mmax , 2Mmax ] is α exp(2αMmax ). Composing this function with the function x (cid:55)→ η 1−x
yields the transition probability, and this latter function has Lipschitz constant η on (0, ∞). Therefore,
the function x (cid:55)→ η
is Lipschitz continuous on [−2Mmax , 2Mmax ], with Lipschitz
constant upper-bounded by ηα exp(2αMmax ). Hence, the Lipschitz constant of the off-diagonal
transition probabilities as a function of the payoffs under the inﬁnity norm is upper-bounded by
2η exp(2αMmax ). Turning our attention to the diagonal elements, we may immediately read off their
Lipschitz constant as being upper-bounded by η−1 × 2ηα exp(2αMmax ) = 2α exp(2αMmax ), and
hence the statement of the lemma follows.

1−xm

(cid:17)

We can now give the full proof of Theorem D.1.
Lipschitz with constant L(α, Mmax ) with respect to the payoffs (Mk (sk , s−k )|k ∈ [K ], s ∈ (cid:81)
Proof of Theorem D.1. By Lemma D.4, we have that all elements of the transition matrix C are
under the inﬁnity norm. Thus, denoting the transition matrix constructed from the empirical payoff
table ˆM by ˆC, we have the following bound for all i, j :

l S l )

|Cij − ˆCij | ≤ εL(α, Mmax ) .

Next, we have that all non-zero elements of Cij are lower-bounded by g(α, η , m, Mmax ) by
Lemma D.3, and hence we have

|Cij − ˆCij | ≤ εL(α, Mmax ) ≤ ε

L(α, Mmax )
g(α, η , m, Mmax )

Cij .

By assumption, the coefﬁcient of Cij on the right-hand side is less than 1/2|S | . We may now appeal
to Theorem D.2, to obtain
|S |−1(cid:88)
for all s ∈ (cid:81)
k S k . Using the trivial bound π(s) ≤ 1 for each s ∈ (cid:81)
k S k yields the result.
With Theorem D.1 established, we can now prove Theorem 3.1.

L(α, Mmax )
g(α, η , m, Mmax )

|π(s) − ˆπ(s)| ≤ 18π(s)ε

(cid:18)|S |

n|S | ,

(cid:19)

n=1

n

.

n=1

n

max

k∈[K ]
s∈S

|Mk (sk , s−k ) − ˆMk (sk , s−k )| <

18L(α, Mmax ) (cid:80)|S |−1
εg(α, η , m, Mmax )

Proof of Theorem 3.1. By Theorem D.1, we have that maxs∈S |π(s) − ˆπ(s)| < ε is guaranteed if
(cid:0)|S |
(cid:1)n|S | <
We separate this into two conditions. Firstly, from the second inequality above, we require
18 (cid:80)|S |−1
(cid:0)|S |
(cid:1)n|S |
2|S |
which is satisﬁed by assumption. Secondly, we have the condition

g(α, η , m, Mmax )
2|S |L(α, Mmax )

|Mk (sk , s−k ) − ˆMk (sk , s−k )| <

(cid:0)|S |
(cid:1)n|S | .
Now, write Ns for the number of trials with the strategy proﬁle s ∈ S . We will next use
the following form of Hoeffding’s inequality: Let X1 , . . . , XN be i.i.d. random variables sup-

ported on [a, b]. Let ε > 0 and δ > 0. Then for N > (b − a)2 log(2/δ)/(2ε2 ), we have

,
18L(α, Mmax ) (cid:80)|S |−1
εg(α, η , m, Mmax )

k∈[K ]
s∈S

max

ε <

n=1

n=1

n

n

(cid:12)(cid:12)(cid:12) > ε
(cid:17)

n=1 Xn − E [X1 ]

N

< δ . Applying this form of Hoeffding’s inequality to the ran-
dom variable ˆM k (sk , s−k ), if we take
(cid:0)|S |
(cid:1)n|S |(cid:17)2

max log(2K |S |/δ)L(α, Mmax )2 (cid:16)(cid:80)|S |−1
648M 2
ε2 g(α, η , m, Mmax )2

max log(2K |S |/δ)
4M 2

(cid:19)2 =

(cid:18)

n=1

n

,

L(α,Mmax )18 (cid:80)|S |−1
εg(α,η ,m,Mmax )

n=1 (|S |

n )n|S |

P (cid:16)(cid:12)(cid:12)(cid:12) 1

(cid:80)N

Ns >

2

then

|Mk (sk , s−k ) − ˆMk (sk , s−k )| <

(cid:0)|S |
(cid:1)n|S |
holds with probability at least 1 − δ/(|S |K ). Applying a union bound over all k ∈ [K ] and s ∈ S
then gives
(cid:0)|S |
(cid:1)n|S |

18L(α, Mmax ) (cid:80)|S |−1
εg(α, η , m, Mmax )

18L(α, Mmax ) (cid:80)|S |−1
εg(α, η , m, Mmax )

|Mk (sk , s−k ) − ˆMk (sk , s−k )| <

max

n=1

n

n=1

n

k∈[K ]
s∈S

with probability at least 1 − δ , as required.

D.2 Proof of Theorem 3.2

Theorem 3.2 (Inﬁnite-α). Suppose all payoffs are bounded in [−Mmax , Mmax ], and that ∀k ∈ [K ]
and ∀s−k ∈ S−k , we have |Mk (σ, s−k ) − Mk (τ , s−k )| ≥ ∆ for all distinct σ, τ ∈ S k , for some
∆ > 0. Let δ > 0. Suppose we construct an empirical payoff table ( ˆMk (s) | k ∈ [K ], s ∈ S )
through Ns i.i.d games for each strategy proﬁle s ∈ S . Then the transition matrix ˆC computed from
payoff table ˆM is exact (and hence all MCCs are exactly recovered) with probability at least 1 − δ , if

Ns > 8∆−2M 2
max log(2|S |K/δ)

∀s ∈ S .

We begin by stating and proving a preliminary result.
Theorem D.5 (Inﬁnite-α conﬁdence bounds). Suppose all payoffs are bounded in [−Mmax , Mmax ].
all distinct σ, τ ∈ S k , for some ∆ > 0. Then if | ˆMk (s) − Mk (s)| < ∆/2 for all s ∈ (cid:81)
Suppose that for all k ∈ [K ] and for all s−k ∈ S−k , we have |Mk (σ, s−k ) − Mk (τ , s−k )| ≥ ∆ for
l S l and all
k ∈ [K ], then we have that ˆC = C.
Proof. From the inequality | ˆMk (s)−Mk (s)| < ∆/2 for all s ∈ S , we have by the triangle inequality
and all distinct σ, τ ∈ S k . Thus, by the assumption of the theorem, ˆMk (σ, s−k ) − ˆM(τ , s−k ) has
the same sign as Mk (σ, s−k ) − Mk (τ , s−k ) for all k ∈ [K ], s−k ∈ S−k , and all distinct σ, τ ∈ S k .
It therefore follows from the expression for ﬁxation probabilities (assuming the Fermi revision
protocol), in the limit as α → ∞, the estimated transition probabilites ˆC exactly match the true
transition probabilities C, and hence the invariant distribution computed from the empirical payoff
table matches that computed from the true payoff table.

that |( ˆMk (σ, s−k ) − ˆM(τ , s−k )) − (Mk (σ, s−k ) − Mk (τ , s−k ))| < ∆ for all k ∈ [K ], s−k ∈ S−k ,

With this result in hand, we may now prove Theorem 3.2.

(cid:80)N

Proof of Theorem 3.2. We use the following form of Hoeffding’s inequality. Let X1 , . . . , XN be i.i.d.
random variables supported on [a, b], and let ε > 0, δ > 0. Then if N > (b − a)2 log(2/δ)/(2ε2 ),
we have P(| 1
n=1 Xi − E[X1 ]| > ε) < δ . Applying this form of Hoeffding’s inequality to an
empirical payoff ˆMk (s), and writing |S | = Πk |S k |, we obtain the result that for

N

Ns >

(2Mmax )2 log(2|S |K/δ)
2(∆/2)2

=

max log(2|S |K/δ)
8M 2
∆2

,

we have
with probability at least 1 − δ/(|S |K ). Applying a union bound over all k ∈ [K ] and all s ∈ (cid:81)
we obtain that if

| ˆMk (s) − Mk (s)| < ∆/2 ,

l S l ,

Ns >

max log(2|S |K/δ)
8M 2
∆2

∀s ∈ (cid:89)

k

S k ,

then by Theorem D.5, we have that the transition matrix ˆC computed from the empirical payoff table
ˆM matches the transition matrix C corresponding to the true payoff table M with probability at least
1 − δ .

E Proofs of results from Section 4

u (s), δ, u, t) (respectively U ( ˆMk
u (s), δ, u, t)) denote

Theorem 4.1. The ResponseGraphUCB algorithm is correct with high probability: Given δ ∈ (0, 1),
for any particular sampling scheme there is a choice of conﬁdence levels such that ResponseGra-
phUCB outputs the correct response graph with probability at least 1 − δ .
Proof. We begin by introducing some notation. For a general strategy proﬁle s ∈ S , denote
the empirical estimator of Mk (s) after u interactions by ˆMk
u (s) and let nt (s) be the number of
interactions of s by time t, and ﬁnally let L( ˆMk
the lower (respectively upper) conﬁdence bound for Mk (s) at some time index t after u interactions
of s, empirical estimator ˆMk
u (s), and conﬁdence parameter δ . We remark that in typical pure
exploration problems, t counts the total number of interactions; in our scenario, since we have a
collection of best-arm identiﬁcation problems, we take a separate time index t for each problem,
counting the number of interactions for strategy proﬁles concerned with each speciﬁc problem. Thus,
for the best-arm identiﬁcation problem concerning two strategy proﬁles s, s(cid:48) , with interaction counts
P(Incorrect output) ≤ (cid:88)
We ﬁrst apply a union bound over each best-arm identiﬁcation problem:
P(Incorrect comparison for strategy proﬁles (σ, s−k ) and (τ , s−k )) .
A standard analysis can now be applied to each best-arm identiﬁcation problem, following the
approach of e.g., Gabillon et al. [17], Kalyanakrishnan et al. [29], Karnin et al. [30]. To reduce
notational clutter, we let s (cid:44) (σ, s−k ) and s(cid:48) (cid:44) (τ , s−k ). Further, without loss of generality taking

ns , ns(cid:48) , we take t = ns + ns(cid:48) .

(σ,s−k ),(τ ,s−k )

Mk (s) > Mk (s(cid:48) ), we have

P(Incorrect ordering of s, s(cid:48) )

(cid:20)

∞(cid:88)

≤ P(∃t, u ≤ t s.t. Mk (s) < L( ˆMk
u (s), δ, u, t) or Mk (s(cid:48) ) > U ( ˆMk
u (s(cid:48) ), δ, u, t))
P(Mk (s) < L( ˆMk
u (s), δ, u, t)) + P(Mk (s(cid:48) ) > U ( ˆMk
u (s(cid:48) ), δ, u, t))

t(cid:88)

≤

.

(cid:21)

t=1

u=1

Note that the above holds for any sampling strategy S . We may now apply an individual concentration
inequality to each of the terms appearing in the sum above, to obtain

P(Incorrect ordering of s, s(cid:48) ) ≤ 2
where f (u, δ, (|S k |)k , t) is an upper bound on the probability of a true mean lying outside a conﬁdence
P(Incorrect output) ≤ |S | (cid:80)K
interval based on u interactions at time t. Thus, overall we have

f (u, δ, (|S k |)k , t) ,

2f (u, δ, (|S k |)k , t) .

∞(cid:88)

t(cid:88)

u=1

t=1

k=1 (|S k | − 1)
2

t=1

u=1

∞(cid:88)

t(cid:88)

|S | (cid:80)K
If f is chosen such that
correctness is complete. It is thus sufﬁcient to choose

k=1 (|S k |−1)
2

t=1

(cid:80)∞

(cid:80)t

f (u, δ, (|S k |)k , t) =

u=1 2f (u, δ, (|S k |)k , t) ≤ δ , then the proof of
π2 |S | (cid:80)K
6δ
k=1 (|S k | − 1)t3

.

Note that this analysis has followed without prescribing the particular form of conﬁdence interval
used, as long as its coverage matches the required bounds above.
Theorem 4.2. The ResponseGraphUCB algorithm, using conﬁdence parameter δ and Hoeffding
conﬁdence bounds, run on an evaluation instance with ∆ = min(sk ,s−k ),(σk ,s−k ) |Mk (sk , s−k ) −
Mk (σk , s−k )| requires at most O(∆−2 log(1/(δ∆))) samples with probability at least 1 − 2δ .

Proof. We adapt the approach of Even-Dar et al. [13], and use the notation introduced in the proof of

Theorem 4.1 First, let ¯U (δ, u, t) = supx [U (x, δ, u, t) − x], and ¯L(δ, u, t) = supx [x − L(x, δ, u, t)].
Mk (s) − Mk (s(cid:48) ) > 2 ¯U (δ, u, t) + 2 ¯L(δ, v , t) ,

Note that if we have counts ns = u and ns(cid:48) = v such that

(7)

then we have

u (s), δ, u, t) − U ( ˆMk
v (s(cid:48) ), δ, v , t) > ˆMk
u (s) − ¯L(δ, u, t) − ( ˆMk
v (s(cid:48) ) + ¯U (δ, v , t))
L( ˆMk
> Mk (s) − 2 ¯L(δ, u, t) − Mk (s(cid:48) ) − 2 ¯U (δ, v , t)
> 0 ,

(a)

where (a) holds with probability at least 1 − 2f (u, δ, (|S k |)k , t). Hence, with probability at least
1 − 2f (u, δ, (|S k |)k , t) the algorithm must have terminated by this point. Thus, if u, v and t are such
that (7) holds, then we have that the algorithm will have terminated with high probability. Writing
∆ = Mk (s) − Mk (s(cid:48) ), with all observed outcomes bounded in [−Mmax , Mmax ], we have

¯U (δ, u, t) = ¯L(δ, u, t) =

max log(2/f (u, δ, (|S k |)k , t))
4M 2
u

.

(cid:114)

(cid:114)

We thus require

(cid:114)

max log(2/f (u, δ, (|S k |)k , t))
max log(2/f (v , δ, (|S k |)k , t))
4M 2
4M 2
∆ > 2
+ 2
u
v
Taking u = v , and using f (u, δ, (|S k |)k , t) =
8u3π2 |S | (cid:80)K
k=1 (|S k | − 1)
3δ

(cid:118)(cid:117)(cid:117)(cid:116) 4M 2

π2 |S | (cid:80)K

(cid:32)

(cid:33)

∆ > 4

log

max

u

.

6δ

k=1 (|S k |−1)t3 as above, we obtain the condition

.

A sufﬁcient condition for this to hold is u = O(∆−2 log( 2
δ∆ )). Thus, if all strategy proﬁles s have
been sampled at least O(∆−2 log( 2
δ∆ )) times, the algorithm will have terminated with probability
at least 1 − 2δ . Up to a log(1/∆) factor, this matches the instance-aware bounds obtained in the
previous section.

F Additional material on ResponseGraphUCB

In this section, we give precise details of the form of the conﬁdence intervals considered in the
ResponseGraphUCB algorithm, described in the main paper.

Hoeffding bounds (UCB).

In cases where the noise distribution on strategy payoffs is known
to be bounded on an interval [a, b], we can use conﬁdence bounds based on the standard Ho-
effding inequality. For a conﬁdence level δ and count index n, and mean estimate x, this in-

terval takes the form (x − (cid:112)(b − a)2 log(2/δ)/2n, x + (cid:112)(b − a)2 log(2/δ)/2n). Optionally,
of the form (x − (cid:112)(b − a)2 log(2/δ)f (t)/n, x + (cid:112)(b − a)2 log(2/δ)f (t)/n), for some function
f : N → (0, ∞).

an additional exploration bonus based on a time index t, measuring the total number of sam-
ples for all strategy proﬁles concerned in the comparison, can be added, yielding an interval

Clopper-Pearson bounds (CP-UCB).

In cases where the noise distribution is known to be
Bernoulli, it is possible to tighten the Hoeffding conﬁdence interval described above, which is valid
for any distribution supported on a ﬁxed ﬁnite interval. The result is the asymmetric Clopper-Pearson
conﬁdence interval: for an empirical estimate x formed from n samples, at a conﬁdence level δ , the
Clopper-Pearson interval [6, 18] takes the form (B (δ/2; nx, n − nx + 1), B (1 − δ/2; nx + 1, n − nx),
where B (p; v , w) is the pth quantile of a Beta(v , w) distribution.

Relaxed variants. As an alternative to waiting for conﬁdence intervals to become fully disjoint
before declaring an edge comparison to be resolved, we may instead stipulate that conﬁdence intervals
need only ε-disjoint (that is, the length of their intersection is < ε). This has the effect of reducing
the number of samples required by the algorithm, and may be practically advantageous in instances
where the noise distributions do not attain the worst case under the conﬁdence bound (for example,
low-variance noise under the Hoeffding bounds); clearly however, such an adjustment breaks any
theoretical guarantees of high-probability correct comparisons.

G Additional material on uncertainty propagation

In this section, we provide details for the high-level approach outlined in Section 5, in particular
giving more details regarding the reduction to response graph selection (in particular, selecting
directions of particular edges within the response graph), and then using the PageRank-style reduction
to obtain a CSSP policy optimization problem.
Reduction to edge direction selection. The inﬁnite-α α-Rank output is a function of the payoff
table M only through the inﬁnite-α limit of the corresponding transition matrix C deﬁned in (1);
This limit is determined by binary payoff comparisons for pairs of strategy proﬁles differing in a
single strategy. We can therefore summarize the set of possible transition matrices C which are
compatible with the payoff bounds L and U by compiling a list E of response graph edges for which
payoff comparisons (i.e., response graph edge directions) are uncertain under L and U. Note that
it may be possible to obtain even tighter conﬁdence intervals on π(s) by keeping track of which
combinations of directed edges in E are compatible with an underlying payoff table M itself, but
by not doing so we only broaden the space of possible response graphs, and hence still obtain valid
conﬁdence bounds. The conﬁdence interval for π(s) could thus be obtained by computing the output
of inﬁnite-α α-Rank for each transition matrix C that arises from all choices of edge directions for
the uncertain edges in E . However, this set is generally exponentially large in the number of strategy
proﬁles, and thus intractable to compute. The next step is to reduce this problem to one which is
solvable using standard dynamic programming techniques to avoid this intractability.
Reduction to CSSP policy optimization. We now use a reduction similar to that used in the
PageRank literature for optimizing stationary distribution mass [10], encoding the problem above
as an SSP optimization problem. For a transition matrix C, let (Xt )∞
Markov chain over the space of strategy proﬁles S , and deﬁne the mean return times λ ∈ [0, ∞]S by
t=0 denote the corresponding
λ(u) = E [inf {t > 0|Xt = u}|X0 = u], for each u ∈ S . By basic Markov chain theory, when C is
such that s is recurrent, the mass attributed to s under the stationary distribution supported on the MCC
containing s is equal to 1/λ(s); thus, maximizing (respectively, minimizing) π(s) over a set of transi-
whereby ϕ(s) = λ(s); then ϕ = (cid:101)Cϕ + 1, where (cid:101)C is the substochastic matrix given by setting the
tion matrices is equivalent to minimizing (respectively, maximizing) λ(s). Deﬁne the mean hitting
time of s starting at u for all u ∈ S by ϕ ∈ [0, ∞]S , where ϕ(u) = E [inf {t > 0|Xt = s}|X0 = u],
column of C corresponding to state s to the zero vector, and 1 ∈ RS is the vector of ones.
Note that ϕ has the interpretation of a value function in an SSP problem, wherein the absorbing
at state s over the set of compatible transition matrices (cid:101)C. We turn this into a standard control
state is s and all transitions before absorption incur a cost of 1. The original problem of maximizing
(respectively, minimizing) π(s) is now expressed as minimizing (respectively, maximizing) this value
problem by specifying the action set at each state u ∈ S as P ({e ∈ E |u ∈ e}), the powerset of the
(cid:101)C corresponding to u. Crucially, the action choices cannot be made independently at each state; if
set of uncertain edges in E incident to u; the interpretation of selecting a subset U of these edges
is that precisely the edges in U will be selected to ﬂow out of u; this then fully speciﬁes the row of
at state u, the uncertain edge between u and u(cid:48) is chosen to ﬂow in a particular direction, then at
state u(cid:48) a consistent action must be chosen, so that the actions at both states agree on the direction

of the edge, thus leading to a constrained SSP optimization problem. We refer to this problem
as CSSP(S, L, U, s). While general solution of CSSPs is intractable, we recall the statement of
Theorem 5.1 that it is sufﬁcient to consider the unconstrained version of CSSP(S, L, U, s) to recover
the same optimal policy.

Theorem 5.1. The unconstrained SSP problem given by removing the action consistency constraints
of CSSP(S, L, U, s) has the same optimal value as CSSP(S, L, U, s).

We conclude by restating the ﬁnal statements of Section 5. In summary, the general approach for
ﬁnding worst-case upper and lower bounds on inﬁnite-α α-Rank ranking weights π(s) for a given
strategy proﬁle s ∈ S is to formulate the unconstrained SSP described above, ﬁnd the optimal policy
(using, e.g., linear programming, policy or value iteration), and then use the inverse relationship
between mean return times and stationary distribution probabilities in recurrent Markov chains to
obtain the bound on the ranking weight π(s) as required.

G.1 MCC detection

Here, we outline a straightforward algorithm for determining whether inf L≤ ˆM≤U π ˆM (s) = 0,
without recourse to the full CSSP reduction described in Section 5. First, we use L and U to split
the edges of the response graph into two disjoint sets EU , edges for which the direction is uncertain
under L and U, and EC , the edges with certain direction. We then construct the set FA ⊆ S of forced
ancestors of s; that is, the set of strategy proﬁles that can reach s using a path of edges contained
in EC , including s itself. We also deﬁne the set FD ⊆ S of forced descendents of s; that is, the set
of strategy proﬁles that can be reached from s using a path of edges in EC , including s itself. If
FD (cid:54)⊆ FA , then s can clearly be made to lie outside an MCC by setting all edges in EU incident to
FD \ FA to be directed into FD . Then there are no edges directed out of FD \ FA , so this set contains
at least one MCC. There also exists a path from s to FD \ FA , and hence s cannot lie in an MCC, so
inf L≤ ˆM≤U π ˆM (s) = 0. If, on the other hand, FD ⊆ FA , we may set all uncertain edges between
FA and its complement to be directed away from FA . We then iteratively compute two sets: FA,out ,
the set of proﬁles in FA for which there exists a path out of FA , and its complement FA \ FA,out .
Any uncertain edges between these two sets are then set to be directed towards FA,out , and the sets
are then recomputed. This procedure terminates when either FA,out = FA , or there are no uncertain
edges left between FA and FA,out . If at this point there is no path from s out of FA , we conclude that
s must lie in an MCC, and so inf L≤ ˆM≤U π ˆM (s) > 0, whilst if such a path does exist, then s does
not lie in an MCC, so inf L≤ ˆM≤U π ˆM (s) = 0.

G.2 Proof of Theorem 5.1

Theorem 5.1. The unconstrained SSP problem given by removing the action consistency constraints
of CSSP(S, L, U, s) has the same optimal value as CSSP(S, L, U, s).
Proof. Let (cid:101)C be the substochastic matrix associated with the optimal unconstrained policy, and
differing only in index k , such that either (i) at state u, the edge direction is chosen to be u → v , and
suppose there are two action choices that are inconsistent; that is, there exist strategy proﬁles u and v
at state v , the edge direction is chosen to be v → u; or (ii) at state u, the edge direction is chosen to
be v → u, and at state v , the edge direction is chosen to be u → v . We show that in either case, there
is a policy without this inconsistency that achieves at least as good a value of the objective as the
inconsistent policy.
policy, and suppose without loss of generality that ϕ(v) ≥ ϕ(u). Let (cid:101)D be the substochastic matrix
We consider ﬁrst case (i). Let ϕ be the associated expected costs under the inconsistent optimal
obtained by adjusting the action at state u so that the edge direction between u and v is v → u,
consistent with the action choice at v . Denote the expected costs under this new transition matrix (cid:101)D
by µ. We can compare ϕ and µ via the following calculation. By deﬁnition, we have ϕ = (cid:101)Cϕ + 1

and µ = (cid:101)Dµ + 1. Thus, we compute

ϕ − µ = ( (cid:101)Cϕ + 1) − ( (cid:101)Dµ + 1)
= (cid:101)Cϕ − (cid:101)Dµ
= (cid:101)Cϕ − (cid:101)Dϕ + (cid:101)Dϕ − (cid:101)Dµ
= ( (cid:101)C − (cid:101)D)ϕ + (cid:101)C(ϕ − µ)
=⇒ ϕ − µ = (I − (cid:101)D)−1 ( (cid:101)C − (cid:101)D)ϕ .

In this ﬁnal line, we assume that I − (cid:101)D is invertible. If it is not, then it follows that (cid:101)D is a strictly
stochastic matrix, thus corresponding to a policy in which no edges ﬂow into s. From this we
immediately deduce that the minimal value of ϕ(s) is ∞; hence, we may assume I − (cid:101)D is invertible
(u, u), and (u, v), and thus the vector ( (cid:101)C − (cid:101)D)ϕ has a particularly simple form; all coordinates
in what follows. Assume for now that s (cid:54)∈ {u, v}. Now note that (cid:101)C and (cid:101)D differ only in two elements:
are 0, except coordinate u, which is equal to η(ϕ(v) − ϕ(u)) ≥ 0. Finally, observe that all entries
of (I − (cid:101)D)−1 = (cid:80)∞
k=0 (cid:101)Dk are non-negative, and hence we obtain the element-wise inequality
ϕ − µ ≥ 0, proving that the policy associated with (cid:101)D is at least as good as (cid:101)C, as required. The
argument is entirely analogous in case (ii), and when one of the strategies concerned is s itself. Thus,
the proof is complete.

H Additional empirical details and results

H.1 Experimental procedures and reproducibility

We detail the experimental procedures here.
The results shown in Fig. 1.1b are generated by computing the upper and lower payoff bounds given
a mean payoff matrix and conﬁdence interval size for each entry, then running the procedure outlined
in Section 5.
As Fig. 4.1 shows an intuition-building example of the ResponseGraphUCB outputs, it was computed
by ﬁrst constructing the payoff table speciﬁed in the ﬁgure, then running ResponseGraphUCB with
the parameters speciﬁed in the caption. The algorithm was then run until termination, with the
strategy-wise sample counts in Fig. 4.1 computed using running averages.
The ﬁnite-α α-Rank results in Fig. 6.1 for every combination of α and  are computed using 20, 5,
and 5 independent trials, respectively, for the Bernoulli, soccer, and poker meta-games. The same
number of trials applies for every combination of δ and ResponseGraphUCB in Fig. 6.3.
The ranking results shown in Fig. 6.3 are computed for 10 independent trials for each game and each
δ .
The parameters swept in our plots are the error tolerance, δ , and desired error . The range of
values used for sweeps is indicated in the respective plots in Section 6, with end points chosen such
that sweeps capture both the high-accuracy/high-sample complexity and low-accuracy/low-sample
complexity regimes.
The sample mean is used as the central tendency estimator in plots, with variation indicated as the
95% conﬁdence interval that is the default setting used in the Seaborn visualization library that
generates our plots. No data was excluded and no other preprocessing was conducted to generate
these plots.
No special computing infrastructure is necessary for running ResponseGraphUCB, nor for reproduc-
ing our plots; we used local workstations for our experiments.

H.2 Full comparison plots

As noted in Section 4, sample complexity and ranking error under adaptive sampling are of particular
interest. To evaluate this, we consider all variants of ResponseGraphUCB in Fig. H.1.

(a) Bernoulli games.

(b) Soccer meta-game.

(c) Poker meta-game.

Figure H.1: ResponseGraphUCB performance metrics versus error tolerance δ for all games. First
and second rows, respectively, show the # of interactions required and response graph edge errors.

H.3 Exploiting knowledge of symmetry in games

(a) Reconstructed response graph.

(b) Strategy-wise sample counts.

Figure H.2: ResponseGraphUCB (δ = 0.1, S =UE, C =UCB) evaluated on the game with payoff
tables shown in Fig. 4.1a, with knowledge of game symmetry exploited to reduce the total number of
samples needed from 185 to 15 and sampling conducted for only a single strategy proﬁle, (0, 1).

Symmetric games. Let SymK denote the symmetric group of degree K over all players. A game
is said to be symmetric if for any permutation ρ ∈ SymK , strategy proﬁle (s1 , . . . , sK ) ∈ S and

index k ∈ [K ], we have Mk (s1 , . . . , sK ) = Mρ(k) (sρ(1) , . . . , sρ(K ) ).

Exploiting symmetry in ResponseGraphUCB. Knowledge of the symmetric constant-sum nature
of a game (e.g., Fig. 4.1a) can signiﬁcantly reduce sample complexity in ResponseGraphUCB: this
knowledge implies that payoffs for all symmetric strategy proﬁles are known a priori (e.g., payoffs
for (0, 0) and (1, 1) are 0.5 in this example); moreover, each observed outcome for a strategy proﬁle
(s1 , . . . , sK ) yields a ‘free’ observation of Mρ(k) (sρ(1) , . . . , sρ(K ) ) for all permutations ρ ∈ SymK ,
strategy proﬁles (s1 , . . . , sK ) ∈ S , and players k ∈ [K ]. For the example in Fig. 4.1a, the symmetry-
exploiting variant of the algorithm is able to reconstruct the true underlying response graph using
only 15 samples of a single strategy proﬁle (0, 1).
In Fig. H.2, we evaluate ResponseGraphUCB on the game shown in Fig. 4.1a, this time exploiting the
knowledge of game symmetry as discussed in Section 4.1. Note that Figs. H.2a and H.2b should be
compared, respectively, with Figs. 4.1b and 4.1c in the main paper. Conﬁdence bounds corresponding
to the symmetry-exploiting sampler (Fig. H.2a) are guaranteed to be tighter than the non-exploiting
sampler (Fig. 4.1b), and so typically we can expect the former to require fewer interactions to arrive
at a ranking conclusion with the same conﬁdence as the latter (under the condition that the payoffs
really are symmetric, as is the case in win/loss two-player games). This is observed in this particular
example, where Fig. 4.1b took 185 interactions to solve, while Fig. H.2a took only 15 samples of a
single strategy proﬁle (0, 1) to correctly reconstruct the response graph.

H.4 Kendall’s distance for partial rankings

We use Kendall’s distance for partial rankings [14] when comparing two rankings, r and ˆr (e.g., as
done in Fig. 6.3)
Consider a pair of partial strategy rankings r and ˆr (i.e., wherein tied rankings are allowed). Deﬁne a
ﬁxed parameter p. The Kendall distance with penalty parameter p is deﬁned,

(cid:88)

{i,j}∈[|S |]

K (r, ˆr ; p) =

¯Ki,j (r, ˆr ; p),

where ¯Ki,j (r, ˆr; p) is:

• 0 when i, j are in distinct buckets in both r, ˆr , but in the same order (e.g., ri > rj and ˆri > ˆrj )
• 1 when i, j are in distinct buckets in both r, ˆr , but in the reverse order (e.g., ri > rj and ˆri < ˆrj )
• 0 when i, j are in the same bucket in both r and ˆr

• p when i, j are in the same bucket in one of r or ˆr , but different buckets in the other.
It can be shown that Kendall’s distance is a metric when p ∈ [0.5, 1]. We use p = 0.5 in our
experiments.

H.5 Preliminary experiments on collaborative ﬁltering-based approaches

The pairing of bandit algorithms and α-Rank seems a natural means of computing rankings in
settings where, e.g., one has a limited budget for adaptively sampling match outcomes. Our use of
bandit algorithms also leads to analysis which is ﬂexible enough to be able to deal with K -player
general-sum games. However, approaches such as collaborative ﬁltering may also fare well in their
own right. We conduct a preliminary analysis of this in here, speciﬁcally for the case of two-player
win-loss games.
For such games, the meta-payoff table is given by a matrix M with all entries lying in (0, 1) (encoding
loss as payoff 0 and win as payoff 1). Taking a matrix completion approach, we might attempt to
reconstruct a low-rank approximation of the payoff table from an incomplete list of (possible noisy)
structure include: (i) the payoff matrix itself; (ii) the logit matrix Lij = log(Mij /(1 − Mij )); and
payoffs, and then run α-Rank on the reconstructed payoffs. Possible candidates for the low-rank
(iii) the odds matrix Oij = exp(Lij ). In particular, Balduzzi et al. [3] make an argument for the
(approximate) low-rank structure of the logit matrix in many applications of interest.

Figure H.3: Ranking errors (Kendall’s distance w.r.t. ground truth) from completion of, respectively,
the sparse payoffs, logits, and odds matrices for Soccer dataset. 20 trials per combo of assumed
matrix ranks and observation rates/density.

We conduct preliminary experiments on this in Fig. H.3, implementing matrix completion calculations
via Alternating Minimization [27]. We compare here the resulting α-Rank errors for the three
reconstruction approaches for the Soccer meta-game. We sweep across the observation rates of payoff
matrix entries and the matrix rank assumed in the reconstruction. Interestingly, conducting low-rank
approximation on the logits (as opposed to the odds) matrix generally yields the lowest ranking error.
Overall, the bandit-based approach may be more suitable when one can afford to play all strategy
proﬁles at least once, whereas matrix completion is perhaps more so when this is not feasible. These
results, we believe, warrant additional study of the performance of related alternative approaches in
future work.

