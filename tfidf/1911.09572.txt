Automatically Generating Macro Research Reports from a Piece of News

Wenxin Hu1 ∗ , Xiaofeng Zhang2 , Gang Yang3

1Harbin Institute of Technology, Shenzhen
huwenxin@stu.hit.edu.com, zhangxiaofeng@hit.edu.com, yanggang@stu.hit.edu.com,

9
1
0
2

v
o

N

1
2

]

L

C

.

s

c

[

1
v
2
7
5
9
0

.

1
1
9
1

:

v

i

X

r

a

Abstract

Automatically generating macro research reports
from economic news is an important yet challeng-
ing task. As we all know, it requires the macro an-
alysts to write such reports within a short period
of time after the important economic news are re-
leased. This motivates our work, i.e., using AI tech-
niques to save manual cost. The goal of the pro-
posed system is to generate macro research reports
as the draft for macro analysts. Essentially, the core
challenge is the long text generation issue. To ad-
dress this issue, we propose a novel deep learning
technique based approach which includes two com-
ponents, i.e., outline generation and macro research
report generation. For the model performance eval-
uation, we ﬁrst crawl a large news-to-report dataset
and then evaluate our approach on this dataset, and
the generated reports are given for the subjective
evaluation.

1 Introduction & Related Works

Nature language generation (NLG) is a fundamental yet chal-
lenging task in the domain of Natural Language Process-
ing (NLP). NLG has long been studied in many applica-
tions such as dialogue system [Akasaki and Kaji, 2017;
Serban et al., 2016], machine translation [Chiang, 2010] and
summarization [See et al., 2017; Chen and Bansal, 2018].
One emerging application of NLG is the macro research re-
port generation which usually takes short news as the input
and outputs long reports.
Generally, the macro research reports could be classiﬁed
into different categories such as company level research re-
port, industry level research report and macro research report.
Different type of reports may have different requirements.
For macro research report, it usually requires the macro ana-
lysts or economists to release their reports as soon as possi-
ble after some important macro-economic news are released.
This is very competitive and requires a high manual cost. This
motivates us to propose this approach to automatically gener-
ate macro research reports from news.

∗Contact Author

Conventionally, text generation is one of the most impor-
tant NLP tasks. Many research efforts have been made to-
wards this end such as poem generation [Liu et al., 2017]
and novel writing [Yang et al., 2011]. There are three
main streams of generative based approaches, e.g., varia-
tional autoencoder based approach [Kingma and Welling,
2014], auto-regressive based approach and generative ad-
versarial networks based approach. Many attempts have
been made to utilize VAEs [Bowman et al., 2016; Miao et
al., 2016] and GANs [Kusner and Hernandezlobato, 2016;
Yu et al., 2017] for the text generation task. However, the
length of the output texts of these approaches are usually less
than 40 words [Guo et al., 2018]. That is, we need to seek
a novel approach for this task to generate a much longer text
from very short textual data.
Recently, sequence-to-sequence model [Dusek and Jurci-
cek, 2016] is proposed and this approach is a two-phase one.
In the ﬁrst phase, it generates a semantic representation ac-
cording to the input sentences. In the second phase, many
coherent sentences are generated according to the input sen-
tences. However, this approach has two major shortcomings.
First, the length of its output is comparably short (less than
140 words) which is not suitable for the generation of long
macro-research reports. Second, the intermediate output of
each iteration is susceptible to the output of its previous re-
sults which results in many repeat terms in the ﬁnal output.
To cope with this challenge, we propose this novel ap-
proach to tackle the issue. If we generate long reports from
very short text, there must exists many hidden information
which is hard to directly model. Intuitively, macro analysts
would draft outlines before writing the macro research re-
ports. Inspired by this, we propose to learn the outlines from
a pair of news and reports via deep neural networks. Then, we
can generate the reports from outlines (feature dimension de-
creased) via a decoder process (feature dimension increased).
The global loss function could then be proposed to minimize
the loss from the input news to the generated reports.

2 The Proposed Approach

The framework of the proposed approach is illustrated in Fig-
ure 1. The general idea is brieﬂy illustrated as follows. From
the bottom, we have a pair of input, e.g., the input news and
the corresponding reports. Through an attention component,
more important terms are kept to form the outline of this re-

 
 
 
 
 
 
port. The input news are embedded to word vectors and are
sent to a Bi-LSTM module to learn the sequence order. The
learned outline is then used to generate reports. We will detail
our approach in the following subsections.

Figure 1: Framework of the proposed approach.

Let X = {x1 , . . . , xm } denote the input news with
xi is one of the N items contained in vocabulary V =
{v1 , . . . , vn}, Y = {y1 , . . . , yL} denote the corresponding
macro research reports and O = {o1 , . . . , oL } denote the out-
line generated with L as its length.

2.1 Outline Generation

The outline is generated using the framework of sequence-
to-sequence model [Xing et al., 2017]. The input news is
ﬁrst embedded into a word vector and then is fed into a Bi-
LSTM model to acquire its hidden state H e = {he
t },
calculated as

1 , . . . , he

t−1 )(cid:107)f←−−enc (xt , he
he
t = f−−→enc (xt , he

t+1 ).

(1)

Then, the H e is used to generate the outline.
To generate the outline, we adopt LSTM network to esti-
mate the probability distribution over the vocabulary set. It
outputs the next token based on the generated summary (de-
noted as a low dimensional vector) of input report Y . At each
step t, the current hidden state st depends on both the token
ot−1 and the hidden state st−1 , written as

sd
t = fdec (ˆot−1 , sd
t−1 )

(2)
To assign higher weights to more important terms, an at-
tention mechanism is introduced here. Accordingly, the dot
product of encoder hidden state he
j and decoder hidden state
i is calculated as

sd

(3)
Then, a softmax function is employed to calculate the weights
based on the scores. The results are given as

j · sd
eij = score(he
j , sd
i ) = he
i .
(cid:80) exp(eij )
exp(eij )
αij · he

T(cid:88)

αij =

ci =

j

(4)

(5)

1

(6)
where ci denote the context vector, ˆst denote the attention
state, and st is the decoder hidden state. Then, the probability

ˆst = tanh(Wc [ct ; st ])

o(cid:88)

of the next token (term to be generated) ot could be computed
as P (ot |o<t , x) = sof tmax(Wc · ˆst ). The objective function
of this process is given as

ˆLoutline (θE , θO ; x) =

−logP (oi = o∗ |S )

(7)

2.2 Macro Research Report Generation

i=1

To generate the ﬁnal report, we use both the news and the
outline trained to generate the report. This process is also
treated as a decoding process. The VAE based approach
is employed as the decoding component. For the input of
VAE, a mixture of news and outline is treated as the in-
put. The original VAE model tends to learn the distribu-
tion of latent variables and samples data from this distribu-
tion. Its logarithm likelihood of sampled data x is acquired
by by maximizing the ELBO problem, given as logP (X ) ≥

Eq(z |x) [logp(x|z )] − K L(q(z |x)||p(z )). In this stage, the de-

coder hidden state hd is updated as

hd
t = fdec ( ˆyt−1 , hd
t−1 )

Similar objective function could be acquired, given as

y(cid:88)

(8)

(9)

ˆLreport (θR ; o) =

−logP (yi = y∗ |H )

i=1

At last, the global loss function of the model could be de-
signed by considering the cost of outline generation Eq. 7
and the cost of report generation Eq. 9. Accordingly, the
global objective function is given as

ˆLmodel (θE , θO , θR ; x) = ˆLoutline + ˆLreport

(10)
Experiments are evaluated on the crawled dataset, and a pair
of news and the generated report are given in Figure 2 for
subjective evaluation.

Figure 2: Sample results: one input news and the corresponding
generated report.

3 Conclusion & Future Work

In this paper, we propose an approach which can automati-
cally generating macro research report from economic news.
This task is very challenging and we output the reports for
subjective evaluations.
In fact, we already computed the
BLEU value which is widely adopted criterion in NLP. How-
ever, we have not implemented other state-of-the-art ap-
proaches due to the limit time. In the near future, we will
implement the BLEU as well as other criteria for more ap-
proaches to evaluate model performance.

Output ReportVAE Generative ModelOutlineAttention DecoderInput NewsInput ReportDecoder Stage TwoDecoder Stage OneEmbeddingEmbeddingBi-LSTMLSTM6月10-11日央行、统计局和海关总署将公布5月宏观经济运行数据,我们对于5月经济数据的前瞻分析如下:5月PMI环比回落1.8个百分点公司国六尾气后处理业务有望持续放量，国六排放标准落地有望加速，材料业务有望成为新的业绩增长点盈利预测与投资评级。维持“买入”评级风险提示：电子胶带下游材料需求不及预期；显示器件市场价格大幅波动的风险；国六标准实施进度低于预期的风险；国六排放标准的升级的风险；电子特气市场竞争风险；产业政策变化的风险风险；专利诉讼风险；本次募投项目进展不达预期的风险；本次募投项目进展不及预期的风险；电子特气产业化进度不及预期的风险；中美贸易摩擦的风险Input NewsGeneration ReportOnJune10-11,thecentralbank,thestatisticalbureauandtheGeneralAdministrationofCustomswillreleasethemacroeconomicoperationdataforMay.Ourforward-lookinganalysisoftheMayeconomicdataisasfollows:MayPMIfell1.8percentagepointsfromthepreviousmonth.Thecompany‘snationalsix-tailgasafter-treatmentbusinessisexpectedtocontinuetobeheavy,andthenationalsixemissionstandardsareexpectedtoaccelerate,andthematerialsbusinessisexpectedtobecomeanewperformancegrowthpointprofitforecastandinvestmentrating.Maintain“Buy”ratingriskwarning:Thedemandfordownstreammaterialsofelectronictapeislessthanexpected;theriskoflargefluctuationsinthemarketpriceofdisplaydevices;thelower-than-expectedriskoftheimplementationofthenationalstandard;theriskofupgradingthenationalsixemissionstandards;Competitiverisk;riskofindustrialpolicychange;patentlitigationrisk;theprogressofthefundraisingprojectisnotuptotheexpectedrisk;theprogressofthefundraisingprojectislessthanexpected;theprogressofelectronicspecialgasindustrializationislessthanexpected;ChinaandtheUnitedStatesTradefrictionriskEnglishneural response generation. national conference on artiﬁ-
cial intelligence, pages 3351–3357, 2017.
[Yang et al., 2011] Jing Yang, Xu Yu, Zhi Qiang Xie, and
Jian Pei Zhang. A novel virtual sample generation method
based on gaussian distribution. Knowledge-Based Sys-
tems, 24(6):740–748, 2011.
[Yu et al., 2017] Lantao Yu, Weinan Zhang, Jun Wang, and
Yong Yu. Seqgan: Sequence generative adversarial nets
with policy gradient. national conference on artiﬁcial in-
telligence, pages 2852–2858, 2017.

References

[Akasaki and Kaji, 2017] Satoshi Akasaki and Nobuhiro
Kaji. Chat detection in an intelligent assistant: Combin-
ing task-oriented and non-task-oriented spoken dialogue
systems. In Meeting of the Association for Computational
Linguistics, 2017.
[Bowman et al., 2016] Samuel R Bowman, Luke Vilnis,
Oriol Vinyals, Andrew M Dai, Rafal Jozefowicz, and
Samy Bengio. Generating sentences from a continuous
space.
conference on computational natural language
learning, pages 10–21, 2016.
[Chen and Bansal, 2018] Yenchun Chen and Mohit Bansal.
Fast abstractive summarization with reinforce-selected
sentence rewriting. meeting of the association for com-
putational linguistics, 1:675–686, 2018.
[Chiang, 2010] David Chiang. Learning to translate with
source and target syntax.
In Meeting of the Association
for Computational Linguistics, 2010.
[Dusek and Jurcicek, 2016] Ondˇrej Dusek and Filip Jurci-
cek. Sequence-to-sequence generation for spoken dialogue
via deep syntax trees and strings. meeting of the associa-
tion for computational linguistics, pages 45–51, 2016.
[Guo et al., 2018] Jiaxian Guo, Sidi Lu, Han Cai, Weinan
Zhang, Jun Wang, and Yong Yu. Long text generation via
adversarial training with leaked information. national con-
ference on artiﬁcial intelligence, pages 5141–5148, 2018.
[Kingma and Welling, 2014] Diederik P Kingma and Max
Welling. Auto-encoding variational bayes.
international
conference on learning representations, 2014.
[Kusner and Hernandezlobato, 2016] Matt
J Kusner and
Jose Miguel Hernandezlobato. Gans for sequences of
discrete elements with the gumbel-softmax distribution.
arXiv: Machine Learning, 2016.
[Liu et al., 2017] Qinyun Liu, Zou Lin, Hongming Che,
Haiyun Wang, Yunzhi Jin, and Hongji Yang. A creative
computing based inspiration assistant to poem generation.
In International Conference on International Symposium
on Pervasive Systems, 2017.
[Miao et al., 2016] Yishu Miao, Lei Yu, and Phil Blunsom.
Neural variational inference for text processing.
interna-
tional conference on machine learning, pages 1727–1736,
2016.
[See et al., 2017] Abigail See, Peter J Liu, and Christopher D
Manning. Get to the point: Summarization with pointer-
generator networks. meeting of the association for compu-
tational linguistics, 1:1073–1083, 2017.
[Serban et al., 2016] Iulian Vlad Serban, Tim Klinger, Ger-
ald Tesauro, Kartik Talamadupula, Bowen Zhou, Yoshua
Bengio, and Aaron C Courville. Multiresolution recur-
rent neural networks: An application to dialogue response
generation. national conference on artiﬁcial intelligence,
pages 3288–3294, 2016.
[Xing et al., 2017] Chen Xing, Wei Wu, Yu Wu, Jie Liu,
Yalou Huang, Ming Zhou, and Weiying Ma. Topic aware

