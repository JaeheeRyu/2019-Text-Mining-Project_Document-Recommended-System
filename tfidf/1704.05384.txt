Online Weighted Matching: Breaking the 1/2 Barrier

Matthew Fahrbach∗ 1 and Morteza Zadimaghaddam† 2

1School of Computer Science, Georgia Institute of Technology
2Google Zürich

November 22, 2019

gave an elegant algorithm in the unweighted case that achieves a tight competitive ratio of 1 − 1/ . In
Abstract
greedy algorithm is 1/2 competitive, and that this is tight for deterministic algorithms. We present the
Online matching and its variants are some of the most fundamental problems in the online algorithms
literature. In this paper, we study the online weighted bipartite matching problem. Karp et al. (STOC 1990)
(cid:27)rst randomized algorithm that breaks this long-standing 1/2 barrier and achieves a competitive ratio of
at least 0.501. In light of the hardness result of Kapralov et al. (SODA 2013) that restricts beating a 1/2
the weighted case, however, we can easily show that no competitive ratio is obtainable without the
commonly accepted free disposal assumption. Under this assumption, it is not hard to prove that the
competitive ratio for the monotone submodular welfare maximization problem, our result can be seen as
strong evidence that solving the weighted bipartite matching problem is strictly easier than submodular
welfare maximization in the online setting. Our approach relies on a very controlled use of randomness,
which allows our algorithm to safely make adaptive decisions based on its previous assignments.

9
1
0
2

v
o

N

1
2

]

S

D

.

s

c

[

2
v
4
8
3
5
0

.

4
0
7
1

:

v

i

X

r

a

∗ Email: matthew.fahrbach@gatech.edu. Supported in part by an NSF Graduate Research Fellowship under grant DGE-1650044.

†Email: zadim@google.com.

 
 
 
 
 
 
Contents

1 Introduction
1.1 Our Contributions .
.
.
.
.
.
.
.
.
.
.
1.2 Related Works .
.
.
.
.
.
.
.
.
.
.
.
.
2 Preliminaries
3 The StochasticGreedy Algorithm
Intuition for Breaking the 1/2 Barrier .
3.1 Variable Descriptions .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
3.2 Case I: Lines 13–30 and Adaptive Assignments .
3.3 Case II: Lines 31–42 .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
3.4
.
.
.
.
.
.
4 Analysis of the Competitive Ratio
4.1 Outline of the Main Proof .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
4.2 Mechanism for Distributing Excess Marginal Gain .
.
.
.
.
4.3
Lower Bounding the Excess Allocated to Each Impression .

1
1
2

.
.

.
.

.
.

.
.

.
.

.
.

.
.

.
.

.
.

.
.

.
.

.
.

.
.

.
.

.
.

.
.

.
.

.
.

.
.

.
.

.
.

.
.

.
.

.
.

.
.

.
.

.
.

.
.

.
.

.
.

3

4
5
7
7
8

.
.
.
.

.
.
.
.

.
.
.
.

.
.
.
.

.
.
.
.

.
.
.
.

.
.
.
.

.
.
.
.

.
.
.
.

.
.
.
.

.
.
.
.

.
.
.
.

.
.
.
.

.
.
.
.

.
.
.
.

.
.
.
.

.
.
.
.

.
.
.
.

.
.
.
.

.
.
.
.

.
.
.
.

.
.
.
.

.
.
.
.

.
.
.
.

9
9
11
12

.
.
.

.
.
.

.
.
.

.
.
.

.
.
.

.
.
.

.
.
.

.
.
.

.
.
.

.
.
.

.
.
.

.
.
.

.
.
.

.
.
.

.
.
.

.
.
.

.
.
.

.
.
.

5 Conclusion

16

A Missing Analysis from Section 3
A.1 Proof of Lemma 3.1 .
.
.
.
.
A.2 Proof of Lemma 3.2 .
.
.
.
.

20
20
20

.
.

.
.

.
.

.
.

.
.

.
.

.
.

.
.

.
.

.
.

.
.

.
.

.
.

.
.

.
.

.
.

.
.

.
.

.
.

.
.

.
.

.
.

.
.

.
.

.
.

.
.

.
.

.
.

.
.

.
.

.
.

.
.

.
.

.
.

.
.

B Missing Analysis from Section 4
B.1 Proof of Lemma 4.6 .
.
.
.
.
B.2 Proof of Lemma 4.7 .
.
.
.
.

22
23
25

.
.

.
.

.
.

.
.

.
.

.
.

.
.

.
.

.
.

.
.

.
.

.
.

.
.

.
.

.
.

.
.

.
.

.
.

.
.

.
.

.
.

.
.

.
.

.
.

.
.

.
.

.
.

.
.

.
.

.
.

.
.

.
.

.
.

.
.

.
.

1 Introduction

Matchings are fundamental structures in graph theory that play a crucial role in combinatorial optimization.
An enormous amount of e(cid:29)ort has gone into designing e(cid:28)cient algorithms for (cid:27)nding maximum matchings
in terms of cardinality or the total weight of its allocation. In particular, matchings in bipartite graphs have
found countless applications in settings where it is desirable to assign entities from one set to those in
another set (e.g., matching students to schools, physicians to hospitals, computing tasks to servers, and
matching a set  of impressions that arrive one by one to a set  of advertisers that are given in advance.
impressions in online media to advertisers). Due to the tremendous growth of matching markets in digital
domains, e(cid:28)cient online matching algorithms have become increasingly important. In particular, search
engine companies have created opportunities for online matching algorithms to have enormous impact
in multi-billion dollar advertising markets. Motivated by these applications, we consider the problem of
When an impression arrives, its edges to the advertisers are revealed and an irrevocable decision has to be
of 1 − 1/ , and they also proved that this is the best competitive ratio that can be achieved.
made about which advertiser the impression should be assigned to. Karp et al. [KMT11] gave an elegant
online bipartite weighted matching problem, each with one advertiser  and two impressions. The weight
online algorithm called Ranking to (cid:27)nd matchings in unweighted bipartite graphs with a competitive ratio
the (cid:27)rst instance and , for some arbitrarily large , in the second instance. The online algorithm has no
of the (cid:27)rst impression to  is 1 in both instances, and the weight of the second impression to  is zero in
The situation in the weighted case is much less clear. This is partly due to the fact that no competitive
algorithm can be designed without the free disposal assumption. To see this, consider two instances of the
impression arrives, the algorithm must decide whether or not to assign this impression to  . Not assigning
it gives a competitive ratio of zero for the (cid:27)rst instance, and assigning it gives a competitive ratio of 1/,
which can be arbitrarily small, for the second. Note that assigning both impressions to  is not an option in
way to distinguish between these two instances, even after the (cid:27)rst impression arrives. When the (cid:27)rst
this setting. This example is the primary motivation for allowing assignments of multiple impressions to a
impression to any given advertiser  ∈ . However, instead of achieving the weights of all edges assigned
by an allocation is equal to ∑∈ max∈T  , , where T is the set of impressions assigned to  and  , is
single advertiser.
to  , we only receive the maximum weight of the edges assigned to  . Concretely, the total weighted achieved
Free Disposal Assumption. In display advertising applications, assigning more impressions to an
the weight of the edge between  and  . In the display advertising literature [FKM+09, KMZ13], the free
advertiser than they paid for only makes them happier. In other words, we can assign more than one
maximization problem, where we can apply known 1/2-competitive greedy algorithms [FNW78, LLN06].
With free disposal, it is not hard to reduce weighted bipartite matching to the monotone submodular welfare
disposal assumption is well received and widely applied because of its natural economic interpretation.
weighted bipartite matching problem that achieves a competitive ratio greater than 1/2 has been a tantalizing
a(cid:28)rmative, breaking the long-standing 1/2 competitive ratio barrier (under the free disposal assumption).
For almost thirty years since the seminal work of Karp et al. [KVV90], (cid:27)nding an algorithm for the online
open problem. In this paper, we introduce the StochasticGreedy algorithm and answer this question in the
Given the hardness result of Kapralov et al. [KPV13] that restricts beating a competitive ratio of 1/2 for
1

Theorem 1.1. There exists a 0.501-competitive algorithm for the online weighted bipartite matching problem.

1.1 Our Contributions

1.2 Related Works

monotone submodular welfare maximization, our algorithm can be seen as strong evidence that solving
weighted bipartite matching is strictly easier than submodular welfare maximization in an online setting.
One of our main technical contributions is the controlled use of randomness in StochasticGreedy,
which allows the algorithm to safely make adaptive decisions based on past assignments and prevents the
cascading of conditional probabilities in our analysis. A more subtle feature of our use of randomness is
that in every step of the algorithm, expected values of random variables are computed over all possible
branches of the randomized algorithm instead of being conditioned on the current state. This ensures that
most sequences of variables in the algorithm are deterministic quantities governed solely by the input
graph and arrival order of the impressions. Our method for making adaptive decisions combined with
the limited randomness of the algorithm allows us to e(cid:28)ciently maintain these expected values (over
all possible branches of the algorithm) using dynamic programming. Lastly, we introduce a mechanism
called DistributeExcess in our analysis, which is not part of the algorithm but allows us to systematically
redistribute the extra marginal gain that the algorithm produces and improve upon the greedy algorithm.
and an improved online correlated selection scheme to achieve an improved competitive ratio of 0.514 for the
We (cid:27)rst draw attention to two very recent works of Huang and Tao [HT19, Hua19] that build on an earlier
version of this paper that appeared on arXiv in 2017 [Zad17]. These works “distill a key ingredient from the
algorithm of Zadimoghaddam” and enhance this idea by using the online primal-dual framework [DJK13]
devoted to achieving competitive ratios greater than 1/2 (usually 1 − 1/ or 1 −  ) by assuming that advertisers
same online weighted bipartite matching problem. In this version of our paper, we have tried to improve
and simplify the presentation of our algorithm and analysis so that our approach is easier to understand.
The literature online weighted bipartite matching algorithms is extensive, but most of these works are
have large capacities or that some stochastic information about the arrival order of the impressions is
(non-bipartite) graphs [HKT+18, GKS19, GKM+19, HPT+19].
known in advance. There are many exciting papers in this area, but we only list a few of the leading works
Large Capacities. Exploiting the large capacities assumption to beat the 1/2 competitive ratio barrier
and refer interested readers to the excellent survey of Mehta [Meh13]. We note that there have recently
competitive ratio of 1 − 1/ assuming that each advertiser has a large capacity, where the capacity denotes the
been several signi(cid:27)cant advances in more general settings, including di(cid:29)erent arrival models and general
dates back two decades ago to [KP00]. Feldman et al. [FKM+09] gave a primal-dual algorithm that achieves a
literature is to provide algorithms with competitive ratio greater than 1/2 without making any assumption
number of impressions that can be assigned to it (e.g., the Display Ads problem). Under similar assumptions,
the same competitive ratio was obtained for the Budgeted Allocation problem [MSVV05, BJN07], in which
since we can replace each advertiser  with capacity  by  identical copies of  , each with unit capacity.
advertisers have some budget constraint on the total weight that can be assigned to them rather than the
number of impressions. From a theoretical point of view, one of the primary goals in the online matching
on the capacities of advertisers. Without loss of generality, we can assume every advertiser has capacity one,
MGS12, MP12, JL13] or that the impressions arrive in a random order [GM08, DH09, FHK+10, MY11, MGZ12,
MWZ15, HTWZ19]. These works achieve a 1 −  competitive ratio if the large capacity assumption holds
Stochastic Arrivals. If we have knowledge about the arrival patterns of impressions, then we can often
leverage this information to design better algorithms. Typical stochastic assumptions include assuming the
in addition to the stochastic assumptions, or at least 1 − 1/ for arbitrary capacities. Korula et al. [KMZ18]
impressions are drawn from some known or unknown distribution [FMMM09, KMT11, DJSW11, HMZ11,
show that the greedy algorithm is 0.505-competitive for the more general problem of submodular welfare
2

2 Preliminaries

The random order assumption is particularly justi(cid:27)ed because Kapralov et al. [KPV13] show that beating 1/2
for submodular welfare maximization in the oblivious adversary model is equivalent to proving NP = RP.
maximization if the impressions arrive in random order, without making any assumptions on the capacities.
Let  be the set of advertisers,  be the set of  impressions, and  , denote the nonnegative weight of the
edge between impression  and advertiser  . If there is not an edge between  and  , we introduce an edge of
one by one at times  = 1, 2, … , . We do not assume  is known to the algorithm. When an impression 
arrives at time  , all edge weights incident to  are revealed to the algorithm, that is,  , for all  ∈ , and
the algorithm has to assign  to one of the advertisers at this time. This is an irrevocable decision and cannot
weight zero to simplify the notation. The set of advertisers is given in advance, and the impressions arrive
the total weight of maximum-valued edges assigned to the advertisers, that is, ∑∈ max∈T  , , where T
the impression with the maximum weight is kept. The rest are discarded and do not counted towards the
be changed later. At the end of the algorithm, if more than one impression is assigned to an advertiser, only
is the set of impressions assigned to  .
total weight of the allocation. This is known as the free disposal assumption. The objective is to maximize
an oblivious adversary, that is, an adversary that does not have access to the outcomes of the random choices
ratio min (, , ), order of  [ALG]/OPT, where ALG is a random variable denoting the value of the objective
In this paper, we assume that the impressions arrive in an adversarial order. Speci(cid:27)cally, we deal with
made by the algorithm, and instead has to (cid:27)x the input graph and arrival order in advance. We use the
standard notion of competitive ratio to measure the performance of our online algorithm. For a randomized
bipartite matching algorithm in this adversarial model, the competitive ratio is de(cid:27)ned to be the worst-case
the objective function) can be achieved by assigning an impression  to an advertiser  at every point in the
function achieved by the algorithm and OPT is the maximum objective value attained o(cid:31)ine.
we let the random variable MaxW denote the maximum weight assigned to  by StochasticGreedy up to
uses randomness in a very controlled manner, so we deliberately use a sans serif font and “upper camel case”
algorithm, we need to keep track of the maximum weight assigned to  by StochasticGreedy. Therefore,
We present our main randomized online algorithm StochasticGreedy in Section 3. This algorithm
(and including) time  for every 0 ≤  ≤  . Since assignments are made at times  = 1, 2, … , , we de(cid:27)ne
to distinguish quantities that are random variables. To evaluate how much marginal value (i.e., increase in
MaxW0 to be zero for all  ∈ . Next, we de(cid:27)ne the random variable Gain , to be the marginal gain of
assigning impression  to advertiser  . Formally, we have Gain , = ( , − MaxW −1
)+ , where ( )+ is the
choices that StochasticGreedy makes before  arrives.
function max{0,  } and  is the arrival time of impression  . We note that Gain , depends on the random
signed to each advertiser contributes to the total weight of the (cid:27)nal allocation, we have ALG = ∑∈ MaxW .
We can also interpret the total weight by letting ALG = ∑∈ MarginalGain , where MarginalGain is the
random variable that denotes how much the assignment of impression  increases the total weight of the
We let ALG be the total weight achieved by StochasticGreedy. Since only the maximum weight edge as-
and also let it be the weight of the allocation, that is, OPT = ∑∈  , ∗ . For the sake of analysis, we can add
and we let  ∗ be the advertiser to which impression  is assigned in OPT. We overload the notation of OPT
allocation at the time of its assignment. We let OPT denote the maximum weight matching of the instance,
completeness of our algorithm, we also introduce a null impression  = 0 with weight 0, = 0 for all  ∈ .
a large enough number of dummy impressions (advertisers) with edges of weight zero to all advertisers
(impressions) so that all vertices (impressions and advertisers) are matched in the optimal solution. For the
3

3 The StochasticGreedy Algorithm

In this section we introduce our randomized online algorithm StochasticGreedy (Algorithm 1). We start
an impression. Lastly, in Section 3.4 we explain why this approach breaks the 1/2 competitive ratio barrier.
by describing the algorithm at a high level, and then we present it formally together with two important
lemmas that highlight its deterministic features. In Section 3.1 we describe the variables in the algorithm.
constructs a set  ⊆  of candidate advertisers that can potentially be matched with  . If there are multiple
Our algorithm builds on the greedy approach. Upon the arrival of impression  , StochasticGreedy (cid:27)rst
Then in Section 3.2 and Section 3.3 we discuss the two main cases the algorithm considers when assigning
candidates, the algorithm considers the top two 1 and 2 , and performs one of the three actions uniformly
at random: (1) greedily assign  to 1 , (2) greedily assign  to 2 , or (3) adaptively choose between 1 and 2
[Gain , ] (where the randomness is over all possible branches of the algorithm up to this point and not
by looking at a past decision of the algorithm. The top candidates are determined by their expected gain
coin (cid:30)ip associated with  , where  is the advertiser in {1 , 2} with the greater adaptive gain. If the
conditioned on the current state) and an adaptive gain value that originates in the proof of Lemma 4.6. In
assignment based on this coin (cid:30)ip was adaptive, then the algorithm chooses between 1 and 2 uniformly at
the event that there are not multiple candidates, the algorithm makes a nonadaptive assignment.
random. Otherwise, the assignment of the past impression   associated with this coin (cid:30)ip was nonadaptive,
assigns  to the other top candidate in {1 , 2}, and if   was not matched to  then  is assigned to  . In
and the algorithm looks at whether or not   was matched to  . If   was matched to  then the algorithm
To adaptively decide between the top two candidates, the algorithm looks at the result of the last
the distribution of many MaxW variables. However, by ensuring that the adaptive decisions are based on
nonnegative parameters  and  that control the thresholds for how greedy and adaptive the algorithm is,
general, adaptive decisions can cause cascading e(cid:29)ects of conditional probabilities that can severely alter
an earlier nonadaptive (i.e., random) assignment, we can prevent this e(cid:29)ect and analyze the algorithm. We
continue this discussion about the bene(cid:27)ts of this kind of adaptive decision in more detail in Section 3.4.
Now we formally present StochasticGreedy in Algorithm 1. This algorithm takes as input two
(see Section 3.2 and Section 3.3). We also acknowledge that the de(cid:27)nitions of the adaptive_gain , variables
and the set  initially appear to be unnatural, but these conditions arise in our analysis and allow us to
respectively. We optimize these constants later in our analysis in Section 4. While Algorithm 1 is initially
cumbersome to parse, we point out that it is comprised of two separate cases that can be analyzed individually
overcome the shortcomings of the greedy algorithm. Before stepping through the details of Algorithm 1,
we (cid:27)rst make two critical observations about how the algorithm uses randomness.

the values of Priority and R . All other variables (e.g., the maximum gains  , all values of adaptive_gain , ,
the sequence of sets  and choices 1 and 2 , all updates to active(), index(), partner(), and the sums  )
Lemma 3.1. The only random variables in StochasticGreedy are the assignments of the impressions and
Lemma 3.2. We can maintain the probability mass function for all random variables Gain , over the course of
are deterministic quantities that depend solely on the instance and the arrival order of the impressions.
algorithm. In particular, we can e(cid:28)ciently compute the value [Gain , ] at any point in StochasticGreedy.

Lemma 3.2 states that we can e(cid:28)ciently compute [Gain , ] for all  ∈  in line 5 of Algorithm 1. This is a
In particular, Lemma 3.1 guarantees that the top candidates in each step are predetermined by the input
instance, even though the assignments of past impressions to these advertisers could have been random.
This property is simple to show by induction but easy to miss because of the complexity of Algorithm 1.
4

3.1 Variable Descriptions

Algorithm 1. When impression  arrives, the algorithm (cid:27)rst computes the maximum expected marginal
competitive ratio proof of the greedy algorithm goes through if we use [Gain , ] instead of their realized
of both of these lemmas to Appendix A.
consequence of the limited randomness in the algorithm and dynamic programming. We defer the proofs
gain  = max∈ [Gain , ] as a benchmark. We remark that it is not hard to show that the standard 1/2
We begin the description of StochasticGreedy by explaining the preprocessing stage in lines 5–11 of
and then it constructs the set of candidates  . There is some slack in how greedy Algorithm 1 is, but an
them to achieve a better result. Next, for every  ∈  the algorithm computes their adaptive gain value,
values. Using expected values, however, has the advantage that if there are two choices with high expected
advertiser must be able to contribute a value of at least (1 −  ) to be considered. We explain the meaning of
gains, then the algorithm can occasionally realize them in a controlled way and exploit the gap between
in Case I (lines 13–30). Otherwise, the algorithm jumps to line 31 and assigns  in Case II (lines 31–42). We
formulas are deterministic and well-de(cid:27)ned. If there are at least two candidates in  , the algorithm assigns 
the variables used in lines 7–11 in the next subsection, but for now we note that all of the quantities in these
All of the following variables with the exception of adaptive_gain , help maintain the state of Algorithm 1.
explain these cases in Section 3.2 and Section 3.3, respectively.
We reiterate that Priority and R are the only variables that are randomized. All other variables (at every
• active() is a boolean value that indicates whether or not advertiser  potentially has an adaptive
gain. At its core, this variable serves as a mechanism for checking if  is a partner in a valid pairing.
time step of the algorithm) are predetermined by the input graph and the arrival order of the impressions.
assigning  to  . If an adaptive assignment is made, the algorithm achieves an additional marginal
• adaptive_gain , represents how much extra marginal gain that algorithm can achieve by adaptively
was nonadaptively assigned to partner() and not  . We give further intuition for this mechanic
gain that is at most proportional to [Gainindex(), ] since it knows the earlier impression index()
• index() records the last impression for which  was considered in a potentially adaptive assignment.
If  is to be matched to 1 or 2 in lines 13–30, the algorithm sets index(1 ) ←  and index(2 ) ←  .
• partner() records the last advertiser with which  was paired in a potentially adaptive assignment.
in Section 3.4 and the derivation of this formula in the proof of Lemma 4.6.
If  is to be matched to 1 or 2 in lines 13–30, then we set partner(1 ) ← 2 and partner(2 ) ← 1 .
• Priority is a random variable for the outcome of the last potentially adaptive assignment involving  .
If  is to be matched to 1 or 2 in lines 13–30, one of the following actions is performed uniformly at
random: (1) greedily assign  to 1 and set Priority1 ← 1, Priority2 ← 2; (2) greedily assign  to 2
and set Priority1 ← 2, Priority2 ← 1; or (3) adaptively assign  to 1 or 2 by looking at Priority ,
where  ∈ {1, 2} is de(cid:27)ned in line 21, and reset Priority1 ← 0, Priority2 ← 0. The intuition here is
that in the (cid:27)rst two actions, the advertiser that is not matched with  receives a higher priority value
•  is an upper bound for the sum of expected gains assigned to  in lines 31–42 since the last time 
was chosen as a candidate in lines 13–30. Whenever  is a choice in lines 13–30,  is reset to zero.
and is therefore more likely to be adaptively assigned a future impression. In the third action, an
adaptive decision is potentially made based on these priority values, and then the priorities are reset.
5

// Not conditioned on the algorithm’s state (see Lemma 3.1)
// Compute adaptive gain values

for  = 1, 2, … , | | do

adaptive_gain , ← 0

for  ∈ {1 , 2} do

 ← max∈ [Gain , ]
adaptive_gain , ← ([Gainindex(), ]/3 − (index(), −  , )+ /3 −  )+ /12
 ← { ∈  ࢼ ( , ≥ index(), −   ) and ([Gain , ] + 2/3 ⋅ adaptive_gain , ≥ (1 −  ) )}
1 ← arg max∈ [Gain , ] + 2/3 ⋅ adaptive_gain ,
2 ← arg max∈{1 } [Gain , ] + 2/3 ⋅ adaptive_gain ,
 ← arg max ∈{1,2} adaptive_gain ,

1 function StochasticGreedy( ,  )
Set active() ← false, index() ← 0, partner() ← 0, Priority ← 0,  ← 0 for all  ∈ 
Algorithm 1: Online weighted bipartite matching algorithm.
Let  be the impression that arrives at time  , i.e.,  = 
for  ∈  do
if active() = true and  , ≥ index(), −   then
Let R be a uniformly random real number in the interval [0, 1)
if | | ≥ 2 then
else
if partner() ∉ {1 , 2} then
Set active() ← true,  ← 0, index() ← 
Set partner(1 ) ← 2 and partner(2 ) ← 1
if R ∈ [0, 1/3) or adaptive_gain , = 0 then
if Priority = 2 and adaptive_gain , > 0 then Assign  to 
if Priority = 1 and adaptive_gain , > 0 then Assign  to partner( )
if Priority = 0 or adaptive_gain , = 0 then
Assign  to 1 or 2 each with probability 1/2
else

Set Priority1 ← 0 and Priority2 ← 0
if R ∈ [1/3, 2/3) then Assign  to 1 and set Priority1 ← 1, Priority2 ← 2
if R ∈ [2/3, 1) then Assign  to 2 and set Priority1 ← 2, Priority2 ← 1

// Case I: There are enough candidates to exploit adaptivity
// Couple the top pair of advertisers 1 and 2

active(partner()) ← false

else

 ← { ∈  ࢼ ( , ≥ index(), −   ) and ([Gain , ] ≥ (1 −  ) )}
 ← { ∈  ࢼ ( , < index(), −   ) and ([Gain , ] ≥ (1 −  ) )}
if | ∪  | = 1 then

// Make a random assignment and prepare for future adaptive decisions
// Note  ⊆ 
// Case II: There is no adaptivity
// Advertiser 2 does not exist

2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42

// Make an adaptive decision if possible

Set 1 ← 1 + 
Assign  to 1 ← arg max∈ [Gain , ]
2 ← arg max∈ {1 } [Gain , ]
else 1 ← arg max∈ [Gain , ]
Set 1 ← 1 +  /2 and 2 ← 2 +  /2

if  ≠ ∅ then 1 ← the only advertiser in 
else
Assign  to 1 or 2 each with probability 1/2

6

// Make a random assignment to the top two choices

3.2 Case I: Lines 13–30 and Adaptive Assignments

If  contains at least two advertisers, then 1 and 2 are chosen as the top two candidates in lines 14–15
based on their value of [Gain , ] + 2/3 ⋅ adaptive_gain , . Before assigning  in lines 21–30, the algorithm
candidates  ∈ {1 , 2}, this step activates  for future a adaptive decision, deactivates its previous partner
performs an update procedure in lines 16–20 to couple the pair of advertisers 1 and 2 . For each of the top
(unless this partner is the other top candidate), and updates partner() to be the other top candidate.
adaptive_gain , is set to zero for all unpaired advertisers  in lines 7–10. Finally, for each  ∈ {1 , 2},
This routine ensures that the active variables are set to true if and only if their advertiser is in a proper
line 17 also updates index() to the current impression and resets the value of  .
Now we focus on the assignment of  in lines 21–30. The algorithm starts by drawing a uniformly
(i.e., the event R ∈ [1/3, 2/3) in line 29), the algorithm matches  to 1 . We note that this assignment does not
as they are simpler to explain and give insight into how the adaptive decision works. With probability 1/3
pairing. This is important because the algorithm uses the active state of an advertiser to ensure that
random real number R from the interval [0, 1). We start by describing the assignment rules in lines 29–30,
assignments to record that the algorithm nonadaptively assigned  to 1 , so we update Priority1 ← 1 and
Priority2 ← 2. Formally, Priority = 1 means that the last time  was chosen as a candidate in lines 13–30,
some sense the complement and means that the last time  was chosen as a candidate in lines 13–30, the
of the algorithm is determined by the input instance (Lemma 3.1). It will be useful for future adaptive
depend on any previous coin (cid:30)ips of the algorithm because the sequence of top candidates over the course
the impression that arrived at this time was nonadaptively assigned to  . The event Priority = 2 is in
impression that arrived was nonadaptively assigned to the other top candidate (i.e., not ). We refer to this
likely to be assigned an impression in the future. With another probability of 1/3 (i.e., when R ∈ [2/3, 1) in
We (cid:27)rst note that after this assignment, Priority1 and Priority2 are reset to 0. To simplify the description
variable for the adaptive state of an advertiser as a priority because if in lines 13–30 a nonadaptive decision
line 30), the algorithm assigns  to 2 and updates the Priority variables accordingly.
is made and the top candidate does not receive the impression, then it is given a higher priority and is more
With the remaining 1/3 probability (i.e., when R ∈ [0, 1/3) in line 22), impression  is assigned adaptively.
of this part of the algorithm, assume that adaptive_gain ,1 ≥ adaptive_gain ,2 . This means  is set to 1
in line 21. The assignment of  is conditioned on the assignment of   , where   is the last impression that
chose 1 as a candidate in lines 13–30. Note that   was the value of index(1 ) immediately before its update
value of Priority1 . If   was matched nonadaptively to 1 , then the algorithm makes an adaptive choice and
assigns  to 2 (i.e., the case where Priority1 = 1 and Priority2 = 2). Intuitively, this is bene(cid:27)cial because
we should consider 2 as the better option. Similarly, if Priority1 = 2 then the algorithm adaptively assigns 
in line 17. The algorithm adaptively makes its assignment conditioned on past events by looking at the
conditioning on the event where   was assigned to 1 decreases the expected gain of assigning  to 1 . Thus,
using a new coin toss that is independent of R . In the event that adaptive_gain ,1 = 0 (which means
to 1 . The last case is when Priority1 = 0. Since we want to prevent the chaining of conditional probabilities,
the algorithm does not make an adaptive choice here and instead assigns  to 1 or 2 uniformly at random
that adaptive_gain ,2 = 0 because we assumed adaptive_gain ,1 ≥ adaptive_gain ,2 ), it su(cid:28)ces for
If the set  contains zero or one advertiser, then the algorithm is forced to make a nonadaptive assignment.
the algorithm to make a random assignment in line 26.
The high level idea in this case is that we want to choose at most two advertisers to be matched with 
7

3.3 Case II: Lines 31–42

lines 13–30 satisfy [MarginalGain ] ≥ ([Gain ,1 ] + [Gain ,2 ])/2 + adaptive_gain ,1 + adaptive_gain ,2 .

• The advertiser with maximum expected gain, arg max∈ [Gain , ], is chosen as one of the candidates.
while ensuring that both of the following conditions are met:
First observe that the de(cid:27)nitions of the sets  and  in lines 32–33 imply that their union  ∪  consists
• If  is not empty and the only advertiser in  has expected gain at least (1 −  ) , it should be chosen.
of all advertisers with expected gain at least (1 −  ) . Therefore, if the conditional statement on line 34
holds, the advertiser arg max∈ [Gain , ] is the only advertiser that meets the (1 −  ) threshold. The
algorithm assigns  to this advertiser and does not consider a second option. Otherwise, the algorithm
selects the only advertiser in  ⊆  (if it exists) as a candidate and chooses one or two additional advertisers
in  with the highest expected gains to be the candidates 1 and 2 . The impression  is then assigned to 1
or 2 with equal probability. The only remaining detail is the variable  , which maintains an upper bound
for the sum of expected gains assigned to  since the last time  was chosen as a candidate in lines 31–42.
We note that  is reset to zero in line 17, and is otherwise incremented in line 36 or line 42 in a way that is
consistent with the probability of impression  being assigned to  .
The key result that gives us a chance to break the 1/2 barrier is Lemma 4.6, which states that all assignments in
Recall from line 7 of Algorithm 1 that adaptive_gain , = ([Gainindex(), ]/3 − (index(), − , )+ /3 −  )+ /12.
We are able to prove Lemma 4.6 because we ensure that adaptive decisions are based on a past nonadaptive
or  is large. The condition  , ≥ index(), −   on line 7 implies that (index(), −  , )+ ≤   is not
assignment, which stops the chaining of conditional dependencies. We (cid:27)rst argue that adaptive_gain ,
can be thought of as some constant fraction of [Gainindex(), ]. If this is not the case, then (index(), −  , )+
too large, so we can bound the drop in [Gainindex(), ]. If  is large, then the algorithm made potential
assignments to  in lines 31–42. Assignments to  in lines 31–42 are favorable because they either agree with
OPT or yield substantially more gain than [Gain , ∗ ]. Ultimately, this allows us to charge the additional
gain from these assignments to adaptive_gain , .
that we are giving the intuition behind our approach and that many details are omitted. For any  ∈ , let
We proceed by assuming that every adaptive_gain , variable is proportional to [Gainindex(), ]. Note
takes values in  over the course of the algorithm. Each of the impressions in   {1} generates enough
 = {1 , 2 , … ,  } be the set of impressions potentially matched with  in lines 13–30. It follows that index()
expected marginal gain value that is lacking is the one associated with the  . For this last impression, we
consider a few di(cid:29)erent cases and forward reference to the additional sources of marginal gain  and 
extra value in the form of adaptive gain to increase the marginal gain of the previous impression. The only
that arise in our lower bound for [ALG] in Lemma 4.1. The following argument is formalized by the
DistributeExcess mechanism in Section 4.2 for reallocating extra marginal gain. Let   be the impression
from it to achieve enough value. Now assume that   arrives after  . If   is assigned in lines 31–42, there is
matched with  in OPT (i.e.,  =  ∗  ). If   arrives before  , then   is large and the mechanism borrows
enough extra value to allocate to [Gain , ] since this is the favorable case. Otherwise,   is assigned in
lines 13–30 and  is not one of the top candidates in {1 , 2}. If  is not one of the top candidates because
  , < index(), −    (which makes the condition on line 7 false), then the value of  is large enough to
make up for this di(cid:29)erence. Otherwise, the sum adaptive_gain  ,1 + adaptive_gain  ,2 is large enough to
cover the marginal gain of all three impressions  , index(1 ), and index(2 ) since the Algorithm 1 accounts
for this 2/3 split in lines 14–15 when choosing 1 and 2 . Putting everything together, we show that all
8

3.4 Intuition for Breaking the 1/2 Barrier

assignments in lines 13–30 satisfy [MarginalGain ] ≥ (1/2 +   ) ⋅ ([Gain ,1 ] + [Gain ,2 ]), for some   > 0.

4 Analysis of the Competitive Ratio

4.1 Outline of the Main Proof

In this section we analyze the competitive ratio of StochasticGreedy and show that it breaks the 1/2 barrier.
We start by presenting the high-level structure of our argument in Section 4.1, deferring the proofs of our
two core lemmas (Lemma 4.2 and Lemma 4.3) to the following subsections. In Section 4.2 we introduce a
mechanism called DistributeExcess, which we use in our analysis to systematically redistribute excess
marginal gain. We stress that DistributeExcess exists solely for the sake of analysis and is not a component
of StochasticGreedy. Then in Section 4.3 we prove Lemma 4.3 by cases and show that DistributeExcess
For every impression  ∈  , we compare [MarginalGain ] to the expected value that StochasticGreedy
allocates enough excess marginal gain to every impression for StochasticGreedy to be 0.501-competitive.
)+ ]. To prove that greedy
algorithms achieve a 1/2 competitive ratio, it usually su(cid:28)ces to show that [MarginalGain ] ≥ [Gain , ∗ ].
Our algorithm StochasticGreedy, however, is designed in such a way that this condition is not necessarily
contributes enough to exceed the standard 1/2 competitive ratio. One of the major technical contributions
satis(cid:27)ed for every impression. Instead, we show that the sum of expected marginal gains over all impressions
is signi(cid:27)cantly greater than this lower bound in aggregate. Intuitively, impressions that beat this benchmark
share their excess marginal gain with other impressions so that at the end of the algorithm, every impression
of this paper is our carefully designed mechanism DistributeExcess (Mechanism 2), which reallocates the
potential excess marginal gain of an assignment to guarantee a uniform lower bound for every impression.
such a way that reveals three additional sources of potential excess gain that can be exploited.
We begin by lower bounding the expected weight of the assignment that StochasticGreedy makes in

could have achieved by assigning  to  ∗ , namely [Gain , ∗ ] = [( , ∗ − MaxW −1 ∗

Lemma 4.1. The expected weight of the assignment of StochasticGreedy, namely [ALG], is at least

MaxW −1 ∗ −  , ∗ + .


MarginalGain − Gain , ∗  +
Proof. We know that [ALG] = ∑∈ [MarginalGain ] = ∑∈  + [Gain , ∗ ]. By the de(cid:27)nition of the ( )+
operator, we have Gain , ∗ =  , ∗ − MaxW −1 ∗ + (MaxW −1 ∗ −  , ∗ )+ , which gives us the  term. So far we
On the other hand, we know [ALG] = ∑∈ [MaxW ] and OPT = ∑∈  , ∗ . Writing  , ∗ − MaxW −1 ∗
yields with the  term. Since ∑∈ [MaxW ∗ ] ≤ ∑∈ [MaxW ] = [ALG], it follows that



[ALG] = ∑∈  +  , ∗ − MaxW −1 ∗  +  .
[ALG] = ∑∈  +  , ∗ − MaxW ∗  +  +  ≥ OPT − [ALG] + ∑∈  +  +  ,

 , ∗ − MaxW ∗  + MaxW ∗ − MaxW −1 ∗ 





MaxW ∗ − MaxW −1 ∗  +



12 OPT + 12 ∑∈

have shown that

as

which completes the proof.

9

Lemma 4.2. The mechanism DistributeExcess( ,  ,  ) computes a value excess for each impression  ∈ 
such that ∑∈ excess ≤ ∑∈  +  +  , where the terms  ,  , and  are de(cid:27)ned in Lemma 4.1. Furthermore,
for every  ∈  , at least a  fraction of  is distributed to excess .

The expectations  and  are nonnegative for all  ∈  since the random variables MaxW ∗ are nondecreasing
in  and by the de(cid:27)nition of the ( )+ operator. The expectations  , however, can sometimes be negative.
For the sake of analysis, we de(cid:27)ne an auxiliary variable excess for each impression  ∈  and show how
sum  +  +  is not necessarily nonnegative. Therefore, we introduce the mechanism DistributeExcess
to assign its value at any given step of the algorithm. We reiterate that the excess variables are not actually
allows us to assign values to all the excess variables so that ∑∈ excess ≤ ∑∈  +  +  , and more
importantly, for every impression  ∈  , we have excess ≥  for some universal constant  > 0. In
used in StochasticGreedy and are only de(cid:27)ned to help us prove the competitive ratio. As noted above, the
at least 1+2+ > 1/2 competitive. Note that we call the routine of assigning values to the excess variables a
to systematically redistribute excess marginal gain that is incurred over the course of the algorithm. This
Theorem 4.4, we exploit these two properties of the excess variables to prove that StochasticGreedy is
aggregate excess marginal gain that StochasticGreedy produces.
mechanism (and not an algorithm) because it is only used in our analysis as a means to argue about the
For now, we abstract away the details of DistributeExcess so that we can understand its role in our
following lemma in Section 4.2.
analysis of the competitive ratio. We present the mechanics of DistributeExcess and the proof of the
bound each excess variable in terms of the maximum expected gain  when  arrives. We note that the
inputs  ,  to StochasticGreedy and  ,  ,  to DistributeExcess are intentionally left as variables so that
crux of our analysis. In particular, Lemma 4.3 shows that we can use DistributeExcess to uniformly lower
The next lemma covers a variety of di(cid:29)erent cases that StochasticGreedy can encounter and is the
Lemma 4.3. For any  ,  ≥ 0, the mechanism DistributeExcess( ,  ,  ) (cid:27)nds a value excess ≥  for
we can optimize them retroactively and so that the case analysis in the proof of Lemma 4.3 is simpler. We
discuss our approach and the supporting lemmas for proving Lemma 4.3 in more detail in Section 4.3.
Theorem 4.4. For any  ,  ≥ 0, the algorithm StochasticGreedy( ,  ) is
Now that we have presented our key prerequisite lemmas, we show how to assemble them to prove our
main result about StochasticGreedy.
For each impression we also have  ≥ [Gain , ∗ ], which is at least [ , ∗ − MaxW −1 ∗
Proof. By combining Lemma 4.1, Lemma 4.2, and Lemma 4.3, we know that
Summing this lower bound over all impressions gives us ∑∈  ≥ OPT − [ALG]. Therefore, it follows
10

19 }. In particular,
is de(cid:27)ned in Lemma 4.3. In particular, if  = 0.082 and  = 0.445, then  ≥ 0.00400802 and StochasticGreedy
1+2+ -competitive, where  = ( ,  )
is 0.501-competitive.

[ALG] ≥ 12 OPT + 12 ∑∈  +  +  ≥ 12 OPT + 12 ∑∈ excess ≥ 12 OPT + 12 ∑∈  .

that [ALG] ≥ 12 OPT + 2 (OPT − [ALG]), or equivalently [ALG] ≥ 1+2+ OPT.

each impression  ∈  , where  is de(cid:27)ned to be  = ( ,  ) = max0≤ , , ≤1 min{  −2 −22
, 1−3 −4 −4
3−2219 , 6  −1−18
18
, 324(1− )2 −361
18468(1− )
, 324(1− )2 −361
18468(1− ) × 18 , 2(1− )
19
, (1 −  ) 1+ × 6(1− )
19
, 18(1− )
19 ×  , 21+ × 18(1− )
4

by setting  = 0.082,  = 0.445,  = 0.955,  = 0.00337198, and  = 0.03362, we have  ≥ 0.00400802.

, 2  −3 −6 −6
6

,

] ≥ [ , ∗ − MaxW ∗ ].

4.2 Mechanism for Distributing Excess Marginal Gain

gain ∑∈  +  +  de(cid:27)ned in Lemma 4.1 to a set of auxiliary variables called excess . This mechanism
In this subsection we introduce the DistributeExcess mechanism, which reallocates the excess marginal
and the excess variables are not part of the StochasticGreedy algorithm, and are used only to analyze
the optimal assignment and assigns a value to each of the excess variables in a way that allows us to show
that [ALG] ≥ 0.501 ⋅ OPT for the current sequence of impressions.
redistributes the sum ∑∈  +  +  across all of the excess variables. Its execution path relies solely on
the competitive ratio. At any time during the online algorithm, this mechanism assumes oracle access to
The mechanism takes as input three parameters  ,  ,  representing allocation proportions that we optimize
later. The allocations of the  and  variables are direct, but the  variables are partitioned and distributed
At a high level, DistributeExcess mirrors the execution of StochasticGreedy and systematically
over time. Before presenting DistributeExcess, we de(cid:27)ne a re(cid:27)nement of  to captures how it evolves.
deterministic quantities in StochasticGreedy (see Lemma 3.1) and is thus independent of its randomness.
De(cid:27)nition 4.5. For every  ∈  , we de(cid:27)ne the time sequence of  (introduced in Lemma 4.1) to be
We let Ɗ denote the backwards di(cid:29)erence for time sequence values, implicitly de(cid:27)ned as Ɗ ( ) =  , −  , −1 .
1 function DistributeExcess( ,  ,  )
Mechanism 2: Mechanism to populate the excess variables.
Initialize excess ← 0 for all  ∈  ∪ {0}
Let  be the impression that arrives at time  , i.e.,  = 
Increase excess by  
for  ∈ {1 , 2} (if 2 does not exist, only consider 1 ) do
Let   be the impression for which  =  ∗ 
if   arrived at or before time  then
if | | ≥ 2 then
for  ∈  do
Increase excess by Ɗ (  )/| |
if index() remains unchanged after time  then
Let   be the impression for which  =  ∗ 
Increase excess by (1 −  ) 
else Increase excessindex(1 ) by  
if 2 exists then Increase excessindex(1 ) and excessindex(2 ) each by   /2
else
Increase excessindex( ∗ ) by  
Set excess ← excess +  − ( +  )
11

// Recall that 0 is the initial value of index()
// Distribute  
// Distribute Ɗ (  )
// Use the value of index() at the beginning of time 
// Use the value of index() at the end of time 
// Distribute (1 −  ) 

for  = 1, 2, … , | | do
 ← { ,   , index()}
for  ∈ {1 , 2 ,  ∗ } do
for  ∈ {1 , 2} do

Increase excessindex() by 2/3 ⋅ adaptive_gain ,

Set excess ← excess +  − ∑∈{1 ,2 , ∗ } 2/3 ⋅ adaptive_gain ,

 , = 0

MaxW ∗ − MaxW −1 ∗ 

if 0 ≤  <  ,
if  ≤  ≤  ,

2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24

valid reallocation. In particular, we show that the (cid:27)nal assignments satisfy ∑∈ excess ≤ ∑∈  +  +  .
part of the statement. At each time step  =  , lines 13–15 and lines 21–24 change the sum ∑ ∈ ∪{0} excess
We conclude this subsection by proving Lemma 4.2, which guarantees that the mechanism performs a
by exactly  . Now we analyze lines 6–11 of Mechanism 2. Assuming that 2 exists, let  1 be the impression
for which 1 =  ∗ 1 and let  2 be the impression for which 2 =  ∗ 2 . Since 1 ≠ 2 we know  1 ≠  2 . Therefore,
Proof of Lemma 4.2. The second claim is immediate from line 5 of Mechanism 2, so we focus on the (cid:27)rst
the change in ∑ ∈ ∪{0} excess at time  =  is at most Ɗ ( 1 ) + Ɗ ( 2 ) = ∑ ∈ Ɗ ( ). If 2 does not exist, we
can also upper bound the change in ∑ ∈ ∪{0} excess at time  =  by Ɗ ( 1 ) = ∑ ∈ Ɗ ( ). Recall that we
have ∑ =1 Ɗ ( ) =  by De(cid:27)nition 4.5. Summing over all time steps  , the total contribution of lines 6–11
variables. The sum ∑ ∈ ∪{0} excess is clearly increased at time  =  by   on line 5. Next, observe that
at the end of Mechanism 2 to ∑∈ ∪{0} excess is at most ∑∈  . Now we analyze the contribution of the 
the total contribution from lines 16–19 to the sum ∑ ∈ ∪{0} excess over the course of the mechanism is at
most (1 −  ) ∑∈  . This follows from the facts that the conditional statement on line 17 evaluates to true
at most once for each  ∈ , and that each impression is matched to at most one advertiser in the optimal
assignment. Since all of these contributions are disjoint, we have ∑ ∈ ∪{0} excess ≤ ∑∈  +  +  at the
end of Mechanism 2. The claim follows because excess0 is always increased by nonnegative amounts.

4.3 Lower Bounding the Excess Allocated to Each Impression

[MarginalGain ] ≥ Gain ,1  + Gain ,2 

2

+ adaptive_gain ,1 + adaptive_gain ,2 .

Lemma 4.6. If impression  is assigned in lines 13–30 of StochasticGreedy, i.e., case | | ≥ 2, we have
Lemma 4.7. For any advertiser  ∈  and impression  ∈  , we have adaptive_gain , ≤ 112 [Gain , ].
Lemma 4.8. For any advertiser  ∈  at time  =  for impression  , we have [Gain , ] ≥ 18(1− )

The rest of our analysis is devoted to proving Lemma 4.3. We will need the following three inequalities
in addition to Lemma 4.2, so we state them together for ease of reference and defer the longer proofs
to Appendix B. First, we show in Lemma 4.6 how the adaptive decisions in lines 23–24 of Algorithm 1
improve the expected marginal gain when assigning impressions. We note that although there are other
potential adaptive opportunities to exploit in Algorithm 1 (e.g., lines 41–42), we use adaptiveness in a very
controlled way and limit its use to the most bene(cid:27)cial parts of the algorithm.
The following two inequalities are derivatives of Lemma 4.6 that show how adaptive_gain , , [Gain , ],
and  relate to one another. We note that the proof of Lemma 4.7 builds directly on the proof of Lemma 4.6.
Proof. This is a direct consequence of Lemma 4.7 and the de(cid:27)nition of the set  in line 11 of Algorithm 1.
the competitive ratio of Algorithm 1. To show that excess ≥  for every impression  ∈  , we (cid:27)rst
consider three di(cid:29)erent scenarios that can occur when  arrives. For each of the three top-level cases, we
analyze a series of subcases, all of which result in lower bounds for excess that are a multiple of  . The
Now we present the proof of Lemma 4.3, which completes the analysis of Mechanism 2 and consequently
adaptivity of Algorithm 1 and the design of Mechanism 2. We note, however, that the subcases themselves
branching structure of these cases is initially di(cid:28)cult to discern, but it is somewhat unavoidable given the
are relatively easy to verify. We present a distilled version of the subcases and their implications in Figure 1.

19  .

12




◦ [Gain , ∗ ] + 2/3 ⋅ adaptive_gain , ∗ ≤ [Gain , ] + 2/3 ⋅ adaptive_gain , for  ∈ {1, 2}

18 , 2(1− )
19

19 ×   
19 ×   

◦  ∗ ∉  ∪    −2 −22 
◦  ∗ ∈   2  −3 −6 −6
◦  ∗ ∈   1−3 −4 −4
4
6
◦  , ∗ < index( ∗ ), ∗ −    6  −1−18
– index(1 ) or index(2 ) changes after   min  324(1− )2 −361
18 
18468(1− ) × 18  
, 324(1− )2 −361
18468(1− )
19 
19 
◦ 1 ,1 ≥  ,1 − 1 and 2 ,2 ≥  ,2 − 2  min  21+ × 18(1− )
◦ 1 ,1 <  ,1 − 1 or 2 ,2 <  ,2 − 2  min (1 −  ) 1+ × 6(1− )
, 18(1− )
19
, 18(1− )
19
, 1−3 −4 −4
, 2  −3 −6 −6
, 3−2219 , 6  −1−18
, 324(1− )2 −361
, 324(1− )2 −361
18468(1− ) ×
, (1 −  ) 1+ × 6(1− )
, 18(1− )
19 ×  , 21+ × 18(1− )
4
6
18
18468(1− )
19

• Case 1:  is assigned in lines 31–42
• Case 2:  is assigned in lines 13–30 and  ∗ ∈ {1 , 2}  3−2219 
• Case 3:  is assigned in lines 13–30 and  ∗ ∉ {1 , 2}
– index(1 ) and index(2 ) remain unchanged after time 
• Impression 1 arrived at or before time  where 1 =  ∗1  2(1− )
• Impression 2 arrived at or before time  where 2 =  ∗2  2(1− )
• Impressions 1 and 2 arrive after time 
Figure 1: Branching structure of the subcases for lower bounding excess in the proof of Lemma 4.3.
Proof of Lemma 4.3. It su(cid:28)ces to show that for any 0 ≤  ,  ,  ≤ 1, mechanism DistributeExcess( ,  ,  )
increases excess by at least min{  −2 −22
during Mechanism 2 for a variety of reasons. Since the sequence  , is nonnegative and nondecreasing in  ,
the updates to the excess variables in line 11 are nonnegative. Similarly,  is nonnegative so the changes
19 } . We start by observing that excess might change
because the adaptive_gain and  variables are nonnegative. The only place where excess might be
reduced is in line 15 or line 24 at  =  . In particular, this happens for small or negative values of  . In this
in line 5 and line 19 are nonnegative. The excess variables also do not decrease in line 14 or lines 21–23
main cases and prove each separately: (1) impression  is assigned in lines lines 31–42 of Algorithm 1, (2) 
proof, instead of tracking all changes to excess , we bound this one-time reduction to excess and show
that excess is increased enough elsewhere to compensate for this potential decrease. We consider three
is assigned in lines 13–30 and  ∗ ∈ {1 , 2}, and (3) impression  is assigned in lines 13–30 and  ∗ ∉ {1 , 2}.
Case 1 ( is assigned in lines 31–42). We start by proving the claim in the simplest case when  is assigned
why we say the only advertiser in  on line 38. Recall that  = [MarginalGain ] − [Gain , ∗ ]. If  ∗ ∉  ∪  ,
in lines 31–42 of Algorithm 1. Since adaptive_gain is always nonnegative,  is a subset of  . According
to the else condition on line 31 of Algorithm 1, the set  , and thus  , contains at most one advertiser. This is
then [Gain , ∗ ] < (1 −  ) by the de(cid:27)nition of sets  and  . On the other hand, arg max∈ [Gain , ] will
be selected as one of the advertisers to which  is assigned, and it achieves a gain of  by de(cid:27)nition. If
Therefore, [MarginalGain ] is at least (1 − /2) , which implies that  ≥ /2 . It follows from line 24 of
there is a second choice (i.e., 2 exists), its gain is at least (1 −  ) by the de(cid:27)nition of the sets  and  .
13

Mechanism 2 that excess is increased by at least  −2 −22  , which proves the claim.
Next, we consider the scenario  ∗ ∈  . In this subcase, Algorithm 1 selects  ∗ as one of at most two
candidates for  . This potential assignment increases  at time  by at least [Gain , ∗ ]/2, which implies that
Ɗ ( ) ≥ [Gain , ∗ ]/2 ≥ (1 −  ) /2. Observe that this situation causes Mechanism 2 to increase excess in
line 11. Since   =  in this subcase, we have | | ≤ 2, which means excess is increased by at least (1 −  ) /4.
Mechanism 2 is at least (1 − /2) −  − ( +  ) = −  +2 +22  . Therefore, at the end of Mechanism 2, the
On the other hand, similar to the argument above, we can show that the change in excess in line 24 of
To conclude Case 1, we assume that  ∗ ∈  . Similar to the argument in the previous paragraph, we
 , which again proves the claim.
Therefore, impression index( ∗ ) arrived before  and was assigned to  ∗ with probability at least 1/3. This
can show that the change in excess is at least −  +2 +22  in line 24 of Mechanism 2. Since  ∗ ∈  , we
know that  , ∗ < index( ∗ ), ∗ −   . Observe that index( ∗ ) ≠ 0, for otherwise we would have  , ∗ < 0.
potential assignment of index( ∗ ) to  ∗ at time index( ∗ ) implies that  ≥ (index( ∗ ), ∗ −  , ∗ )/3 >   /3 by
the de(cid:27)nitions of  and  . Lemma 4.2 implies that a  fraction of  is distributed to excess , so when
Case 2 ( is assigned in lines 13–30 and  ∗ ∈ {1 , 2}). Now we consider the case where  is assigned in
 , as desired.
lines 13–30 of Algorithm 1 and  ∗ is equal to either 1 or 2 . We show that excess does not decrease too much
Since  ∗ ∈ {1 , 2}, the change in excess in line 15 at time  =  is at least
in line 15 of Mechanism 2, and then we lower bound its increments on other occasions. Using Lemma 4.6,

value of excess is at least 1−4  −  +2 +22  = 1−3 −4 −4
4
Mechanism 2 ends, excess is at least   −  +2 +22  ≥  3  −  +2 +22  = 2  −3 −6 −6
6

we know [MarginalGain ] is at least ([Gain ,1 ] + [Gain ,2 ])/2 + adaptive_gain ,1 + adaptive_gain ,2 .

 − ∑∈{1 ,2 } 2/3 ⋅ adaptive_gain , = [MarginalGain ] − Gain , ∗  − ∑∈{1 ,2 } 2/3 ⋅ adaptive_gain ,
= 12 Gain ,1  + 2/3 ⋅ adaptive_gain ,1 + Gain ,2  + 2/3 ⋅ adaptive_gain ,2  − Gain , ∗  (1)

≥  ∑∈{1 ,2 } 1/2 ⋅ Gain ,  + adaptive_gain , − Gain , ∗  − ∑∈{1 ,2 } 2/3 ⋅ adaptive_gain ,
19 

where the last inequality holds because 1 and 2 are both in the set  and because [Gain , ∗ ] ≤  .
Now, since  ∗ is the same as 1 or 2 , variable  increases by at least [Gain , ∗ ]/3 at time  because 1/3 is
a lower bound on the probability of assigning  to  ∗ . Furthermore, Lemma 4.8 implies [Gain , ∗ ] ≥ 18(1− )
because  ∗ ∈  . This means that at time  =  on line 11 of Mechanism 2, excess is increased by at least
19  since we have  =   and | | ≤ 2. Therefore, we conclude that at the end
19  −  = 3−2219  , which proves the claim for Case 2.
Case 3 ( is assigned in lines 13–30 and  ∗ ∉ {1 , 2}). The (cid:27)nal case is when  is assigned in lines 13–30
of Mechanism 2, the value excess is at least 3(1− )
The new idea we can use here is that since 1 and 2 have been selected as the top two choices in the set 
and  ∗ ∉ {1 , 2}. We (cid:27)rst bound the reduction of excess in line 15 of Mechanism 2 at time  =  , and
(lines 14–15 of Algorithm 1) and  ∗ has not been chosen, at least one of the following inequalities holds:
then we prove it is increased enough in other occasions. Like in Case 2, we start by applying Lemma 4.6.
both  ∈ {1, 2}. We start by proving the claim in the (cid:27)rst scenario. To do this, we need to introduce the new
14

 , ∗ < index( ∗ ), ∗ −   or [Gain , ∗ ] + 2/3 ⋅ adaptive_gain , ∗ ≤ [Gain , ] + 2/3 ⋅ adaptive_gain , , for

≥ (1 −  ) −  = − ,

Ɗ ( )/| | ≥ [Gain , ∗ ]/6 ≥ 3(1− )

12

≥ − 118 +  ,

− − 23 ⋅ adaptive_gain , ∗ ≥ − − 23 ⋅ Gain , ∗ 

Gain ,  + 23 ⋅ adaptive_gain , ≥ Gain , ∗  + 23 ⋅ adaptive_gain , ∗ .

12  ∑ ∈{1,2} Gain ,  + 23 ⋅ adaptive_gain ,  − Gain , ∗  − 23 ⋅ adaptive_gain , ∗ ≥ 0.

2/3 ⋅ adaptive_gain , ∗ term into (1) from Case 2 to address the fact that  ∗ ∉ {1 , 2}. Working from (1),
we can say that the change in excess in line 15 of Mechanism 2 is at least
where the (cid:27)rst inequality holds by Lemma 4.7. Since we (cid:27)rst assume  , ∗ < index( ∗ ), ∗ −   , impression
with probability at least 1/3 in Algorithm 1. Therefore, it follows that  ≥ (index( ∗ ), ∗ −  , ∗ )/3 >   /3.
index( ∗ ) exists (i.e., it is not zero). We know that impression index( ∗ ) arrived before  and is assigned to  ∗
Applying Lemma 4.2, we know Mechanism 2 increases excess on line 5 by at least    /3 upon termination.
Therefore, the (cid:27)nal value of excess is at least (  3 − 118 −  ) = 6  −1−18
18  , which proves the claim for
To complete the analysis for Case 3, we focus on the second subcase where we assume for  ∈ {1, 2} that
To address the assumption that  ∗ ∉ {1 , 2}, we adapt (1) as follows and then combine it with the inequalities
the (cid:27)rst scenario.
above to get the following lower bound on the change of excess in line 15 of Mechanism 2:
Therefore, excess is not reduced in line 15 of Mechanism 2. To complete Case 3, it su(cid:28)ces to show that
are set to true. For now, we assume that at least one of these two indices changes after time  . (We prove
excess is increased enough in other places.
the claim later if this assumption does not hold.) Let   be the (cid:27)rst time that this happens and let   be the
We note that in line 17 of Algorithm 1, index(1 ) and index(2 ) are set to  and active1 and active2
impression that arrives at time   . Without loss of generality, we assume that index(1 ) is the one among
these two that changes from  to   at time   . (The following argument also holds even if both of them change
at   .) At time   , line 14 of Mechanism 2 increases excess by 2/3 ⋅ adaptive_gain  ,1 because index(1 ) is
equal to  before it is set to   . In the time period [ + 1,   − 1], the indices of 1 and 2 remain unchanged, and
state still holds. Since 1 is one of the two advertisers that Algorithm 1 selects for   , we know 1 is in the
thus they are always active. Note that if   =  + 1, the time period is empty and the claim about their active
set  at time   , which further implies   ,1 ≥  ,1 −    . Therefore, the if condition on line 7 of Algorithm 1
that excess is increased at time   by
We bound each of the three terms in (2) separately. Since 1 is in set  when both  and   arrive, we
19   by Lemma 4.8. The term ( ,1 −   ,1 )+ is zero if
(2)
Applying the inequality  ,1 −   ,1 ≤    from the de(cid:27)nition of set  shows that excess is increased by
before   and also has a larger weight to advertiser 1 . Since  ≥ [Gain ,1 ], we also have  ≥ 18(1− )
at least

is true for 1 at time   , and adaptive_gain  ,1 is set to ([Gain , ]/3 − ( ,1 −   ,1 )+ /3 − 1 )+ /12. Recall
the weights satisfy  ,1 ≤   ,1 . If we have  ,1 >   ,1 , then [Gain ,1 ] ≥ [Gain  ,1 ] because  arrives

19   .

have [Gain ,1 ] ≥ 18(1− )
19  and [Gain  ,1 ] ≥ 18(1− )

23 ⋅ adaptive_gain  ,1 =  Gain ,1 

−  ,1 −   ,1 +

54
54
− 118 + .
54 ⋅ 18(1 −  ) − 118 + =  324(1 −  )2 − 361
19 
18468(1 −  )  − 118 + .

 18(1 −  )
54 ⋅ 19

−

15

that [Gain , ] ≥ 18(1− )
19

assumption Ɗ >  1 , we have Ɗ >  [Gain1 ,1 ] ≥  ([Gain ,1 ] − Ɗ −  ). Applying Lemma 4.8, we know

that [Gain ,1 ] ≥ 18(1− )
19  , which implies Ɗ >  ( 18(1− )
19  − Ɗ −  ). It follows that Ɗ > 1+ ( 18(1− )
19  −  )+ .

that has been assigned to 1 in lines 31–42 of Algorithm 1 during the time period [ + 1,   − 1]. For each
Now we focus on lower bounding 1 . At time   , 1 is the expected sum of   for every impression  
of these impressions, Mechanism 2 increases excess in lines 21-22 by    /2 or    depending on the
existence of 2 for   . Note that this is consistent with how Algorithm 1 increases 1 . Therefore, excess
is increased in total by at least  324(1− )2 −361
its minimum occurs when either 1 or the expression in the ( )+ operator is zero. In both cases, the lower
18468(1− )  − 118 + +  1 This lower bound proves the claim because
bound is at least  .
We have reached the (cid:27)nal step of Case 3 where we consider the scenario when index(1 ) and index(2 )
both remain unchanged after time  , and thus are active until the end of the algorithm. If for some  ∈ {1 , 2},
impression   (the impression with  =  ∗  ) arrived at or before time  =  , we can lower bound the increase
in excess similar to in Case 2. Since  is assigned to  with probability at least 1/3, Ɗ (  ) ≥ [Gain , ]/3.
Mechanism 2 increases excess by at least one third of this amount in line 11. Since  ∈  , we also know
19  , which proves
such that 1 =  ∗1 and 2 =  ∗2 . If 1 ,1 <  ,1 −  1 , then we prove the claim as follows. First, let Ɗ be the
Now we consider the case where the impressions 1 and 2 arrive after time  , where 1 and 2 are de(cid:27)ned
by Lemma 4.8. Therefore, excess is increased by at least 2(1− )
where  is the value of 1 at the time   when 1 arrives. This lower bound holds because Ɗ compensates for
the claim.
left-hand side of the inequality  ,1 − 1 ,1 >  1 . We note that [Gain1 ,1 ] is at least [Gain ,1 ] − Ɗ −  ,
assigned to 1 between the times between the arrivals of  and 1 . By de(cid:27)nition, 1 ≥ [Gain1 ,1 ]. Using the
how much smaller 1 ,1 is compared to  ,1 and  is an upper bound on the total marginal gains of the edges
Note that we can introduce the ( )+ operator because Ɗ >  1 is positive. Since  arrived before 1 and is
assigned to 1 with probability at least 1/3, variable 1 is at least Ɗ /3. Mechanism 2 increases excess
by exactly (1 −  )1 on line 19 because of our assumption that index(1 ) remains unchanged. Therefore,
claim. The subcase when 2 ,2 <  ,2 −  2 follows similarly.
excess is greater than (1 −  ) 1+ ( 6(1− )
To complete the proof, we now assume 1 ,1 ≥  ,1 −  1 and 2 ,2 ≥  ,2 −  2 . If one of 1 or 2
19  − 3 )+ +   ≥  at the end of the mechanism, which proves the
showed the claim for the scenario that at least one of index(1 ) or index(2 ) changes after time  . To see
this, observe that if 1 is assigned in lines 13–30, then Mechanism 2 increases excessindex( ∗1 ) = excess by
2/3 ⋅ adaptive_gain1 ,1 on line 14 at time 1 . Otherwise, excess is increased in line 23 by  (1 + 2 )
when 1 and 2 arrive since index( ∗1 ) and index( ∗2 ) are still equal to  . Like in the previous paragraph,
value of 1 when 1 arrives. By noting that Ɗ1 ≤  1 and applying Lemma 4.8, we have the inequality
is assigned in lines 13–30, the proof of the claim is identical to the part above starting near (2) where we
derive an analogous lower bound for 2 by replacing Ɗ1 with Ɗ2 and   with   . Therefore, excess is
19  −   )+ . Observe that we can apply the ( )+ operator because 1 is nonnegative. We can
the claim because its minimum occurs when either one of the ( )+ terms are zero, or both   and   are
increased by at least  11+ ( 18(1− )
19  −   )+ +    . This lower bound proves
We give the (cid:27)rst algorithm for online weighted bipartite matching with competitive ratio greater than 1/2
zero. This completes Case 3 and therefore concludes the proof of Lemma 4.3.
(under the free disposal assumption), resolving a central open problem in the literature of online algorithms
16

we can show that 1 ≥ [Gain1 ,1 ] ≥ [Gain ,1 ] − Ɗ1 −   , where Ɗ1 = ( ,1 − 1 ,1 )+ and   is the

1 ≥ 11+ ( 18(1− )

19  −   )+ +    +  11+ ( 18(1− )

5 Conclusion

since the seminal work of Karp et al. [KVV90] thirty years ago. Given the hardness result of Kapralov et
al. [KPV13], our algorithm can be seen as strong evidence that solving the weighted bipartite matching
problem is strictly easier than submodular welfare maximization in online settings. Our main technical
contributions in this work include a novel method for making adaptive decisions that is amenable to
analysis, using the expectation of random variables over all possible branches of the randomized algorithm
to force key variables of the algorithm to be deterministic, and a mechanism that we design solely for the
sake of analysis to systematically reallocate extra marginal gain that the algorithm produces.

References

[BJN07]

[DH09]

[DJK13]

Niv Buchbinder, Kamal Jain, and Joseph Se(cid:28) Naor. Online primal-dual algorithms for maxi-
mizing ad-auctions revenue. In European Symposium on Algorithms, pages 253–264. Springer,
2007.
Nikhil R. Devanur and Thomas P. Hayes. The adwords problem: online keyword matching
with budgeted bidders under random permutations. In Proceedings of the 10th ACM Conference
on Electronic Commerce (EC), pages 71–78. Association for Computing Machinery, 2009.
Nikhil R Devanur, Kamal Jain, and Robert D Kleinberg. Randomized primal-dual analysis of

ranking for online bipartite matching. In Proceedings of the Twenty-Fourth Annual ACM-SIAM

Proceedings of the 12th ACM Conference on Electronic Commerce (EC), pages 29–38. Association

Symposium on Discrete Algorithms (SODA), pages 101–107. Society for Industrial and Applied
Mathematics, 2013.
[DJSW11] Nikhil R. Devanur, Kamal Jain, Balasubramanian Sivan, and Christopher A. Wilkens. Near opti-
mal online algorithms and fast approximation algorithms for resource allocation problems. In
[FHK+10]
[FKM+09]

for Computing Machinery, 2011.
Jon Feldman, Monika Henzinger, Nitish Korula, Vahab S. Mirrokni, and Cli(cid:29) Stein. Online
stochastic packing applied to display ad allocation. In European Symposium on Algorithms,
pages 182–194. Springer, 2010.
Jon Feldman, Nitish Korula, Vahab Mirrokni, Shanmugavelayutham Muthukrishnan, and
Martin Pál. Online ad assignment with free disposal. In International Workshop on Internet and
Network Economics, pages 374–385. Springer, 2009.
[FMMM09] Jon Feldman, Aranyak Mehta, Vahab Mirrokni, and Shan Muthukrishnan. Online stochastic

matching: Beating 1 − 1/ . In Proceedings of the 50th Annual IEEE Symposium on Foundations

of Computer Science (FOCS), pages 117–126. Institute of Electrical and Electronics Engineers,
2009.
[FNW78] Marshall L Fisher, George L Nemhauser, and Laurence A Wolsey. An analysis of approxima-
[GKM+19] Buddhima Gamlath, Michael Kapralov, Andreas Maggiori, Ola Svensson, and David Wajc.
tions for maximizing submodular set functions—ii. In Polyhedral Combinatorics, pages 73–87.
Springer, 1978.
Online matching with general arrivals. arXiv preprint arXiv:1904.08255, 2019.

17

[GKS19]

Buddhima Gamlath, Sagar Kale, and Ola Svensso. Beating greedy for stochastic bipartite

matching. In Proceedings of the Thirtieth Annual ACM-SIAM Symposium on Discrete Algorithms

[HMZ11]
[HPT+19]

applications to adwords. In Proceedings of the Nineteenth Annual ACM-SIAM Symposium on

match when all vertices arrive online. In Proceedings of the 50th Annual ACM SIGACT Symposium

(SODA), pages 2841–2854. Society for Industrial and Applied Mathematics, 2019.
[GM08]
Gagan Goel and Aranyak Mehta. Online budgeted matching in random input models with
[HKT+18] Zhiyi Huang, Ning Kang, Zhihao Gavin Tang, Xiaowei Wu, Yuhao Zhang, and Xue Zhu. How to
Discrete Algorithms (SODA), pages 982–991. Society for Industrial and Applied Mathematics,
2008.
on Theory of Computing (STOC), pages 17–29. Association for Computing Machinery, 2018.
Bernhard Haeupler, Vahab S Mirrokni, and Morteza Zadimoghaddam. Online stochastic
weighted matching: Improved approximation algorithms. In International Workshop on Internet
and Network Economics, pages 170–181. Springer, 2011.
Zhiyi Huang, Binghui Peng, Zhihao Gavin Tang, Runzhou Tao, Xiaowei Wu, and Yuhao Zhang.
Tight competitive ratios of classic matching algorithms in the fully online model. In Proceedings
Society for Industrial and Applied Mathematics, 2019.
bipartite matching: Beating 1 − 1/ with random arrivals. ACM Transactions on Algorithms
Zhiyi Huang and Runzhou Tao. Understanding Zadimoghaddam’s edge-weighted online
matching algorithm: Unweighted case. arXiv preprint arXiv:1910.02569, 2019.
[HTWZ19] Zhiyi Huang, Zhihao Gavin Tang, Xiaowei Wu, and Yuhao Zhang. Online vertex-weighted
(TALG), 15(3):38, 2019.
Zhiyi Huang. Understanding Zadimoghaddam’s edge-weighted online matching algorithm:

of the Thirtieth Annual ACM-SIAM Symposium on Discrete Algorithms (SODA), pages 2875–2886.

[Hua19]

[HT19]

Weighted case. arXiv preprint arXiv:1910.03287, 2019.

[ JL13]

[KMT11]

Patrick Jaillet and Xin Lu. Online stochastic matching: New algorithms with better bounds.

Mathematics of Operations Research, 39(3):624–646, 2013.

Chinmay Karande, Aranyak Mehta, and Pushkar Tripathi. Online bipartite matching with

[KMZ13]

unknown distributions. In Proceedings of the Forty-Third Annual ACM Symposium on Theory of

Maximizing weight and cardinality. In International Conference on Web and Internet Economics,

Computing (STOC), pages 587–596. Association for Computing Machinery, 2011.
maximization: Greedy beats 1/2 in random order. SIAM Journal on Computing, 47(3):1056–1086,
Nitish Korula, Vahab S. Mirrokni, and Morteza Zadimoghaddam. Bicriteria online matching:
pages 305–318. Springer, 2013.
[KMZ18] Nitish Korula, Vahab Mirrokni, and Morteza Zadimoghaddam. Online submodular welfare
 -matching. Theoretical Computer Science, 233(1-2):319–325, 2000.
2018.
Bala Kalyanasundaram and Kirk R. Pruhs. An optimal deterministic algorithm for online
18

[KP00]

[KPV13]

[KVV90]

[LLN06]

[Meh13]

[MGS12]

[MGZ12]

[MP12]

Michael Kapralov, Ian Post, and Jan Vondrák. Online submodular welfare maximization: Greedy

is optimal. In Proceedings of the Twenty-Fourth Annual ACM-SIAM Symposium on Discrete

Algorithms (SODA), pages 1216–1225. Society for Industrial and Applied Mathematics, 2013.
Richard Karp, Umesh Vazirani, and Vijay Vazirani. An optimal algorithm for on-line bipartite

matching. In Proceedings of the Twenty-Second Annual ACM Symposium on Theory of Computing

(STOC), pages 352–358. Association for Computing Machinery, 1990.
Benny Lehmann, Daniel Lehmann, and Noam Nisan. Combinatorial auctions with decreasing
marginal utilities. Games and Economic Behavior, 55(2):270–296, 2006.
Aranyak Mehta. Online matching and ad allocation. Foundations and Trends in Theoretical
Computer Science, 8(4):265–368, 2013.
Vahideh H. Manshadi, Shayan Oveis Gharan, and Amin Saberi. Online stochastic matching:
Online actions based on o(cid:31)ine statistics. Mathematics of Operations Research, 37(4):559–573,
2012.
Vahab S. Mirrokni, Shayan Oveis Gharan, and Morteza Zadimoghaddam. Simultaneous
approximations for adversarial and stochastic online budgeted allocation. In Proceedings of the

Twenty-Third Annual ACM-SIAM Symposium on Discrete Algorithms (SODA), pages 1690–1701.

Society for Industrial and Applied Mathematics, 2012.
Aranyak Mehta and Debmalya Panigrahi. Online matching with stochastic rewards.

In

Proceedings of the 53rd Annual IEEE Symposium on Foundations of Computer Science (FOCS),

pages 728–737. Institute of Electrical and Electronics Engineers, 2012.
[MSVV05] Aranyak Mehta, Amin Saberi, Umesh Vazirani, and Vijay Vazirani. Adwords and generalized

on-line matching. In Proceedings of the 46th Annual IEEE Symposium on Foundations of Computer

Science (FOCS), pages 264–273. Institute of Electrical and Electronics Engineers, 2005.
[MWZ15] Aranyak Mehta, Bo Waggoner, and Morteza Zadimoghaddam. Online stochastic matching with

unequal probabilities. In Proceedings of the Twenty-Sixth Annual ACM-SIAM Symposium on

Discrete Algorithms (SODA), pages 1388–1404. Society for Industrial and Applied Mathematics,
2015.
Mohammad Mahdian and Qiqi Yan. Online bipartite matching with random arrivals: An
approach based on strongly factor-revealing LPs. In Proceedings of the Forty-Third Annual
Morteza Zadimoghaddam. Online weighted matching: Beating the 12 barrier. arXiv preprint
ACM Symposium on Theory of Computing (STOC), pages 597–606. Association for Computing
Machinery, 2011.

arXiv:1704.05384, 2017.

[MY11]

[Zad17]

19

A Missing Analysis from Section 3

A.1 Proof of Lemma 3.1

the values of Priority and R . All other variables (e.g., the maximum gains  , all values of adaptive_gain , ,
the sequence of sets  and choices 1 and 2 , all updates to active(), index(), partner(), and the sums  )
Lemma 3.1. The only random variables in StochasticGreedy are the assignments of the impressions and
are deterministic quantities that depend solely on the instance and the arrival order of the impressions.

Proof. We proceed by induction on  . At the beginning of Algorithm 1 when  = 0, all variables are initialized
the variables at time  ≥ 1. Recall that impression  arrives at time  =  and is predetermined by the arrival
order. We begin by considering lines 4–12. The maximum expected gain  is an expected value over all
branches of the randomized algorithm up to time  and is deterministic by de(cid:27)nition. For each  ∈ , the
deterministically. Assuming the claim as the induction hypothesis, we proceed by analyzing the state of all
current values of active(), index(), and  are deterministic by the induction hypothesis. Therefore, all
values of adaptive_gain , and the set of candidates  at time  are also deterministic.
If we have | | ≥ 2, then the algorithm executes lines 13–30. In this case, the advertisers 1 and 2 are the
partner() in line 18 is (cid:27)xed by the induction hypothesis. The branching in lines 22–30 possibly depend
Therefore, the claim holds for all time steps  by induction.
impression  and updates to the variables Priority1 and Priority2 are made. In the second case, if | | ≤ 1,
maximizers of deterministic quantities and therefore deterministic themselves, assuming ties are broken
on the random values of R and Priority , but in all of these conditional statements, only the assignment of
lexicographically. The updates that occur in lines 16–21 do not rely on any randomness since the value of
then the algorithm executes lines 31–42. The only randomness here is the assignment of  to either 1 or 2 .

A.2 Proof of Lemma 3.2

algorithm. In particular, we can e(cid:28)ciently compute the value [Gain , ] at any point in StochasticGreedy.
Lemma 3.2. We can maintain the probability mass function for all random variables Gain , over the course of

of the impressions. Furthermore, recall that Gain , = ( , − MaxW −1
the values of Priority . To be speci(cid:27)c, Lemma 3.1 shows that the values of adaptive_gain , and the top
candidates 1 and 2 are deterministic and depend solely on the underlying instance and the arrival order
)+ . Therefore, it su(cid:28)ces to maintain
Proof. First, recall that Algorithm 1 is essentially deterministic except for the impression assignments and
the probability mass function for the random variables MaxW at each time step of the algorithm.
We proceed by induction on  . Let   = {0, , 1 , , 2 , , … ,  −1 , } denote the set of possible weights
assigned to advertiser  at the beginning of time step  ≥ 1, and recall that 0, = 0. Note that we use  to
denote the impression that arrives at time  . Next, let   ࢼ   → Ó≥0 be the probability mass function
for the random variable MaxW at the beginning of iteration  . Since the state of advertiser  , namely
Priority , is randomized, we re(cid:27)ne the distribution   into three conditional distributions. Let   ,2 be the
probability that Priority = 2 at the beginning of time  , and let   ,2 be the distribution for MaxW given that
Priority = 2 at the beginning of time  . The probabilities   ,1 ,   ,0 and conditional distributions   ,1 ,   ,0
are de(cid:27)ned similarly. We note that if a distribution is not well-de(cid:27)ned because   , = 0 for some  ∈ {0, 1, 2},
and always pull information from reachable states. Note also that we have  , = ∑2 =0   ,   , by the law of
No impressions have been assigned at the beginning of time  = 1, so for every advertiser  ∈  the
probabilities are set to  1 ,0 = 1,  1 ,1 = 0,  1 ,2 = 0 and the distributions satisfy  1 , (0) = 1 for all  ∈ {0, 1, 2}.
our analysis still holds since because these update rules simulate all branches of the randomized algorithm
total probability if we treat the addition and scalar multiplication of probability distributions like vectors.
20

Case 1 (| | ≤ 1 and | ∪  | = 1).

not change, so set   +11 , ←  1 , for  ∈ {0, 1, 2}. Since  is assigned to 1 , we update   +11 , ← max( ,1 ,  1 , )

Priority1 and Priority2 remain unchanged, so set   +11 , ←  1 , and   +12 , ←  2 , for  ∈ {0, 1, 2}. Impression 

This completes the base case when  = 1, so now assume  ≥ 1 and that we have computed   , and   , for
all  ∈  and  ∈ {0, 1, 2}. We show how to compute all of these quantities at time  + 1. Let 1 and 2 be the
top candidates at time  , and let  =  . (If 2 does not exist, we can ignore this term.) For every advertiser
 ∈   {1 , 2}, the state of Priority does not change and  does not receive impression  . Therefore, we
have   +1 , ←   , and   +1 , ←   , for  ∈ {0, 1, 2}. Now we focus on updating these values for 1 and 2 .
First consider the case where 2 does not exist. The state Priority1 does
for each  ∈ {0, 1, 2}. Here, the max operator transfers all probability mass from entries less than  ,1 to
the value  ,1 .
Case 2 (| | ≤ 1 and | ∪ | ≥ 2). Now suppose | | ≤ 1 and that there are two candidates 1 and 2 . The states
is assigned to 1 or 2 with equal probability 1/2, independent of their current states. Therefore, we update
Case 3 (| | ≥ 2 and adaptive_gain , = 0). Now suppose that | | ≥ 2 and that adaptive_gain , = 0.
sets Priority1 ← 0 and Priority2 ← 0 on line 27. The impression  is randomly assigned to 1 or 2 with
Recall that adaptive_gain , is a deterministic quantity governed solely by the instance and arrival order.
The algorithm is guaranteed to enter the conditional statement on line 25, so we update the priority
equal probability on line 26, so we update the two well-de(cid:27)ned conditional distributions to be
The values of the other distributions   +11 ,1 ,   +11 ,2 ,   +12 ,1 ,   +12 ,2 do not matter, so we leave them unchanged. We
note that the equations above can be simpli(cid:27)ed using the distributive property of the max operator, but the
Case 4 (| | ≥ 2 and adaptive_gain , > 0). Now assume | | ≥ 2 and adaptive_gain , > 0, where 
determine how  is assigned. First, observe that we should set   +11 , ← 13 and   +12 , ← 13 for  ∈ {0, 1, 2}
recurrences are easier to verify in their current form.
is de(cid:27)ned in line 21 of Algorithm 1. In this case, the randomness of R and the current states of 1 and 2
then we can determine how the algorithm branched at time  based on the value of R . This provides us with
distributions   +1 , for  ∈ {0, 1, 2}. If we condition on the value of Priority at the beginning of time  + 1,
since each of the three main branches is equally likely. Now let us focus on computing the conditional
recurrence relations for the distributions   +1 , , where we consider all possible previous states of Priority

the conditional distributions to be   +11 , ← 12 max( ,1 ,  1 , ) + 12  1 , and   +12 , ← 12  2 , + 12 max( ,2 ,  2 , ),
probabilities to be   +11 ,0 ← 1,   +11 ,1 ← 0,   +11 ,2 ← 0 and   +12 ,0 ← 1,   +12 ,1 ← 0,   +12 ,2 ← 0 since the algorithm
  +11 ,0 ← 12 2∑ =0  1 , max  ,1 ,  1 ,  + 12 2∑ =0  1 ,  1 , ,
  +12 ,0 ← 12 2∑ =0  2 ,  2 , + 12 2∑ =0  2 , max  ,2 ,  2 ,  .

for each  ∈ {0, 1, 2}.

in each equation:  +1 ,0 ←   ,2 max  , ,   ,2 +   ,1 1 ,1 +   ,0  12 max  , ,   ,0 + 12   ,0 ,
  +1 ,2 ←   ,2  ,2 +   ,1  ,1 +   ,0  ,0 .
  +1 ,1 ←   ,2 max  ,   ,2 +   ,1 max  ,   ,1 +   ,0 max  ,   ,0 ,

21

B Missing Analysis from Section 4

  +13− ,0 ←   ,2 3− +   ,1 2∑ =0  3− , max  ,3− ,  3− ,  +   ,0  12  3− + 12 2∑ =0  3− , max  ,3− ,  3− ,  ,
  +13− ,2 ←  3− ,2 3− ,2 +  3− ,1 3− ,1 +  3− ,0 3− ,0 .
  +13− ,1 ←  3− ,2 max  ,3− ,  3− ,2 +  3− ,1 max  ,3− ,  3− ,1 +  3− ,0 max  ,3− ,  3− ,0 ,

We can compute the distributions   +13− , for  ∈ {0, 1, 2} similarly, though in the adaptive decisions we need
to account for the probabilities   ,  . We use the equality   = ∑2 =0   ,   , in the following recurrences:
for each  ∈  and  ∈ {0, 1, 2}. Therefore, by induction, we can maintain the distribution   for MaxW at
each time step  , and hence compute the exact values of [Gain , ].
In all four cases, we have shown how to compute the probabilities   +1 , and conditional distributions   +1 ,
We use the following properties of the operators ( )+ and [X ࢼ  ] throughout this section.
value of X conditioned on  . If this conditional probability is not well-de(cid:27)ned, we assume that [X ࢼ  ] = 0.
For any random variable X and event  , de(cid:27)ne [X ࢼ  ] to be [X ࢯ  ] Pr( ) where [X ࢯ  ] is the expected
Before we give the proofs of Lemma 4.6 and Lemma 4.7, we present two self-contained, prerequisite lemmas.
Proof. There are 3! = 6 possible orderings for  ,  , and  . We consider each case separately:
• Case 1 ( ≥  ≥  ). Property 1 is equivalent to  −  ≥ ( −  ) − 0, which is true. Property 2 is
equivalent to  −  ≥ ( −  ) + 0 − 0, which is also true.
• Case 2 ( ≥  ≥  ). Property 1 is equivalent to  −  ≥ ( −  ) − ( −  ), which is true. Property 2 is
equivalent to  −  ≥ ( −  ) + ( −  ) − 0, which is also true.
• Case 3 ( ≥  ≥  ). Property 1 is equivalent to 0 ≥ 0 − ( −  ), which is true. Property 2 is equivalent
to 0 ≥ 0 + 0 − 0, which is also true.
• Case 4 ( ≥  ≥  ). Property 1 is equivalent to 0 ≥ 0 − ( −  ), which is true. Property 2 is equivalent
• Case 5 ( ≥  ≥  ). Property 1 is equivalent to 0 ≥ ( −  ) − ( −  ), which is true. Property 2 is
to 0 ≥ 0 + 0 − ( −  ), which is also true.
• Case 6 ( ≥  ≥  ). Property 1 is equivalent to 0 ≥ 0 − ( −  ), which is true. Property 2 is equivalent
equivalent to  −  ≥ 0 + ( −  ) − ( −  ), which is also true.
to 0 ≥ 0 + ( −  ) − ( −  ), which is also true.
This completes the proof.

Lemma B.1. For any three real numbers  ,  ,  ∈ Ó, we have:

1. ( − max{ ,  })+ ≥ ( −  )+ − (max{ ,  } −  ),
2. ( −  )+ ≥ ( − max{ , })+ + ( −  )+ − ( −  )+ .

22

[X] ≥ ∑=1 [X ࢼ  ].

Lemma B.2. For any  disjoint events 1 , 2 , … ,  and nonnegative random variable X, we have
If these disjoint events span the probability space, that is, ∑=1 Pr( ) = 1, the inequality can be replaced by
equality, even if X is not nonnegative.

Proof. Let  be the event that none of 1 , 2 , … ,  occur. The  + 1 events  , 1 , 2 , … ,  are all disjoint
and span the entire probability space. Therefore, for any random variable X (not necessarily nonnegative),
which proves the second part of the claim since Pr( ) = 0 if 1 , 2 , … ,  span the probability space. For
the law of total expectation gives us
the (cid:27)rst part, if X is nonnegative then [X ࢼ  ] is also nonnegative, and therefore [X] ≥ ∑=1 [X ࢼ  ],
which concludes the proof.

[X] = X ࢯ   Pr  + ∑=1 [X ࢯ  ] Pr( ) = X ࢼ   + ∑=1 [X ࢼ  ],

B.1 Proof of Lemma 4.6

+ adaptive_gain ,1 + adaptive_gain ,2 .

Lemma 4.6. If impression  is assigned in lines 13–30 of StochasticGreedy, i.e., case | | ≥ 2, we have

[MarginalGain ] ≥ Gain ,1  + Gain ,2 
[MarginalGain ] ≥ [Gain ,1 ] + [Gain ,2 ]

Proof. Recall that the variables adaptive_gain , and  are not random variables as show in Lemma 3.1—
they are completely determined by the arrival order of the impressions. Recall also that  ∈ {1, 2} is set in
we also have adaptive_gain ,3− = 0, so we need to show that
line 21 of Algorithm 1 such that adaptive_gain , ≥ adaptive_gain ,3− . If adaptive_gain , = 0, then
This is evident because  is assigned to 1 or 2 with equal probability on line 26. Therefore, we focus on
the case adaptive_gain , > 0 for the rest of the proof.
Therefore, conditioning on the event adaptive_gain , > 0 does not a(cid:29)ect the distribution of R variables.
The adaptive_gain , variables are computed in terms of the expected values [Gain , ] and therefore
to 1 or 2 symmetrically. We also note that Priority ∈ {1, 2} is associated with the events R  ∈ [1/3, 2/3) or
do not depend on the state of the algorithm (i.e., independent of the previous coin tosses R ) by Lemma 3.2.
of the algorithm) unless R ∈ [0, 1/3) and Priority ∈ {1, 2}. This is the only cases where  is not assigned
Impression  is assigned to 1 or 2 with equal probability (and also independently to prior decisions
R  ∈ [2/3, 1), where   is equal to index( ) at the beginning of time  =  when  arrives. Let   =   < 
be the time that   arrives. Observe that the random variables R and R  are independent of each other.
Therefore, by the law of total expectation, the expected marginal gain [MarginalGain ] is equal to

2

2

.

Gain ,1  + Gain ,2 

+ Gain , ࢼ 2  + Gain ,3− ࢼ 1 

+ Gain , ࢼ 0  + Gain ,3− ࢼ 0 

,

3

3

6

23

2

6

.

6

6

≥ 2 ⋅ adaptive_gain ,

the space, so Lemma B.2 implies ([Gain , ࢼ 2 ] + [Gain , ࢼ 0 ])/6 = ([Gain , ] − [Gain , ࢼ 1 ])/6.

Gain ,1  + Gain ,2 
+ Gain , ࢼ 2  + Gain ,3− ࢼ 1 
− Gain , ࢼ 1  + Gain ,3− ࢼ 2 
[Gain , ࢼ 2 ] − [Gain , ࢼ 1 ]
[Gain ,3− ࢼ 1 ] − [Gain ,3− ࢼ 2 ] ≥ 0.
Gain , =  , − MaxW −1 + ≥  , − MaxW  −1 + − MaxW −1 − MaxW  .
Gain , ࢼ 2  ≥  , − MaxW  −1 + ࢼ 2  − MaxW −1 − MaxW  ࢼ 2 
≥  , − MaxW  −1 +
− MaxW −1 − MaxW  ,
)+ ࢼ 2 ] = [( , − MaxW  −1

where 2 , 1 , and 0 are the events that Priority is equal to 2, 1, and 0, respectively. These events partition
An analogous decomposition also holds for [Gain ,3− ]/6. Therefore, [MarginalGain ] is equal to
Since adaptive_gain , ≥ adaptive_gain ,3− , it su(cid:28)ces to prove the inequalities
(3)
and
Proof of Inequality (3). Conditioning on the event 2 , we can apply Property 1 in Lemma B.1 to get
(4)
to complete the proof.
Therefore, we have MaxW  −1 = MaxW  . Taking the conditional expectation implies that
Recall that   = index( ) and note that no impression is assigned to  at time   according to the event 2 .
where the second inequality holds for the following reasons. The random variable MaxW  −1
)+ ]/3. We can also apply Lemma B.2 for
(5)
In this interval, index( ) has remained the same since it was last set to   at time   . Therefore, any
The term MaxW −1 − MaxW  represents all assignments to  in the time range [  + 1,  − 1], inclusive.
is independent
impression   assigned to  in this time period has been allocated in lines 31–42 of the algorithm (i.e., the
case | | ≤ 1). Since we increment  for each of these assignments accordingly (by either   or   /2
depending on whether  is the only candidate or not), the expected increase of MaxW in this time period
is upper bounded by  . Formally, we have [MaxW −1 − MaxW  ] ≤  . Therefore, it follows from (5) that
(6)
By Property 2 of Lemma B.1, we always have the inequality

Gain , ࢼ 2  ≥  , − MaxW  −1 + 
 , − MaxW  −1 + ≥  , − max  , , MaxW  −1 + +   , − MaxW  −1 + −   , −  , + .
[Gain , ࢼ 2 ] ≥  , − max  , , MaxW  −1 +
+   , − MaxW  −1 + 
−   , −  , +
≥ Gain , ࢼ 1  + Gain  , 
−   , −  , +

of event 2 , so [( , − MaxW  −1

the nonnegative term MaxW −1 − MaxW  and to get [MaxW −1 − MaxW  ࢼ 2 ] ≤ [MaxW −1 − MaxW  ].

3

(7)

− 

(8)

−  .

3
−  ,

By combining inequalities (6) and (7), it follows that

3

3

3

3

3

24

3

≥ Gain , ࢼ 1 ,
[Gain , ࢼ 2 ] − [Gain , ࢼ 1 ] ≥ 12 ⋅ adaptive_gain , ,

Thus, Gain , = ( , − MaxW −1 )+ is at most ( , − max{  , , MaxW  −1 })+ . Observing that MaxW  −1
independent of 1 , we have , − max  , , MaxW  −1 + 

where (8) is proved as follows. Conditioning on event 1 , we know that impression   was assigned to  .
is
which proves (8). Note that the only assumption we used to prove (8) is that adaptive_gain , is positive.
Therefore, the de(cid:27)nition of adaptive_gain , in line 8 of Algorithm 1 and inequality (8) imply that
Impression   is assigned to either  or  . The index values of these two advertisers are unchanged
Proof of Inequality (4). Let  be partner( ) at time  − 1 (right before 3− becomes the partner of  ).
which completes the proof of inequality (3).
contradicts the positivity of adaptive_gain , . Moreover, the advertiser  ≠ 0 is well-de(cid:27)ned by the
in the interval [  + 1,  − 1], otherwise active( ) would be false at the beginning of time  , which
positivity of adaptive_gain , since active ( ) = true. Therefore, all impressions assigned to  or 
and has neither  nor  as its candidates is also assigned independent to the value of R  . In particular,
assigned independent to the value that R  takes for impression   . Each impression that arrives before time 
conditioning on the events Priority = 1 or Priority = 2 does not change the distribution of MaxW −1
in this interval must be non-adaptive decisions (i.e., are assigned in lines 31–42). These impressions are
which proves the inequality. If 3− =  , then all assignments to 3− at times  = {1, 2, … ,  − 1}  { }
are independent of both events Priority = 1 and Priority = 2. If we (cid:27)x the set of impressions assigned
to 3− at times in  , conditioning on Priority = 1 compared to conditioning on Priority = 2 can only
for
decrease MaxW −13− , which further implies [Gain ,3− ࢼ 1 ] ≥ [Gain ,3− ࢼ 2 ]. This completes the proof
of inequality (4), and therefore the proof of the lemma.
Proof. If adaptive_gain , = 0, then the claim is trivial. Therefore, assume that
where   is equal to index() at the beginning of time  =  . Indices are set on line 17 of Algorithm 1, so
adaptive_gain , > 0 implies that   is assigned to an advertiser in lines 13–30. Without loss of generality,
assume  is the (cid:27)rst choice of   (i.e.,  = 1 and  ≠ 2 ). The proof for the other case is identical by swapping
Priority = 1 or Priority = 2. Let 1 be the event that   is assigned to  in line 29, or, equivalently, the
We focus on the events where   is assigned in lines 29–30 of Algorithm 1, which correspond to setting
event Priority = 1. We note that   might be assigned to  in lines 13–30 but this is not included in the

any  ∈   { , }. Therefore, if 3− is not the same as  , then [Gain ,3− ࢼ 1 ] = [Gain ,3− ࢼ 2 ],

Lemma 4.7. For any advertiser  ∈  and impression  ∈  , we have adaptive_gain , ≤ 112 [Gain , ].

B.2 Proof of Lemma 4.7

adaptive_gain , = Gain  , /3 −   , −  , + /3 − 

> 0,

12

the Priority = 1 and Priority = 2 terms.

25

event 1 . Similarly, let 2 be the event that   is assigned to the second candidate in line 30. Let   =   be
the time   arrives. Conditioning on 2 , we can apply (8) from the proof of Lemma 4.6, which gives us
have [Gain , ] ≥ [Gain , ࢼ 2 ] ≥ 12 ⋅ adaptive_gain , by Lemma B.2 since Gain , random variables are
Since adaptive_gain , > 0, inequality (9) implies [Gain , ࢼ 2 ] ≥ 12 ⋅ adaptive_gain , . Therefore, we
(9)
always nonnegative.

Gain , ࢼ 2  ≥ Gain , ࢼ 1  + Gain  , 
≥ Gain  , 
−   , −  , +

−  .

3

−   , −  , +

3

− 

3

3

26

