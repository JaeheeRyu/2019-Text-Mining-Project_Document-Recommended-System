9
1
0
2

v
o

N

1
2

]

L

C

.

s

c

[

1
v
3
7
3
9
0

.

1
1
9
1

:

v

i

X

r

a

Entity Extraction with Knowledge from Web
Scale Corpora

Zeyi Wen1 , Zeyu Huang2 , and Rui Zhang2

1The University of Western Australia, 2The University of Melbourne

zeyi.wen@uwa.edu.au, {z.huang56@student.,rui.zhang@}unimelb.edu.au

Abstract. Entity extraction is an important task in text mining and
natural language processing. A popular method for entity extraction is
by comparing substrings from free text against a dictionary of entities.
In this paper, we present several techniques as a post-processing step for
improving the eﬀectiveness of the existing entity extraction technique.
These techniques utilise models trained with the web-scale corpora which
makes our techniques robust and versatile. Experiments show that our
techniques bring a notable improvement on eﬃciency and eﬀectiveness.
Keywords: Entity Extraction · String Matching · Pre-trained Model.

1

Introduction

Entity extraction is widely used in text mining and natural language processing.
For example, it can be used for pre-processing unstructured text: tagging and
highlighting the named entities of interest. A common approach for approximate
entity extraction is by comparing a substring against an entity. The approach
identiﬁes the candidate substrings from free text that match a given list of named
entities. For ease of presentation, we use “dictionary” to refer to the list and
“entities” to refer to the named entities. Our previous work [16] developped the
“2ED” algorithm, which this paper is built on, represents a string matching
approach for entity extraction. 2ED is based a distance that considers both
character-level edit-distance and token-level edit-distance between a substring
from the text and an entity from the dictionary.
Although 2ED reaches a high F1 score with improved eﬃciency compared to
other techniques, the limitation of 2ED is that 2ED is based on lexical evidence
of the text and the dictionary, which lacks the ability to catch syntactical and
semantical evidence within the text and the dictionary. To improve 2ED, we pro-
pose multiple techniques including (i) using web-scale corpora for distinguishing
a typo from an intended token in the substring, (ii) estimating word similarity
using word embedding, and (iii) other improvements including more advanced
tokenisation. We implement our proposed techniques as a post-processing step
for the 2ED algorithm. According to our evaluation, the post-processing brings
47% improvement measured by area under the Receiver Operating Characteris-
tic (ROC) curve, in predicting whether a matched substring represents a valid
entity in the dictionary.

 
 
 
 
 
 
2

Z. Wen, Z. Huang and R. Zhang

2 Related Work

Another widely adopted approach to entity extraction is machine Learning such
as Recurrent Neural Networks (RNN) [15], Convolutional Neural Networks (CNN) [2]
and SVMs [17]. Most of the machine learning based approaches do not require
a dictionary consisting of entities of interest. In this paper, we mainly focus on
the string matching approach which ﬁnds the nearest neighbour of an entity [6].
There have been various research in the area of entity extraction. Chiu et al. [2]
used an architecture combining Long Short-Term Memory (LSTM) and CNN for
named entity extraction, which can utilise both token-level and character-level
evidence. Wei et al. [15] applied an RNN to entity extraction tasks in the medi-
cal domain. Besides the machine learning approaches, there are also researches
focused on string matching for entity extraction, like the concept recognition
system in [14] which implemented a dictionary based entity extraction tool as a
component of an NLP pipeline.
Recent works in language models also inspired our work in this paper. Heaﬁeld
[5] proposed an eﬃcient language model enabling fast queries which also uses
space-eﬃcient data structures like TRIE. The BerkeleyLM pro ject [5] enables the
storage of large n-gram language models with a fast and small data structure.
Progress in the research of word embedding has facilitated many text mining
tasks. Mikolov et al. [12] explored the performance of representations of words
in a vector-space and demonstrated that these vectors captures various features
and rules in the language without human intervention during the training phase.
Whitelaw et al. [18] proposed an spell-checking and autocorrection method that
utilises the web as a noisy corpus. The approach used shares some common fea-
tures with our method in this paper as it makes use of the web as a source
for training a language model. One ma jor diﬀerence is that in their approach a
machine learning based classiﬁer was further used while we rely on a rule-based
method for classiﬁcation.

3 The 2ED Algorithm

The previous 2ED algorithm proposed a novel method for estimating the distance
(similarity) between a candidate substring and an entity called FuzzyED [16].
The novelty of FuzzyED is in that it proposed a function for measuring similarity
between two strings that consist of a sequence of tokens, by taking into consider-
ation both the character-level and the token-level edit-distances. Together with
the function is a series of techniques for improving the performance by produc-
ing highly promising candidate sub-strings in an eﬃcient way [16]. We will ﬁrst
introduce some key features of the 2ED algorithm which are related to the post-
processing work that we will discuss in this paper. We will also point out some
potential weaknesses of this previous work that enlightened the improvements
we will propose.

Entity Extraction with Knowledge from Web Scale Corpora

3

3.1 Features of the 2ED Algorithm

Using IDF to assign weights to tokens : One important idea that is exploited
throughout the process of 2ED is that tokens in an entity should have discrim-
inated weights. The algorithm proposed using (normalised) Inverted Document
Frequency (IDF) as the weight for each token [16], which makes use of informa-
tion from the dictionary. The IDF of a token in the dictionary is a representation
of its relative importance in an entity. If a token is rarely seen in the dictionary,
the 2ED algorithm assumes it is more substantial in a named entity. Such tokens
are called “core” tokens and should form an essential part of an entity [16]. The
weights of tokens are widely used in several steps of the algorithm including the
sub-string generation step, where “core” tokens are used as the starting point for
spanning; the spanning step, for determining the point for terminating spanning
of the sub-string; the shrinking step, for updating the lower bound dissimilarity;
and ﬁnally the computation of FuzzyED.
Pruning and ﬁltering : Since the computation of FuzzyED is relatively expen-
sive, multiple methods for reducing the number of candidates are proposed. The
algorithm uses sophisticated spanning and shrinking techniques for generating
candidate substrings from the text which is proven to be more eﬃcient than enu-
meration based substring generation algorithms [16]. In addition, some general
ﬁlters utilizing information from IDF are used to further reduce the candidates
for FuzzyED computation.
Calculation of FuzzyED score : The ﬁnal step for determining whether a can-
didate matches an entity is to calculate similarity between the two strings by
applying the FuzzyED algorithm. The formal deﬁnition is F uzzyED(E , S ) =
CD (S ) + CI (E ) + CS (E , S ), were E and S denotes the entity and the substring
respectively. CD (·) denotes the deletion cost of removing a token; CI (·) denotes
the insertion cost of inserting a token; CS (·) denotes the substitution cost of
substituting a token in S with a similar token in E .
As we can see, both character-level and token-level edit-distances are consid-
ered. The character-level edit-distance is in the substitution cost part, where the
cost of substituting a token with another is related to the edit-diﬀerent of the
two tokens. The token-level edit-distance is calculated in a way similar to the
character-level version, which is a well studied dynamic programming problem.
The resulting FuzzyED score is in the range of [0,1] representing the similarity
between the substring and the entity, where a score of 1 means exact match.
The pairs with a FuzzyED score greater or equal to the threshold will be added
to the extracted entity list.
Parameters in 2ED : The 2ED algorithm features hyper-parameters that can
be used to tune the thresholds for two levels of similarities [16]. The hyper-
parameters include δ for token-level similarity threshold and τ for character-level
similarity thershold. The two hyper-parameters are in domain [0,1].

3.2 Drawbacks of the 2ED algorithm

Applicability of IDFs as token weights : As described above, the 2ED algorithm
uses the dictionary as the source for obtaining Inverted Document Frequency.

4

Z. Wen, Z. Huang and R. Zhang

The intuition of this approach is to assign diﬀerent weights to the tokens within
an entity so that they can reﬂect their relative importance in the entity. Such
weights are ﬁnally used to calculate the FuzzyED score as in formula (1). How-
ever, the approach for weighing diﬀerent tokens directly from the dictionary
may have some potential vulnerabilities. Suppose a data analysis practitioner
is interested in Australian educational institutes on Wikipedia and a dictionary
speciﬁcally designed for this purpose is used. Then the IDF of tokens such as
“Australian” and “University” can be much smaller than desired. In this case,
IDF from a more comprehensive corpus might be a better ﬁt. In fact, the experi-
mental data sets used in the validation of the 2ED algorithm contain dictionaries
of millions of named entities, which makes it more appropriate to use IDF as
token weights.
Eﬀectiveness of token-level edit-distance : The proposed function for FuzzyED
calculates the cost of token-level edit by the sum of three operations: insertion,
deletion and substitution. Thus the cost of transforming “Alpha Beta” to “Beta
Alpha” is one deletion and one insertion, which is the sum of weights of the
two words. This is because the same operations are used for token-edit and
character-edit. But as an observation from the English language, token-edit and
character-edit are diﬀerent. In the above example, we might have over estimated
the distance between “Alpha Beta” and “Beta Alpha”. Consequently, more oper-
ations on the token level needs to be introduced and their costs should be studied
to reﬂect the linguistic “distance”. Another concern about including token-level
edit-distance in the FuzzyED algorithm is its signiﬁcance in real world use cases,
i.e. how many matched pairs (between a substring and an entity) truly incur a
token-level edit operation. We will show that this concern is valid for the cor-
pus and dictionary we have chosen in the next section where statistics of the
validation of the current 2ED algorithm will be presented.

4

Improvement on 2ED

The eﬀectiveness of the 2ED algorithm in terms of precision and recall was
studied in previous works [16]. The metrics used are listed in the table below.

Table 1: Metrics used for evaluating 2ED

notation
tp
f p
f n
p
r

description
deﬁnition
True Positive count # of correctly returned entities
False Positive count # of wrongly returned entities
False Negative count
# of missed entities
p = tp
Precision
Recall
r = tp

tp+f p

tp+f n

According to the study in [16], 2ED reached a recall of above 96% when
token-level threshold is set to 0.9, and above 99% when is set to 0.85 on some

Entity Extraction with Knowledge from Web Scale Corpora

5

data sets. While the results above are impressive, we also explored the previous
2ED algorithm in greater detail by focusing on its performance on two kinds
of edit-distances respectively. We use a lower threshold of at 0.8 to allow us to
observe as many matched pairs of substrings and entities (hereafter “matched
pairs”) as reasonably possible. The data set used at this stage is a corpus of IMDb
reviews [8] and a dictionary of movie titles obtained from the IMDb website [4].
The results of running 2ED algorithm on this data set are as follows: (i) the
number of sub-strings matched (with δ = τ = 0.8) is 39908, and (ii) the number
of sub-strings approximately matched (score<1) is 7540.
The table below shows a summary of the labelled matched pairs.

Table 2: Summary of the labelled matched pairs from 2ED

Summary of labelled matched pairs
Total f p tp
number of matched pairs labelled
200 47 153
matched pairs with token-level edit-distance
42
20 22
matched pairs with character-level edit-distance 156
27 129
matched pairs with both levels of edit-distance
2
0
2

Although the number of substrings labelled is relatively small, we can still
draw some qualitative conclusions: First, the number of approximately matched
substrings (score<1) takes a considerable proportion of the number of all matched
substrings when the threshold δ is set to 0.8. Secondly, the true positive matches
takes up over 75 percent of the manually labelled sample. Thirdly, out of the
manually labelled sample, substrings with character-level edit-distance takes a
dominant ma jority and also contributes to the biggest proportion of the true
positive matches. Lastly, substrings with both levels of edit distance takes a lit-
tle proportion in the sample. According to the analysis above, we will focus on
improving the eﬀectiveness of the previous work for extracting substrings with
character-level edit-distance.

4.1 Distinguishing a typo from an intended token

Limitation of lexical edit-distances Previous experimental studies of the
2ED algorithm provided some intuitions for our improvement work. For example,
the following two matched pairs have a very close 2ED score but pair #1 is an
invalid match while pair #2 is valid.

Table 3: Examples from 2ED

#
Substring
Entity
2ED
1 about the premise about the promise 0.844754
2
code of honor
code of honour
0.862025

6

Z. Wen, Z. Huang and R. Zhang

To be more general, 2ED measures the similarity of two tokens by lexical
edit-distance. In example #1, the diﬀerence between this matched pair is within
the word pair “premise” and “promise”. 2ED measures the distance between the
word pair by number of character-level operations including insertion, deletion
and substitution. Thus, the diﬀerence is represented by a substitution opera-
tion that turns the letter “e” in word “premise” into the letter “o” in word
“promise”. Similarly in example #2, the distance is represented by an insertion
operation that turns the word “honor” into “honour”. However, the validity
of the two matched pairs is not represented by the lexical edit-distance within
these pairs. The pair in example #2 is valid because token “honor” is an vari-
ation of token “honour” in English; while the pair in example #1 is not valid
because “premise” and “promise” are two diﬀerent words that share little simi-
larity in their grammatical position and semantical meaning. In fact, the lexical
edit-distance between two tokens is more applicable as a representation of their
similarity (distance) when one of them is a mis-spelled version (typo) of the
other. It is common that words that look similar may or may not have close
meanings and grammatical position (e.g. part-of-speech). Thus, some criteria
for distinguishing a typo from an intended token should be introduced to help
us judge whether it is appropriate to apply the FuzzyED which is based on
lexical edit-distance.

4.2 Using language models

We propose the following conditions for identifying a typo based on the above
analysis: suppose the substring contains a token ts that has a close lexical edit-
distance with the corresponding token te in the entity. We assume ts is not a
typo if and only if (i) ts is a valid word in the language and (ii) ts ﬁts in the
context in the substring. The next step is to model these two conditions in a
feasible way. In fact, the conditions (i) and (ii) above can both be judged with
a corpus of its language, where the validity of a single token can be measured
by its frequency in the corpus and the validity of its context by the frequency of
word phrases or (token-level) n-grams. Since single tokens are just (token-level)
1-grams, the above conditions can be simpliﬁed as judging whether the n-grams
generated around ts are valid n-grams in the language, where n is in range [1, k ]
and k ≥ 2. N-grams and language models The application of (token-level) n-
grams in NLP tasks is versatile, one of them being a statistical language model.
A language model can help us tell (i) how likely a given n-gram will appear in
a language or (ii) the conditional probability that an n-gram is followed by a
certain word. According to the analysis above, we are using statistics of the n-
grams in a language without domain-speciﬁc knowledge or linguistic rules. Thus
it is appropriate to use a statistical language model trained from a large corpus.
Utilising web-scale corpora: Since our task is to speciﬁcally use the lan-
guage model as a comprehensive corpus of a language, it is critical to ﬁnd a
source where we can obtain large scale n-grams. In 2012, Lin et al. [7, 8] pub-
lished the second version of the “Google Books Ngram Corpus, where frequencies
of n-grams in the Google Books collection are collected with historical statistics.

Entity Extraction with Knowledge from Web Scale Corpora

7

The corpus “reﬂects 6% of all books ever published” [7]. Considering the scale
and coverage of the corpus, we ﬁnd it a great source for our purpose. Train-
ing from such a large corpus consisting of hundreds of gigabytes of data is not
trivial. Even if we leave out all historical information and use n-gram frequen-
cies collectively, maintaining a map of n-grams and their frequencies is still a
memory consuming task to be executed on a single machine. Besides memory
consumption of the training process, the complexity of the model is another
concern. Based on the above complexity analysis, we adopt a relatively sim-
ple model in this paper, the BerkeleyLM [13], which features a trained n-gram
based model using the Google Books Ngram Corpus. The model provides inter-
faces for querying (i) the (conditional) log-probability of an n-gram and (ii) the
raw count of an n-gram in the Google Books Ngram Corpus. The trained model
uses stupid back-oﬀ for estimating the (conditional) probability of an n-gram as

follows P (wi |w1 , w2 , ..., wi−1 ) = count(w1 ,w2 ,...,wi )
count(w1 ,w2,...,wi−1 ) .

4.3 Estimating word similarity

As discussed in Section 4.1, we are interested in the case where an unmatched
token in the substring is not a typo and thus the applicability of the previous
2ED algorithm needs to be carefully reviewed. Here, the method we propose is
to use word embedding for measuring the distance between two tokens. From the
experimental results in Section 4.1, an observation is that the unmatched tokens
in a matched pair can belong to various cases. For example, (i) ts and te may be
variations of each other (e.g. “honour” and “honour”); (ii) ts may be the plural
form of te or vice versa (e.g. “survivors” and “survivor”). A word embedding
will help us capture the “distance” between the unmatched tokens. The word
embedding adopted in this paper is Google’s word2vec [1] which represents words
in a corpus by vectors of ﬂoats. According to Mikolov et al. [12,11,10], the trained
model can capture syntactic and semantic information of words in a language.

4.4 Other improvements

Besides the introduction of language models and word embedding to replace
some functionality of the FuzzyED distance metric, we have also made other
minor improvements. The previous 2ED algorithm does not separate the period
from the last word in a sentence during its tokenisation phrase. Since many
approximately matched pairs are exact matches if we strip the period (dot)
from the last token in the substring, we make this operation an optional feature
in the implemented post-processing algorithm.

5

Implementation

The improved methods described in Section 4 is implemented in a pipeline.
All improvements are applied as post-processing steps to ﬁlter, examine and
(possibly) re-score matched pairs selected by previous 2ED algorithm. We will
walk through the pipeline step by step in the rest of this section.

8

Z. Wen, Z. Huang and R. Zhang

5.1 Obtain candidate pairs

The ﬁrst step is ﬁnding matched pairs with character-level edit-distance as can-
didates for future re-scoring work. We apply the previous 2ED algorithm to our
corpus against a dictionary with token-level similarity threshold δ = 0.8 and
character-level similarity threshold τ = 0.8. was chosen according to the param-
eter optimization work in [16]; was chosen for error tolerance with the previous
algorithm. After obtaining the list of matched pairs, we ﬁlter out exact matches
(i.e. 2ED score = 1) because they are not the part of the result we are trying
to improve. For the rest of the matched pairs, we apply the following steps for
each pair.

5.2 Rescore candiadte pairs

Filter out pairs with tokenisation problems: An approximately matched
pairs with tokenisation problems as described in Section 4.2 is not processed into
the next step. Rather, we simply strip the ending period from the substring and
assume it an exact match. The stripping step is operational and improvement
from this operation is separately analysed as in Section 6.
Generate n-gram to check validity: With an approximately matched
pair, we compare each token pair in the corresponding position of the substring
and the entity to identify whether there is (only) a character-level edit-distance
between this pair. In this process, we have also obtained the position of the ts and
te as per notation in Section 4.1 if the token pair does exist. We then generate
(token-level) n-gram pairs surrounding ts and te in the substring and the entity
term respectively. We ﬁrst generate 3-grams. If the substring is too short that
it contains less than 3 tokens. We use the substring and the entity as a whole,
i.e. 2-grams or 1-gram, for a pair. The next step is to check the validity of these
n-grams in a language model to help us distinguish a typo from an intended
word according to the conditions described in Section 4.1. We use a tolerant
criteria for this validity check, where the unmatched token in the substring ts
is considered an intended word as long as any n-gram pair from ts and te are
both valid or both invalid in the language model. For each (token-level) n-gram,
we use two thresholds accounting for the log-probability and raw count in the
language model respectively to help check its validity. The thresholds are set
according to empirical observations. The threshold for log-probability is -10.8
and the threshold for raw count is 0.
Apply cosine similarity to rescore: When an unmatched token is identi-
ﬁed as an intended word in the last step, we use the trained word2vec embedding
to calculate the similarity between the token pair ts and te. For the examples in
Section 4.1, the cosine similarity are shown in the table below. As we can see,
these scores represents the distance between the token pairs in English: the sim-
ilarity between “honor” and “honour” is signiﬁcantly higher than that between
“premise” and “promise” since the former consists of variations of the same word
while the latter consists of two distinct words.

Entity Extraction with Knowledge from Web Scale Corpora

9

Table 4: Cosine similarity for examples in Table 3

#
Substring
Entity
Cosine similarity
1 about the premise about the promise 0.245628
2
code of honor
code of honour
0.637478

The cosine similarity between ts and te is normalised using the following
formula. This formula is chosen according to empirical observation of the distri-
bution of the cosine similarity (denoted by cos in the formula). It guarantees the
following features: (i) normalised edit-distance is 0 when cos is 1, i.e. the edit-
distance is 0 for two identical words; (ii) normalised edit-distance is 1 whencos
is 0 (although cos is within range [-1,1], we observe that most empirical results
sits in [0,1]); (iii) normalised edit-distance punishes low cos scores using an ex-
ponential formula; (iv) base is a tunable parameter aﬀecting the curve of the
normalization function: EDnorm = base1−cos−1
. A ﬁnal score is applied to the
post-processed pair of substring and entity using the following formula. We take
the length of the entity as a normalizing parameter. This approach is similar to
the normalization in FuzzyED when we assign a uniform weight to the tokens
in the entity: Rescore = 1 − EDnorm

base−1

length(entity) .

6 Experimental studies

Evaluation setup : The validation was performed on the NeCTAR research cloud [3]
using a 12-core computing instance with 48 GB of RAM. The test data set was
obtained from a public corpus of Amazon reviews [9]. The corpus consists of
(i) millions of reviews on the Amazon website, further divided into subsets by
product category; (ii) metadata of the products available on the Amazon web-
site. From the review data set, we selected the books subset which contains more
than 8 million reviews and sampled 1000 reviews as the text for the task. From
the metadata which contains information about 9.4 billion product items, we
extracted only the titles of these items and use the result as the dictionary for
the task. Due to missing ﬁelds in the metadata, the dictionary consists of 7.99
million product titles.
The corpus for training word2vec embeddings is obtained from various re-
sources on the web using the script provided in the toolset on [1]. The resulting
training set consists of 6.1 billion tokens. After two runs of the word2phrase
pre-processing [1], we train the word2vec embedding with the Continuous Bag
of Word (CBOW) method and a vector size of 300 dimensions on the data set.
Another training method skipgram was also attempted but achieved lower pre-
cision using the provided validation tool in [1]; and it was not used in future
steps. The training process takes less than a day to ﬁnish on the cloud instance.
We use (i) the distribution of the scores of re-visited pairs for positive pairs
and negative pairs respectively, and (ii) the Receiver Operating Characteristic
(ROC) curve to validate the eﬀectiveness of the post-processing. We manually

10

Z. Wen, Z. Huang and R. Zhang

labelled 113 pairs from the re-scored set, which comprises over 10% of its size
and use that labelled data to evaluate the result.
Eﬀectiveness : Figure 1 shows the distribution of the re-visited scores sepa-
rated by their labels, where label Y means the matched pair is valid (according
to human evaluation) and label N means the pair is not. As we can see, by ap-
plying the post-processing, the two groups have their scores distributed in two
clusters in distinct centroids.

(a) Label = Y

(b) Label = N

Fig. 1: Histogram of post-processing scores

The two ROC curves below compare the performance of using 2ED score and
post-processed score to predict validity of extracted substrings. As we can see,
the post-processed score achieves a higher true positive rate without sacriﬁcing
the false positive rate, while 2ED score performs like random guess in evaluation
of extracted substrings. Overall the post-processed score achieved an area under
curve (AUC) of 0.72, and outperforms 2ED score by 47%.

(a) Label = Y

(b) Label = N

Fig. 2: ROC curves for 2ED score and post-processing score

Eﬃciency The eﬃciency of the algorithm is not the ma jor concern in this
paper. We evaluate the performance of the post-processing algorithm by the

Entity Extraction with Knowledge from Web Scale Corpora

11

time taken to complete the task on the data set described earlier in this section.
The task typically ﬁnishes within 10 minutes, depending on the status of the
cloud instance. Ma jority of the time taken is on loading trained models into
memory, so the performance should also depend on the physical RAM available
on the evaluation instance. After loading the models, the application ﬁnishes
processing over 2000 items in less than 5 seconds. Therefore, this algorithm
should be suitable as a post-processing step for the previous 2ED algorithm in
terms of its performance.

7 Conclusion and Future Work

In this paper, we proposed several improvements to our previous entity extrac-
tion algorithm called “2ED”. Our proposed improvements include language mod-
els for typo detection, word embedding to measure word distances to capture
semantic features, and more advanced tokenization. We have implemented the
proposed techniques as a post-processing step on top of 2ED. Our proposed
techniques bring signiﬁcant improvement to 2ED. The improvement mainly lies
in the introduction of web-scale corpora used for training relatively comprehen-
sive and versatile models. This ﬁnding shows that more information from the
web-scale corpora can facilitate entity extraction.
Some improvements and extensions to this work can be made to further gen-
eralise its applicability, boost its performance and make better use of the web-
scale corpus. First, it is possible to combine evidence from postags. The new
version of Google Books Ngram Corpus features part-of-speech tag (i.e., postag)
information. Such labels can be further utilised for measuring the distance be-
tween a token pair in addition to the n-gram used in the current implementa-
tion. Furthermore, beyond the post-processing approach, postags can facilitate
the candidate substring generation process in 2ED. Second, it is promising to
tokenise text with punctuations. 2ED uses a tokenisation method which does not
separate the period of a sentence with its ending word. One ma jor concern of
stripping periods from the ending words in the previous implementation is it is
hard to distinguish a “true” punctuation which ends a sentence or clause from an
ending dot of an abbreviation lexically. With the language models introduced in
this paper, potentially we are able to ﬁnd eﬀective ways to improve the tokenisa-
tion using linguistic evidence from these models. Third, learning parameters for
2ED is also helpful for users. The current implementation uses empirical settings
of parameters for judging n-gram validity and for normalising the cosine similar-
ity of a token pair. These parameters can be learned from labelled data. Finally,
our approach extracts entities in English, and our approach can be extended to
other languages.

References

1. Y. Bengio, R. Ducharme, P. Vincent, and C. Jauvin. A neural probabilistic lan-
guage model. Journal of machine learning research, 3(Feb):1137–1155, 2003.

12

Z. Wen, Z. Huang and R. Zhang

2. J. P. Chiu and E. Nichols. Named entity recognition with bidirectional lstm-cnns.
Transactions of the Association for Computational Linguistics, 4:357–370, 2016.
3. S. A. CORBET and E. S. DELFOSSE. Honeybees and the nectar of echium
plantagineum l. in southeastern australia. Australian Journal of Ecology, 9(2):125–
139, 1984.
4. F. M. Harper and J. A. Konstan. The movielens datasets: History and context.
Acm transactions on interactive intel ligent systems (tiis), 5(4):19, 2016.
5. K. Heaﬁeld. Kenlm: Faster and smaller language model queries. In Wworkshop on
statistical machine translation, pages 187–197. ACL, 2011.
6. H. V. Jagadish, B. C. Ooi, K.-L. Tan, C. Yu, and R. Zhang. idistance: An adaptive
b+-tree based indexing method for nearest neighbor search. TODS, 30(2):364–397,
2005.
7. Y. Lin, J.-B. Michel, E. L. Aiden, J. Orwant, W. Brockman, and S. Petrov. Syn-
tactic annotations for the google books ngram corpus. In ACL, pages 169–174,
2012.
8. A. L. Maas, R. E. Daly, P. T. Pham, D. Huang, A. Y. Ng, and C. Potts. Learning
word vectors for sentiment analysis. In Proceedings of the 49th annual meeting of
the association for computational linguistics: Human language technologies-volume
1, pages 142–150. Association for Computational Linguistics, 2011.
9. J. McAuley and J. Leskovec. Hidden factors and hidden topics: understanding
rating dimensions with review text. In Proceedings of the 7th ACM conference on
Recommender systems, pages 165–172. ACM, 2013.
10. T. Mikolov, K. Chen, G. Corrado, and J. Dean. Eﬃcient estimation of word
representations in vector space. arXiv preprint arXiv:1301.3781, 2013.
11. T. Mikolov, I. Sutskever, K. Chen, G. S. Corrado, and J. Dean. Distributed repre-
sentations of words and phrases and their compositionality. In Advances in neural
information processing systems, pages 3111–3119, 2013.
12. T. Mikolov, W.-t. Yih, and G. Zweig. Linguistic regularities in continuous space
word representations. In Proceedings of the 2013 Conference of the North Amer-
ican Chapter of the Association for Computational Linguistics: Human Language
Technologies, pages 746–751, 2013.
13. A. Pauls and D. Klein. Faster and smaller n-gram language models. In ACL, pages
258–267. ACL, 2011.
14. E. Tseytlin, K. Mitchell, E. Legowski, J. Corrigan, G. Chavan, and R. S. Jacob-
son. Noble–ﬂexible concept recognition for large-scale biomedical natural language
processing. BMC bioinformatics, 17(1):32, 2016.
15. Q. Wei, T. Chen, R. Xu, Y. He, and L. Gui. Disease named entity recognition by
combining conditional random ﬁelds and bidirectional recurrent neural networks.
Database, 2016, 2016.
16. Z. Wen, D. Deng, R. Zhang, and R. Kotagiri. An eﬃcient entity extraction algo-
rithm using two-level edit-distance. In ICDE, pages 998–1009. IEEE, 2019.
17. Z. Wen, R. Zhang, K. Ramamohanarao, J. Qi, and K. Taylor. Mascot: fast and
highly scalable svm cross-validation using gpus and ssds. In ICDM, pages 580–589.
IEEE, 2014.
18. C. Whitelaw, B. Hutchinson, G. Y. Chung, and G. Ellis. Using the web for language
independent spellchecking and autocorrection. In EMNLP, pages 890–899. ACL,
2009.

