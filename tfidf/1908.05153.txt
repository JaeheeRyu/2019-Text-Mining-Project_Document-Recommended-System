Semi-supervised Learning with Adaptive
Neighborhood Graph Propagation Network

Bo Jiang, Leiling Wang, Jin Tang and Bin Luo

School of Computer Science and Technology, Anhui University
jiangbo@ahu.edu.cn

9
1
0
2

v
o

N

1
2

]

V

C

.

s

c

[

2
v
3
5
1
5
0

.

8
0
9
1

:

v

i

X

r

a

Abstract—Graph methods have been commonly employed for
visual data representation and analysis in computer vision area.
Graph data representation and learning becomes more and more
important in this area. Recently, Graph Convolutional Networks
(GCNs) have been employed for graph data representation and
learning. Existing GCNs usually use a ﬁxed neighborhood graph
which is not guaranteed to be optimal
for semi-supervised
learning tasks. In this paper, we propose a uniﬁed adaptive
neighborhood feature propagation model and derive a novel
Adaptive Neighborhood Graph Propagation Network (ANGPN)
for data representation and semi-supervised learning. The aim
of ANGPN is to conduct both neighborhood graph construction
(from pair-wise distance information) and graph convolution
simultaneously in a uniﬁed formulation and thus can learn an
optimal neighborhood graph that best serves graph convolution
for graph based semi-supervised learning. Experimental results
demonstrate the effectiveness and beneﬁt of the proposed ANGPN
on image data semi-supervised classiﬁcation tasks.

I . INTRODUCT ION

Compact data representation and learning is a fundamen-
tal problem in computer vision area. Graph methods have
been commonly employed for visual data representation and
analysis, such as classiﬁcation, segmentation, etc. The core
technique in graph methods is graph representation and learn-
ing. Recently, there is an increasing attention on trying to
generalize CNNs to Graph Convolutional Networks (GCNs)
to deal with graph data [1] [2] [3] [4]. For example, Bruna
et al. [5] employ a eigen-decomposition of graph Laplacian
matrix to deﬁne graph convolution operation. Henaff et al. [6]
introduce a spatially constrained spectral ﬁlters to deﬁne
graph convolution. Defferrard et al. [1] propose to utilize
Chebyshev expansion to approximate the spectral ﬁlters for
graph convolution operation. Kipf et al. [7] present a simple
Graph Convolutional Network (GCN). The main idea of GCN
is to explore the ﬁrst-order approximation of spectral ﬁlters to
compute graph convolution more efﬁciently. Monti et al. [2]
propose mixture model CNNs (MoNet) for graph data learning
and analysis. Veli ˇckovi ´c et al. [4] propose Graph Attention
Networks (GAT) for graph based semi-supervised learning
which can aggregate the features of neighboring nodes in
an adaptive weighting manner. Hamilton et al. [3] present
a general inductive representation and learning framework
(GraphSAGE) by sampling and aggregating features from
a node’s local neighborhood. Klicpera et al. [8] propose
to integrate PageRank propagation into GCN in layer-wise
propagation. Petar et al. [9] propose Deep Graph Infomax

(DGI) to learn the node representation in unsupervised manner.
Jiang et al., [10] recently propose Graph Diffusion-Embedding
Network (GEDNs) for graph node semi-supervised learning.
The above state-of-the-art methods can be widely used
in many graph learning tasks, such as graph based semi-
supervised learning, low-dimensional representation, cluster-
ing etc. In computer vision applications, for these learning
tasks, existing methods usually need to use a two-stage frame-
work, i.e., 1) obtaining/constructing a neighborhood graph
from data and 2) conducting graph convolutional representa-
tion and learning on this graph. However, one main issue is that
such a two-stage framework does not fully mine the correlation
between graph construction and graph convolutional learning,
which may lead to weak suboptimal solution. Previous works
generally focus on graph convolution operation while little
attention has been put on graph construction. To address this
issue, Li et al. [11] present an optimal graph CNNs, in which
the graph is learned adaptively by employing a metric learning
method. Veli ˇckovi ´c et al. [4] propose Graph Attention Net-
works (GAT) for graph based semi-supervised learning. Jiang
et al. [12] propose Graph Optimized Convolutional Network
(GOCN) by integrating graph construction and convolution
together in a uniﬁed operation model. They also [13] propose
Graph mask Convolutional Network (GmCN) by selecting
neighbors adaptively in GCN operation.
In this paper, motivated by recent graph learning [14], [15]
and graph optimized GCN works [8], [12], [13], we propose
a novel graph neural network, named Adaptive Neighborhood
Graph Propagation Network (ANGPN), for graph based semi-
supervised learning task. Similar to [12], [13], ANGPN aims to
conduct both neighborhood graph learning and graph convolu-
tion together and formulate graph construction and convolution
simultaneously. Different from previous works [4], [12], [13],
ANGPN aims to learn an optimal neighborhood graph for
graph convolution from pair-wise distance information and can
be used in many computer vision tasks which do not have any
prior graph structures. Overall, the main contributions of this
paper are summarized as follows:
• We propose a novel Adaptive Neighborhood Graph Prop-
agation Network (ANGPN) which integrates both neigh-
borhood graph construction and graph convolution coop-
eratively in a uniﬁed formulation for data representation
and semi-supervised learning problem.
• We propose a novel model of Adaptive Neighborhood

 
 
 
 
 
 
Feature Propagation (ANFP) which provides a context-
aware representation for data by jointly employing both
unary feature of each data and contextual features from
its neighbors together.

Experimental results on several datasets demonstrate the effec-
tiveness of ANGPN on semi-supervised learning tasks.

I I . ADA P T IVE N E IGHBORHOOD FEATURE PRO PAGAT ION

In the following, we ﬁrst introduce a neighborhood graph
feature propagation (NFP) model. Then we extend it to Adap-
tive NFP (ANFP) by constructing the neighborhood graph in
NFP adaptively.

A. Neighborhood feature propagation

Let M ∈ Rn×n be the adjacency matrix of a neighborhood
graph with Mii = 0, i.e., if node vj ∈ N (vi ) then Mij = 1,
otherwise Mij = 0. Let A = M D−1 be the row-normalization
of M , where D is a diagonal matrix with Dii = Pj Mij .
Thus, we have Pn
j=1 Aij = 1, Aij ≥ 0 and Aii = 0. Let
H = (h1 , h2 · · · hn ) ∈ Rn×d be the feature descriptors of
graph nodes. Then, the aim of our Neighborhood Graph based
Feature Propagation is to learn a kind of feature representation
F = P (A, H ) = (f1 , f2 · · · fn ) ∈ Rn×d for graph nodes
by incorporating the contextual information of their neighbors.
Here, we propose a scheme that is similar to the one in label
propagation [16] but iteratively propagates the features H on
neighborhood graph A. Formally, for each node vi , we absorb
a fraction of feature information from its neighbors in each
propagation step as follows,

In particular, the converged solution of Eq.(3) is the optimal
solution that minimizes the following optimization problem,

min

F

Tr(F T (I − A)F ) + µkF − H k2

F

(4)

where α = 1
1+µ . Tr(·) denotes the trace function and k · kF
denotes the Frobenious norm of matrix. Then, inspired by [14],
[15], we incorporate graph learning into Eq.(4) and construct
a uniﬁed model as

min

S,F

Xn

i,j=1

Dx
ij Sij + γ kS k2

F + βTr(F T (I − S )F )

+ µkF − H k2

F

s.t. Xn

j=1

Sij = 1, Sij ≥ 0

(5)

(6)

where Dx
ij = kxi − xj k2 denotes the Euclidean distance
between original input feature xi and xj of data1 . The optimal
Sij can be regarded as a conﬁdence (or probability) that data
xj is connected to xi as a neighbor. A larger distance Dx
should be assigned with smaller conﬁdence Sij . The parameter
γ > 0 is used to control the sparsity of learned graph S [14].
We call it as Adaptive Neighborhood Feature Propagation
(ANFP). The optimal S and F can be obtained via a simple
algorithm which alternatively conducts the following Step 1
and Step 2 until convergence.
Step 1. Solve S while ﬁxing F , the problem becomes

ij

min

S

Xn

i,j=1

Dx
ij Sij + γ kS k2

F + βTr(F T (I − S )F )

s.t. Xn

j=1

Sij = 1, Sij ≥ 0

f (t+1)

i

= α Xn

j=1

Aij f (t)

j + (1 − α)hi

(1)

which is equivalent to

(7)

(8)

where t = 0, 1 · · · and f (0)
j = hj . Parameter α ∈ (0, 1) is the
fraction of feature information that node vi receives from its
neighbors. We can rewrite Eq.(7) more compactly as

F (t+1) = αAF (t) + (1 − α)H

(2)

where F (0) = H . It is known that the above propagation will
converge to an equilibrium solution as

F ∗ = (1 − α)(I − αA)−1H

(3)

B. Adaptive neighborhood feature propagation

In the above feature propagation, one important aspect is
the construction of neighborhood graph A. One popular way
is to construct a human established graph, such as k-nearest
neighbor graph. However, such a pre-deﬁned graph may have
no clear structure and thus also be not guaranteed to best serve
the feature propagation and learning task. To overcome this
issue, we propose to learn an adaptive neighborhood graph S
to better capture the intrinsic neighborhood relationship among
data in the above NFP. We call it as Adaptive Neighborhood
Feature Propagation (ANFP).
In order to do so, we ﬁrst show that the above propagation
has an equivalent optimization formulation [10], [12], [16].

min

S

Xn

i,j=1

(Dx − βF F T )ij Sij + γ kS k2

F

s.t. Xn

j=1

Sij = 1, Sij ≥ 0

This problem has a simple closed-form solution which is given
as [14],

Sij = max (cid:8) −

1
2γ

(Dx − βF F T )ij + η , 0(cid:9)

(9)

2kγ Pk

j=1 Dx

ij .

k + 1

where η = 1
Remark. Without loss of generality, here we suppose that
Dx
i2 · · · Dx
in are ordered from small to large values, as
discussed in work [14].
Step 2. Solve F while ﬁxing S , the problem becomes

i1 , Dx

min

F

Tr(F T (I − S )F ) + µkF − H k2

F

(10)

This is similar to Eq.(10) and the optimal solution is

F = (1 − α)(I − αS )−1H

(11)

1 For efﬁciency consideration, we use the original feature X to compute
Dx which is ﬁxed in the following training process. Here, one can also use
the current learned representation H to obtain/update more accurate Dh .

I I I . ANGPN ARCH IT ECTURE

B. Perceptron layer

In this section, we present our Adaptive Neighborhood
Graph Propagation Network (ANGPN) based on the above
ANFP model formulation. Similar to the overall architecture
of traditional GCN [7], we employ one input layer, several
hidden propagation layers and one ﬁnal perceptron layer for
ANGPN. They are introduced below.

A. Hidden propagation layer

Given an input feature matrix H (k) ∈ Rn×dk and pair-wise
distance matrix Dx ∈ Rn×n , ANGPN hidden layer outputs
feature map matrix H (k+1) ∈ Rn×dk+1 by using the above
ANFP and non-linear transformation. More speciﬁcally, let
F = P (Dx , H ) be the optimal solution of ANFP, then our
ANGPN conducts layer-wise propagation in hidden layer as

(12)

H (k+1) = σ(cid:0)P (Dx , H (k) )W (k) (cid:1)
where Dx denotes the distance matrix and W (k) ∈ Rdk×dk+1
is the k -th layer-speciﬁc trainable weight matrix. σ(·) denotes
an activation function.
Directly calculating ANFP solution F = P (Dx , H ) is time
consuming due to (A) inversion operation in computing F
(Eq.(11)) and (B) alternative computation of Step 1 and Step
2 until convergence. To reduce the issue of high computa-
tional complexity, similar to [12], we derive an approximate
algorithm to conduct them in the following. For inversion
operation (A), we adopt the strategy similar to GCN [7] and
approximate the optimization of F via an one-step power
iteration algorithm. More concretely, instead of calculating
Eq.(11) directly, we use the following update to compute F .

F = αSH + (1 − α)H = (αS + (1 − α)I )H

(13)

For computation (B), we can also use a truncated T -step
alternative iteration to optimize ANFP approximately. Based
on the above analysis, we summarize the whole propagation
algorithm to compute P (Dx , H (k) ) in hidden propagation
layer of ANGPN in Algorithm 1.

Algorithm 1 ANGPN propagation layer

1: Input: Feature matrix H (k) ∈ Rn×dk and ordered dis-
tances D ∈ Rn×n , parameters γ , β and α, maximum
iteration T
2: Output: Feature map H (k+1)
3: Initialize F = H (k)
4: Compute η as η = 1
5: for t = 1, 2 · · · T do
Compute S as

2kγ Pk

k + 1

6:

j=1 Dx

ij

7:

Sij = max (cid:8) − 1

2γ (Dx − βF F T )ij + η , 0(cid:9)

Compute F as
F = (αS + (1 − α)I )H (k)

8: end for
9: Return P (Dx , H (k) ) = F

For graph node classiﬁcation, the last layer of ANGPN
outputs the ﬁnal label prediction Z ∈ Rn×c for graph nodes
where c denotes the number of class. We add a softmax
activation function on each row of the ﬁnal output feature
map H (K ) ∈ Rn×c as,

Z = softmax(H (K ) )

(14)

where each row Zi of matrix Z denotes the corresponding
label prediction vector for the i-th node. The optimal network
weight parameters {W (0) , W (1) , · · · W (K−1)} of ANGPN are
trained by minimizing cross-entropy loss function deﬁned
as [7],

LSemi-ANGPN = − Xi∈L Xc

j=1

Yij lnZij

(15)

where L indicates the set of labeled nodes and Yi· , i ∈ L
denotes the corresponding label indication vector for the i-th
labeled node.

C. Computational complexity

Empirically, the optimization in each layer of ANGPN gen-
erally does not lead to very high computational cost because it
can be solved via a simple (efﬁcient) algorithm (Algorithm 1)
in practical. Overall, ANGPN has similar time consuming with
GAT [4]. However, ANGPN outperforms GAT (shown in Table
I). Theoretically, in each iteration, the complexity of our graph
construction and convolution are O(kn)+O(dn2 ) and O(n2 d),
where n, d denote the graph size and feature dimension in each
layer. The overall complexity is O(T (kn + 2dn2 )), where T is
number of maximum steps. In our experiments, we set T = 2
which can return desired better results.

IV. EX P ER IMENT S

A. Datasets

We test our method on four benchmark datasets, including
SVHN [17], 20News [18], CIFAR [19] and CoraML [20]. The
details of these datasets and their usages in our experiments
are introduced below.
SVHN. This dataset contains 73257 training and 26032 testing
images [17]. Each image is a 32 × 32 RGB color image which
contains multiple number of digits and the task is to recognize
the digit in the image center. In our experiments, we select 500
images for each class and obtain 5000 images in all for our
evaluation. We have not use all of images because it requires
large storage and high computational complexity for graph
convolution operation in our ANGPN and other related GCN
based methods. We extract a commonly used CNN feature
descriptor for each image data.
CIFAR. This dataset contains 50000 natural images which
are falling into 10 classes [19]. Each image in this dataset is
a 32 × 32 RGB color image. In our experiments, we select
500 images for each class and use 5000 images in all for our
evaluation. For each image, we extract a widely used CNN
feature descriptor for it.

20News. The 20 Newsgroups data set is a collection of approx-
imately 20,000 newsgroup documents, which are partitioned
nearly across 20 different newsgroups [18]. In our experiments,
we use a subset with 3970 data points in 4 classes. Each data
is represented by a 8014 dimension feature descriptor.
CoraML. The CoraML dataset consists of 1617 scientiﬁc
publication documents which are classiﬁed into one of seven
class. Each document is represented by a 8329 dimension
feature descriptor.

the effectiveness of ANGPN on conducting semi-supervised
learning tasks.
Table II shows the results of ANGPN vs. NGPN on four
datasets. Here, we can note that, ANGPN consistently per-
forms better than the baseline model NGPN. This further
demonstrates the advantages of ANGPN by conducting graph
construction and graph convolution cooperatively and thus can
boost their respective performance.

B. Experimental setting

For all datasets, we randomly select 10%, 20% and 30%
samples in each class as labeled data for training the network
and use the other 5% of labeled data for validation purpose.
The remaining samples are used as the unlabeled test samples.
All
the reported results are averaged over ﬁve runs with
different groups of training, validation and testing data splits.
The number of hidden convolution layers in ANGPN is set
as 2. The number of units in each hidden layer is set as
50. Some additional experiments across different number of
convolution layers are presented in Parameter analysis. We
train our ANGPN for a maximum of 10000 epochs (training
iterations) by using an ADAM algorithm [24] initialized by
Glorot initialization [25] with learning rate 0.005. The param-
eters {β , α} in ANGPN are set to {0.3, 0.5}, respectively.

C. Comparison results

Baselines. We compare our method against some other
related graph approaches which contain i) graph based semi-
supervised learning method Label Propagation (LP) [22], Man-
ifold Regularization (ManiReg) [21], and ii) graph neural net-
work methods including DeepWalk [23], Graph Convolutional
Network (GCN) [7], Graph Attention Networks (GAT) [4],
Deep Graph Informax(DGI) [9] and GraphSAGE [3]. The
codes of these comparison methods were provided by authors
and we use them directly on our data setting in experiments. In
addition, as discussed before, the core aspect of the proposed
ANGPN is that it conducts graph construction and graph con-
volution cooperatively to boost their respective performance.
To demonstrate the effectiveness of this cooperative learning,
we construct a baseline method, NGPN, which ﬁrst constructs
the graph S via Eq.(15) with β = 0 and then conducts feature
propagation on S via Eq.(19).
Comparison results. Table I summarizes the comparison
results on four benchmark datasets. The best results are marked
as bold in Table I. We can note that, (1) ANGPN outperforms
the baseline method GCN [7] on all datasets, demonstrating
the desired ability of the proposed ANGPN network on
conducting semi-supervised learning tasks by learning neigh-
borhood graph adaptively in data representation and learning.
(2) ANGPN consistently performs better than recent graph
network model GAT [4], Deep Graph Informax(DGI) [9] and
GraphSAGE [3]. (3) ANGPN can obtain better performance
than other graph based semi-supervised learning methods, such
as LP [22], ManiReg [21] and DeepWalk [23]. It demonstrates

y
c
a

r

u
c
c

A

0.65

0.6

0.55

0.5

0.45

0.4

0.92

0.9

0.88

y
c
a

r

u
c
c

A

0.86

0.84

0.82

0.8

0.78

0.76

CIFAR

GCN
ANGPN

2

3

4

5

6

Number of layers

20 news

GCN
ANGPN

0.8

0.75

y
c
a

r

u
c
c

A

0.7

0.65

0.6

0.7

0.65

0.6

y
c
a

SVHN

GCN
ANGPN

2

3

4

5

6

Number of layers

CoraML

GCN
ANGPN

0.55

r

u
c
c

A

0.5

0.45

0.4

2

3

4

5

Number of layers

6

2

3

4

5

6

Number of layers

Fig. 1: Results of ANGPN across different number of convolutional
layers on four datasets.

D. Parameter analysis

In this section, we evaluate the performance of ANGPN
model with different network settings. Figure 2 shows the
performance of ANGPN method across different number of
convolutional layers on four datasets, respectively. One can
note that ANGPN can obtain better performance with dif-
ferent number of layers, which indicates the insensitivity of
ANGPN w.r.t. model depth. Also, ANGPN always performs
better than GCN under different model depths, which further
demonstrates the beneﬁt and better performance of ANGPN
comparing with the baseline method.

V. CONCLU S ION

This paper proposes a novel Adaptive Neighborhood
Graph Propagation Network (ANGPN) for graph based semi-
supervised learning problem. ANGPN integrates neighbor-
hood graph construction and graph convolution architecture
together in a uniﬁed formulation, which can learn an optimal
neighborhood graph structure that best serves the proposed
graph propagation network for data representation and semi-
supervised learning problem. Experimental results on four
benchmark datasets demonstrate the effectiveness and advan-
tage of ANGPN model on various semi-supervised learning
tasks. In the future,, we will explore ANGPN model for some
other machine learning tasks, such as graph embedding, data
clustering etc.

TABLE I: Comparison results of different methods on four benchmark datasets. The best results are marked as bold.

Dataset
No. of label
ManiReg [21]
LP [22]
DeepWalk [23]
DGI [9]
GraphSage [3]
GCN [7]
GAT [4]
ANGPN

Dataset
No. of label
ManiReg [21]
LP [22]
DeepWalk [23]
DGI [9]
GraphSage [3]
GCN [7]
GAT [4]
ANGPN

10%
71.91± 1.50
55.56 ± 0.62
74.54 ± 0.87
72.05 ± 0.99
73.70 ± 0.92
75.10 ± 0.60
72.60 ± 0.81
76.85 ± 0.83

10%
89.80 ± 0.93
88.34 ± 1.35
88.62 ± 0.70
89.47 ± 0.69
90.46 ± 0.55
89.94 ± 0.49
88.00 ± 0.16
90.80 ± 0.34

SVHN
20%
76.43 ± 0.63
66.29 ± 0.78
76.72 ± 0.83
74.83 ± 0.46
77.22 ± 0.58
76.45 ± 0.61
75.13 ± 0.38
78.69 ± 0.53

20News
20%
91.08 ± 0.41
90.92 ± 0.35
90.31 ± 0.66
90.76 ± 0.90
91.79 ± 0.73
91.40 ± 0.46
89.07 ± 0.47
92.35 ± 0.37

30%
78.67 ± 0.71
70.52 ± 0.72
77.35 ± 1.36
75.77 ± 0.64
78.78 ± 0.31
77.27 ± 0.57
75.17 ± 0.59
79.62 ± 0.55

30%
91.85 ± 0.23
91.55 ± 0.81
91.12 ± 0.38
91.02 ± 0.36
92.13 ± 0.44
91.45 ± 0.74
89.05 ± 1.10
92.83 ± 0.48

10%
59.44 ± 1.24
52.66 ± 1.40
58.82 ± 0.96
58.18 ± 0.57
61.85 ± 0.98
60.36 ± 1.00
59.62 ± 0.35
62.42 ± 1.04

10%
44.90 ± 1.85
54.38 ± 1.93
60.28 ± 2.59
62.88 ± 0.86
63.51 ± 2.01
63.33 ± 1.44
58.16 ± 1.76
65.53 ± 1.25

CIFAR
20%
65.11 ± 0.79
57.66 ± 0.39
61.62 ± 0.54
61.66 ± 0.18
63.58 ± 0.68
63.17 ± 0.61
61.09 ± 0.54
65.43 ± 0.66

CoraML
20%
58.00 ± 1.64
61.42 ± 1.15
64.14 ± 1.63
65.16 ± 0.60
68.43 ± 0.46
68.22 ± 0.72
60.77 ± 0.55
69.22 ± 1.50

30%
66.04 ± 0.58
60.36 ± 1.05
63.76 ± 0.82
64.08 ± 0.39
65.21 ± 0.38
63.87 ± 0.56
62.19 ± 0.34
65.90 ± 0.49

30%
64.61 ± 1.60
64.42 ± 0.84
66.17 ± 1.76
68.43 ± 1.01
68.91 ± 0.65
68.16 ± 1.18
61.64 ± 1.53
69.55 ± 1.44

TABLE II: Comparison results of ANGPN vs. NGPN on four
different benchmark datasets.

Dataset

SVHN

CIFAR

20News

CoraML

method
10%
NGPN
75.73±0.67
ANGPN 76.85±0.83
NGPN
61.41±0.77
ANGPN 62.42±1.04
NGPN
90.01±0.35
ANGPN 90.80±0.34
NGPN
63.42±1.75
ANGPN 65.53±1.25

20%
77.72±0.64
78.69±0.53
64.51±0.74
65.43±0.66
91.64±0.54
92.35±0.37
66.40±0.71
69.22±1.50

30%
78.49±0.59
79.62±0.55
64.65±0.58
65.90±0.49
91.97±0.71
92.83±0.48
67.88±1.92
69.55±1.44

RE F ERENCE S

[1] M. Defferrard, X. Bresson, and P. Vandergheynst, “Convolutional neural
networks on graphs with fast localized spectral ﬁltering,” in Advances
in Neural Information Processing Systems, 2016, pp. 3844–3852.
[2] F. Monti, D. Boscaini, J. Masci, E. Rodola, J. Svoboda, and M. M.
Bronstein, “Geometric deep learning on graphs and manifolds using
mixture model cnns,” in IEEE Conference on Computer Vision and
Pattern Recognition, 2017, pp. 5423–5434.
[3] W. Hamilton, Z. Ying, and J. Leskovec, “Inductive representation
learning on large graphs,” in Advances in Neural Information Processing
Systems, 2017, pp. 1024–1034.
[4] P. Velickovic, G. Cucurull, A. Casanova, A. Romero, P. Lio, and
Y. Bengio, “Graph attention networks,” arXiv preprint arXiv:1710.10903,
2017.
[5] J. Bruna, W. Zaremba, A. Szlam, and Y. LeCun, “Spectral networks and
locally connected networks on graphs,” in International Conference on
Learning Representations, 2014.
[6] M. Henaff, J. Bruna, and Y. LeCun, “Deep convolutional networks on
graph-structured data,” arXiv preprint arXiv:1506.05163, 2015.
[7] T. N. Kipf and M. Welling, “Semi-supervised classiﬁcation with graph
convolutional networks,” arXiv preprint arXiv:1609.02907, 2016.
[8] J. Klicpera, A. Bojchevski, and S. G ¨unnemann, “Predict then propagate:
Graph neural networks meet personalized pagerank,” in ICLR, 2019.
[9] P. Veli ˇckovi ´c, W. Fedus, W. L. Hamilton, P. Li `o, Y. Bengio, and R. D.
Hjelm, “Deep Graph Infomax,” in International Conference on Learning
Representations, 2019.
[10] B. Jiang, D. Lin, J. Tang, and B. Luo, “Data representation and learning
with graph diffusion-embedding networks,” in IEEE Conference on
Computer Vision and Pattern Recognition, 2019, pp. 10 414–10 423.
[11] L. Ruoyu, W. Sheng, Z. Feiyun, and H. Junzhou, “Adaptive graph convo-
lutional neural networks,” in AAAI Conference on Artiﬁcial Intelligence,
2018, pp. 3546–3553.

[12] B. Jiang, Z. Zhang, J. Tang, and B. Luo, “Graph optimized convolutional
networks,” arXiv:1904.11883, 2019.
[13] B. Jiang, B. Wang, J. Tang, and B. Luo, “Graph mask convolutional
network,” arXiv preprint arXiv: arXiv:1910.01735v2, 2019.
[14] F. Nie, X. Wang, and H. Huang, “Clustering and projected clustering
with adaptive neighbors,” in Acm Sigkdd International Conference on
Knowledge Discovery and Data Mining, 2014.
[15] F. Nie, W. Zhu, and X. Li, “Unsupervised feature selection with struc-
tured graph optimization,” in AAAI conference on artiﬁcial intelligence,
2016.
[16] F. Wang and C. Zhang, “Label propagation through linear neigh-
borhoods,” IEEE Transactions on Knowledge and Data Engineering,
vol. 20, no. 1, pp. 55–67, 2008.
[17] Y. Netzer, T. Wang, A. Coates, A. Bissacco, B. Wu, and A. Y. Ng,
“Reading digits in natural images with unsupervised feature learning,”
in NIPS workshop on deep learning and unsupervised feature learning,
2011.
[18] K. Lang, “Newsweeder: Learning to ﬁlter netnews,” in Proceedings of
the Twelfth International Conference on Machine Learning, 1995, pp.
331–339.
[19] A. Krizhevsky and G. Hinton, “Learning multiple layers of features from
tiny images,” Citeseer, Tech. Rep., 2009.
[20] P. Sen, G. Namata, M. Bilgic, L. Getoor, B. Galligher, and T. Eliassi-
Rad, “Collective classiﬁcation in network data,” AI magazine, vol. 29,
no. 3, p. 93, 2008.
[21] M. Belkin, P. Niyogi, and V. Sindhwani, “Manifold regularization: A
geometric framework for learning from labeled and unlabeled examples,”
Journal of machine learning research, vol. 7, no. Nov, pp. 2399–2434,
2006.
[22] X. Zhu, Z. Ghahramani, and J. D. Lafferty, “Semi-supervised learning
using gaussian ﬁelds and harmonic functions,” in Proceedings of the
20th International conference on Machine learning (ICML-03), 2003,
pp. 912–919.
[23] B. Perozzi, R. Al-Rfou, and S. Skiena, “Deepwalk: Online learning
of social representations,” in Proceedings of the 20th ACM SIGKDD
international conference on Knowledge discovery and data mining, 2014,
pp. 701–710.
[24] D. P. Kingma and J. Ba, “Adam: A method for stochastic optimization,”
in International Conference on Learning Representations, 2015.
[25] X. Glorot and Y. Bengio, “Understanding the difﬁculty of training deep
feedforward neural networks,” in International conference on artiﬁcial
intelligence and statistics, 2010, pp. 249–256.

