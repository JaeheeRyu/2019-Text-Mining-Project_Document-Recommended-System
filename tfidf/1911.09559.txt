9
1
0
2

v
o

N

1
2

]

T

I

.

s

c

[

1
v
9
5
5
9
0

.

1
1
9
1

:

v

i

X

r

a

Article

Generalizing Information to the Evolution of Rational Belief

Jed A. Duersch and Thomas A. Catanach

Sandia National Laboratories, Livermore, CA 94550, United States
Email: jaduers@sandia.gov and tacatan@sandia.gov

Abstract:
Information theory provides a mathematical foundation to measure uncertainty in belief.
Belief is represented by a probability distribution that captures our understanding of an outcome’s
plausibility. Information measures based on Shannon’s concept of entropy include realization information,
Kullback-Leibler divergence, Lindley’s information in experiment, cross entropy, and mutual information.

We derive a general theory of information from ﬁrst principles that accounts for evolving belief and
recovers all of these measures. Rather than simply gauging uncertainty, information is understood in this
theory to measure change in belief. We may then regard entropy as the information we expect to gain
upon realization of a discrete latent random variable.

This theory of information is compatible with the Bayesian paradigm in which rational belief is updated
as evidence becomes available. Furthermore, this theory admits novel measures of information with
well-deﬁned properties, which we explore in both analysis and experiment. This view of information
illuminates the study of machine learning by allowing us to quantify information captured by a predictive
model and distinguish it from residual information contained in training data. We gain related insights
regarding feature selection, anomaly detection, and novel Bayesian approaches.

Keywords:
information, Bayesian inference, entropy, self information, mutual
Kullback-Leibler divergence, Lindley information, maximal uncertainty, proper utility

information,

1. Introduction

This work integrates essential properties of information embedded within Shannon’s derivation of
entropy [1] and the Bayesian perspective, which identiﬁes probability with plausibility. We pursue this
investigation in order to understand how to rigorously apply information-theoretic concepts to the theory
of inference and machine learning. Speciﬁcally, we would like to understand how to quantify the evolution
of predictions given by machine learning models.
We begin in 1.1 with a thought experiment that illustrates shortcomings with the way standard
information measures would partition prediction information and residual information during machine
learning training.

1.1. Shortcomings with standard approaches

We would like to partition label information in training data into a component that is predicted by a
machine learning model and the residual component that remains unpredicted. Ideally, the sum of these
two components would be a conserved quantity, which would allow us to view training as simply shifting
information from the residual partition to the predicted partition. More importantly, however, predictive
information should clearly capture prediction quality.
Let q0 (x) be a probability distribution that represents uninformed predictions regarding labels of
interest, such as uniform probability over all outcomes. Let q1 (x) represent prediction probabilities from

 
 
 
 
 
 
2 of 31

(cid:16) q1 (x)
(cid:16) r(x)

q0 (x)

q1 (x)

(cid:17)

(cid:17)
(cid:16) 1
(cid:16) 1

a suitably trained machine learning model. Finally, we represent training labels as r(x), which assigns
full probability to speciﬁed outcomes. See 2.3 and 2.4 for additional details regarding notation. Two
approaches to measuring predictive and residual information that are related to entropy would use either
the Kullback-Leibler divergence [2,3] or Lindley’s information in experiment [4].
Prediction information: (cid:82) dx q1 (x) log
1. Kullback-Leibler.
Residual information: (cid:82) dx r(x) log
Prediction information: (cid:82) dx q1 (x) log
(cid:17) − (cid:82) dx q0 (x) log
2. Lindley.
Residual information: (cid:82) dx r(x) log
(cid:17) − (cid:82) dx q1 (x) log
Now suppose we learn that all of the training data had been mislabeled so that predictions q1 (x)
are worthless. We replace r(x) above with corrected labels ˆr(x) and examine how these measurements
change. Using KL-divergence, the prediction information remains unchanged and residual information
explodes, which highlights how total information is not conserved; variations in intermediate belief q1 (x)
can dramatically alter the sum. More problematic, however, is the fact that prediction information is
indifferent to prediction quality.
The Lindley formulation is substantially less satisfying. Although total information is conserved,
neither prediction information nor residual information changes, thus leaving no indication that the model
is catastrophically mistrained.

(cid:16) 1
(cid:16) 1
(cid:17)

q1 (x)

r(x)

(cid:17)

q0 (x)

q1 (x)

1.2. Our contributions

In the course of pursuing a consistent framework in which information measurements may be
understood, we have derived a theory of information from ﬁrst principles that places all entropic
information measures in a uniﬁed interpretable context. By specifying the properties of information
we desire, we show how a unique formulation follows that subsumes critical properties of Shannon’s
construction of entropy.
This theory fundamentally understands entropic information as a form of expectation that measures
changes in belief. Expectation is not necessarily taken with respect to the distributions that represent
the shift in belief, but rather with respect to another distribution representing a point of view. As a
consequence, information associated with a change in belief is not a ﬁxed quantity. Just as rational belief
must evolve as new evidence becomes available, so also does the information we would rationally assign
to previous shifts in belief. By emphasizing the role of rational belief, this theory recognizes that the degree
of validity we assign to past states of belief is both dynamic and potentially subjective as our state of
knowledge matures. We ﬁnd compelling foundations for this perspective within the Bayesian philosophy
of probability as a logic for expressing and updating uncertainty [5,6].
As a consequence of enforcing consistency with rational belief, a second additivity property emerges;
just as entropy can be summed over independent distributions, information gained over a sequence of
observations can be summed over intermediate belief updates. Total information over such a sequence
is independent of how results are grouped or ordered. This provides a compelling solution to the
thought experiment above. Label information in training data is a conserved quantity and we motivate
a formulation of prediction information that is directly tied to prediction quality. Notably, predictive
information trained with incorrect labels above would become negative in the view of corrected labels ˆr(x)
using this theory. We experimentally observe this phenomenon in 7.2.

3 of 31

Sooﬁ, Ebrahimi, and others [7–10] identify key contributions to information theory in the decade
following Shannon’s paper that are intrinsically tied to entropy. These are the Kullback-Leibler divergence,
Lindley’s information in experiment, and Jaynes’ construction of entropy-maximizing distributions that are
consistent with speciﬁed expectations. We show how this theory recovers these measures of information
and admits new forms that may not have been previously associated with entropic information, such as
the log pointwise posterior predictive measure of model accuracy [11]. We also show how this theory
admits novel information-optimizing probability distributions analogous to that of Jaynes’. Having a
consistent interpretation of information illuminates how it may be applied and what properties will hold
in a given context.
This notion of evolving information is a natural consequence of the foundations reason in the
Bayesian perspective that enables us to solve multiple challenges in Bayesian learning. One such challenge
is understanding how efﬁciently a given model incorporates new data. This theory provides bounds on
the information gained by a model resulting from inference and allows us to characterize the information
contained in individual observations by evaluating their consistency with rational belief inferred from the
entire dataset.
The rest of this paper is organized as follows. Section 2 discusses notation and background regarding
entropic information and Bayesian reasoning. Section 3 contains postulates that express properties of
information we desire as well as the formulation of information that follows and other related measures
of information. Section 4 analyzes general consequences and properties of this formulation. Section 5
examines connections with foundations of information theory. Section 6 discusses further implications
with respect to Bayesian inference and machine learning. Section 7 explores negative information with
computational experiments that illustrate when it occurs, how it may be understood, and why it is useful.
Section 8 summarizes these results and offers a brief discussion of future work. Appendix A proves our
principal result. Appendix B contains all corollary proofs. Appendix C provides key computations used in
experiments.

2. Background and notation

Shannon’s construction of entropy [1] shares a fundamental connection with thermodynamics. The
motivation is to facilitate analysis of complex systems which can be decomposed into independent
subsystems. The essential idea is simple — when probabilities multiply, entropy adds. This abstraction
allows us to compose uncertainties across disparate sources by simply adding results.
Shannon applied this perspective to streams of symbols called channels. The number of possible
outcomes grows exponentially with the length of a symbol sequence, whereas entropy grows linearly. This
facilitates a rigorous formulation of the rate of information conveyed by a channel as well as analysis of
what is possible in the presence of noise.
The property of independent additivity is used in standard training practices for machine learning.
Just as thermodynamic systems and streams of symbols break apart, so does an ensemble of predictions
over independent observations. This allows us to partition training sets into batches and compute
cross-entropy averages. MacKay [12] gives a comprehensive discussion of information in the context of
learning algorithms. Tishby [13] examines information trends during neural network training.
A second critical property of entropy, which is implied by Shannon and further articulated by both
Barnard [14] and Rényi [15], is that entropy is an expectation. Given a latent random variable z, we denote
the probability distribution over outcomes as p(z). Stated as an expectation, entropy is deﬁned as

(cid:90)

S[ p(z) ] =

dz p(z) log

= Ep(z) log

(cid:18) 1

(cid:19)

p(z)

(cid:18) 1

p(z)

(cid:19)

.

4 of 31

Following Shannon, investigators developed a progression of divergence measures between general
probability distributions, q0 (z) and q1 (z). Notable cases include the Kullback-Leibler divergence, Rényi’s
(cid:18) q1 (z)
(cid:18) q1 (z)
information of order-α [15], and Csiszár ’s f -divergence [16]. We can also write the KL divergence as an
expectation
Ebrahimi, Sooﬁ, and Soyer [10] offer an examination of these axiomatic foundations and generalizations
with a primary focus on entropy and the KL divergence.

DKL [ q1 (z) (cid:107) q0 (z) ] =

= Eq1 (z) log

(cid:19)

.

(cid:19)

(cid:90)

dz q1 (z) log

q0 (z)

q0 (z)

2.1. Bayesian reasoning

The Bayesian view of probability, going back to Laplace [17] and championed by Jeffreys [18] and
Jaynes [6], focuses on capturing our beliefs. This perspective considers a probability distribution as an
abstraction that attempts to model these beliefs. This view subsumes all potential sources of uncertainty
and provides a comprehensive scope that facilitates analysis in diverse contexts.
Belief may express our understanding of what is consistent with physical models of natural laws. For
example, the state of a particle in quantum theory is represented by a wave function, which transforms to
probability distributions regarding position and momentum. Bell [19] shows that this form of uncertainty
is physically intrinsic within the theory of quantum mechanics. Likewise, we model the evolution of
uncertainty in thermodynamic systems as stochastic dynamical processes which capture ﬂuctuations
among compatible microstates [20].
Uncertainty may also be purely epistemic, however. Laplace considers a card drawn from one of two
urns, each containing a known ratio of black to white cards. A participant is given a card and is then asked
to infer the plausibility of each urn. In this context, uncertainty exists strictly within the participant’s mind.
In the Bayesian framework, the prior distribution p(z) expresses initial beliefs about some latent
variable z. Statisticians, scientists, and engineers often have well-founded views about real-world systems
that from the basis for priors. Examples include physically realistic ranges of model parameters or plausible
responses of a dynamical system. In the case of total ignorance, one applies the principle of insufﬁcient
reason [21] — we should not break symmetries of belief without justiﬁcation. Jaynes’ construction of
maximally uncertainty distributions [22] generalizes this principle, which we discuss further in 3.4 and 5.5.
As observations x become available, we update belief from the prior distribution to obtain the posterior
distribution p(z | x), which incorporates this new knowledge. This update is achieved by applying Bayes’
theorem
p(x | z)p(z)
p(z | x) =
dz p(x | z)p(z).
where p(x) =
p(x)
The likelihood distribution p(x | z) expresses the probability of observations given any speciﬁed value of
z. The normalization constant p(x) is also the probability of x given the prior belief that has been speciﬁed.
Within Bayesian inference, this is also called model evidence and it is used to evaluate a model structure’s
plausibility for generating the observations.
Integrating the maturing notion of belief found within the Bayesian framework with information
theory recognizes that our perception of how informative observations are depends on how our beliefs
develop, which is dynamic as our state of knowledge grows.

(cid:90)

2.2. Foundations of reason

Bernardo [23] shows that integrating entropy-like information measures with Bayesian inference
provides a foundation for rational experimental design. He considers potential utility functions, or
objectives for optimization, which are formulated as kernels of expectation over posterior belief updated

5 of 31

by the outcome of an experiment. Bernardo then distinguishes the belief a scientist reports from belief that
is justiﬁed by inference.
For a utility function to be proper, the Bayesian posterior must be the unique optimizer of expected
utility over all potentially reported beliefs. In other words, a proper utility function must not provide an
incentive to lie. His analysis shows that Lindley information

DL [ q1 (z) (cid:107) q0 (z) ] = S[ q1 (z) ] − S[ q0 (z) ]

is a proper utility function. Thus entropy-based information measures are not simply ad hoc objectives
— they are tied to foundations of reason by admitting optimization-based analysis that recovers rational
belief.

2.3. Probability notation

Random variables are denoted in boldface such as x. Typically x and y will imply observable
measurements and z will indicate either a latent explanatory variable or unknown observable. Each
random variable is implicitly associated with a corresponding probability space including the set of all
possible outcomes Ωz , a σ-algebra Fz of measurable subsets, and a probability measure Pz which maps
subsets of events to probabilities. We then express the probability measure as a distribution function p(z).
A realization, or speciﬁc outcome, will be denoted with either a check ˇz or, for discrete distributions
only, a subscript zi where i ∈ [n] and [n] = {1, 2, . . . , n}. If it is necessary to emphasize the value of a
marginalization is obtained by p(x) = (cid:82) dz p(x, z). When two distributions are equivalent over all subsets
distribution at a speciﬁc point or realization, we will use the notation p(z = ˇz). Conditional dependence
is denoted in the usual fashion as p(z | x). The joint distribution is then p(x, z) = p(z | x)p(x) and
of nonzero measure, we use notation q0 (z) ≡ p(z) or q1 (z) ≡ p(z | x).
The probability measure allows us to compute expectations over functions f (z) which are denoted

Details regarding the support of integration or summation are left implicit unless stated. For example, in
both the discrete case above and continuous cases, such as a distribution on the unit interval z ∈ R
integral notation should be interpreted respectively as

[0,1] , the

Ep(z) f (z) =

dz p(z) f (z).

(cid:90)

(cid:90)

(cid:90)

dz p(z) f (z) =

n∑

i=1

p(z = zi ) f (zi )

and

dz p(z) f (z) =

d ˇz p(z = ˇz) f ( ˇz).

(cid:90) 1

0

2.4. Sequences of belief

The postulates and theory in this work concern a sequence of beliefs beginning with an initial
state q0 (z) which is updated to a state q1 (z). In general, any third state of belief r(z) may serve as the
distribution over which expectation is taken. When we wish to emphasize this general perspective, we
refer to this as the view of expectation. The support of r(z) refers to the set of outcomes for which r(z) > 0,
which is also the support of expectation.
In the Bayesian context, we identify these distributions with prior belief q0 (z) ≡ p(z) and the ﬁrst
posterior q1 (z) ≡ p(z | x), which is conditioned upon an observation x. Then r(z) denotes the state of
rational belief. Rational belief is logically consistent with the entire body of available evidence affecting the
variable z. See 6.1 for further discussion. By denoting the partition of evidence that complements x as y,
rational belief corresponds to a second inference r(z) ≡ p(z | x, y).

6 of 31

We will also denote realization using notation r(z | ˇz), which assigns all probability mass to the
speciﬁed outcome ˇz. In the continuous setting this corresponds to a Dirac delta distribution r(z | ˇz) ≡
δ(z − ˇz).

3. Information and evolution of belief

In order to provide context for comparison, we begin by presenting the properties of entropic
information originally put forward by Shannon using our notation.

3.1. Shannon’s properties of entropy
1. Given a discrete probability distribution p(z) for which z ∈ {zi | i ∈ [n]}, the entropy S[ p(z) ] is
continuous in the probability of each outcome p(z = zi ).
2. If all outcomes are equally probable, namely p(z = zi ) = 1/n, then S[ p(z) ] is monotonically
increasing in n.
3. The entropy of a joint random variable S[ p(z, w) ] can be decomposed using a chain rule expressing
conditional dependence

S[ p(z, w) ] = S[ p(z) ] + Ep(z) S[ p(w | z) ] .

The ﬁrst point is aimed at extending Shannon’s derivation, which employs rational probabilities, to
real-valued probabilities. The second point drives at understanding entropy as a measure of uncertainty;
as the number of possible outcomes increases, each realization becomes less predictable. This results in
entropy taking positive values. The third point is critical — not only does it encode independent additivity,
it implies that entropic information is computed as an expectation.
We note that Fadeeve [24] gives a simpliﬁed set of postulates. Rènyi [15] generalizes information
by replacing the last point with a weaker version which simply requires independent additivity, but not
conditional expectation. This results in α-divergences. Csiszàr [16] generalizes this further using convex
functions f to obtain f -divergences.

3.2. Postulates

Rather than repeating direct analogs of Shannon’s properties in the context of evolving belief, it is
both simpler and more illuminating to be immediately forthcoming regarding the key requirement of
information in the perspective of this theory.

Postulate 1. Entropic information associated with the change in belief from q0 (z) to q1 (z) is quantiﬁed as an
expectation over belief r(z), which we call the view of expectation. As an expectation, it must have the functional
form

(cid:90)

Ir(z) [ q1 (z) (cid:107) q0 (z) ] =

dz r(z) f (r(z), q1 (z), q0 (z)) .

Postulate 2. Entropic information is additive over independent belief processes. Taking joint distributions associated
with two independent random variables z and w to be q0 (z, w) = q0 (z)q0 (w), q1 (z, w) = q1 (z)q1 (w), and
r(z, w) = r(z)r(w) gives

Ir(z)r(w) [ q1 (z)q1 (w) (cid:107) q0 (z)q0 (w) ] = Ir(z) [ q1 (z) (cid:107) q0 (z) ] + Ir(w) [ q1 (w) (cid:107) q0 (w) ] .

Postulate 3. If belief does not change then no information is gained regardless of the view of expectation

Ir(z) [ q0 (z) (cid:107) q0 (z) ] = 0.

7 of 31

Postulate 4. The information gained from any normalized prior state of belief q0 (z) to an updated state of belief
r(z) in the view of r(z) must be nonnegative

Ir(z) [ r(z) (cid:107) q0 (z) ] ≥ 0.

combining this with the second postulate, it is possible to show that f (r, q, p) = log (cid:0)rγ qα pβ (cid:1) for constants
The ﬁrst postulate requires information to be reassessed as belief changes. The most justiﬁed state of
belief, based on the entirety of observations, will correspond to the most justiﬁed view of information. By
α, β, γ. See A for details. The third postulate then constrains these exponential constants and the fourth
simply sets the sign of information.

3.3. Principal result

Theorem 1. Information as a measure of change in belief.
Information measurements that satisfy these
postulates must take the form
(cid:18) q1 (z)

(cid:19)

(cid:90)

Ir(z) [ q1 (z) (cid:107) q0 (z) ] = α

dz r(z) log

q0 (z)

for some α > 0.

Proof is given in A. As Shannon notes regarding entropy, α corresponds to a choice of units. Typical
choices are natural units α = 1 and bits α = log(2)−1 . We employ natural units in analysis and bits in
experiments.
Although it would be possible to combine Postulate 1 and Postulate 2 into an analog of Shannon’s
chain rule as a single postulate, doing so would obscure the reasoning behind the construction. We leave
the analogous chain rule as a consequence in Corollary 1. Regarding Shannon’s proof that entropy is
the only construction that satisﬁes properties he provides, we observe that he has restricted attention to
functionals acting upon a single distribution. The interpretation of entropy is discussed in 4.1.
Cox [5] discusses the properties of reasonable expectation and shows why r(z) must be understood
as a proper (normalized) probability distribution. As for q0 (z) and q1 (z), however, nothing postulated
prevents analysis respecting improper or non-normalizable probability distributions. In the Bayesian
context, such distributions merely represent relative plausibility among subsets of outcomes. We caution
that such analysis is a further abstraction, which requires some subtlety for interpretation.
We remark that although Rènyi and Csiszàr were able to generalize information by weakening
Shannon’s chain rule to independent additivity,
inclusion of the ﬁrst postulate prevents such
generalizations.

3.4. Regarding the support of expectation

The proof given assumes q0 (z) and q1 (z) take positive values over the support of the integral, which
is also the support of r(z). In the Bayesian context, we also have
p(x | z)q0 (z)
p(y | z, x)q1 (z)
p(y | x)
p(x)

and r(z) =

q1 (z) =

Accordingly, if for some ˇz we have q1 ( ˇz) = 0 it follows that r( ˇz) = 0. Likewise, q0 ( ˇz) = 0 would imply
both q1 ( ˇz) = 0 and r( ˇz) = 0. This forbids information contributions that fall beyond the scope of the
proof. Even so, the resulting form is analytic and admits analytic continuation.

Since both limε→0 [ε log ε] = 0 and limε→0
(cid:18) q1
(cid:16) q1
q0

(cid:19)

ε log

ε log

ε→0

ε→0

lim

lim

ε

,

(cid:2)ε log ε−1 (cid:3) = 0, limits of information of the form
,
lim
ε log
,
and lim
ε log

(cid:18) ε

(cid:16) ε

(cid:19)

(cid:17)

(cid:17)

ε→0

ε

ε→0

q0

8 of 31

are consistent with restricting the domain of integration (or summation) to the support of r(z). We gain
further insight by considering limits of the form

(cid:18) ε

(cid:19)

(cid:16) q

(cid:17)

.

r log

lim

ε→0

and lim

r log

ε→0

ε

q
Information diverges to −∞ in the ﬁrst case and +∞ in the second. This is consistent with the fact that no
ﬁnite amount of data will recover belief over a subset that has been strictly forbidden from consideration,
which bears ramiﬁcations for how we understand rational belief.
If belief is not subject to inﬂuence from evidence, it is difﬁcult to credibly construe an inferred outcome
as having rationally accounted for that evidence. Lindley calls this Cromwell’s rule [25]; we should not
eliminate a potential outcome from consideration unless it is logically false. The principle of insufﬁcient
reason goes further by avoiding unjustiﬁed creation of information that is not inﬂuenced by evidence.

3.5. Information density

The Radon-Nikodym theorem [26] formalizes the notion of density that relates two measures. If we
assign both probability and a second measure to any subset within a probability space, then there exists a
density function, unique up to subsets of measure zero, such that the second measure is equivalent to the
integral of said density over any subset.

Deﬁnition 1. Information density. We take the Radon-Nikodym derivative to obtain information density of the
change in belief from q0 (z) to q1 (z)
(cid:18) q1 (z)

dIr(z) [ q1 (z) (cid:107) q0 (z) ]

(cid:19)

D[ q1 (z) (cid:107) q0 (z) ] =

dr(z)

= log

.

q0 (z)

The key property we ﬁnd in this construction is independence from the view of expectation. As
such, information density encodes all potential information outcomes one could obtain from this theory.
Furthermore, this formulation is amenable to analysis of improper distributions. For example, it proves
useful to consider information density corresponding to constant unit probability density q1 (z) ≡ 1,
which is discussed further in 4.1.

3.6. Information pseudometrics

The following pseudometrics admit interpretations as notions of distance between belief states that
remain compatible with Postulate 1. This is achieved by simply taking the view of expectation r(z) to be
the weight function in weighted-L p norms of information density. These constructions then satisfy useful
properties of pseudometrics:
1. Positivity, L p
2. Symmetry, L p
3. Triangle inequality,

r(z) [ q1 (z) (cid:107) q0 (z) ] ≥ 0,
r(z) [ q1 (z) (cid:107) q0 (z) ] = L p
r(z) [ q0 (z) (cid:107) q1 (z) ],

L p

r(z) [ q2 (z) (cid:107) q0 (z) ] ≤ L p
r(z) [ q2 (z) (cid:107) q1 (z) ] + L p
r(z) [ q1 (z) (cid:107) q0 (z) ] .

9 of 31

Deﬁnition 2. L p information pseudometrics. We may construct pseudometrics between states of belief q0 (z)
and q1 (z) with the view of expectation r(z), by taking weighted-L p norms of information density where the view of
(cid:18) q1 (z)
expectation serves as the weight function

(cid:18)(cid:90)

L p

r(z) [ q1 (z) (cid:107) q0 (z) ] =

dz r(z)

for some

p ≥ 1.

(cid:12)(cid:12)(cid:12)(cid:12)log

(cid:19)(cid:12)(cid:12)(cid:12)(cid:12) p(cid:19)1/ p

q0 (z)

Symmetry is obvious from inspection and the other properties follow by construction as a seminorm.
With regard to positivity, we observe that if q0 (z) and q1 (z) are measurably distinct over the support
of r(z) then the measured distance must be greater than zero. We may regard states of belief q0 (z) and
q1 (z) as weakly equivalent in the view of r(z) if their difference is immeasurable over the support of r(z).
That is, if q0 (z) and q1 (z) only differ over subsets of outcomes that are deemed by r(z) to be beyond
plausible consideration, then in the view of r(z) they are equivalent. As such, these pseudometrics could
be regarded as subjective metrics in the view of r(z).
The homogeneity property of seminorms implies that these constructions retain the units of measure of
information density; if information density is measured in bits, these distances have units of bits as well.
Note that taking p = 1 results in a pseudometric that is also a pure expectation.
The natural deﬁnition of information variance also satisﬁes the properties of a pseudometric and is
also interpretable as a standard statistical construct.

Deﬁnition 3. Information variance. Information variance between belief states q0 (z) and q1 (z) in the view of
expectation r(z) is simply the variance of information density
(cid:18) q1 (z)

(cid:19)2

(cid:18)

(cid:19)

(cid:90)

dz r(z)

log

− ϕ

q0 (z)

Varr(z) [ q1 (z) (cid:107) q0 (z) ] =
where ϕ = Ir(z) [ q1 (z) (cid:107) q0 (z) ].

4. Corollaries and Interpretations

The following corollaries examine primary consequences of Theorem 1. Note that multiple random
variables may be expressed as a single joint variable such as z = (z1 , z2 , . . . , zn ). The following corollaries
explore one or two components at a time such as variables z1 and z2 or observations x and y. Extensions
joint distribution q(z1 , z2 ), we can compute the marginalization q(z1 ) ≡ (cid:82) dz2 q(z1 , z2 ) and conditional
to multiple random variables easily follow.
Note that the standard formulation of conditional dependence holds in Corollary 1. That is, given any
dependence follows by the Radon-Nikodym derivative to obtain q(z2 | z1 ) ≡ q(z1 ,z2 )
q(z1 ) . All proofs are
contained in B.

Corollary 1. Chain rule of conditional dependence. Information associated with joint variables decomposes as

Ir(z1 ,z2 ) [ q1 (z1 , z2 ) (cid:107) q0 (z1 , z2 ) ] = Ir(z1 ) [ q1 (z1 ) (cid:107) q0 (z1 ) ]
Ir(z2 | z1 ) [ q1 (z2 | z1 ) (cid:107) q0 (z2 | z1 ) ] .

+ Er(z1 )

Corollary 2. Additivity over belief sequences. Information gained over a sequence of belief updates is additive
within the same view. Given initial belief q0 (z), intermediate states q1 (z) and q2 (z), and the view r(z) we have

Ir(z) [ q2 (z) (cid:107) q0 (z) ] = Ir(z) [ q2 (z) (cid:107) q1 (z) ] + Ir(z) [ q1 (z) (cid:107) q0 (z) ] .

Corollary 3. Antisymmetry. Information from q1 (z) to q0 (z) is the negative of information from q0 (z) to q1 (z)

Ir(z) [ q0 (z) (cid:107) q1 (z) ] = −Ir(z) [ q1 (z) (cid:107) q0 (z) ] .

4.1. Entropy

10 of 31

Shannon’s formalization of entropy as uncertainty may be consistently understood as the expectation
of information gained by realization. We ﬁrst reconstruct information contained in realization. We then
deﬁne the general form of entropy in the discrete case, which is cross entropy, and ﬁnally the standard
form of entropy follows.
Corollary 4. Realization information (discrete). Let z be a discrete random variable z ∈ {zi | i ∈ [n]}.
Information gained by realization ˇz from q(z) in the view of realization r(z | ˇz) is
Ir(z | ˇz) [ r(z | ˇz) (cid:107) q(z) ] = D[ 1 (cid:107) q(z = ˇz) ] .
Corollary 5. Cross entropy (discrete). Let z be a discrete random variable z ∈ {zi | i ∈ [n]} and ˇz be a
hypothetical realization. Expectation over the view r( ˇz) of information gained by realization from belief q(z) recovers
cross entropy

Er( ˇz)

Ir(z | ˇz) [ r(z | ˇz) (cid:107) q(z) ] = Ir(z) [ 1 (cid:107) q(z) ] = Sr(z) [ q(z) ] .

Corollary 6. Entropy (discrete). Let z be a discrete random variable z ∈ {zi | i ∈ [n]} and ˇz be a hypothetical
realization. Expectation over plausible realizations q( ˇz) of information gained by realization from belief q(z)
recovers entropy

Eq( ˇz)

Ir(z | ˇz) [ 1 (cid:107) q(z) ] = Iq(z) [ 1 (cid:107) q(z) ] = S[ q(z) ] .

Shannon proved that this is the only construction as a functional acting on a single distribution
q(z) that satisﬁes his properties. As mentioned earlier, the information notation Iq(z) [ 1 (cid:107) q(z) ] requires
some subtlety of interpretation. Probability density 1 over all possible outcomes z ∈ Ωz is not generally
normalized. Although these formulas are convenient abstractions that result from formal derivations as
expectations in the discrete case, nothing prevents us from applying them in continuous settings, which
recovers the typical deﬁnitions in such cases.
We emphasize that this deﬁnition of entropy on the continuum is not consistent with taking the limit
of a sequence of discrete distributions that converges in probability density to that of a continuous limiting
distribution. The entropy of such a sequence diverges to inﬁnity, which matches our intuition; the amount
of data required to specify a continuous (real) random variable also diverges.
4.2. Potential information
z as a latent variable. Given belief p(z), the probability of an observation w is p(w) = (cid:82) dz p(w | z)p(z) as
We now consider expectations over hypothetical future observations w that would inﬂuence belief in
usual.

Corollary 7. Consistent future expectation. Let the view p(z) express present belief in the latent variable z and
w represent a future observation. The expectation over plausible w of information in the belief-shift from q0 (z) to
q1 (z) in the view of rational future belief p(z | w) is equal to information in the present view

Ep(w)

Ip(z | w) [ q1 (z) (cid:107) q0 (z) ] = Ip(z) [ q1 (z) (cid:107) q0 (z) ] .

11 of 31

Corollary 8. Mutual information. Let the view p(z) express present belief in the latent variable z and w represent
a future observation. Expectation of information gained by a future observation w is mutual information

Ep(w)

Ip(z | w) [ p(z | w) (cid:107) p(z) ] = Ip(z,w) [ p(z, w) (cid:107) p(z)p(w) ] .

Corollary 9. Realization limit. Let z be a latent variable and ˇz be the limit of increasing observations to obtain
arbitrary precision over plausible values of z. Information gained from q0 (z) to q1 (z) in the realization limit r(z | ˇz)
is pointwise information density

Ir(z | ˇz) [ q1 (z) (cid:107) q0 (z) ] = D[ q1 (z = ˇz) (cid:107) q0 (z = ˇz) ] .

5. Implications regarding foundations

As discussed in 2, Kullback and Leibler, Lindley, and Jaynes have been identiﬁed as contributing key
insights to our understanding of entropic information. In our view, Bernardo’s contribution also carries
profound implications regarding the fundamental connection between information and reason. We review
these contributions within the context of this theory and conclude this section with remarks on Fisher
information.

5.1. Kullback-Leibler divergence
As discussed in 2.4, we regard q0 (z) ≡ p(z) as the prior and q1 (z) ≡ p(z | x) as the posterior
conditioned on the observation of x in the Bayesian setting. Without any additional evidence, we must
take r(z) ≡ p(z | x) to be rational belief. This implies that the Kullback-Liebler divergence is the rational
measure of information gained by the observation of x. It bears repeating that once additional evidence
y has been observed, this will no longer hold. In general, Theorem 1 gives the rational measure of
information gained by the observation of x, where the view of expectation is taken to be updated rational
belief r(z) ≡ p(z | x, y).

5.2. Lindley information

In the context of this theory, Lindley’s measure is not the information gained by the experiment; it is
simply the difference in uncertainty due to the experiment. This distinction is highlighted by the following
thought experiment.
Suppose z describes the value of two lottery tickets. Comparing the ﬁrst ticket with winning numbers
is represented by x, which we will call the ﬁrst experiment. Lindley proposed the following measure of
information
DL [ p(z | x) (cid:107) p(z) ] = S[ p(z | x) ] − S[ p(z) ]
If the odds of winning are 1 : 220 , or roughly one in a million, the uncertainty difference in reading a
single ticket is always 2.04 × 10−5 bits, regardless of the outcome. Realization information, however, is
1.38 × 10−6 bits for a losing ticket (the predictable outcome) verses 20 bits for a winning ticket. In contrast,
Ip(z | x) [ p(z | x) (cid:107) p(z) ] recovers realization information gained by reading the ﬁrst ticket.
Reading the second ticket y and updating information Ip(z | x,y) [ p(z | x) (cid:107) p(z) ] also recovers
realization information regardless of the outcome of the second ticket. This is because once the ﬁrst
random variable is realized, thus eliminating all but one possible outcome, belief in that outcome may no
longer change.

12 of 31

5.3. Bernardo proper utility

As discussed in 2.2, Bernardo considers utility functions that are formulated as expectations over
inferred belief. By distinguishing optimal belief that is potentially reported from belief inferred by the
evidence, Bernardo identiﬁes a proper utility function as an optimization objective for which both are
equivalent. He then shows that Lindley information is proper.
Corollary 10. Information in inference yields proper utility. Taking the view of rational belief p(z | x) over
the latent variable z conditioned upon an experimental outcome x, the information Ip(z | x) [ q(z) (cid:107) p(z) ] from prior
belief p(z) to reported belief q(z) is a proper utility function. That is, the unique optimizer recovers rational belief
q∗(z) ≡ p(z | x).

We would like to go a step further and show that when information from q0 (z) to q1 (z) is positive
in the view of r(z), we may claim that q1 (z) is closer to r(z) than q0 (z). For this claim to be consistent,
we must show that any perturbation that unambiguously drives belief q1 (z) toward the view r(z) must
also increase information. The complementary perturbation response with respect to q0 (z) immediately
follows by Corollary 3.

Corollary 11. Proper perturbation response. Let q1 (z) be measurably distinct from the view r(z) and
Ir(z) [ q1 (z) (cid:107) q0 (z) ] be ﬁnite. Let the perturbation η(z) preserve normalization and drive belief toward r(z)
on all measurable subsets. It follows

lim

ε→0

∂
∂ε

Ir(z) [ q1 (z) + εη(z) (cid:107) q0 (z) ] > 0.

It bears repeating, by Corollary 8, that mutual information captures expected proper utility, which
provides a basis for rational experimental design and feature selection.

5.4. Discrepancy functions

Ebrahimi, Sooﬁ, and Soyer [10] discuss information discrepancy functions, which have two key
properties. First, a discrepancy function is nonnegative D [ q1 (z) (cid:107) q0 (z) ] ≥ 0 with equality if and only if
q1 (z) ≡ q0 (z). Second, if we hold q0 (z) ﬁxed then D [ q1 (z) (cid:107) q0 (z) ] is convex in q1 (z).
One of the reasons information discrepancy functions are useful is that they serve to identify
independence. Random variables x and z are independent if and only if p(x) ≡ p(x | z). Therefore,
we have D [ p(x, z) (cid:107) p(x)p(z) ] ≥ 0 with equality if and only if x and z are independent, noting that
p(x, z) ≡ p(x | z)p(z). This has implications regarding sensible generalizations of mutual information.
Theorem 1 does not satisfy information discrepancy properties unless the view of expectation is taken
to be r(z) ≡ q1 (z), which is the KL divergence. We note, however, that information pseudometrics and
information variance given in 3.6 satisfy a weakened formulation. Speciﬁcally, L p
with equality if and only if q0 (z) and q1 (z) are weakly equivalent in the view of r(z). Likewise, these
formulations are convex in information density D[ q1 (z) (cid:107) q0 (z) ].
5.5. Jaynes maximal uncertainty

r(z) [ q1 (z) (cid:107) q0 (z) ] ≥ 0

Jaynes uses entropy to analytically construct probability distributions for which uncertainty is
maximal while maintaining consistency with a speciﬁed set of expectations. This construction avoids
unjustiﬁed creation of information and places the principle of insufﬁcient reason into an analytic framework

13 of 31

within which the notion of symmetry generalizes to informational symmetries conditioned upon observed
expectations.
We review how Jaynes constructs the resulting distribution r∗(z). Let such kernels of expectation be
denoted f i (z) for i ∈ [n] and the observed expectations be Er(z) [ f i (z)] = ϕi . The objective of optimization
is
r∗(z) = argmax
subject to Er(z) f i (z) = ϕi ∀ i ∈ [n].

S[ r(z) ]

r(z)

The Lagrangian, which captures both the maximal uncertainty objective and the expectation constraints, is

L[ r(z), λ ] =

dz r(z)

log

− n∑

λi ( f i (z) − ϕi )

.

r(z)
where λ ∈ Rn is the vector of Lagrange multipliers. This Lagrangian formulation satisﬁes the variational
principle in both r(z) and λ. Variational analysis yields the optimizer

i=1

(cid:90)

(cid:32)

(cid:18) 1

(cid:19)

(cid:33)

(cid:33)

(cid:32) n∑

i=1

r∗(z) ∝ exp

λi f i (z)

.

5.5.1. Information-critical distributions
Rather than maximizing entropy, we may minimize Ir(z) [ r(z) (cid:107) q0 (z) ] while maintaining consistency
with speciﬁed expectations. Since the following corollary holds for general distributions q0 (z), including
the improper case q0 (z) ≡ 1, this includes Jaynes’ maximal uncertainty as a minimization of negative
entropy.
Corollary 12. Minimal information. Given kernels of expectation f i (z) and speciﬁed expectations Er(z) [ f i (z)] =
ϕi for i ∈ [n], the distribution r∗(z) that satisﬁes these constraints while minimizing information Ir(z) [ r(z) (cid:107) q0 (z) ]
is given by

(cid:33)

(cid:32) n∑

i=1

λi f i (z)

for some λ ∈ Rn .

r∗(z) ∝ q0 (z) exp

5.6. Remarks on Fisher information

Fisher provides an analytic framework to assess the suitability of a pointwise latent description of a
probability distribution [27]. As Kullback and Leibler note, the functional properties of information in
Fisher ’s construction are quite different from Shannon’s and thus we do not regard Fisher information as a
form of entropic information. Fisher ’s construction, however, can be rederived and understood within this
theory. He begins with the assumption that there is some latent realization ˇz for which p(x | ˇz) is an exact
description of the true distribution of x. We can then deﬁne the Fisher score as the gradient of information
from any independent prior belief q0 (x) to a pointwise latent description p(x | z), in the view p(x | ˇz)

f = ∇z Ip(x | ˇz) [ p(x | z) (cid:107) q0 (x) ] .

Note that ˇz is ﬁxed by assumption, despite remaining unknown. By the variational principle, the score
must vanish at the optimizer z∗. By Corollary 10, the optimizer must be z∗ = ˇz. We can then assess the

sensitivity of information to the parameter z at the optimizer z∗ by computing the Hessian. This recovers
an equivalent construction of the Fisher matrix within this theory

14 of 31

Fi j =

∂2
∂zi ∂z j

Ip(x | ˇz) [ p(x | z) (cid:107) q0 (x) ] .

The primary idea behind this construction is that high-curvature in z implies that a pointwise description
is both suitable and a well-conditioned optimization problem.

5.6.1. Generalized Fisher matrix

We may eliminate the assumption of an exact pointwise description and generalize analogous
formulations to arbitrary views of expectation.

Deﬁnition 4. Generalized Fisher score. Let r(x) be the view of expectation regarding an observable x. The
gradient with respect to z of information from independent prior belief q0 (x) to a pointwise description p(x | z)
gives the score

f = ∇z Ir(x) [ p(x | z) (cid:107) q0 (x) ] .

Deﬁnition 5. Generalized Fisher matrix. Let r(x) be the view of expectation regarding an observable x. The
Hessian matrix with respect to components of z of information from independent prior belief q0 (x) to the pointwise
description p(x | z) gives the generalized Fisher matrix

Fi j =

∂2
∂zi ∂z j

Ir(x) [ p(x | z) (cid:107) q0 (x) ] .

Again, a local optimizer z∗ must satisfy the variational principle and yield a score of zero. The
generalized Fisher matrix would typically be evaluated at such an optimizer z∗.

6. Information in inference and machine learning

We begin this section with a brief discussion of rational views that may be used to measure information.
Then we discuss the model information and predictive information provided by inference. Once we have
deﬁned these information measurements, we derive upper and lower bounds between them that we
anticipate being useful for future work. Finally, we show how inference information may be constrained,
which addresses some challenges in Bayesian inference.

6.1. Discussion of rational views

Given a state of prior belief, Bayesian inference assimilates evidence to update belief and obtain the
posterior distribution. As described in 2.4, we regard this as rational belief as opposed to an arbitrary belief
that may serve as the view of expectation. Within the Bayesian philosophy, however, we may disagree
about whether information can only be measured with respect to rational belief. This disagreement
corresponds to objective versus subjective views of Bayesian probability, see [6] for a discussion.
Within the objective view, one’s beliefs must be consistent with the entirety of evidence and prior
knowledge must be justiﬁed by sound principles of reason. Therefore, anyone with the same body of
evidence must hold the same rational belief. In contrast, the subjective view holds that one’s personal
beliefs do not need justiﬁcation and do not require consistency with others. Note, however, that the
subjective view does not imply that all beliefs are equally valid. It simply allows validity to be derived

15 of 31

from other notions of utility, such as computational feasibility, in addition to consistency with evidence
and prior knowledge.
While Theorem 1 does not require adoption of either perspective, these philosophies do inﬂuence
allowable views of expectation. The objective philosophy implies that an information measurement is
justiﬁed to the same degree as the view of expectation that deﬁnes it, whereas the subjective philosophy
admits information analysis with any view of expectation.

6.2. Machine learning information

(cid:110)(cid:16)

Akaike [28] ﬁrst introduced information-based complexity criteria as a strategy for model selection.
These ideas were further developed by Schwarz, Burnham, and Gelman [11,29,30]. We anticipate these
notions will prove useful in future work to both understand and control the problem of memorization
in machine learning training. Accordingly, we discuss how this theory views model complexity and
x( j) , y( j) (cid:17) (cid:12)(cid:12)(cid:12) j ∈ [T ]
distinguishes formulations of predictive information and residual information.
In machine learning, observations correspond to matched pairs of inputs and labels Y =
. For each sample j of T training examples, we would like to map the input x( j) to
an output label y( j) . Latent variables θ are unknown model parameters from a speciﬁed model family
or computational structure. A model refers to a speciﬁc parameter state and the predictions that the
model computes are p(y( j) | x( j) , θ). Since the deﬁnitions and derivations that follow hold with respect to
either single cases or the entire training ensemble, we will use shorthand notation p(y | θ) to refer to both
scenarios.
We denote the initial state of belief in model parameters as q0 (θ) and updated belief during training
as qi (θ) for i ∈ [n]. We can then compute predictions from any state of model belief by marginalization

(cid:111)

qi (y) ≡ (cid:82) dθ p(y | θ)qi (θ).

Deﬁnition 6. Model information. Model information from initial belief q0 (θ) to updated belief qi (θ) in the view
of r(θ) is given by
i ∈ [n].

Ir(θ) [ qi (θ) (cid:107) q0 (θ) ]

for

When we compute information contained in training labels, the label data obviously provide the
rational view. This is represented succinctly by r(y | ˇy), which assigns full probability to speciﬁed outcomes.
Again, if we need to be explicit then this could be written as r(y | x( j) , y( j) ) for each case in the training set.
Deﬁnition 7. Predictive label information. The realization of training labels is the rational view r(y | ˇy) of label
plausibility. We compute information from prior predictive belief q0 (y) to predictive belief qi (y) in this view as

Ir(y | ˇy) [ qi (y) (cid:107) q0 (y) ] .

In the continuous setting, this formulation is closely related to log pointwise predictive density [11]. We
can also deﬁne complementary label information that is not contained in the predictive model.
Deﬁnition 8. Residual label information (discrete). Residual information in the label realization view r(y | ˇy)
is computed as

Ir(y | ˇy) [ r(y | ˇy) (cid:107) qi (y) ] .

Residual information is equivalent to cross-entropy if the labels are full realizations. We note, however,
that if training labels are probabilistic and leave some uncertainty then replacing r(y | ˇy) above with a
general distribution r(y) would correctly calibrate residual information so that if predictions were to

16 of 31

match label distributions then residual information would be zero. Just as the limiting form of entropy
discussed in 4.1 diverges, so also does residual information diverge in the continuous setting.
As a consequence of Corollary 2, the sum of predictive label information and residual label information
is always constant. This allows us to rigorously frame predictive label information as a fraction of the total
information contained in training labels. Moreover, Corollary 11 assures us that model perturbations that
drive predictive belief toward the label view must increase predictive information. This satisﬁes our initial
incentive for this investigation.
There is a second type of predictive information we may rationally construct, however. Rather than
considering predictive information with respect to speciﬁed label outcomes, we might be interested in the
information we expect to obtain about new samples from the generative process. If we regard marginalized
predictions qi (y) as our best approximation of this process, then we would simply measure change in
predictive belief in this view.
from model belief qi (θ) using the predictive marginalization qi (y) ≡ (cid:82) dθ p(y | θ)qi (θ). If we hold this to be the
Deﬁnition 9. Predictive generative approximation. We may approximate the distribution of new outcomes
rational view of new outcomes from the generative process, predictive information is

Iqi (y) [ qi (y) (cid:107) q0 (y) ] .

6.3. Inference information bounds
In Bayesian inference, we have prior belief in model parameters q0 (θ) ≡ p(θ) and the posterior
inferred from training data q1 (θ) ≡ p(θ | ˇy). The predictive marginalizations are called the prior predictive
and posterior predictive distributions respectively
p(y) ≡
and p(y | ˇy) ≡

dθ p(y | θ)p(θ | ˇy).

(cid:90)

(cid:90)

dθ p(y | θ)p(θ)

We derive inference information bounds for Bayesian networks [31]. Let y, θ1 , and θ2 represent a
directed graph of latent variables. In general, the joint distribution can always be written as p(y, θ1 , θ2 ) =
p(θ2 | θ1 , y)p(θ1 | y)p(y). The property of local conditionality means p(θ2 | θ1 , ˇy) ≡ p(θ2 | θ1 ). That is, belief
dependence in θ2 is totally determined by that of θ1 just as belief in θ1 is computed from ˇy.

Corollary 13. Joint local inference information. Inference information in θ1 gained by having observed ˇy is
equivalent to the inference information in both θ1 and θ2 .

Ip(θ1 ,θ2 | ˇy) [ p(θ1 , θ2 | ˇy) (cid:107) p(θ1 , θ2 ) ] = Ip(θ1 | ˇy) [ p(θ1 | ˇy) (cid:107) p(θ1 ) ] .

Corollary 14. Monotonically decreasing local inference information. Inference information in θ2 gained by
having observed ˇy is bound above by inference information in θ1 .

Ip(θ2 | ˇy) [ p(θ2 | ˇy) (cid:107) p(θ2 ) ] ≤ Ip(θ1 | ˇy) [ p(θ1 | ˇy) (cid:107) p(θ1 ) ] .

This shows that inference yields nonincreasing information as we compound inference on locally
conditioned latent variables, which is relevant for sequential predictive computational models such as
neural networks. We observe that the inference sequence from training data ˇy to model parameters θ to
new predictions y is also a locally conditioned sequence. If belief in a given latent variable is represented as
a probability distribution, this places bounds on what transformations are compatible with the progression
of information.

17 of 31

Corollary 15. Inferred information upper bound. Model information in the posterior view is less than or equal
to predictive label information resulting from inference

Ip(θ | ˇy) [ p(θ | ˇy) (cid:107) p(θ) ] ≤ Ir(y | ˇy) [ p(y | ˇy) (cid:107) p(y) ] .

This is noteworthy because it tells us that inference always yields a favorable tradeoff between
increased model complexity and predictive information. Combining Corollary 14 and Corollary 15, we
have upper and lower bounds on model information due to inference

Ip(y | ˇy) [ p(y | ˇy) (cid:107) p(y) ] ≤ Ip(θ | ˇy) [ p(θ | ˇy) (cid:107) p(θ) ] ≤ Ir(y | ˇy) [ p(y | ˇy) (cid:107) p(y) ] .

6.4. Inference information constraints

Practitioners of Bayesian inference often struggle when faced with inference problems for models
structures that are not well suited to the data. An under expressive model family is not capable of
representing the process being modeled. As a consequence, the posterior collapses to a small set of
outcomes that are least inconsistent with the evidence. In contrast, an over expressive model admits
multiple sufﬁcient explanations of the process.
Both model and predictive information measures offer means to understand and address these
challenges. By constraining the information gained by inference, we may solve problems associated
with model complexity. In this section, we discuss explicit and implicit approaches to enforcing such
constraints.

6.4.1. Explicit information constraints

Our ﬁrst approach to encode information constraints is to explicitly solve a distribution that satisﬁes
expected information gained from the prior to the posterior. We examine how information-critical
distributions can be constructed from arbitrary states of belief qi (θ) for i ∈ [n]. Again, we may obtain
critical distributions with respect to uncertainty by simply setting q0 (θ) ≡ 1. By applying this to inference,
so that n = 1 and q1 (θ) ≡ p(θ | y), we recover likelihood annealling as a means to control model
information.

Corollary 16. Constrained information.
Given states of belief qi (θ) and information constraints
Ir(θ) [ qi (θ) (cid:107) q0 (θ) ] = ϕi for i ∈ [n], the distribution r∗(θ) that satisﬁes these constraints while minimizing
Ir(θ) [ r(θ) (cid:107) q0 (θ) ] has the form

(cid:19)λi

(cid:18) qi (θ)
q0 (θ)

r∗(θ) ∝ q0 (θ)

n∏

i=1

for some λ ∈ Rn .

Corollary 17. Information-annealed inference. Annealed belief r(θ) for which information gained from prior
to posterior belief is ﬁxed Ir(θ) [ p(θ | ˇy) (cid:107) p(θ) ] = ϕ and information Ir(θ) [ r(θ) (cid:107) p(θ) ] is minimal must take the
form
for some λ ∈ R.

r(θ) ∝ p( ˇy | θ)λ p(θ)

Note that the bounds in 6.3 still apply if we simply include λ as a ﬁxed model parameter in the
deﬁnition of the likelihood function so that p( ˇy | θ) (cid:55)→ p( ˇy | θ, λ) ≡ p( ˇy | θ)λ . This prevents the model
from learning too much, which may be useful for under-expressive models or for smoothing out the
posterior distribution to aid exploration during learning.

18 of 31

6.4.2. Implicit information constraints

Our second approach introduces hyper-parameters, λ and ψ, into the Bayesian inference problem,
which allows us to deﬁne a prior on those hyper-parameters that implicitly encodes information constraints.
This approach gives us a way to express how much we believe we can learn from the data and model that
we have in hand. Doing so may prevent overconﬁdence when there are known modeling inadequacies or
underconﬁdence from overly broad priors.
As above, λ parameters inﬂuence the likelihood and can be though of as controlling annealing or
an embedded stochastic error model. The ψ parameters control the prior on the model parameters θ.
For example, these parameters could be the prior mean and co-variance if we assume a Gaussian prior
distribution. Therefore the inference problem takes the form

p(θ, λ, ψ | ˇy) =

p( ˇy)

p( ˇy | θ, λ)p(θ | ψ)p(λ, ψ)

.

p(λ, ψ) = g (cid:0) ϕθ , ϕy

(cid:1) where
In order to encode the information constraints, we must construct the hyper-prior distribution

ϕθ = Ip(θ | ˇy,λ,ψ) [ p(θ | ˇy, λ, ψ) (cid:107) p(θ | ψ) ]
ϕy = Ir(y | ˇy) [ p(y | ˇy, λ, ψ) (cid:107) p(y | λ, ψ) ]

and
control model information and predictive information, respectively. The function g (cid:0) ϕθ , ϕy
(cid:1) is the
likelihood of λ and ψ given the speciﬁed model and prediction complexities. For example, this could be
an indicator function as to whether the information gains are within some range. Note that we may also
consider other forms of predictive information such as the predictive generative approximation.
The posterior distribution on model parameters and posterior predictive distribution can be formed
by marginalizing over hyper-parameters
p(θ | ˇy) =
p(y | ˇy) =

dλdψ p(θ, λ, ψ | ˇy)
and
dλdψ p(y | θ, λ)p(θ, λ, ψ | ˇy).

(cid:90)
(cid:90)

7. Negative information

As mentioned in the introduction, information may be found to be negative under some circumstances.
The following experiments illustrate such cases and motivate the utility of negative information, which
can be interpreted as identifying intermediate belief states that are later found to be less consistent with a
more justiﬁed view than prior belief had been.

7.1. Negative information in continuous inference
In the following set of experiments we have a latent variable θ ∈ R2 , which is distributed as
N (θ | 0, I ). Each sample y( j) ∈ R2 corresponds to realization of an independent latent variable x( j) ∈ R2
so that y( j) = θ + x( j) . Each x( j) is distributed as N (x( j) | 0, σ2
1 I ) where σ1 = 1/2. Both prior belief in
plausible values of θ and prior predictive belief in plausible values of y are visualized in Figure 1. Deciles
separate annuli of probability 1/10. The model information we expect to gain by observing 10 samples of
y, which is also mutual information from Corollary 8, is Ip(y,θ) [ p(y, θ) (cid:107) p(y)p(θ) ] = 5.36 bits. See C for
details.

19 of 31

Figure 1. Prior distribution of θ and prior predictive distribution of individual y samples. The domain of
plausible θ values is large before any observations are made.

Figure 2. Typical inference of θ from observation of 10 samples of y (left) followed by 10, 20, and 40
additional samples respectively. Both prior belief and ﬁrst inference deciles of θ are shown in gray. As
observations accumulate, the domain of plausible θ values tightens.

The ﬁrst observation consists of 10 samples of y followed by inference of θ. Subsequent observations
each add another 10, 20, and 40 samples respectively. A typical inference sequence is shown in Figure 2.
Model information gained by inference from the ﬁrst observation in the same view is 5.72 bits. As
additional observations become available the model information provided by ﬁrst inference is eventually
reﬁned to 5.10 bits. Typically the region of plausible models θ resulting from each inference is consistent
with what was previously considered plausible.
By running 1 million independent experiments, we construct a histogram of the model information
provided by ﬁrst inference in subsequent views. This is shown in Figure 3. As a consequence of
Postulate 4, the model information provided by ﬁrst inference must always be positive before any
additional observations are made. The change in model covariance in this experiment provides a stronger
lower bound of 3.95 bits after ﬁrst inference, which can be seen in the ﬁrst view on the left. This bound is
saturated in the limit when the inferred mean is unchanged. Additional observations may indicate that the
ﬁrst inference was less informative than initially believed. We may regard the rare cases showing negative
information as being misinformed after ﬁrst inference. The true value of the model θ may be known to
arbitrary precision if we collect enough observations. This is the realization limit on the right. Under
this experimental design, this limit converges to the Laplace distribution centered at mutual information
L(µ, (log 2)−1 ) computed in the prior view.
From these million experiments, we can select the most unusual cases for which the information
provided by ﬁrst inference is later found to an extreme. Figure 4 visualizes the experiment for which the

20 of 31

Figure 3. Histogram of ﬁrst inference information in observation sequence. The vertical line at 5.36 bits is
mutual information. Information is positive after ﬁrst inference, but may drop with additional observations.
The limiting view of inﬁnite samples (realization) is shown on the right.

Figure 4. Minimum ﬁrst inference information out of 1 million independent experiments. This particularly
rare case shows how ﬁrst samples can mislead inference, which is later corrected by additional observations.
The fourth inference (right) bears remarkably little overlap with the ﬁrst.

information provided by ﬁrst inference is found to be the minimum after observing 160 total samples from
the generative process. Although model information assessed following the ﬁrst observation is a fairly
typical value, additional samples quickly show that the ﬁrst samples were unusual. This becomes highly
apparent in the fourth view, which includes 80 samples in total.
Figure 5 visualizes the complementary case in which we select the experiment for which the
information provided by ﬁrst inference is later found to be the maximum. The explantory characteristic
of this experiment is the rare value that the true model has taken. High information in inference shows
a high degree of surprise from what the prior distribution deemed plausible. Each inference indicates a
range of plausible values of θ that is quite distant from the plausible region indicated by prior belief. The
change in belief due to ﬁrst inference is conﬁrmed by additional data in fourth inference.
Finally, we examine a scenario in which the ﬁrst 10 samples are generated from a different process
than subsequent samples. We proceed with inference as before and assume a single generative process,
despite the fact that this assumption is actually false. Figure 6 shows the resulting inference sequence. After
ﬁrst inference, nothing appears unusual because there is no data that would contradict inferred belief. As
soon as additional data become available, however, information in ﬁrst inference becomes conspicuously
negative. Note that the one-in-a-million genuine experiment exhibiting minimum information, Figure 4,
gives −11.53 bits after 70 additional samples. In contrast, this experiment yields −47.91 bits after only 10
additional samples.

21 of 31

Figure 5. Maximum ﬁrst inference information out of 1 million independent experiments. The true value
of θ has taken an extremely rare value. As evidence accumulates, plausible ranges of θ conﬁrm the ﬁrst
inference.

Figure 6. Inconsistent inference. The ﬁrst 10 samples are drawn from a different ground truth than
subsequent samples, but inference proceeds as usual. As additional data become available, ﬁrst inference
information becomes markedly negative.

By comparing this result to the information distribution in the realization limit, we see that the
probability of a genuine experiment exhibiting information this negative would be less than 2−155 . This
shows how highly negative information may ﬂag anomalous data. We explore this further in the next
section.

7.2. Negative information in MNIST model with mislabeled data

We also explore predictive label information in machine learning models by constructing a small
neural network to predict MNIST digits [32]. This model was trained with 50,000 images with genuine
labels. Training was halted using cross-validation from 10,000 images that also had genuine labels. To
investigate how predictive label information serves as an indicator of prediction accuracy, we randomly
mislabeled a fraction of unseen cases. Prediction information was observed on 10,000 images for which
50% had been randomly relabeled, which resulted in 5,521 original labels and 4,479 mismatched labels.
The resulting distribution of information outcomes is plotted in Figure 7, which shows a dramatic
difference between genuine labels and mislabeled cases. In all cases, prediction information is quantiﬁed
from the uninformed probabilities q0 (y = yi ) = 1/10 for all outcomes i ∈ [10] to model predictions, which
are conditioned on the image input q1 (y | x), in the view of the label r(y | yi ). Total label information, the
sum of predictive label information and residual label information, is

Ir(y | yi ) [ r(y | yi ) (cid:107) q0 (y) ] = Ir(y | yi ) [ q1 (y | x) (cid:107) q0 (y) ] + Ir(y | yi ) [ r(y | yi ) (cid:107) q1 (y | x) ] = log2 (10) bits

22 of 31

Figure 7. Histogram of information outcomes for genuine labels (left) and mislabeled (right) cases. Correct
label information is highly concentrated at 3.2 bits, which is 95.9% of the total information contained
in labels. Mislabeled cases have mean information at -18 bits and information is negative for 99.15% of
mislabeled cases.

Figure 8. Original MNIST labels. The top row shows lowest predictive label information among original
labels. Notably, the two leading images appear to be genuinely mislabeled in the original dataset.
Subsequent predictions are poor. The bottom row shows the highest information among original labels.
Labels and predictions are consistent in these cases.

Figure 9. Mislabeled digits. The top row shows the lowest predictive label information among mislabeled
cases. In each case, the claimed label is implausible and the prediction is correct. The bottom row shows
the highest prediction information among mislabeled cases. Although claimed labels are incorrect, most
images share identiﬁable features with the claim.

or roughly 3.32 bits for each case. Both Figure 8 and Figure 9 show different forms of anomaly detection
using negative information.
Figure 8 shows that genuine labels may exhibit negative information when predictions are poor. Only
1.1% of correct cases exhibit negative predictive label information. The distribution mean is 3.2 bits for

23 of 31

this set. Notably, the ﬁrst two images appear to be genuinely mislabeled in the original dataset, which
underscores the ability of this technique to detect anomalies.
In contrast, over 99.1% of mislabeled cases exhibit negative information with the distribution mean at
−18 bits. The top row of Figure 9 shows that information is most negative when the claimed label is not
plausible and model predictions clearly match the image. Similarly to 7.1, strongly negative information
indicates anomalous data. When incorrect predictions match incorrect labels, however, information can be
positive as shown in the bottom row. The cases appear to share identiﬁable features with the claim.
Negative information indicates that the prediction is farther from the label view than uninformed
predictions; we would be more likely to guess the claimed label without observing model predictions.
We remark that the claimed label is assumed to be correct. Negative information strictly implies that the
prediction is inconsistent with the view of expectation.

8. Conclusion

Just as belief matures with accumulation of evidence, we hold that the information associated with a
shift in belief must also mature. By formulating principles that articulate how we may regard information
as an expectation that measures change in belief, we derived a theory of information that places existing
measures of entropic information in a coherent uniﬁed framework. These measures include Shannon’s
original description of entropy, cross-entropy, realization information, Kullback-Leibler divergence, and
Lindley information (uncertainty difference) due to an experiment.
Moreover, we found other explainable information measures that may be adapted to speciﬁc scenarios
from ﬁrst principles including the log pointwise predictive measure of model accuracy. We derived useful
properties of information including the chain rule of conditional dependence, additivity over belief updates,
consistency with respect expected future observations, and expected information in future experiments
as mutual information. We also showed how this theory generalizes information-critical probability
distributions that are consistent with observed expectations analogous to that of Jaynes’. In the context
of Bayesian inference, we showed how information constraints recover and illuminate useful annealed
inference practices.
We also examined the phenomenon of negative information, which occurs when a more justiﬁed point
of view, based on a broader body of evidence, indicates that a previous change of belief was misleading.
Experiments demonstrated that negative information reveals anomalous cases of inference or anomalous
predictions in the context of machine learning.
The primary value of this theoretical framework is the consistent interpretation and corresponding
properties of information that guide how it may be assessed in a given context. The property of additivity
over belief updates within the present view allows us to partition information in a logically consistent
manner. For machine learning algorithms, we see that total information from the uninformed state
to a label-informed state is a constant that may be partitioned into the predicted component and the
residual component. This insight suggests new approaches to model training, which will be the subject of
continuing research.

8.1. Future work

The challenges we seek to address with this theory relate to real-world applications of inference and
machine learning. Although Bayesian inference provides a rigorous foundation for learning, poor choices
of prior or likelihood can lead to results that elude or contradict human intuition when analyzed after the
fact. This only becomes worse as the scale of learning problems increases, as in deep neural networks,
where human intuition cannot catch inconsistencies. Information provides a metric to quantify how well

24 of 31

a model is learning that may be useful when structuring learning problems. Some related challenges
include:

1. Controlling model complexity in machine learning to avoid memorization,
2. Evaluating the inﬂuence of different experiments and data points to identify outliers or poorly
supported inferences,
3. Understanding the impact of both model-structure and ﬁdelity of variational approximations on
learnability.

Funding: The Department of Homeland Security sponsored the production of this material under DOE Contract
Number DE-NA0003525 for the management and operation of Sandia National Laboratories. This work was also
funded in part by the Department of Energy Ofﬁce of Advanced Scientiﬁc Computing Research.
Acknowledgments: We would like to extend our earnest appreciation to Michael Bierma, Christopher Harrison,
Steven Holtzen, Philip Kegelmeyer, Jaideep Ray, and Jean-Paul Watson for helpful discussions on this topic.
Sandia National Laboratories is a multimission laboratory managed and operated by National Technology
and Engineering Solutions of Sandia, LLC., a wholly owned subsidiary of Honeywell International, Inc., for the
U.S. Department of Energy’s National Nuclear Security Administration under contract DE-NA-0003525. This paper
describes objective technical results and analysis. Any subjective views or opinions that might be expressed in the
paper do not necessarily represent the views of the U.S. Department of Energy or the United States Government.

Appendix A. Proof of principal result
Lemma A1. Let g(·) : R+ (cid:55)→ R be a function such that g(x1 x2 ) = g(x1 ) + g(x2 ) for all x1 , x2 > 0. It follows
that g(x) = a log(x) where a is a constant.

Proofs of Lemma A1 given by Erdös [33], Fadeev [24], Rényi [15].
for all r1 , q1 , p1 , r2 , q2 , p2 > 0. It follows that f (r, q, p) = log (cid:0)rγ qα pβ (cid:1).
Lemma A2. Let f (·, ·, ·) : R3
+ (cid:55)→ R be a function such that f (r1 r2 , q1 q2 , p1 p2 ) = f (r1 , q1 , p1 ) + f (r2 , q2 , p2 )
Proof of Lemma A2. We begin by deﬁning g1 (x) ≡ f (x−1 , x, x), g2 (y) ≡ f (y, y−1 , y), and g3 (z) ≡
f (z, z, z−1 ). It follows that g1 (x1 x2 ) = g(x1 ) + g(x2 ). From Lemma A1, we have g1 (x) = a log(x) for some
constant a. Similarly, g2 (y) = b log(y) and g3 (z) = c log(z) for constants b and c. We may now construct
rq and observe f (r, q, p) = f (x−1 yz, xy−1 z, xyz−1 ) =
positive quantities x =
f (x−1 , x, x) + f (y, y−1 , y) + f (z, z, z−1 ) = g1 (x) + g2 (y) + g1 (z). The desired result follows by identifying
q p, y =
pr, and z =
constants α = (c + a)/2, β = (a + b)/2, and γ = (b + c)/2

√

√

√

Proof of Theorem 1. We proceed by combining Postulate 1 with Postulate 2, which gives

dz dw r(z)r(w) f (r(z)r(w), q1 (z)q1 (w), q0 (z)q0 (w))

Ir(z)r(w) [ q1 (z)q1 (w) (cid:107) q0 (z)q0 (w) ]
=
= Ir(z) [ q1 (z) (cid:107) q0 (z) ] + Ir(w) [ q1 (w) (cid:107) q0 (w) ]
=

(cid:90)

(cid:90)
(cid:90)
(cid:90)

dz r(z) f (r(z), q1 (z), q0 (z)) +
dw r(w) f (r(w), q1 (w), q0 (w))
The last line follows by multiplying each term in the previous line by (cid:82) dw r(w) = 1 and (cid:82) dz r(z) = 1,
dz dw r(z)r(w) [ f (r(z), q1 (z), q0 (z)) + f (r(w), q1 (w), q0 (w))] .
respectively. Since this must hold for arbitrary r(z)r(w), this implies

=

f (r(z)r(w), q1 (z)q1 (w), q0 (z)q0 (w)) = f (r(z), q1 (z), q0 (z)) + f (r(w), q1 (w), q0 (w)) .

25 of 31

(cid:90)

(cid:90)

(cid:16)

r(z)γ q0 (z)α+β (cid:17)

By Lemma A2, we have f (r(z), q1 (z), q0 (z)) = log (cid:0)r(z)γ q1 (z)α q0 (z)β (cid:1). From Postulate 3, we require
dz r(z) log
dz r(z) log q0 (z) = 0.
Since this must hold for arbitrary r(z) and q0 (z), this implies γ = 0 and β = −α. Thus we see that
Ir(z) [ r(z) (cid:107) q0 (z) ] = αDKL [ r(z) (cid:107) q0 (z) ], which is a constant α times the Kullback-Leibler divergence [2].
(cid:18) r(z)
(cid:18) q0 (z)
Jensen’s inequality easily shows nonnegativity of the form
dz r(z) log
r(z)
= − log(1) = 0.

dz r(z) log
≥ − log

dz r(z) log r(z) + (α + β)

q0 (z)
q0 (z)

(cid:18)(cid:90)

dz r(z)

= −

(cid:19)

(cid:19)

(cid:19)

= γ

(cid:90)

(cid:90)

(cid:90)

r(z)

It follows from Postulate 4 that α > 0. As Shannon notes, the scale is arbitrary and simply deﬁnes the unit
of measure.

Appendix B. Corollary proofs

Proof of Corollary 1. We unpack Theorem 1 and write joint distributions as the marginalization times the
corresponding conditional distribution such as r(z1 , z2 ) ≡ r(z2 | z1 )r(z1 ). This gives
(cid:18) q1 (z1 , z2 )
dz1 dz2 r(z1 , z2 ) log
dz2 r(z2 | z1 ) log

Ir(z1 ,z2 ) [ q1 (z1 , z2 ) (cid:107) q0 (z1 , z2 ) ]
=

(cid:18) q1 (z1 )q1 (z2 | z1 )
q0 (z1 , z2 )
q0 (z1 )q0 (z2 | z1 )

dz1 r(z1 )

(cid:19)

(cid:19)

(cid:90)

=

(cid:90)
(cid:90)
(cid:90)

(cid:90)

=

dz1 r(z1 ) log

(cid:19)

(cid:18) q1 (z1 )
q0 (z1 )

(cid:90)

(cid:18) q1 (z2 | z1 )

(cid:19)

dz2 r(z2 | z1 ) log

dz1 r(z1 )

+
= Ir(z1 ) [ q1 (z1 ) (cid:107) q0 (z1 ) ] + Er(z1 )

q0 (z2 | z1 )
Ir(z2 | z1 ) [ q1 (z2 | z1 ) (cid:107) q0 (z2 | z1 ) ] .

Proof of Corollary 2. Again, we simply unpack Theorem 1 and apply the product property of the
logarithm as
(cid:18) q2 (z)
(cid:18) q2 (z)

(cid:18) q2 (z)q1 (z)
q0 (z)
q1 (z)q0 (z)

Ir(z) [ q2 (z) (cid:107) q0 (z) ] =

(cid:18) q1 (z)

dz r(z) log

dz r(z) log

(cid:90)
(cid:90)
(cid:90)

(cid:19)

(cid:19)

(cid:19)

(cid:19)

(cid:90)

=

dz r(z) log

=
+
q1 (z)
q0 (z)
= Ir(z) [ q2 (z) (cid:107) q1 (z) ] + Ir(z) [ q1 (z) (cid:107) q0 (z) ] .

dz r(z) log

Proof of Corollary 3. Swapping q0 (z) and q1 (z) reciprocates the argument of the logarithm in Theorem 1,
which gives the negative of the original ordering.

Proof of Corollary 4. After realization ˇz = z j , probability is distributed as r(z = zi | z j ) = δi j . Restricting
support to z = ˇz gives

(cid:18)

(cid:19)

Ir(z | ˇz) [ r(z | ˇz) (cid:107) q(z) ] = log

1
q(z = ˇz)

= D[ 1 (cid:107) q(z = ˇz) ] .

26 of 31

Proof of Corollary 5. Computing the expectation value as given easily reconstructs the standard
formulation of cross entropy

Sr(z) [ q(z) ] = Er( ˇz)

Ir(z | ˇz) [ r(z | ˇz) (cid:107) q(z) ]

(cid:90)

(cid:18) 1

(cid:19)

dz r(z) log

=
= Ir(z) [ 1 (cid:107) q(z) ] .

q(z)

Proof of Corollary 6. This follows by simply replacing r(z) with q(z) in cross entropy.
w recovers present belief (cid:82) dw p(z | w)p(w) = p(z). It follows
Proof of Corollary 7. Plausible joint values of z and w are p(z, w) = p(z | w)p(w). Marginalizing over
(cid:18) q1 (z)
dw p(w)

Ip(z | w) [ q1 (z) (cid:107) q0 (z) ] =

Ep(w)

(cid:19)

(cid:90)

(cid:90)
(cid:90)

(cid:18) q1 (z)
dz p(z | w) log

(cid:19)

q0 (z)

dz p(z) log

=
q0 (z)
= Ip(z) [ q1 (z) (cid:107) q0 (z) ] .

Proof of Corollary 8. We compute the expectation value stated and simply rewrite the product of the
marginalization and conditional distribution as the joint distribution p(z | w)p(w) ≡ p(z, w). This gives
(cid:18) p(z | w)p(w)
dz p(z | w) log
(cid:18) p(z, w)
p(w)p(z)
p(z)p(w)

Ip(z | w) [ p(z | w) (cid:107) p(z) ]

dw dz p(z, w) log

dw p(w)

Ep(w)

(cid:90)
(cid:90)

(cid:19)

(cid:19)

(cid:90)

=

=
= Ip(z,w) [ p(z, w) (cid:107) p(z)p(w) ] .

Proof of Corollary 9. The limit of increasing precision yields the Dirac delta function p(z | ˇz) ≡ δ(z − ˇz).
(cid:18) q1 (z)
(cid:18) q1 (z = ˇz)
It follows
q0 (z = ˇz)

Ip(z | ˇz) [ q1 (z) (cid:107) q0 (z) ] =

dz δ(z − ˇz) log

q0 (z)

= log

(cid:19)

(cid:19)

(cid:90)

.

27 of 31

variations η(z) must maintain normalization (cid:82) dz η(z) = 0 and are otherwise arbitrary. Taking the Gâteaux
Proof of Corollary 10. We consider differential variations at the optimizer q1 (z) = q∗(z) + εη(z) where
derivative (with respect to the differential element ε) and applying the variational principle gives
(cid:20) p(z | x)
q∗(z)

dz η(z)

(cid:21)

0 =

(cid:90)

.

To satisfy the normalization constraint for otherwise arbitrary η(z), the term in brackets must be constant.
The stated result immediately follows.
Proof of Corollary 11. Let measurable disjoint subsets of outcomes be Ω> = {z | r(z) > q1 (z) > 0} and
Ω< = {z | r(z) < q1 (z)}. If η(z) drives belief toward r(z) on all measurable subsets then η(z) ≥ 0
for z almost everywhere in Ω> . Likewise, η(z) ≤ 0 almost everywhere in Ω< . Finally, η(z) = 0
(cid:82) dz η(z) = 0. If information is ﬁnite, then we may express r(z) = q1 (z)(1 + δ(z)) almost everywhere
almost everywhere on the complement Ωz \ (Ω> ∪ Ω< ). In order to retain normalization, we note that
(except an immeasurable subset that is not contained in Ω> ∪ Ω< for which we could have q1 (z) = 0 and
r(z) > 0) and observe that δ(z) > 0 for z ∈ Ω> just as δ(z) < 0 for z ∈ Ω< . Since η(z) has the same sign
as δ(z) almost everywhere, it follows

lim

ε→0

∂
∂ε

= lim

ε→0

(cid:90)

Ir(z) [ q1 (z) + εη(z) (cid:107) q0 (z) ]
(cid:18) q1 (z) + εη(z)
∂
q0 (z)
∂ε

dz r(z) log

(cid:19)

(cid:90)
(cid:90)
(cid:90)

=

=

=

> 0.

dz r(z)

η(z)
q1 (z)
dz (1 + δ(z)) η(z)

dz δ(z)η(z)

(cid:32)

(cid:90)

(cid:19)

L[ r(z), λ ] =

Proof of Corollary 12. We proceed by constructing the Lagrangian
(cid:18) r(z)
dz r(z)
log
.
maintain normalization (cid:82) dz η(z) = 0 and are otherwise arbitrary. Taking the Gâteaux derivative and
We consider differential variations at the optimizer r(z) = r∗(z) + εη(z) where variations η(z) must
applying the variational principle gives
(cid:18) r∗(z)

λi ( f i (z) − ϕi )

− n∑

q0 (z)

(cid:19)

(cid:34)

(cid:35)

(cid:90)

i=1

(cid:33)

0 =

dz η(z)

log

+ 1 − n∑

λi ( f i (z) − ϕi )

.

q0 (z)

i=1

To satisfy the normalization constraint for otherwise arbitrary η(z), the variational principle requires the
term in brackets to be constant. The stated result immediately follows.

28 of 31

Proof of Corollary 13. We unpack Theorem 1 and apply the local conditionality property to write
p(θ1 , θ2 | ˇy) ≡ p(θ2 | θ1 )p(θ1 | ˇy). This gives
(cid:18) p(θ1 , θ2 | ˇy)
dθ1 dθ2 p(θ1 , θ2 | ˇy) log
(cid:18) p(θ1 | ˇy)
dθ1 p(θ1 | ˇy) log
dθ1 p(θ1 | ˇy) log

Ip(θ1 ,θ2 | ˇy) [ p(θ1 , θ2 | ˇy) (cid:107) p(θ1 , θ2 ) ]
=

(cid:18) p(θ2 | θ1 )p(θ1 | ˇy)
p(θ1 , θ2 )
p(θ2 | θ1 )p(θ1 )

dθ2 p(θ2 | θ1 )

(cid:90)
(cid:90)
(cid:90)

(cid:19)

(cid:19)

(cid:19)

(cid:90)

=

=
p(θ1 )
= Ip(θ1 | ˇy) [ p(θ1 | ˇy) (cid:107) p(θ1 ) ] .

(cid:82) dθ1 p(θ2 | θ1 )p(θ1 | ˇy). Then we apply Jensen’s inequality followed by Bayes’ Theorem to obtain
Proof of Corollary 14. As a consequence of local conditional dependence, we observe that p(θ2 | ˇy) =
(cid:18) p(θ2 | ˇy)
log
p(θ2 | ˇy)

Ip(θ2 | ˇy) [ p(θ2 | ˇy) (cid:107) p(θ2 ) ]
dθ1 p(θ2 | θ1 )p(θ1 | ˇy)
=
dθ2
dθ2 p(θ2 | θ1 )
p(θ2 )
p(θ1 | θ2 )p(θ2 | ˇy)
p(θ1 )

dθ1 p(θ1 | ˇy) log
dθ1 p(θ1 | ˇy) log

(cid:18)(cid:90)
(cid:18)(cid:90)

(cid:19)
(cid:19)

(cid:20)(cid:90)

p(θ2 )

(cid:19)

(cid:21)

≤

=

(cid:90)
(cid:90)
(cid:90)
(cid:90)

=

dθ1 p(θ1 | ˇy)

log

+ log

dθ2

p(θ1 | θ2 )p(θ2 | ˇy)

p(θ1 | ˇy)

(cid:20)

dθ2

(cid:18) p(θ1 | ˇy)

(cid:19)

p(θ1 )

(cid:18)(cid:90)

The ﬁrst term provides the upper bound we seek. It remains to show that the second term is bound from
above by zero, which follows from a second application of Jensen’s inequality

Ip(θ2 | ˇy) [ p(θ2 | ˇy) (cid:107) p(θ2 ) ]
≤ Ip(θ1 | ˇy) [ p(θ1 | ˇy) (cid:107) p(θ1 ) ] + log
= Ip(θ1 | ˇy) [ p(θ1 | ˇy) (cid:107) p(θ1 ) ] .

(cid:18)(cid:90)

dθ1 dθ2 p(θ1 | θ2 )p(θ2 | ˇy)

(cid:19)(cid:21)

.

(cid:19)

29 of 31

Proof of Corollary 15. The denominator of the ﬁrst log argument is model evidence and we apply Bayes’
Theorem to the second log argument. Denominators of log arguments cancel and Jensen’s inequality
implies that the ﬁrst term must be greater than the second.
(cid:18)(cid:82) dθ p(y | θ)p(θ | ˇy)
(cid:18) p(θ | ˇy)
dy δ(y − ˇy) log
(cid:18)(cid:82) dθ p( ˇy | θ)p(θ | ˇy)
dθ p(θ | ˇy) log
(cid:18) p( ˇy | θ)
dθ p(θ | ˇy) log
p( ˇy)
p( ˇy)
dθ p( ˇy | θ)p(θ | ˇy)
dθ p(θ | ˇy) log(p( ˇy | θ)) ≥ 0.

Ir(y | ˇy) [ q(y | ˇy) (cid:107) p(y) ] − Ip(θ | ˇy) [ p(θ | ˇy) (cid:107) p(θ) ]
(cid:82) dθ p(y | θ)p(θ)

(cid:19)
(cid:19)

(cid:18)(cid:90)

(cid:90)
(cid:90)

= log

= log

(cid:19)

(cid:19)

(cid:19)

p(θ)

(cid:90)

−

−

(cid:90)

−

=

Proof of Corollary 16. The stated result immediately follows by taking f i (θ) ≡ log
Corollary 12.
Proof of Corollary 17. We take n = 1, q0 (θ) ≡ p(θ), and q1 (θ) ≡ p(θ | ˇy) and apply Corollary 16 with
Bayes’ theorem to obtain
(cid:18) p(θ | ˇy)
(cid:18) p( ˇy | θ)
p( ˇy)

r(θ) ∝ p(θ)

and applying

(cid:19)λ

(cid:19)λ

= p(θ)

p(θ)

.

(cid:17)

(cid:16) qi (θ)
q0 (θ)

Appendix C. Information computations in experiment

p(y( j) | θ) ≡ N (y( j) | θ, Σ).

Appendix C.1. Inference
Prior belief is p(θ) ≡ N (θ | 0, A) and p(x( j) ) ≡ N (x( j) | 0, Σ). Since y( j) = θ + x( j) , we have
If we let n samples have an average ¯y, it easily follows that p( ¯y | θ) ≡
N ( ¯y | θ, 1
Σ). Bayes’ rule gives p(θ | ¯y) ∝ p( ¯y | θ)p(θ) or
2 θT A−1 θ − n
2

p(θ | ¯y) ∝ exp

(cid:21)

n

(θ − ¯y)T Σ−1 (θ − ¯y)
(θ − µ)T B−1 (θ − µ)

(cid:21)

(cid:20) −1
(cid:20) −1

2

∝ exp

where B−1 = A−1 + nΣ−1 and µ = nBΣ−1 ¯y. Normalization yields p(θ | ¯y) ≡ N (θ | µ, B).

30 of 31

n

(cid:90)

Appendix C.2. Mutual information
(cid:82) dθ p(θ)p( ¯y | θ). This gives p( ¯y) ≡ N ( ¯y | 0, A + 1
We marginalize over plausible θ to obtain corresponding probability of observing ¯y as p( ¯y) =
(cid:18) p( ¯y, θ)
Σ). Mutual information, which is the expected
information gained by observing ¯y according to present belief, is computed
d ¯y dθ p( ¯y, θ) log
(cid:18) p( ¯y | θ)
d ¯y p( ¯y | θ) log
p( ¯y)
(cid:12)(cid:12)(cid:12)2π ( A + 1

2 ( ¯y − θ)T Σ−1 ( ¯y − θ)
2 ¯yT ( A + 1

(cid:16) −n
(cid:12)(cid:12)(cid:12)−1/2

(cid:19)


d ¯y p( ¯y | θ) log

(cid:12)(cid:12)(cid:12)−1/2

(cid:12)(cid:12)(cid:12)2π 1

(cid:16) −1

(cid:17)
(cid:17)



p( ¯y)p(θ)

Σ)−1 ¯y

dθ p(θ)

dθ p(θ)

(cid:90)
(cid:90)

(cid:90)
(cid:90)

(cid:19)

exp

exp

Σ)

=

=

Σ

n

n

n

(cid:16)

(cid:17)

=

1
2

log det

nΣ−1 A + I

Appendix C.3. First inference information in a subsequent view
Let the state of belief before an experiment be p(θ) ≡ N (θ | 0, A). After observing ¯y(1) inference gives
p(θ | ¯y(1) ) ≡ N (θ | µ, B). Additional observations ¯y(2) yield p(θ | ¯y(1) , ¯y(2) ) ≡ N (θ | ν, C ). Information
(cid:13)(cid:13)(cid:13) p(θ)
gained in the ﬁrst observation in the view of inference following the second observation is computed
 |2π B|−1/2 exp
|2π A|−1/2 exp
( A−1 − B−1 )C
+ tr

2 (θ − µ)T B−1 (θ − µ)
2 θT A−1 θ
+ νT A−1 ν − (ν − µ)T B−1 (ν − µ)

dθ N (θ | ν, C ) log
AB−1 (cid:17)

p(θ | ¯y(1) )

(cid:16) −1

(cid:16) −1

p(θ | ¯y(1) , ¯y(2) )



log det

(cid:17)

(cid:17)

(cid:16)

(cid:16)

(cid:16)

(cid:17)

(cid:17)

(cid:104)

(cid:105)

(cid:90)

=

=

I

.

1
2

References

1.

2.

3.
4.

5.
6.
7.

8.

9.

10.

Shannon, C.E. A Mathematical Theory of Communication. Bell System Technical Journal 1948, 27, 379–423.
doi:10.1002/j.1538-7305.1948.tb01338.x.
Kullback, S.; Leibler, R.A. On Information and Sufﬁciency. The Annals of Mathematical Statistics 1951, 22, 79–86.
doi:10.1214/aoms/1177729694.
Kullback, S. Information theory and statistics; Courier Corporation, 1997.
Lindley, D.V. On a measure of the information provided by an experiment. The Annals of Mathematical Statistics
1956, 27, 986–1005.
Cox, R.T. Probability, frequency and reasonable expectation. American journal of physics 1946, 14, 1–13.
Jaynes, E.T. Probability theory: The logic of science; Cambridge university press, 2003.
Sooﬁ, E.S. Capturing the intangible concept of information. Journal of the American Statistical Association 1994,
89, 1243–1254.
Sooﬁ, E.S. Principal information theoretic approaches.
95, 1349–1353.
Ebrahimi, N.; Sooﬁ, E.S.; Zahedi, H. Information properties of order statistics and spacings. IEEE Transactions
on Information Theory 2004, 50, 177–183.
Ebrahimi, N.; Sooﬁ, E.S.; Soyer, R. Information measures in perspective. International Statistical Review 2010,
78, 383–412.

Journal of the American Statistical Association 2000,

31 of 31

11.

25.

26.
27.

28.

16.

17.

14.

15.

18.
19.
20.
21.
22.
23.
24.

Gelman, A.; Hwang, J.; Vehtari, A. Understanding predictive information criteria for Bayesian models. Statistics
and Computing 2013, 24, 997–1016. doi:10.1007/s11222-013-9416-2.
12. MacKay, D.J.C. Information Theory, Inference and Learning Algorithms; Cambridge University Press, 2003.
13.
Tishby, N.; Zaslavsky, N. Deep learning and the information bottleneck principle. 2015 IEEE Information
Theory Workshop (ITW). IEEE, 2015, pp. 1–5.
Barnard, G. The theory of information. Journal of the Royal Statistical Society. Series B (Methodological) 1951,
13, 46–64.
Rényi, A. On Measures of Entropy and Information. Proceedings of the Fourth Berkeley Symposium on
Mathematical Statistics and Probability, Volume 1: Contributions to the Theory of Statistics; University of
California Press: Berkeley, Calif., 1961; pp. 547–561.
Csiszár, I. Information-type measures of difference of probability distributions and indirect observation. Studia
Scientiarum Mathematicarum Hungarica 1967, 2, 229–318.
LaPlace, P.S. Mémoire sur la probabilité des causes par les événements. Mém. de math. et phys. présentés à l’Acad.
roy. des sci 1774, 6, 621–656.
Jeffreys, H. The theory of probability; OUP Oxford, 1998.
Bell, J.S. On the Einstein Podolsky Rosen paradox. Physics Physique Fizika 1964, 1, 195.
Gibbs, J.W. On the equilibrium of heterogeneous substances. American Journal of Science 1878, pp. 441–458.
Bernoulli, J. Ars conjectandi; Impensis Thurnisiorum, fratrum, 1713.
Jaynes, E.T. Information theory and statistical mechanics. Physical review 1957, 106, 620.
Bernardo, J.M. Expected information as expected utility. The Annals of Statistics 1979, pp. 686–690.
Fadeev, D.
Zum Begriff der Entropie einer endlichen Wahrscheinlichkeitsschemas.
Arbeiten zur
Informationstheorie I. Deutscher Verlag der Wissenschaften 1957, pp. 85–90.
Lindley, D.V. The Bayesian Approach to Statistics. Technical report, University of California, Berkeley,
Operations Research Center, 1980.
Nikodym, O. Sur une généralisation des intégrales de MJ Radon. Fundamenta Mathematicae 1930, 15, 131–179.
Fisher, R.A. Theory of statistical estimation. Mathematical Proceedings of the Cambridge Philosophical Society.
Cambridge University Press, 1925, Vol. 22, pp. 700–725.
Akaike, H. A new look at the statistical model identiﬁcation. IEEE Transactions on Automatic Control 1974,
19, 716–723. doi:10.1109/tac.1974.1100705.
Schwarz, G.
Estimating the Dimension of a Model.
doi:10.1214/aos/1176344136.
Burnham, K.P.; Anderson, D.R. Kullback-Leibler information as a basis for strong inference in ecological
studies. Wildlife Research 2001, 28, 111. doi:10.1071/wr99107.
Pearl, J. Probabilistic reasoning in intelligent systems: networks of plausible inference; Elsevier, 2014.
LeCun, Y.; Cortes, C.; Burges, C. MNIST handwritten digit database. AT&T Labs, 2010.
Erdös, P. On the Distribution Function of Additive Functions.
The Annals of Mathematics 1946, 47, 1.
doi:10.2307/1969031.

The Annals of Statistics 1978, 6, 461–464.

29.

30.

31.
32.
33.

