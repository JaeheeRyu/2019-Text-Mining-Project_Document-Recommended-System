9
1
0
2

v
o

N

1
2

]

G

L

.

s

c

[

2
v
8
1
1
3
0

.

9
0
9
1

:

v

i

X

r

a

TRAD ING -O FF S TAT IC AND DYNAM IC R EGR ET IN ON L IN E
L EA S T-SQUAR E S AND B EYOND

A PR E PR IN T

Jianjun Yuan

Andrew Lamperski

Department of Electrical and Computer Engineering
University of Minnesota
Minneapolis, MN, 55455

Department of Electrical and Computer Engineering
University of Minnesota
Minneapolis, MN, 55455

yuanx270@umn.edu

alampers@umn.edu

November 22, 2019

AB STRACT

Recursive least-squares algorithms often use forgetting factors as a heuristic to adapt to non-
stationary data streams. The ﬁrst contribution of this paper rigorously characterizes the effect of
forgetting factors for a class of online Newton algorithms. For exp-concave and strongly convex
objectives, the algorithms achieve the dynamic regret of max{O(log T ), O(
T V )}, where V is a
bound on the path length of the comparison sequence. In particular, we show how classic recursive
least-squares with a forgetting factor achieves this dynamic regret bound. By varying V , we ob-
tain a trade-off between static and dynamic regret. In order to obtain more computationally efﬁcient
algorithms, our second contribution is a novel gradient descent step size rule for strongly convex
functions. Our gradient descent rule recovers the order optimal dynamic regret bounds described
above. For smooth problems, we can also obtain static regret of O(T 1−β ) and dynamic regret of
O(T β V ∗ ), where β ∈ (0, 1) and V ∗ is the path length of the sequence of minimizers. By varying β ,
we obtain a trade-off between static and dynamic regret.

√

1

Introduction

Online learning algorithms are designed to solve prediction and learning problems for streaming data or batch data
whose volume is too large to be processed all at once. Applications include online auctions [1], online classiﬁcation
and regression [2], online subspace identiﬁcation [3], as well as online resource allocation [4].
The general procedure for online learning algorithms is as follows: at each time t, before the true time-dependent
objective function ft (θ) is revealed, we need to make the prediction, θt , based on the history of the observations fi (θ),
i < t. Then the value of ft (θt ) is the loss suffered due to the lack of the knowledge for the true objective function
ft (θ). Our prediction of θ is then updated to include the information of ft (θ). This whole process is repeated until
termination. The functions, ft (θ), can be chosen from a function class in an arbitrary, possibly adversarial manner.
The performance of an online learning algorithm is typically assessed using various notions of regret. Static regret,
Rs , measures the difference between the algorithm’s cumulative loss and the cumulative loss of the best ﬁxed decision
in hindsight [5]:

T(cid:88)

t=1

T(cid:88)

t=1

Rs =

ft (θt ) − min

θ∈S

ft (θ),

where S is a constraint set. For convex functions, variations of gradient descent achieve static regret of O(
T ), while
for strongly convex functions these can be improved to O(log T ) [6]. However, when the underlying environment is
changing, due to the ﬁxed comparator the algorithms converge to, static regret is no longer appropriate.

√

 
 
 
 
 
 
A PR E PR IN T - NOV EMB ER 22 , 2019

In order to better track the changes of the underlying environment, dynamic regret is proposed to compare the cumu-
lative loss against that incurred by a comparison sequence, z1 , . . . , zT ∈ S :

T(cid:88)

ft (θt ) − T(cid:88)

t=1

t=1

Rd =

ft (zt )

√

The classic work on online gradient descent [7] achieves dynamic regret of the order O(
T (1 + V )), where V is a
bound on the path length of the comparison sequence:
This has been improved to O((cid:112)T (1 + V )) in [8] by applying a meta-optimization over step sizes.
In works such as [9, 10], it is assumed that zt = θ∗
t = argminθ∈S ft (θ). We denote that particular version of dynamic
regret by:

(cid:107)zt − zt−1 (cid:107) ≤ V .

T(cid:88)

t=2

R∗

d =

ft (θt ) − T(cid:88)

t=1

T(cid:88)
T(cid:88)

t=1

(cid:13)(cid:13)θ∗

ft (θ∗
t )

(cid:13)(cid:13) ,

In particular, if V ∗ is the corresponding path length:

t=2

t−1

V ∗ =

t − θ∗
(1)
then [9] shows that for strongly convex functions, R∗
d of order O(V ∗ ) is obtained by gradient descent. However, as
pointed out by [8], V ∗ metric is too pessimistic and unsuitable for stationary problems, which will result in poor gen-
eralization due to the random perturbation caused by sampling from the same distribution. Thus, a trade-off between
static regret Rs and dynamic regret R∗
d is desired to maintain the abilities of both generalization to stationary problem
and tracking to the local changes.
Adaptive regret [11] is another metric when dealing with changing environment, which is deﬁned as the maximum
static regret over any contiguous time interval. Although it shares the similar goal as the dynamic regret, their relation-
ship is still an open question.
Closely related to the problem of online learning is adaptive ﬁltering, in which time series data is predicted using a
ﬁlter that is designed from past data [12]. The performance of adaptive ﬁlters is typically measured in an average case
setting under statistical assumptions. One of the most famous adaptive ﬁltering techniques is recursive least squares,
which bears strong resemblance to the online Newton method of [13]. The work in [13] proves a static regret bound of
O(log T ) for online Newton methods, but dynamic regret bounds are not known.
In order to have an algorithm that adapts to non-stationary data, it is common to use a forgetting factor. For the recursive
least squares, [14] analyzed the effect of the forgetting factor in terms of the tracking error covariance matrix, and [15]
made the tracking error analysis with the assumptions that the noise is sub-Gaussian and the parameter follows a
drifting model. However, none of the analysis mentioned is done in terms of the regret, which eliminates any noise
assumption. For the online learning, [16] analyzed the discounted UCB, which uses the discounted empirical average
as the estimate for the upper conﬁdence bound. [17] used the weighted least-squares to update the linear bandit’s
underlying parameter.
The contributions of this paper are:

√

max{O(log T ), O(

1. For exp-concave and strongly convex problems, we propose a discounted Online Newton algorithm
which generalizes recursive least squares with forgetting factors and the original online Newton method
of [13]. We show how tuning the forgetting factor can achieve a dynamic regret bound of Rd ≤
T V )}. This gives a rigorous analysis of forgetting factors in recursive least squares
and improves the bounds described in [8]. However, this choice requires a bound on the path length, V . For
an alternative choice of forgetting factors, which does not require path length knowledge, we can simultane-
ously bound static regret by Rs ≤ O(T 1−β ) and dynamic regret by Rd ≤ max{O(T 1−β ), O(T β V )}. Note
that tuning β produces a trade-off between static and dynamic regret.
2. Based on the analysis of discounted recursive least squares, we derive a novel step size rule for online gradient
descent. Using this step size rule for smooth, strongly convex functions we obtain a static regret bound of
Rs ≤ O(T 1−β ) and a dynamic regret bound against θt = argminθ∈S ft (θ) of R∗
improves the trade-off obtained in the exp-concave case, since static regret or dynamic regret can be made
small by appropriate choice of β ∈ (0, 1).

d ≤ O(T β (1 + V ∗ )). This

2

A PR E PR IN T - NOV EMB ER 22 , 2019

√

√

max{O(log T ), O(

max{O(log T ), O(

3. We show how the step size rule can be modiﬁed further so that gradient descent recovers the
T V )} dynamic regret bounds obtained by discounted Online Newton methods. How-
ever, as above, these bounds require knowledge of the bound on the path length, V .
4. Finally, we describe a meta-algorithm,
similar
to that used in [8], which can recover
the
T V )} dynamic regret bounds without knowledge of V . These bounds are tighter than
those in [8], since they exploit exp-concavity to reduce the loss incurred by running an experts algorithm. Fur-
thermore, we give a lower bound for the corresponding problems, which matches the obtained upper bound
for certain range of V .
Notation. For the n dimensional vector θ ∈ Rn , we use (cid:107)θ(cid:107) to denote the (cid:96)2 -norm. The gradient of the function ft
at time step t in terms of the θ is denoted as ∇ft (θ).
For the matrix A ∈ Rm×n , its transpose is denoted by A(cid:62) and A(cid:62)A denotes the matrix multiplication. The inverse
of A is denoted as A−1 . When m = n, we use (cid:107)A(cid:107)2 to represent the induced 2 norm of the square matrix. For the
two square matrix A ∈ Rn×n and B ∈ Rn×n , A (cid:22) B means A − B is negative semi-deﬁnite, while A (cid:23) B means
A − B is positive semi-deﬁnite. For a positive deﬁnite matrix, M , let (cid:107)x(cid:107)2
M = x(cid:62)M x. The standard inner product
between matrices is given by (cid:104)A, B (cid:105) = Tr(A(cid:62)B ). The determinant of a square matrix, A is denoted by |A|. We use I
to represent the identity matrix.

2 Discounted Online Newton Algorithm

As described above, the online Newton algorithm from [13] strongly resembles the classic recursive least squares
algorithm from adaptive ﬁltering [12]. Currently, only the static regret of the online Newton method is studied. To
obtain more adaptive performance, forgetting factors are often used in recursive least squares. However, the regret
of forgetting factor algorithms has not been analyzed. This section proposes a class of algorithms that encompasses
recursive least squares with forgetting factors and the online Newton algorithm. We show how dynamic regret bounds
for these methods can be obtained by tuning the forgetting factor.
First we describe the problem assumptions. Throughout the paper we assume that ft : S → R are convex, differen-
tiable functions, S is a compact convex set, (cid:107)x(cid:107) ≤ D for all x ∈ S , and (cid:107)∇ft (x)(cid:107) ≤ G for all x ∈ S . Without loss of
generality, we assume throughout the paper that D ≥ 1.
In this section we assume that all of the objective functions, ft : S → R are α-exp-concave for some α > 0. This
means that e−αft (θ) is concave.
If ft is twice differentiable, it can be shown that ft is α-exp-concave if and only if

∇2 ft (x) (cid:23) α∇ft (x)∇ft (x)(cid:62)

(2)

for all x ∈ S .
For an α-exp-concave function ft , Lemma 4.2 of [6] implies that for all ρ ≤ 1
holds for all x and y in S :

2 min{ 1

4GD , α}, the following bound

ft (y) ≥ ft (x) + ∇ft (x)(cid:62) (y − x) +

(x − y)(cid:62)∇ft (x)∇ft (x)(cid:62) (x − y).

ρ
2

(3a)

In some variations on the algorithm, we will require extra conditions on the function, ft . In particular, in one variation
we will require (cid:96)-strong convexity. which means that there is a number (cid:96) > 0 such that

(3b)
for all x and y in S . For twice-differentiable functions, strong convexity implies α-exp-concavity for α ≤ (cid:96)/G2 on S .
In another variant, we will require that the following bound holds for all x and y in S :

ft (y) ≥ ft (x) + ∇ft (x)(cid:62) (y − x) +

(cid:107)x − y(cid:107)2

(cid:96)
2

ft (y) ≥ ft (x) + ∇ft (x)(cid:62) (y − x) +

(cid:107)x − y(cid:107)2∇2 ft (x) .

1
2

(3c)

This bound does not correspond to a commonly used convexity class, but it does hold for the important special case of
2 (cid:107)yt − Atx(cid:107)2 . This fact will be important for analyzing the classic discounted recursive
quadratic functions: ft (x) = 1
least-squares algorithm. Note that if yt and At are restricted to compact sets, α can be chosen so that ft is α-exp-
concave.

3

A PR E PR IN T - NOV EMB ER 22 , 2019

Additionally, the algorithms for strongly convex functions and those satisfying (3c) will require that the gradients
∇ft (x) are u-Lipschitz for all x ∈ S (equivalently, ft (x) is u-smooth), which means the gradient ∇ft (x) satisﬁes the
relation

(cid:107)∇ft (x) − ∇ft (y)(cid:107) ≤ u (cid:107)x − y(cid:107) , ∀t.

This smoothness condition is equivalent to ft (y) ≤ ft (x) + ∇ft (x)T (y − x) + u
2 (cid:107)y − x(cid:107)2 and implies, in particular,

that ∇2 ft (x) (cid:22) uI .

Algorithm 1 Discounted Online Newton Step
Given constants  > 0, η > 0, and γ ∈ (0, 1).
Let θ1 ∈ S and P0 = I .
for t=1,. . . ,T do
Play θt and incur loss ft (θt )
Observe ∇t = ∇ft (θt ) and Ht = ∇2 ft (θt ) (if needed)
Update Pt :

Pt = γPt−1 + ∇t∇(cid:62)
Pt = γPt−1 + Ht

t

(cid:17)

(cid:16)

Update θt : θt+1 = ΠPtS

end for

θt − 1
η P −1

t ∇t

(Quasi-Newton)
(Full-Newton)

(4a)
(4b)

is

Pt

2 min{ 1

To accommodate these three different cases, we propose Algorithm 1, in which ΠPtS (y) = argminz∈S (cid:107)z − y(cid:107)2
the projection onto S with respect to the norm induced by Pt .
By using Algorithm 1, the following theorem can be obtained:
Theorem 1. Consider the following three cases of Algorithm 1:
1. ft is α-exp-concave. The algorithm uses η ≤ 1
4GD , α},  = 1 1 , and (4a).
2. ft is α-exp-concave and (cid:96)-strongly convex while ∇ft (x) is u-Lipschitz. The algorithm uses η ≤ (cid:96)/u,  = 1,
and (4b).
3. ft is α-exp-concave and satisfy (3c) while ∇ft (x) is u-Lipschitz. The algorithm uses η ≤ 1,  = 1, and (4b).
For each of these cases, there are positive constants a1 , . . . a4 such that
for all z1 , . . . , zT ∈ S such that (cid:80)T
Due to space limits, the proof is in the Appendix. Now we describe some consequences of the theorem.
Corollary 1. Setting γ = 1 − T −β with β ∈ (0, 1) leads to the following form:

t=1 (ft (θt ) − ft (zt )) ≤ −a1T log γ − a2 log(1 − γ ) + a3
1−γ V + a4
t=2 (cid:107)zt − zt−1 (cid:107) ≤ V .

(cid:80)T

(cid:80)T

t=1 (ft (θt ) − ft (zt )) ≤ O(T 1−β + β log T + T β V )

Proof. The ﬁrst term is bounded as:

−T log γ = −T log(1 − T −β ) ≤ T 1−β
1 − T −β = O(T 1−β ),

where the inequality follows from − log(1 − x) ≤ x
1−x for 0 ≤ x < 1.
The other terms follow by direct calculation.
This corollary guarantees that the static regret is bounded in the order of O(T 1−β ) since V = 0 in that case. The
dynamic regret is of order O(T 1−β + T β V ). By choosing β ∈ (0, 1), we are guaranteed that both the static and

1The value used here is only for proof simplicity, please see Meta-algorithm Section for more discussion.

4

A PR E PR IN T - NOV EMB ER 22 , 2019

dynamic regrets are both sublinear in T as long as V < O(T ). Also, small static regret can be obtained by setting β
near 1.
In the setting of Corollary 1, the algorithm parameters do not depend on the path length V . Thus, the bounds hold for
any path length, whether or not it is known a priori. The next corollary shows how tighter bounds could be obtained if
knowledge of V were exploited in choosing the discount factor, γ .

Corollary 2. Setting γ = 1 − 1

2

leads to the form:

(cid:113) max{V ,log2 T /T }
2DT

T(cid:88)

(ft (θt ) − ft (zt )) ≤ max{O(log T ), O(

√

T V )}

t=1

This matches the bounds obtained in [13]. For positive path lengths bounded by V , we improve the O((cid:112)T (1 + V ))
The proof is similar to the proof of Corollary 1.
Note that Corollary 2 implies that the discounted Newton method achieves logarithmic static regret by setting V = 0.
dynamic bounds from [8]. However, the algorithm above current requires knowing a bound on the path length, whereas
[8] achieves its bound without knowing the path length, a priori.
1 = z1 , . . . , zT can vary over S like in [18], and use this as a pre-ﬁxed
value to allow the comparator sequence to vary arbitrarily over the set of admissible comparator sequence {zT
If we view V as the variation budget that zT
(cid:107)zt − zt−1 (cid:107) ≤ V }, we can tune γ in terms of V .

1 ∈ S :

T(cid:80)

t=2

In order to bound the dynamic regret without knowing a bound on the path length, the method of [8] runs a collection
of gradient descent algorithms in parallel with different step sizes and then uses a meta-optimization [5] to weight
their solutions. In a later section, we will show how a related meta-optimization over the discount factor leads to
T V )} dynamic regret bounds for unknown V .
For the Algorithm 1, we need to invert Pt , which can be achieved in time O(n2 ) for the Quasi-Newton case in (4a) by
utilizing the matrix inversion lemma. However, for the Full-Newton step (4b), the inversion requires O(n3 ) time.

max{O(log T ), O(

√

3 From Forgetting Factors to a Step Size Rule

In the next few sections, we aim to derive gradient descent rules that achieve similar static and regret bounds to
the discounted Newton algorithm, without the cost of inverting matrices. We begin by analyzing the special case of
quadratic functions of the form:

(cid:107)θ − yt(cid:107)2 ,

1
2

ft (θ) =

(5)
where yt ∈ S . In this case, we will see that discounted recursive least squares can be interpreted as online gradient
descent with a special step size rule. We will show how this step size rule achieves a trade-off between static regret
and dynamic regret with the speciﬁc comparison sequence θ∗
t = yt = argminθ∈S ft (θ). For a related analysis of more
2 (cid:107)At θ − yt(cid:107)2 , please see the appendix.
general quadratic functions, ft (θ) = 1
1 ∈ S . The analysis
Note that the previous section focused on dynamic regret for arbitrary comparison sequences, zT
techniques in this and the next section are specialized to comparisons against θ∗
t = argminθ∈S ft (θ), as studied in
initial matrix P0 = 0. When ft is deﬁned as in (5), we have that Pt = (cid:80)t−1
works such as [9, 10].
Classic discounted recursive least squares corresponds to Algorithm 1 running with full Newton steps, η = 1, and
k=0 γ k I . Thus, the update rule can be
expressed in the following equivalent ways:

θ∈S

γ i−1 ft+1−i (θ)
θt+1 = argmin
γ − γ t
1 − γ
=
1 − γ t θt +
1 − γ t yt
= θt − P −1
t ∇ft (θt )
= θt − ηt∇ft (θt ),

t(cid:88)

i=1

5

(6a)

(6b)

(6c)
(6d)

A PR E PR IN T - NOV EMB ER 22 , 2019

where ηt = 1−γ
1−γ t . Note that since yt ∈ S , no projection steps are needed.
The above update is the ubiquitous gradient descent with a changing step size. The only difference from the standard
methods is the choice of ηt , which will lead to the useful trade-off between dynamic regret R∗
d and static regret to
maintain the abilities of both generalization to stationary problem and tracking to the local changes.
By using the above update, we can get the relationship between θt+1 − θ∗
t and θt − θ∗
t as the following result:
t = argminθS ft (θ) in Eq.(5). When using the discounted recursive least-squares update in Eq.(6),
we have the following relation:

Lemma 1. Let θ∗

θt+1 − θ∗
t =

γ − γ t
1 − γ t (θt − θ∗
t )

Proof. Since θ∗

t = argmin ft (θ) = yt , for θt+1 − θ∗
θt+1 − θ∗

t , we have:

t = θt+1 − yt
= γ−γ t
1−γ t θt + 1−γ
1−γ t yt − yt
= γ−γ t

1−γ t (θt − yt ) = γ−γ t
1−γ t (θt − θ∗
t )

Theorem 2. Let θ∗

Recall from (1) that the path length of optimizer sequence is denoted by V ∗ . With the help of Lemma 1, we can upper
bound the dynamic regret R∗
d in the next theorem:
t be the solution to ft (θ) in Eq.(5). When using the discounted recursive least-squares update in
1 (cid:107) + V ∗ (cid:1)
Eq.(6) with 1 − γ = 1/T β , β ∈ (0, 1), we can upper bound the dynamic regret as:
Proof. According to the Mean Value Theorem, there exists a vector x ∈ {v |v = δθt + (1 − δ)θ∗
t , δ ∈ [0, 1]} such that
t (cid:107). For our problem, (cid:107)∇ft (x)(cid:107) = (cid:107)x − yt(cid:107) ≤ (cid:107)x(cid:107) + (cid:107)yt(cid:107).
For (cid:107)x(cid:107), we have:

d ≤ 2DT β (cid:0) (cid:107)θ1 − θ∗
ft (θt ) − ft (θ∗
t ) = ∇ft (x)T (θt − θ∗
t ) ≤ (cid:107)∇ft (x)(cid:107) (cid:107)θt − θ∗
(cid:107)x(cid:107) = (cid:107)δθt + (1 − δ)θ∗
≤ δ (cid:107)θt(cid:107) + (1 − δ) (cid:107)yt(cid:107)

R∗

(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)(cid:13) t−1(cid:80)

i=1

t−1(cid:80)

i=1

γ i−1 yt−i

γ i−1

(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)(cid:13) + (1 − δ) (cid:107)yt(cid:107)
t (cid:107)

= δ

≤ D

T(cid:80)

ft (θ∗
t )

(cid:17) ≤ 2D

where the second inequality is due to (cid:107)yi (cid:107) ≤ D , ∀i.
As a result, the norm of the gradient can be upper bounded as (cid:107)∇ft (x)(cid:107) ≤ 2D . Then we have R∗
(cid:107)θt − θ∗
t (cid:107). Now we could instead upper bound
(cid:13)(cid:13)θt − θ∗
(cid:107)θt − θ∗
t (cid:107), which can be achieved as follows:
T −1(cid:80)
T −1(cid:80)

(cid:107)θt − θ∗

T(cid:80)

T(cid:80)

T(cid:80)

T(cid:80)

(cid:13)(cid:13)

d =

t=1

t=1

t=1

t=1

t=2

t=1

t

(cid:13)(cid:13)
T(cid:80)
(cid:13)(cid:13)θ∗
T(cid:80)
(cid:13)(cid:13)θ∗
T(cid:80)
(cid:13)(cid:13)θ∗

t−1 − θ∗
t − θ∗
t − θ∗
t − θ∗

t−1

t=2

t=2

t−1 + θ∗
(cid:107)θt+1 − θ∗
t (cid:107) +
1−γ t (cid:107)θt − θ∗
t (cid:107) +
1−γ t (cid:107)θt − θ∗
t (cid:107) +

γ−γ t

≤ (cid:107)θ1 − θ∗

t (cid:107) = (cid:107)θ1 − θ∗
1 (cid:107) +
1 (cid:107) +
= (cid:107)θ1 − θ∗
1 (cid:107) +
1 (cid:107) +

(cid:13)(cid:13)
(cid:13)(cid:13)

t−1

t−1

T(cid:80)

t=1

(cid:16)

ft (θt ) −

≤ (cid:107)θ1 − θ∗
where in the second equality, we substitute the result from Lemma 1.
From the above inequality, we get
(cid:17) (cid:107)θt − θ∗
t (cid:107) ≤ (cid:107)θ1 − θ∗

T(cid:88)

(cid:16)

γ−γ t

t=1

1 − γ − γ t
1 − γ t

t=1

6

t=2

(cid:13)(cid:13)θ∗

t − θ∗

t−1

(cid:13)(cid:13)

T(cid:88)

t=2

1 (cid:107) +

A PR E PR IN T - NOV EMB ER 22 , 2019

(cid:16)

Since

1 − γ−γ t

1−γ t

(cid:17)

Thus, Rd ≤ 2D

T(cid:80)

t=1

= 1−γ

1−γ t ≥ 1 − γ , we get
(cid:107)θt − θ∗

T(cid:80)

t=1

t (cid:107) ≤ 1

1−γ (cid:107)θ1 − θ∗
1 (cid:107) + 1
= T β ((cid:107)θ1 − θ∗
1 (cid:107) +

T(cid:80)

1−γ

t−1

t − θ∗
t − θ∗

(cid:13)(cid:13))

t−1

(cid:13)(cid:13)

(cid:13)(cid:13)θ∗

t=2

T(cid:80)
(cid:13)(cid:13)θ∗
(cid:13)(cid:13)).

(cid:107)θt − θ∗

t (cid:107) ≤ 2DT β ((cid:107)θ1 − θ∗
1 (cid:107) +

(cid:13)(cid:13)θ∗

T(cid:80)

t=2

t=2

t − θ∗

t−1

Theorem 2 shows that if we choose the discounted factor γ = 1 − T −β we obtain a dynamic regret of O(T β (1 + V ∗ )).
This is a reﬁnement of the Corollary 1 since the bound no longer has the T 1−β term. Thus, the dynamic regret can be
made small by choosing a small β .
In the next theorem, we will show that this carefully chosen γ can also lead to useful static regret, which can give us a
trade-off and solve the dilemma of generalization for stationary problems versus the tracking for local changes.
Theorem 3. Let θ∗ be the solution to min
ft (θ). When using the discounted recursive least-squares update in
Eq.(6) with 1 − γ = 1/T β , β ∈ (0, 1), we can upper bound the static regret as:

T(cid:80)

t=1

Rs ≤ O(T 1−β )

Recall that the algorithm of this section can be interpreted both as a discounted recursive least squares method, and as
a gradient descent method. As a result, this theorem is actually a direct consequence of Corollary 1, by setting V = 0.
However, we will give a separate proof in the appendix, since the techniques extend naturally to the analysis of more
general work on gradient descent methods of the next section.
Our Theorems 2 and 3 build a trade-off between dynamic and static regret by the carefully chosen discounted factor
γ . Compared with the result from the last section, there are two improvements: 1. The two regrets are decoupled so
that we could reduce the β to make the dynamic regret R∗
d result smaller than bound from Corollary 1; 2. The update
is the ﬁrst-order gradient descent, which is computationally more efﬁcient than second order methods.
In the next section, we will consider the strongly convex and smooth case, whose result is inspired by this section’s
analysis.

4 Online Gradient Descent for Smooth, Strongly Convex Problems

In this section, we generalize the results of the previous section idea to functions which are (cid:96)-strongly convex and
u-smooth. We will see that similar bounds on Rs and R∗
d can be obtained.
Our proposed update rule for the prediction θt+1 at time step t + 1 is:

(cid:107)θ − (θt − ηt∇ft (θt ))(cid:107)2

(7)

θt+1 = argmin
(cid:96)(γ−γ t )+u(1−γ ) and γ ∈ (0, 1).

1−γ

θ∈S

where ηt =
This update rule generalizes the step size rule from the last section.
Before getting to the dynamic regret, we will ﬁrst derive the relation between (cid:107)θt+1 − θ∗
t (cid:107) and (cid:107)θt − θ∗
t (cid:107) to try to
mimic the result in Lemma 1 of the quadratic case:
t ∈ S be the solution to ft (θ) which is strongly convex and smooth. When we use the update in Eq.(7),
the following relation is obtained:

Lemma 2. Let θ∗

(cid:115)

(cid:107)θt+1 − θ∗

t (cid:107) ≤

1 − l(1 − γ )
u(1 − γ ) + lγ

(cid:107)θt − θ∗

t (cid:107)

Due to space limits, please refer to the appendix for the proof.
Now we are ready to present the dynamic regret result:

7

Theorem 4. Let θ∗

t be the solution to ft (θ), θ ∈ S . When using the update in Eq.(7) with 1 − γ = 1/T β , β ∈ (0, 1),
we can upper bound the dynamic regret:

d ≤ G(cid:0)2(T β − 1) + u/l(cid:1)((cid:107)θ1 − θ∗
1 (cid:107) + V ∗ )

R∗

A PR E PR IN T - NOV EMB ER 22 , 2019

The proof follows the similar steps in the proof of Theorem 2. Due to space limits, please refer to the appendix.
Theorem 4’s result seems promising in achieving the trade-off, since it has a similar form of the result from quadratic
problems in Theorem 2. Next, we will present the static regret result, which assures that the desired trade-off can be
obtained.
Theorem 5. Let θ∗ be the solution to min
θ∈S
we can upper bound the static regret:

ft (θ). When using the update in Eq.(7) with 1 − γ = 1/T β , β ∈ (0, 1),

T(cid:80)

t=1

Rs ≤ O(T 1−β )

The proof follows the similar steps in the proof of Theorem 3. Due to space limits, please refer to the appendix.
The regret bounds of this section are similar to those obtained for simple quadratics. Thus, this gradient descent rule
maintains all of the advantages over the discounted Newton method that were described in the previous section and
the advantages of trading off static regret and dynamic regret R∗
d .

5 Online Gradient Descent for Strongly Convex Problems

In this section, we extend step size idea from previous section to problems which are (cid:96)-strongly convex, but not
necessarily smooth. We obtain a dynamic regret of Rd ≤ max{O(log T ), O(
T V )}, similar to the discounted online
Newton method. However, our analysis does not lead to the clean trade-off of Rs ≤ O(T 1−β ) and R∗
V ∗ )) obtained when smoothness is also used.
The update rule is online gradient descent:

d ≤ O(T β (1 +

√

θt+1 = argmin

θ∈S

(cid:107)θ − (θt − ηt∇ft (θt ))(cid:107)2

(8)

(cid:96)(1−γ t ) , and γ ∈ (0, 1).

where ηt = 1−γ
We can see that the update rule is the same as the one in Eq.(7) while the step size ηt is replaced with
By using the new step size with the update rule in Eq.(8), we can obtain the following dynamic regret bound:
Theorem 6. If using the update rule in Eq.(8) with ηt = 1−γ
(cid:96)(1−γ t ) and γ ∈ (0, 1), the following dynamic regret can be
obtained:

1−γ
(cid:96)(1−γ t ) .

(cid:16)

(cid:17) ≤ 2D(cid:96)
ft (θt ) − ft (zt )

T(cid:88)

t=1

T(cid:88)

t=1

ηt

1
1 − γ

V +

G2
2

Proof. According to the non-expansive property of the projection operator and the update rule in Eq.(8), we have

The reformulation gives us

(cid:107)θt+1 − zt(cid:107)2 ≤ (cid:107)θt − ηt∇ft (θt ) − zt(cid:107)2
= (cid:107)θt − zt(cid:107)2 − 2ηt∇ft (θt )T (θt − zt ) + η2
t (cid:107)∇ft (θt )(cid:107)2
(cid:0) (cid:107)θt − zt(cid:107)2 − (cid:107)θt+1 − zt(cid:107)2 (cid:1) + ηt
∇ft (θt )T (θt − zt ) ≤ 1
2 (cid:107)∇ft (θt )(cid:107)2
(cid:0) (cid:107)θt − zt(cid:107)2 − (cid:107)θt+1 − zt(cid:107)2 (cid:1) + ηt
to ∇ft (θt )T (θt − zt ) ≥ ft (θt ) − ft (zt ) + (cid:96)
ft (θt ) − ft (zt ) ≤ 1
2 (cid:107)∇ft (θt )(cid:107)2 − (cid:96)
2 (cid:107)zt − θt(cid:107)2

(9)
Moreover, from the strong convexity, we have ft (zt ) ≥ ft (θt ) + ∇ft (θt )T (zt − θt ) + (cid:96)
2 (cid:107)zt − θt(cid:107)2 , which is equivalent
2 (cid:107)zt − θt(cid:107)2 . Combined with Eq.(9), we have

2ηt

2ηt

Then we can lower bound (cid:107)θt+1 − zt(cid:107)2 by

(cid:107)θt+1 − zt(cid:107)2 = (cid:107)θt+1 − zt+1 (cid:107)2 + (cid:107)zt+1 − zt(cid:107)2 + 2(θt+1 − zt+1 )(cid:62) (zt+1 − zt )
≥ (cid:107)θt+1 − zt+1 (cid:107)2 − 4D(cid:107)zt+1 − zt(cid:107)

8

(10)

(11)

Combining (10) and (11) gives

ft (θt ) − ft (zt ) ≤ 1

2ηt

(cid:0) (cid:107)θt − zt(cid:107)2 − (cid:107)θt+1 − zt+1(cid:107)2 (cid:1) + 2D

ηt

A PR E PR IN T - NOV EMB ER 22 , 2019

(cid:107)zt+1 − zt(cid:107) + ηt
2 (cid:107)∇ft (θt )(cid:107)2 − (cid:96)
2 (cid:107)zt − θt(cid:107)2

Summing over t from 1 to T , dropping the term − 1
(cid:107)∇ft (θt )(cid:107)2 ≤ G2 , and re-arranging gives

2ηT

(cid:107)θT +1 − zT +1 (cid:107)2 , setting zT +1 = zT , using the inequality

(cid:16)

T(cid:80)

≤ 1

t=1
2 ( 1
η1

ft (θt ) − ft (zt )
− (cid:96))(cid:107)θ1 − z1(cid:107)2 + 1
≤ 2D(cid:96) 1

1−γ V + G2
2

ηt

2

(cid:17)
T(cid:80)

t=1

T(cid:80)

t=1

− 1

ηt−1

( 1

ηt

− (cid:96))(cid:107)θt − zt(cid:107)2 + 2D

T −1(cid:80)

t=1

(cid:107)zt+1 − zt(cid:107) + G2

2

1
ηt

T(cid:80)

t=1

ηt

where for the second inequality, we use the following results: 1
1−γ , and the deﬁnition of V .

= (cid:96)(1−γ t )
1−γ ≤ (cid:96)

η1

1
ηt

− (cid:96) = 0, 1

ηt

− 1

ηt−1

− (cid:96) = (cid:96)(1−γ )(γ t−1−1)
1−γ

≤ 0,

Similar to the case of discounted online Newton methods, if a bound on the path length, V , is known, the discount
factor can be tuned to achieve low dynamic regret:
Corollary 3. By setting γ = 1 − 1

, the following bound can be obtained:

(cid:113) max{V ,log2 T /T }
2DT

2

T(cid:88)

(cid:16)

(cid:17) ≤ max{O(log T ), O(
ft (θt ) − ft (zt )

√

T V )}.

t=1

This result is tighter than the O((cid:112)T (1 + V )) bound obtained by [8] on convex functions, but not directly comparable
to the O(V ∗ ) bounds obtained in [9] for smooth, strongly convex functions.
Similar to the Corollary 2 on discounted online Newton methods, Corollary 3 requires knowing V . In the next section,
we will see how a meta-algorithm can be used to obtain the same bounds without knowing V .

6 Meta-algorithm

In previous sections, we discussed the results on dynamic regret for both α-exp-concave and (cid:96)-strongly convex objec-
tives. The tightest regret bounds were obtained by choosing a discount factor that depends on V , a bound on the path
length. In this section, we solve this issue by running multiple algorithms in parallel with different discount factors.
For online convex optimization, a similar meta-algorithm has been used by [8] to search over step sizes. However, the
method of [8] cannot be used directly in either the α-exp-concave or (cid:96)-strongly convex case due to the added O(
regret from running multiple algorithms. In order to remove this factor, we exploit the exp-concavity in the experts
algorithm, as in Chapter 3 in [5].
In this section, we will show that by using appropriate parameters and analysis designed speciﬁcally for our cases, the
meta-algorithm can be used to solve our issues.

√

T )

6.1 Exp-concave case

Before showing the regret result, we ﬁrst show that the cumulative loss of the meta-algorithm is comparable to all
Aγ ∈ H:
Lemma 3. If ft is α-exp-concave and λ = α, the cumulative loss difference of Algorithm 2 for any γ ∈ H is bounded
as:

T(cid:88)

t=1

(ft (θt ) − ft (θγ
t )) ≤ 1
α

log

1
wγ

1

9

A PR E PR IN T - NOV EMB ER 22 , 2019

Algorithm 2 Meta-Algorithm
Given step size λ, and a set H containing discount factors for each algorithm.
Activate a set of algorithms {Aγ |γ ∈ H} by calling Algorithm 1 (exp-concave case) or the update in Eq.(8) (strongly
convex case) for each parameter γ ∈ H.
Sort γ in descending order γ1 ≥ γ2 ≥ · · · ≥ γN , and set wγi
for t=1,. . . ,T do
Play θt = (cid:80)
Obtain θγ
t from each algorithm Aγ .
t , and incur loss ft (θγ
t ) for each θγ
t .
γ∈H
Update wγ
t by

i(i+1) with C = 1 + 1/|H|.

1 = C

t θγ

wγ

wγ

t+1 =

t exp(−λft (θγ
wγ
t ))
t exp(−λft (θµ
wµ
t ))

.

(cid:80)

µ∈H

Send back the gradient ∇ft (θγ
t ) for each algorithm Aγ .

end for

√

This result shows how O(
T ) regret incurred by running an experts algorithm is reduced in the α-exp-concave case.
The result is similar to Proposition 3.1 of [5].
Based on the above lemma, if we can show that there exists an algorithm Aγ , which can bound the regret
T V )}, then we can combine these two results and show that the

t ) − ft (zt )) ≤ max{O(log T ), O(

(cid:80)T

t=1 (ft (θγ

√

regret holds for θt , t = 1, . . . , T as well:
Theorem 7. For any comparator sequence z1 , . . . , zT ∈ S , setting H =
√
log2 T )(cid:101) + 1, and λ = α leads to the result:
where ηi = 1

2i−1 , N = (cid:100) 1

2 log2 ( 2DT 2

log T
T
2D

2

γi = 1 − ηi

(cid:12)(cid:12)(cid:12)i = 1, . . . , N

(cid:111)

with T ≥ 2

(cid:110)

√

T(cid:88)

t=1

(ft (θt ) − ft (zt )) ≤ O(max{log T ,

T V })

(cid:80)T

√

t=1 (ft (θγ

As described previously, the proof ’s main idea is to show that we could both ﬁnd an algorithm Aγ bounding the regret
T V )} and cover the V with O(log T ) different γ choices. Please see

t ) − ft (zt )) ≤ max{O(log T ), O(

the appendix for the formal proof.
In practice, we include the additional case when γ = 1 to make the overall algorithm explicitly balance the static
regret. Also, the free parameter  used in Algorithm 1 is important for the actual performance. If it is too small, the
update will be easily effected by the gradient to have high generalization error. In practice, it can be set to be equal to
4GD , α} like in [6].

2 min{ 1
1/(ρ2D2 ) or 1/(ρ2D2N ) with ρ = 1

6.2 Strongly convex case

For the strongly convex problem, since the parameter γ used in Corollary 3 is the same as the one in Corollary 2, it
seems likely that the meta-algorithm should work with the same setup in as Theorem 7. The only parameter that needs
to be changed is λ, which was set above to α, the parameter of α-exp-concavity.
To proceed, we ﬁrst show that the (cid:96)-strongly convex function with bounded gradient (e.g.,(cid:107)∇ft(cid:107) ≤ G) is also (cid:96)/G2 -
exp-concave. Previous works also pointed out this, but their statement only works when ft is second-order differen-
tiable, while our result is true when ft is ﬁrst-order differentiable.
Lemma 4. For the (cid:96)-strongly convex function ft with (cid:107)∇ft(cid:107) ≤ G, it is also α-exp-concave with α = (cid:96)/G2 .
Lemma 4 indicates that running Algorithm 2 with strongly convex function leads to the same result as in Lemma 3.
Thus, using the similar idea as discussed in the case of α-exp-concavity and Algorithm 2, the theorem below can be
obtained:
Theorem 8. For any comparator sequence z1 , . . . , zT ∈ S , setting H =
with T ≥ 2
√
log2 T )(cid:101) + 1, and λ = (cid:96)/G2 leads to the result:
where ηi = 1

(cid:12)(cid:12)(cid:12)i = 1, . . . , N

2i−1 , N = (cid:100) 1

γi = 1 − ηi

2 log2 ( 2DT 2

(cid:110)

(cid:111)

2

log T
T
2D

T(cid:88)

(ft (θt ) − ft (zt )) ≤ O(max{log T ,

T V })

√

t=1

10

As discussed in the previous subsection, in practice, we also include the case when γ = 1 to make the overall algorithm
explicitly balance the static regret and set  accordingly as in the exp-concave case.

A PR E PR IN T - NOV EMB ER 22 , 2019

6.3 A Lower bound

In the previous subsections, we demonstrate how to achieve the dynamic regret max{O(log T ), O(
T V )} for both
the exp-concave and strongly convex problems without knowing V . In this subsection, we will give a lower bound,
which approaches the upper bound for large and small V .
Proposition 1. For losses of the form ft (θ) = (θ − t )2 , for all γ0 ∈ (0, 1) and all V = T
(cid:107)zt − zt−1(cid:107) ≤ V and
sequence zT
1 such that

4−γ0 , there is a comparison

T(cid:80)

2+γ0

√

Rd ≥ max{O(log T ), O(cid:0)(V T )

2 (cid:1)}.

γ0

t=2

(cid:16)

(cid:17)

The above result has the following indications: 1. For V = o(T ) but approaching to T , it is impossible to achieve
better bound of Rd ≥ O
with α0 < 1. 2. For other ranges of V like V = O(
T ), its lower bound is not
established and still an open question.

(V T )

√

α0
2

7 Conclusion

In this paper, we propose a discounted online Newton algorithm that generalizes recursive least squares with forgetting
factors and existing online Newton methods. We prove a dynamic regret bound max{O(log T ), O(
T V )} which
provides a rigorous analysis of forgetting factor algorithms. In the special case of simple quadratic functions, we
demonstrate that the discounted Newton method reduces to a gradient descent algorithm with a particular step size
rule. We show how this step size rule can be generalized to apply to strongly convex functions, giving a substantially
more computationally efﬁcient algorithm than the discounted online Newton method, while recovering the dynamic
regret guarantees. The strongest regret guarantees depend on knowledge of the path length, V . We show how to
use a meta-algorithm that optimizes over discount factors to obtain the same regret guarantees without knowledge
of V as well as a lower bound which matches the obtained upper bound for certain range of V . Finally, when the
functions are smooth we show how this new gradient descent method enables a static regret of Rs ≤ O(T 1−β ) and
d ≤ O(T β (1 + V ∗ )), where β ∈ (0, 1) is a user-speciﬁed trade-off parameter.

R∗

√

References

[1] Avrim Blum, Vijay Kumar, Atri Rudra, and Felix Wu. Online learning in online auctions. Theoretical Computer
Science, 324(2-3):137–146, 2004.
[2] Koby Crammer, Ofer Dekel, Joseph Keshet, Shai Shalev-Shwartz, and Yoram Singer. Online passive-aggressive
algorithms. Journal of Machine Learning Research, 7(Mar):551–585, 2006.
[3] Jianjun Yuan and Andrew Lamperski. Online adaptive principal component analysis and its extensions.
International Conference on Machine Learning, pages 7213–7221, 2019.
[4] Jianjun Yuan and Andrew Lamperski. Online convex optimization for cumulative constraints. In Advances in
Neural Information Processing Systems, pages 6137–6146, 2018.
[5] Nicolo Cesa-Bianchi and G ´abor Lugosi. Prediction, learning, and games. Cambridge university press, 2006.
[6] Elad Hazan. Introduction to online convex optimization. Foundations and Trends R(cid:13) in Optimization, 2(3-4):157–
325, 2016.
[7] Martin Zinkevich. Online convex programming and generalized inﬁnitesimal gradient ascent. In Proceedings of
the 20th International Conference on Machine Learning (ICML-03), pages 928–936, 2003.
[8] Lijun Zhang, Shiyin Lu, and Zhi-Hua Zhou. Adaptive online learning in dynamic environments. In Advances in
Neural Information Processing Systems, pages 1323–1333, 2018.
[9] Aryan Mokhtari, Shahin Shahrampour, Ali Jadbabaie, and Alejandro Ribeiro. Online optimization in dynamic
environments: Improved regret rates for strongly convex problems. In 2016 IEEE 55th Conference on Decision
and Control (CDC), pages 7195–7201. IEEE, 2016.

In

11

A PR E PR IN T - NOV EMB ER 22 , 2019

[10] Tianbao Yang, Lijun Zhang, Rong Jin, and Jinfeng Yi. Tracking slowly moving clairvoyant: Optimal dynamic
regret of online learning with true and noisy gradient. In International Conference on Machine Learning, pages
449–457, 2016.
[11] Elad Hazan and Comandur Seshadhri. Efﬁcient learning algorithms for changing environments. In Proceedings
of the 26th annual international conference on machine learning, pages 393–400. ACM, 2009.
[12] Ali H Sayed. Adaptive ﬁlters. John Wiley & Sons, 2011.
[13] Elad Hazan, Amit Agarwal, and Satyen Kale. Logarithmic regret algorithms for online convex optimization.
Machine Learning, 69(2):169–192, 2007.
[14] Lei Guo, Lennart Ljung, and Pierre Priouret. Performance analysis of the forgetting factor rls algorithm. Inter-
national journal of adaptive control and signal processing, 7(6):525–537, 1993.
[15] Peng Zhao, Xinqiang Wang, Siyu Xie, Lei Guo, and Zhi-Hua Zhou. Distribution-free one-pass learning. IEEE
Transactions on Knowledge and Data Engineering, 2019.
[16] Aur ´elien Garivier and Eric Moulines. On upper-conﬁdence bound policies for switching bandit problems. In
International Conference on Algorithmic Learning Theory, pages 174–188. Springer, 2011.
[17] Yoan Russac, Claire Vernade, and Olivier Capp ´e. Weighted linear bandits for non-stationary environments. In
Advances in Neural Information Processing Systems, pages 12017–12026, 2019.
[18] Omar Besbes, Yonatan Gur, and Assaf Zeevi. Non-stationary stochastic optimization. Operations research,
63(5):1227–1244, 2015.
[19] Yurii Nesterov. Introductory lectures on convex optimization: A basic course, volume 87. Springer Science &
Business Media, 2013.
[20] Jacob Abernethy, Peter L Bartlett, Alexander Rakhlin, and Ambuj Tewari. Optimal strategies and minimax lower
bounds for online convex games. 2008.

12

A PR E PR IN T - NOV EMB ER 22 , 2019

Appendix:

The supplementary material contains proofs of the some
results of the paper along with supporting results.

Proof of Theorem 1: Before proving the theorem, the
following observation is helpful.
Lemma 5. If Pt is updated via (4a) then (cid:107)Pt(cid:107) ≤  + G2
1−γ ,
while if Pt is updated via (4b), then (cid:107)Pt(cid:107) ≤  + u
1−γ .

Proof. First consider the quasi-Newton case. The bound
holds at P0 = I , so assume that it holds at time t − 1 for
t ≥ 1. Then, by induction we have

(cid:107)Pt(cid:107) = (cid:107)γPt−1 + ∇t∇t(cid:107)
≤ γ (cid:107)Pt−1 (cid:107) + G2
≤ γ  +
G2
1 − γ
G2
1 − γ

≤  +

.

The full-Newton case is identical, except it uses the bound
(cid:107)Ht(cid:107) ≤ u.

The generalized Pythagorean theorem implies that

(cid:107)θt+1 − zt(cid:107)2

Pt

≤

(cid:13)(cid:13)(cid:13)(cid:13)θt − 1

P −1
t ∇t − zt
η
= (cid:107)θt − zt(cid:107)2
1
t P −1
+
t (θt − zt ).
(cid:16)(cid:107)θt − zt(cid:107)2

(cid:13)(cid:13)(cid:13)(cid:13)2

Pt
t ∇t

Pt

η2 ∇(cid:62)

− 2

η

∇(cid:62)

Re-arranging shows that

∇(cid:62)

t (θt − zt ) ≤ 1
t P −1
t ∇t +
2η
− (cid:107)θt+1 − zt(cid:107)2

∇(cid:62)

η
2

Pt

Pt

(cid:17)

(12)

Let c1 be the upper bound on (cid:107)Pt(cid:107) from Lemma 5. Then
we can lower bound (cid:107)θt+1 − zt(cid:107)2
by

Pt
= (cid:107)θt+1 − zt+1(cid:107)2
Pt

(cid:107)θt+1 − zt(cid:107)2

Pt

+ (cid:107)zt+1 − zt(cid:107)2
+ 2(θt+1 − zt+1 )(cid:62)Pt (zt+1 − zt )
− 4Dc1 (cid:107)zt+1 − zt(cid:107)

Pt

≥ (cid:107)θt+1 − zt+1(cid:107)2
Pt

(13)

Combining (12) and (13) gives

∇(cid:62)

t (θt − zt ) ≤ 1
(cid:0)(cid:107)θt − zt(cid:107)2
t P −1
t ∇t + 2Dc1 η(cid:107)zt+1 − zt(cid:107)
2η
η
2

∇(cid:62)

Pt

− (cid:107)θt+1 − zt+1(cid:107)2
Pt

(cid:1)

Summing over t, dropping the term −(cid:107)θT +1 − zT +1 (cid:107)2
setting zT +1 = zT , and re-arranging gives

PT

,

T(cid:88)

t=1

∇(cid:62)

t (θt − zt ) ≤ T(cid:88)
(cid:107)θ1 − z1(cid:107)2 +
η
(θt − zt )(cid:62) (Pt − Pt−1 )(θt − zt )
2

t=1

1
2η

∇(cid:62)

t P −1
t ∇t + 2Dc1 ηV

+

η
2

T(cid:88)

t=1

(14)

Now we will see how the choices of η enable the ﬁnal sum
that η(Pt − Pt−1 ) (cid:22) η∇t∇(cid:62)
from (14) to cancel the terms from (3). In Case 1, we have
t and the bound from (3a)
holds for ρ = η . In Case 2, η(Pt − Pt−1 ) (cid:22) ηHt (cid:22) (cid:96)I .
In Case 3, η(Pt − Pt−1 ) (cid:22) ηHt (cid:22) Ht . Thus in all cases,
η has been chosen so that combining the appropriate term
of (3) with (14) gives

(cid:80)T

t=1 (ft (θt ) − ft (zt )) ≤ (cid:80)T
t P −1
+2Dc1 ηV + 2ηD2

t=1

1

2η ∇(cid:62)

t ∇t

(15)
Now we will bound the ﬁrst sum of (15). Note that
, ∇t∇(cid:62)
t (cid:105). In Case 1, we have that
t = Pt − γPt−1 , while in Cases 2 and 3, we have
that ∇t∇(cid:62)
α (Pt − γPt−1 ). So, in Case 1, let
c2 = 1 and in Cases 2 and 3, let c2 = 1/α. Then in all
cases, we have that

∇(cid:62)
∇t∇(cid:62)

t P −1
t ∇t = (cid:104)P −1

t

t (cid:22) 1
α Ht = 1

∇(cid:62)

t P −1
t ∇t ≤ c2 (cid:104)P −1

t

, Pt − γPt−1 (cid:105).

(16)

Lemma 4.5 of [6] shows that
(cid:104)P −1

t

, Pt−γPt−1 (cid:105) ≤ log

|Pt |

|γPt−1 | = log

|Pt |

|Pt−1 | −n log γ ,

(17)

where n is the dimension of xt .
Combining (16) with (17), summing, and then using the
bound that (cid:107)PT (cid:107) ≤ c1 gives,

T(cid:88)

t=1

∇(cid:62)

t P −1
t ∇t ≤ c2 log |PT | − c2n log  − nT log γ
≤ c2n log
− c2nT log γ
c1


(18)

Recall that c1 =  + c3
depending on the case. Then a more explicit upper bound
on (18) is given by:

1−γ , where c3 = G2 or c3 = u,

t(cid:88)

t=1

∇(cid:62)

t P −1
t ∇t ≤ c2n log

(cid:18)

1 +

c3
(1 − γ )

(cid:19)

−c2nT log γ .

(19)

Combining (15) and (19) gives the bound:

(cid:80)T

t=1 (ft (θt ) − ft (zt )) ≤ − c2 nT
2η log γ+
1 + c3
 + c3
2η log
+ 2Dη

c2 n

(cid:16)

(1−γ )

(cid:17)

(cid:16)

1−γ

(cid:17)

V + 2ηD2

13

A PR E PR IN T - NOV EMB ER 22 , 2019

The desired regret bound can now be found by simplifying
the expression on the right, using the fact that

1

1−γ > 1.

The following integral bound will be used in a few places.
Lemma 6. If γ ∈ (0, 1), then

T(cid:88)

t=1

1 − γ t ≤ 1
1
1 − γ

+ T − 1 +

log(1 − γ )
log γ

Proof.

T(cid:88)

t=1

1 − γ t ≤ 1
1
1 − γ
1
1 − γ
1
1 − γ

+

(cid:90) T
(cid:16)

1

1
1 − γ t dt
t − ln(1 − γ t )
ln(γ )
ln(1 − γ )
+ T − 1 +
ln γ
ln(1 − γ )
ln γ

=

+

(cid:17)(cid:12)(cid:12)(cid:12)T

1

=

− ln(1 − γ T )
ln γ

≤ 1

1 − γ

+ T − 1 +

.

Proof of Theorem 3:

Proof. To proceed, recall that the update in Eq.(6) is

θt+1 = γ−γ t
1−γ t θt + 1−γ
1−γ t yt

= θt − ηt∇ft (θt )

where ηt = 1−γ
Then we get the relationship between ∇ft (θt )T (θt − θ∗ )
and (cid:107)θt − θ∗ (cid:107)2 − (cid:107)θt+1 − θ∗ (cid:107)2 as:

1−γ t .

(cid:107)θt+1 − θ∗ (cid:107)2 = (cid:107)θt − ηt∇ft (θt ) − θ∗ (cid:107)2
= (cid:107)θt − θ∗ (cid:107)2 − 2ηt∇ft (θt )T (θt − θ∗ )
t (cid:107)∇ft (θt )(cid:107)2
+η2
∇ft (θt )T (θt − θ∗ ) = 1

2ηt
+ ηt

Eq.(7), we have (cid:0)∇ft (θt ) + 1
According to the optimality condition of the update rule in
(cid:0) (cid:107)θt − θ∗ (cid:107)2 − (cid:107)θt+1 − θ∗ (cid:107)2 (cid:1)
θt+1 )T (θ − θt+1 ). Then combine with Eq.(21), we have
(22)
From the smoothness of ft (θ), we have ft (θt+1 ) ≤
Moreover, we write ft (θ∗ ) as ft (θ∗ ) = ft (θt ) +
2 (cid:107)θ∗ − θt(cid:107)2 , which combined with
2 (cid:107)θt+1 − θt(cid:107)2 . Since
(cid:0) (cid:107)θt − θ∗ (cid:107)2 − (cid:107)θt+1 − θ∗ (cid:107)2 (cid:1)
≥ u, we have ft (θt ) +
the previous equation gives us the following equation:

2 (cid:107)∇ft (θt )(cid:107)2

∇ft (θt )T (θ∗ − θt ) + 1

ft (θt ) − ft (θ∗ ) = 1
2 (cid:107)∇ft (θt )(cid:107)2 − 1
2 (cid:107)θ∗ − θt(cid:107)2
≤ 2D2 ηt + 1
(cid:107)θt+1 − θ∗ (cid:107)2 (cid:1) − 1
2 (cid:107)θ∗ − θt(cid:107)2

2ηt
+ ηt

2ηt

(cid:0) (cid:107)θt − θ∗ (cid:107)2 −

where the inequality is due to (cid:107)∇ft (θt )(cid:107) ≤ 2D as shown
in Theorem 2.

Sum the above inequality from t = 1 to T , we get:

T(cid:80)

t=1

(cid:16)

ft (θt ) − ft (θ∗ )
≤ 2D2

(cid:17)

T(cid:80)

t=1

ηt + 1/η1−1
2

(cid:2)( 1
(cid:107)θ1 − θ∗ (cid:107)2 + 1
(cid:107)θT +1 − θ∗ (cid:107)2

2

T(cid:80)

t=2

ηt

− 1

ηt−1

− 1) (cid:107)θ∗ − θt(cid:107)2 (cid:3) − 1
Since ηt = 1−γ
the static regret, we have:

2ηT

1−γ t , η1 = 1, 1

ηt

− 1

ηt−1

− 1 < 0. Then for

Rs =
≤ 2D2

T(cid:80)
(cid:16)
T(cid:80)

t=1

ft (θt ) − ft (θ∗ )
ηt = 2D2 (1 − γ )

(cid:17)

t=1

T(cid:80)

t=1

1−γ t
1

(20)

Now we will use the integral bound from Lemma 6 to
bound the regret. Since 1 − γ = 1/T β ,

log(1−γ )
log γ

=
) . Since log(1 + x) ≥ 1
2 x, x ∈ (0, 1), log(1 +
log γ ≤ 2β (T β −

β log T
log(1+ 1
T β −1 ) ≥ 1
1
2

T β −1

1

T β −1 . Thus, we have log(1−γ )
1−γ t = O(T 1−β ), which re-

1) log T . Then (1 − γ )

T(cid:80)

t=1

1

sults in Rs ≤ O(T 1−β ).

Proof of Lemma 2:

Proof. The proof follows the analysis in Chapter 2 of
[19].
From the strong convexity of ft (θ), we have

ft (θ) ≥ ft (θt ) + ∇ft (θt )T (θ − θt ) + (cid:96)
2 (cid:107)θ − θt (cid:107)2
= ft (θt ) + ∇ft (θt )T (θ − θt ) + ∇ft (θt )T (θt+1 − θt )
−∇ft (θt )T (θt+1 − θt ) + (cid:96)
2 (cid:107)θ − θt (cid:107)2
= ft (θt ) + ∇ft (θt )T (θt+1 − θt )
+∇ft (θt )T (θ − θt+1 ) + (cid:96)
2 (cid:107)θ − θt (cid:107)2

(21)

ηt

(θt+1 − θt )(cid:1)T
(θ − θt+1 ) ≥
0, ∀θ ∈ S , which is ∇ft (θt )T (θ − θt+1 ) ≥ 1
(θt −

ηt

ft (θ) ≥ ft (θt ) + ∇ft (θt )T (θt+1 − θt )
+ 1
(θt − θt+1 )T (θ − θt+1 ) + (cid:96)
2 (cid:107)θ − θt(cid:107)2

ηt

ft (θt ) + ∇ft (θt )T (θt+1 − θt ) + u

1
ηt

= (cid:96)(γ−γ t )+u(1−γ )
1−γ

∇ft (θt )T (θt+1 − θt ) ≥ ft (θt+1 ) − 1
ft (θ) ≥ ft (θt+1 ) − 1
(cid:107)θt+1 − θt(cid:107)2
(θt − θt+1 )T (θ − θt+1 ) + (cid:96)
2 (cid:107)θ − θt(cid:107)2
+ 1
= ft (θt+1 ) + 1
(cid:107)θt+1 − θt(cid:107)2
(θt − θt+1 )T (θ − θt ) + (cid:96)
2 (cid:107)θ − θt(cid:107)2
+ 1

2ηt

(cid:107)θt+1 − θt(cid:107)2 .

Then combined with inequality (22), we have

2ηt

ηt

2ηt

ηt

(23)

14

A PR E PR IN T - NOV EMB ER 22 , 2019
(cid:0)ft (θt ) − ft (θ∗
t )(cid:1) ≤ G 1
(cid:13)(cid:13) (cid:1) ≤ G(cid:0)2(T β −1)+u/(cid:96)(cid:1)(cid:0) (cid:107)θ1 − θ∗
1 (cid:107) +
1 (cid:107)+

By setting θ = θ∗
t and using the fact ft (θ∗
we reformulate the above inequality as:

t ) ≤ ft (θt+1 ),

(θt − θt+1 )T (θ∗
t − θt )

≤ −

2(cid:96)(γ−γ t )+2u(1−γ ) (cid:107)θ∗
(cid:96)(1−γ )

t − θt(cid:107)2 − 1

2 (cid:107)θt+1 − θt(cid:107)2

(24)
t (cid:107)2 , we have
(cid:1) (cid:107)θt − θ∗
(cid:1) (cid:107)θt − θ∗

Since (cid:107)θt+1 − θ∗

t (cid:107)2 = (cid:107)θt+1 − θt + θt − θ∗
(cid:107)θt+1 − θ∗
t (cid:107)2 = (cid:107)θt+1 − θt(cid:107)2 + (cid:107)θt − θ∗
≤ (cid:0)1 − (cid:96)(1−γ )
+2(θt − θt+1 )T (θ∗
t − θt )

t (cid:107)2

≤ (cid:0)1 −

(cid:96)(1−γ )
(cid:96)(γ−γ t )+u(1−γ )
(cid:96)γ+u(1−γ )

t (cid:107)2

t (cid:107)2

(25)

Proof of Theorem 4:

Proof. We use the same steps as in the previous sec-
tion. First, according to the Mean Value Theorem,
t (cid:107), where x ∈ {v |v = δθt + (1 −
t , δ ∈ [0, 1]}. Due to the assumption on the upper
bound of the norm of the gradient, we have ft (θt ) −
t (cid:107). As a result,
(cid:107)θt − θ∗
t (cid:107).

we have ft (θt ) − ft (θ∗
t ) = ∇ft (x)T (θt − θ∗
t ) ≤
(cid:107)∇ft (x)(cid:107) (cid:107)θt − θ∗
δ)θ∗
(cid:0)ft (θt ) −
t ) ≤ G (cid:107)θt − θ∗
ft (θ∗
t )(cid:1) ≤ G
ft (θ∗

T(cid:80)

t=1

T(cid:80)
T(cid:80)
T(cid:80)

t=1

Now we
(cid:13)(cid:13)θt − θ∗
(cid:107)θt − θ∗
t (cid:107).
(cid:107)θ1 − θ∗

need

to
upper
bound
the
term
(cid:107)θt − θ∗
t (cid:107) is equal
to (cid:107)θ1 − θ∗
(cid:13)(cid:13), which
t−1 − θ∗

T(cid:80)
T(cid:80)

t=1

t=1

1 (cid:107) +

t=2

t−1 + θ∗
1 (cid:107) +

t

is

less
t − θ∗

than

t=1

(cid:107)θt+1 − θ∗
t (cid:107) +

T(cid:80)

t=2

(cid:13)(cid:13)θ∗

t−1

(cid:13)(cid:13).

According to Lemma 2, we have
(cid:107)θt − θ∗
t (cid:107) with ρ =
(cid:107)θt − θ∗
t (cid:107) ≤ (cid:107)θ1 − θ∗
(cid:13)(cid:13), which
t − θ∗
(cid:107)θt − θ∗

T(cid:80)

t=1
1 − l(1−γ )

(cid:107)θt+1 − θ∗

t (cid:107) ≤

ρ

T(cid:80)
T(cid:80)
T(cid:80)
(cid:13)(cid:13)θ∗
T(cid:80)

t=1

(cid:113)

u(1−γ )+lγ . Then we
(cid:107)θt − θ∗

have

t=1

1 (cid:107) + ρ

T(cid:80)
(cid:13)(cid:13)θ∗

t=1

t (cid:107) +

t=2

t−1

can

be

reformulated
t − θ∗

as

t=1

t (cid:107) ≤ 1

1 − ρ = 1 − (cid:113)
1−ρ ((cid:107)θ1 − θ∗
1 (cid:107) + +
1 − a0

T(cid:80)

t=2

t−1

(cid:13)(cid:13)).

b0−a0
a0 = (cid:96) and b0 = (cid:96)γ+u(1−γ )
b0
b0
1−γ
b0−a0 )
(cid:96)(T β −1)+u(cid:0)√
b0
b0−a0
b0 (
b0+
(cid:96)(T β −1)+u−(cid:96)(cid:1)
a0
2(cid:0)(cid:96)(T β −1)+u(cid:1)
(cid:96)

=

√

b0−√
√
, where
. Thus, 1/(1 −
√
√
√
√
√
b0−√
. After plug-
ging in the expression of 1 − γ = 1/T β , 1/(1 −

ρ) =

=

ρ) =

√

(cid:96)(T β −1)+u+
(cid:96)

√

≤

= 2(T β − 1) + u/(cid:96)

Then Rd =
t − θ∗
t − θ∗

T(cid:80)

t=1

1−ρ

(cid:0) (cid:107)θ1 − θ∗

+

T(cid:80)
T(cid:80)

t=2

(cid:13)(cid:13)θ∗
(cid:13)(cid:13)θ∗

t−1

+

t=2

t−1

(cid:13)(cid:13) (cid:1).

Proof of Theorem 5:

Proof. The proof follows the similar steps in the proof of
Theorem 3.
According to the non-expansive property of the projection
operator and the update rule in Eq.(7), we have

(cid:107)θt+1 − θ∗ (cid:107)2 ≤ (cid:107)θt − ηt∇ft (θt ) − θ∗ (cid:107)2
= (cid:107)θt − θ∗ (cid:107)2 − 2ηt∇ft (θt )T (θt − θ∗ )
t (cid:107)∇ft (θt )(cid:107)2
+η2

The reformulation gives us

∇ft (θt )T (θt − θ∗ ) ≤ 1

2ηt
+ ηt

(cid:0) (cid:107)θt − θ∗ (cid:107)2 − (cid:107)θt+1 − θ∗ (cid:107)2 (cid:1)
(26)
Moreover, from the strong convexity, we have ft (θ∗ ) ≥
2 (cid:107)θ∗ − θt(cid:107)2 , which is
2 (cid:107)θ∗ − θt(cid:107)2 . Combined with Eq.(26), we have
(cid:0) (cid:107)θt − θ∗ (cid:107)2 − (cid:107)θt+1 − θ∗ (cid:107)2 (cid:1)
Summing up from t = 1 to T with (cid:107)∇ft (θt )(cid:107)2 ≤ G2 , we
get

2 (cid:107)∇ft (θt )(cid:107)2

ft (θt ) + ∇ft (θt )T (θ∗ − θt ) + (cid:96)
equivalent to ∇ft (θt )T (θt − θ∗ ) ≥ ft (θt ) − ft (θ∗ ) +

(cid:96)

ft (θt ) − ft (θ∗ ) ≤ 1

2ηt
+ ηt

2 (cid:107)∇ft (θt )(cid:107)2 − (cid:96)
2 (cid:107)θ∗ − θt(cid:107)2

T(cid:80)

t=1

(cid:0)ft (θt ) − ft (θ∗ )(cid:1)
2 G2 − T(cid:80)

≤ T(cid:80)

t=1

1
2ηt

(cid:0) (cid:107)θt − θ∗ (cid:107)2 − (cid:107)θt+1 − θ∗ (cid:107)2 (cid:1)

+
≤ G2 /2

T(cid:80)
T(cid:80)
(cid:104)
T(cid:80)

t=1

ηt

t=1
ηt + 1/η1−(cid:96)
2

(cid:96)

2 (cid:107)θ∗ − θt (cid:107)2

t=1

− (cid:96)) (cid:107)θ∗ − θt (cid:107)2 (cid:105)
(cid:107)θ1 − θ∗ (cid:107)2

+ 1

2

t=2

( 1

ηt

− 1

ηt−1

(27)

Since ηt =

1−γ
(cid:96)(γ−γ t )+u(1−γ ) , 1/η1 = u and 1
ηt
(cid:96)(γ t−1−1)(1−γ )
1−γ

− 1

ηt−1

−(cid:96) =

≤ 0.

For the term

T(cid:80)
T(cid:80)

t=1

ηt =

T(cid:80)

t=1

1−γ

(cid:96)(γ−γ t )+u(1−γ ) , it can be refor-

mulated as 1
T −1(cid:80)

u + 1
u
u
t=1
u(1−γ )
u + 1−γ
(cid:96)(γ−γ t ) = 1
(cid:96)γ

u(1−γ )
(cid:96)(γ−γ t )
1+ u(1−γ )
(cid:96)(γ−γ t )

= 1

T(cid:80)

t=2
1−γ t−1 = 1
1
1−γ t ≤
1

u(1−γ )
(cid:96)(γ−γ t )
1+ u(1−γ )
(cid:96)(γ−γ t )

≤

u + 1
1
u
1−γ
(cid:96)γ

T(cid:80)

t=2

T(cid:80)

t=2

u +

t=1

1

1−γ t . For

T −1(cid:80)

t=1

1

1−γ t , we know that

T −1(cid:80)

t=1

15

A PR E PR IN T - NOV EMB ER 22 , 2019

O(T ) as shown in the proof of Theorem 3. For the term
(cid:96)(T β −1) . Combining these two terms’ in-
equalities, we get that

1−γ
(cid:96)γ , 1−γ
(cid:96)γ =

1

T(cid:80)

t=1

ηt ≤ O(T 1−β ).
(cid:0)ft (θt ) − ft (θ∗ )(cid:1) ≤ O(T 1−β )

As a result, the inequality (27) can be reduced to

T(cid:88)

t=1

Proof of Corollary 3:

Proof. Since γ = 1 − 1

2

(cid:113) max{V ,log2 T /T }
2DT

and V ∈

[0, 2DT ], 1/2 ≤ γ < 1.

Next, we upper bound each term on the right-
hand-side of Theorem 6 individually.
T V ). In order to bound the
second term, Lemma 6 implies that (1 − γ )
).

1

1−γ V

=

2

(cid:113)

max{V ,log2 T /T } V ≤ O(
2DT

√

T(cid:80)

t=1

1−γ t ≤
1

1 + (1 − γ )(T + ln(1−γ )

ln γ

In this case, the logarithm terms can be bounded by:

ln(1−γ )
ln γ
− ln( 1
− ln(1− 1
− ln( 1

=

2

(cid:113)
(cid:113)

(cid:113) max{V ,log2 T /T }
max{V ,log2 T /T }
2DT
max{V ,log2 T /T }
2DT
2DT
max{V ,log2 T /T }
2DT
max{V ,log2 T /T }
2DT

)

2

)

=

2

)

ln

(cid:16)

1+

1
2
1− 1
2

(cid:114)
(cid:114)

(cid:17)
(cid:113)

≤ ln(2
≤ O(ln(T / log T ) T
≤ O(T )

(cid:113)

max{V ,log2 T /T } )4
2DT
max{V ,log2 T /T }
2DT
log T )
(cid:113) max{V ,log2 T /T }
2DT

where the ﬁrst inequality follows by using ln(1 + x) ≥

1

2 x, x ∈ [0, 1], and 1 − 1
< 1.
1−γ t ≤ max{O(log T ), O(

2

Thus, (1 − γ )
The ﬁnal result follows by adding the two terms.

T(cid:80)

t=1

1

√

T V )}.

Proof of Lemma 3:

Proof. The ﬁrst part of the proof is the same as the ﬁrst
part of the result in the Proof of Lemma 1 in [8], which
follows methods of [5]. We deﬁne Lγ
i ), and

t =

t(cid:80)

i=1

fi (θγ

Wt = (cid:80)

γ∈H

1 exp(−αLγ
wγ
t ).

The following update is equivalent to the update rule in
Algorithm 2:

wγ
1 exp(−αLγ
wγ
t =
1 exp(−αLµ
t−1 )
t ≥ 2.
wµ
t−1 )
log WT = log (cid:0) (cid:80)
T )(cid:1)
≥ log (cid:0) max
wγ
1 exp(−αLγ
T )(cid:1)
(cid:0)Lγ
γ∈H wγ
1 exp(−αLγ
= −α min
α log 1

(cid:80)

µ∈H

,

(28)

First, we have

γ∈H

γ∈H
Then we bound the quantity log(Wt/Wt−1 ). For t ≥ 2,
we get

T + 1

wγ

1

(cid:1).

(29)

log

(cid:16) Wt
Wt−1

(cid:17)
(cid:16) (cid:80)
(cid:16) (cid:80)
(cid:16) (cid:80)

= log

1 exp(−αLγ
γ∈H wγ
t )
γ∈H wγ
1 exp(−αLγ
1 exp(−αLγ
t−1 ) exp(−αft (θγ
t−1 )
γ∈H wγ
t ))
1 exp(−αLγ
γ∈H wγ
t−1 )

(cid:80)

(cid:17)

= log

(cid:80)

(cid:17)

= log

γ∈H

t exp(−αft (θγ
wγ
t ))

(cid:17)

(30)

where the last equality is due to Eq.(28).

When t = 1, log W1 = log

(cid:16) (cid:80)

γ∈H
Then log WT can be expressed as:

1 exp(−αf1 (θγ
wγ
1 ))

(cid:17)

.

log WT = log W1 +

T(cid:80)
(cid:16) (cid:80)

t=2

log
wγ
t exp(−αft (θγ
t ))

(cid:16) Wt
Wt−1

(cid:17)

=

T(cid:80)

t=1

log

γ∈H
Due to the α-exp-concavity, exp(−αft ((cid:80)
The rest of the proof is new.
t )), which is equivalent to

(cid:17)

.

(31)

γ∈H wγ

t θγ

t )) ≥

(cid:80)

t exp(−αft (θγ
γ∈H wγ

log

(cid:16) (cid:80)

t exp(−αft (θγ
γ∈H wγ
t ))

(cid:17) ≤ −αft

(cid:16) (cid:80)

γ∈H wγ

t θγ
t

(cid:17)

= −αft (θt )

(32)

Combining the Inequalities (29), (31), and (32), we get

−α min

γ∈H

(cid:0)Lγ
T +

1
α

log

1
wγ

1

(cid:1) ≤ −α

T(cid:88)

t=1

ft (θt )

which can be reformulated as

T(cid:88)

t=1

ft (θt ) ≤ min

γ∈H

(cid:16) T(cid:88)

t=1

ft (θγ
t ) +

1
α

log

1
wγ

1

(cid:17)

Since it holds for the minimum value, it is true for all γ ∈
H, which completes the proof.

16

A PR E PR IN T - NOV EMB ER 22 , 2019

Proof of Theorem 7:

1 − η∗ , we have (cid:80)T
Proof. When γ = γ ∗ = 1 − 1
T V )} based on the Corollary
2.
Since 0 ≤ V ≤ 2T D , 1

2

log T
T

(cid:113) max{ T
2D

log2 T

V ,1}

=
t ) − ft (zt )) ≤

t=1 (ft (θγ ∗

max{O(log T ), O(

√

2

log T
T
2D

√

≤ η∗ ≤ 1
2 .
√
According to our deﬁnition of ηi , min ηi = 1
and
2 ≤ max ηi < 1, which means for any value of V , there
always exists a ηk such that

2

log T
T
2D

1

1
log T
2k−1 ≤ η∗ ≤ 2ηk = ηk+1
ηk =
2
T
2D
2 log2 (max{ T
log2 T V , 1})(cid:99) + 1.

√

where k = (cid:98) 1
Now we claim that that running the algorithm with γk in-
curs at most a constant factor increase in the dynamic re-
gret.
Since 0 < ηk ≤ 1
According to Theorem 1, we have

2 , 1

2 ≤ γk = 1 − ηk < 1 and γk ≥ γ ∗ .

(cid:80)T

t=1 (ft (θγk

t ) − ft (zt )) ≤ −a1T log γk − a2 log(1 − γk )
V + a4 .

+ a3
1−γk

Now we bound each term of the regret in terms of the
value obtained by using γ ∗ . For the ﬁrst term on the RHS,

−T log γk = T log 1

γk

≤ T log 1

γ ∗ .

For the second one, − log(1 − γk ) = − log 1
2 2ηk . Since 1 ≥ 2ηk ≥ η∗ , 1
leads to − log 1

2 (2 − 2γk ) =
− log 1
2 2ηk ≥ 1
2 2ηk ≤ − log 1
2 η∗ and − log(1 − γk ) ≤
− log 1
2 η∗ = log 2 − log(1 − γ ∗ ).
V = 1
V = 2

2 η∗ , which

For the third one,
1−γ ∗ V . Thus the claim has been proved.
Since using γk in place of γ ∗ increases the regret by at
most a constant factor, Corollary 2 implies that:

1−γk
1

ηk

2ηk

V ≤ 2
η∗ V =

2

T(cid:88)

t=1

t ) − ft (zt )) ≤ max{O(log T ), O(
(ft (θγk

√

T V )}

(33)

Furthermore, from Lemma 3 we get

T(cid:80)

t=1

(ft (θt ) − ft (θγk
t )) ≤ 1
α log 1
α log(k(k + 1))
≤ 2 1
≤ O(log(log T ))
α log(k + 1)

w

γk
1

≤ 1

(34)

Combining the above inequalities (33) and (34) completes
the proof.

Proof of Lemma 4:

Proof. Let g(x) = exp(−αf (x)). To prove the concavity
of g(x), it is equivalent to show (cid:104)∇g(x)−∇g(y), x−y(cid:105) ≤
it is equivalent to prove that (cid:104)exp(−αf (x))∇f (x) −
exp(−αf (y))∇f (y), x − y(cid:105) ≥ 0, which can be reformu-
lated as

0, x, y ∈ S . Since ∇g(x) = exp(−αf (x))(−α)∇f (x),

exp(−αf (x))(cid:104)∇f (x), x − y(cid:105) ≥ exp(−αf (y))(cid:104)∇f (y), x − y(cid:105)

(35)

Without loss of generality, let us assume f (x) ≥ f (y).
Due to (cid:96)-strong convexity, f (x) ≥ f (y) + (cid:104)∇f (y), x −
2 (cid:107)x − y(cid:107)2 , which leads to

y(cid:105) + (cid:96)
(cid:104)∇f (y), x − y(cid:105) ≤ f (x) − f (y) − (cid:96)
2
What’s more, f (y) ≥ f (x)+ (cid:104)∇f (x), y − x(cid:105)+ (cid:96)

(cid:107)x − y(cid:107)2

(36)
2 (cid:107)x− y(cid:107)2 ,

which leads to

(cid:104)∇f (x), x − y(cid:105) ≥ f (x) − f (y) +

(cid:96)
2

(cid:107)x − y(cid:107)2

(37)

Combining inequalities (35), (36), and (37), it is enough

to prove that exp(−αf (x))(f (x) − f (y) + (cid:96)
y(cid:107)2 ) ≥ exp(−αf (y))(f (x) − f (y) − (cid:96)
2 (cid:107)x − y(cid:107)2 ),
2 (cid:107)x − y(cid:107)2 (exp(−αf (x)) +
exp(−αf (y))) ≥ (f (x) − f (y))(exp(−αf (y)) −
1 + exp (cid:0)α(cid:0)f (x) − f (y)(cid:1)(cid:1)(cid:17) ≥

2 (cid:107)x −

which can be reformulated as (cid:96)
exp(−αf (x))). When x − y = 0, it is always true.
Let us consider the case when (cid:107)x − y(cid:107) > 0. Then we
need to show that (cid:96)

α(cid:0)f (x)−f (y)(cid:1)(cid:17)
2

(cid:16)

f (x)−f (y)

(cid:107)x−y(cid:107)
(cid:107)x−y(cid:107)
. Due to bounded gradi-
ent and Mean value theorem, f (x)−f (y)
≤ G, which
(cid:107)x−y(cid:107)
means it is enough to show that

exp

(cid:16)

−1

(cid:96)
2G

(cid:16)

1+exp (cid:0)α(cid:0)f (x)−f (y)(cid:1)(cid:1)(cid:17) ≥ exp

(cid:16)

α(cid:0)f (x) − f (y)(cid:1)(cid:17) − 1

(cid:107)x − y(cid:107)

(38)

According to the Taylor

series,

α(cid:0)f (x) −
= 1 + α(cid:0)f (x) − f (y)(cid:1) + 1
2! α2 (cid:0)f (x) −
exp
n! αn (cid:0)f (x) − f (y)(cid:1)n
n! αn (cid:0)f (x) −
=
+
2 α2 (f (x) − f (y)) f (x)−f (y)

(cid:16)

f (y)(cid:1)(cid:17)
f (y)(cid:1)2

+ · · · + 1

, n → ∞.

Thus,

α(cid:0)f (x)−f (y)(cid:1)(cid:17)
f (y)(cid:1)n−1 f (x)−f (y)
exp
1
α(cid:0)f (x)−f (y)(cid:1)(cid:17)

(cid:16)

−1
(cid:107)x−y(cid:107) + · · · + 1
(cid:107)x−y(cid:107) , n → ∞. Since f (x)−f (y)
(cid:107)x−y(cid:107) ≤ G, we

(cid:107)x−y(cid:107)

α f (x)−f (y)

(cid:107)x−y(cid:107)

have

exp

(cid:16)

−1

(cid:107)x−y(cid:107)

n! αn (cid:0)f (x) − f (y)(cid:1)n−1
≤ αG + 1
2 α2 (f (x) − f (y))G + . . .
+ 1
G
2G (f (x) − f (y)) + 1
2G (f (x) − f (y))2
G + α (cid:96)
2G (f (x) − f (y))n , n → ∞

(39)

For the LHS of inequality (38), it is equal to

(cid:96)

2! α2 (cid:96)

+ · · · + 1

n! αn (cid:96)

(40)

17

A PR E PR IN T - NOV EMB ER 22 , 2019

If we compare the coefﬁcients of the RHS from the in-
equality (39) with the one in (40) and plug in α = (cid:96)/G2 ,
we see that it is always smaller or equal, which completes
the proof.

Proof of Theorem 8:

regret (cid:80)T
Proof. As in the proof of Theorem 7, all we need to show
is that there exists an algorithm Aγ , which can bound the
have (cid:80)T
When γ = γ ∗ = 1− 1
based on the Corollary 3.
Since 0 ≤ V ≤ 2T D , 1

t=1 (ft (θγ

t ) − ft (zt )) ≤ O(max{log T ,

√

T V }).

2

log T
T

(cid:113) max{ T
2D

log2 T

V ,1}

= 1−η∗ , we

t=1 (ft (θγ ∗

t ) − ft (zt )) ≤ O(max{log T ,

√

T V })

2

log T
T
2D

√

≤ η∗ ≤ 1
2 .
√
According to our deﬁnition of ηi , min ηi = 1
and
2 ≤ max ηi < 1, which means for any value of V , there
always exists a ηk such that

2

log T
T
2D

1

1
log T
2k−1 ≤ η∗ ≤ 2ηk = ηk+1
ηk =
2
T
2D
2 log2 (max{ T
log2 T V , 1})(cid:99) + 1.
2 ≤ γk = 1 − ηk < 1 and γk ≥ γ ∗ .
(cid:0)ft (θγk
t )−ft (zt )(cid:1) ≤ 2D(cid:96)
1 − γk

√

where k = (cid:98) 1
Since 0 < ηk ≤ 1
According to Theorem 6, we have

2 , 1

T(cid:88)

t=1

V +

G2
(cid:96)

(1−γk )

T(cid:88)

t=1

1
1 − γ t

k

For the ﬁrst term on the RHS,

1−γk
1

V = 1

ηk

V = 2

2ηk

V ≤

2

η∗ V = 2

1−γ ∗ V .

For the second one, 1 − γk ≤ 1 − γ ∗ . According to the
proof in Corollary 3,
.

T(cid:80)

t=1

1
1−γ t

k

≤ 1

1−γk

+ T + log(1−γk )
log γk

log(1 − γk )
log γk

log ηk
=
log(1 − ηk )
2 η∗ , log ηk ≥ log 1
0 < − log ηk ≤ − log
η∗ = log 2 − log η∗ .
1
2
2 η∗ , 1 − ηk ≤ 1 − 1
2 η∗ . Then log(1 − ηk ) ≤

=

− log ηk
− log(1 − ηk )

.

(41)

Since ηk ≥ 1

2 η∗ and

(42)

Since ηk ≥ 1
2 η∗ ), which results in

log(1 − 1
− log(1 − ηk ) ≥ − log(1 − 1
2

η∗ ) > 0.

(43)

Combining inequalities (42) and (43) with Eq.(41), we get

log(1−γk )
log γk

≤ log 2−log η∗
− log(1− 1
2 η∗ )
log 2
− log(1− 1
2 η∗ ) +

=

− log η∗
− log(1− 1
2 η∗ )

(44)

For the ﬁrst term on the RHS,

− log(1 − 1
2 η∗ ) = log

(cid:16)
(cid:16)

1

1− 1

4

(cid:113)

max{V ,log2 T /T }
2DT
max{V ,log2 T /T }
2DT
max{V ,log2 T /T }
4
2DT
max{V ,log2 T /T }
2DT
max{V ,log2 T /T }
2DT

(cid:17)

= log

1 +

1
4

(cid:113)
(cid:113)

1− 1
(cid:113) max{V ,log2 T /T }
1− 1
2DT
(cid:113) max{V ,log2 T /T }
2DT

(cid:17)

≥ 1
≥ 1

2

1
4

(cid:113)
(cid:113)

4

8

where the ﬁrst inequality is due to log(1 + x) ≥ 1
[0, 1] and the second one is due to
As a result,

2 x, x ∈
> 0.

log 2
− log(1− 1

2 η∗ ) ≤ 8

(cid:113)

max{V ,log2 T /T } log 2
2DT

≤ 8 T

log T

√

2D log 2 < O(T ).

For the second term on the RHS of Eq.(44),

− log η∗ = log
2
≤ log 2 + 1
2 log 2D + 1
2 log T

(cid:16)

(cid:113)

2DT
max{V ,log2 T /T }

(cid:17)

log T

Combining the inequalities for − log η∗ and − log(1 −
2 η∗ ), we get

1

− log η∗
− log(1− 1

2 η∗ ) ≤ (log 2 + 1
2 log 2D +
2D ≤ O(T ).

1

2 log T

log T )8 T
log T

√

As a result, log(1−γk )

log γk

≤ O(T ) and

T(cid:80)

t=1

1
1−γ t

k

≤ O(T ).
(cid:17) ≤ O(max{log T ,
t ) − ft (zt )
ft (θγk

Since using γk does not increase the order when used in
place of γ ∗ , we get

T(cid:88)

t=1

(cid:16)

√

T V })

which combining with the result of Lemma 3 completes
the proof.

Proof of Proposition 1:

Proof. Since strongly convex problem with bounded gra-
dient is also exp-concave due to Lemma 4 shown in the
next section, we will only consider the strongly convex
problem.
For the case when V = 0, Rd reduces to the static re-
gret Rs , which has the lower bound O(log T ) as shown in
[20].
Let us now consider the case when V > 0. The analy-
sis is inspired by [10]. We will use ft (θ) = (θ − t )2
as the special case to show the lower bound. Here T
1 is
a sequence of independently generated random variables
from {−2σ, 2σ} with equal probabilities. For the dynamic
regret Rd =

T(cid:80)

t=1

ft (θt ) − min

zT

1 ∈SV

T(cid:80)

t=1

ft (zt ) ≥ T(cid:80)

t=1

ft (θt ) −

18

A PR E PR IN T - NOV EMB ER 22 , 2019

T(cid:80)
T(cid:80)

t=1

ft (zt ), where SV = {zT
1 :
ft (θt ) − T(cid:80)
E[−2θt t + 3

T(cid:80)

t=2

(cid:107)zt − zt−1(cid:107) ≤ V }, and

zt = 1

2 t . As a result, the expected value of

T(cid:80)

t=1

ft (θt ) −

t=1

ft (zt ) is E[
t )] ≥ T(cid:80)

T(cid:80)

t=1

t=1
4 2

ft (zt )] = E[

T(cid:80)

t=1

(θ2

t −

2θt t + 3

4 2

t=1

t ] = 3σ2T . This implies
that Rd ≥ 3σ2T . For the path length,
(cid:107)zt − zt−1(cid:107) ≤
and γ0 ∈ (0, 1). Then
4−γ0 . Then

T(cid:80)

t=2

2σT . Let us set σ = T

− 2(1−γ0 )
4−γ0

V = 2σT = 2T
4−γ0 and (V T )
2 ≥ 3T

2+γ0

γ0

2 = 2

γ0

2 T
4−γ0 ≥ 0. In

3γ0

Rd − 3√
other words, Rd ≥ O

2

(V T )

γ0

3γ0

4−γ0 − 3√
2

2

γ0

2 T

3γ0

(cid:16)

(V T )

γ0
2

(cid:17)

, ∀γ0 ∈ (0, 1) with

V = 2T

2+γ0
4−γ0 .

In summary, we have that there always exist a exist a se-
quence of loss functions f T
1 and a comparison sequence
1 such that

Rd ≥ max{O(log T ), O(cid:0)(V T )
(cid:107)zt − zt−1(cid:107) ≤ V = O(T
2 (cid:1)}, ∀γ0 ∈ (0, 1)
zT
4−γ0 ) and

T(cid:80)

t=2

2+γ0

γ0

Online Least-Squares Optimization Consider the on-

line least-squares problem with:

ft (θ) =

1
2

(cid:107)yt − At θ(cid:107)2

(45)
where At ∈ Rm×n , AT
t At has full rank with lI (cid:22)
t At (cid:22) uI , and yt ∈ Rm comes from a bounded set
with (cid:107)yt(cid:107) ≤ D .
In the main paper, we analyzed the dynamic regret
of discounted recursive least squares against compari-
son sequences z1 , . . . , zT with a path length constraint
t=2 (cid:107)zt − zt−1(cid:107) ≤ V . Additionally, we analyzed
the trade-off between static and dynamic regret of a
gradient descent rule with comparison sequence θ∗
argminθ∈S ft (θ). In this appendix, we analyze the trade-
off between static regret and dynamic regret with compar-
ison sequence θ∗
t achieved by discounted recursive least
squares. We will see that the discounted recursive least
squares achieves trade-offs depend on the condition num-
ber, δ = u/l. In particular, low dynamic regret is only
guaranteed for low condition numbers.
Recall that discounted recursive least squares corresponds
to Algorithm 1 running with a full Newton step and η = 1.
In this case, Pt =
t At , and the update rule can be written more explicitly
as

AT

(cid:80)T

t =

t(cid:80)

i=1

γ i−1AT

t+1−iAt+1−i = γPt−1 +

AT

θt+1 =

(cid:16) t(cid:88)

i=1

γ i−1AT

t+1−iAt+1−i

(cid:17)−1(cid:16) t(cid:88)

i=1

γ i−1AT

t+1−i yt+1−i

(cid:17)

(46)

The above update rule can be reformulated as:

θt+1 = θt − P −1
t ∇ft (θt ).

(47)

Before we analyze dynamic and static regret for the
update (47), we ﬁrst show some supporting results for
(cid:107)yt − Atx(cid:107) and (cid:107)∇ft (x)(cid:107), where x ∈ {v |v = β θt +
Lemma 7. Let θt be the result of Eq.(47), and θ∗

(1 − β )θ∗
t , β ∈ [0, 1]}.
argmin ft (θ). For x ∈ {v |v = β θt + (1 − β )θ∗
t , β ∈
t =
[0, 1]}, If (cid:107)yt(cid:107) ≤ D , then (cid:107)yt − Atx(cid:107) ≤ (u/l + 1)D .
(cid:112)σ1 (AT
t At ) ≤ √
(cid:107)β θt + (1 − β )θ∗
t (cid:107) ≤ β (cid:107)θt(cid:107) + (1 − β ) (cid:107)θ∗

Proof. (cid:107)yt − Atx(cid:107) ≤ (cid:107)At(cid:107)2 (cid:107)x(cid:107) + (cid:107)yt(cid:107), and (cid:107)At(cid:107)2 =
u. For (cid:107)x(cid:107), we have (cid:107)x(cid:107) =
(cid:17)−1(cid:16) t−1(cid:80)
t (cid:107).
(cid:107)θt(cid:107),
term
be
upper

For

the

(cid:107)θt(cid:107)

=

(cid:13)(cid:13)(cid:13)(cid:13)(cid:16) t−1(cid:80)
(cid:13)(cid:13)(cid:13)(cid:13)(cid:16) t−1(cid:80)
(cid:13)(cid:13)(cid:13)(cid:13)(cid:16) t−1(cid:80)

i=1

γ i−1AT

t−iAt−i

i=1

γ i−1AT

t−i yt−i

(cid:17)(cid:13)(cid:13)(cid:13)(cid:13),

which

can

bounded

by

i=1

γ i−1AT

t−iAt−i

(cid:17)−1(cid:13)(cid:13)(cid:13)(cid:13)2
(cid:17)−1(cid:13)(cid:13)(cid:13)(cid:13)2

(cid:13)(cid:13)(cid:13)(cid:13)(cid:16) t−1(cid:80)

i=1

γ i−1AT

t−i yt−i

(cid:17)(cid:13)(cid:13)(cid:13)(cid:13).

Then we upper bound these two terms individually.

i=1

γ i−1AT

t−iAt−i

=

1
γ i−1AT
t−iAt−i )
1−γ t−1

σn (

t−1(cid:80)

i=1

.

Since lI (cid:22) AT
t−1(cid:80)
t−1(cid:80)

t−iAt−i (cid:22) uI ,
1−γ lI (cid:22)
1−γ uI .

i=1

γ i−1AT
t−iAt−i )
γ i−1AT

(cid:22)

1−γ t−1

Thus,

σn (

i=1

t−iAt−i ) ≥ l 1−γ t−1

1−γ , which results in
≤ 1−γ

(cid:13)(cid:13)(cid:13)(cid:13)(cid:16) t−1(cid:80)
(cid:13)(cid:13)(cid:13)(cid:13)(cid:16) t−1(cid:80)

i=1

γ i−1AT

t−iAt−i

(cid:17)−1(cid:13)(cid:13)(cid:13)(cid:13)2
(cid:13)(cid:13)(cid:13)(cid:13)(cid:16) t−1(cid:80)
(cid:17)(cid:13)(cid:13)(cid:13)(cid:13) ≤ t−1(cid:80)

l(1−γ t−1 ) .

For
t−1(cid:80)

the

term

i=1

γ i−1AT
γ i−1 (cid:13)(cid:13)AT
γ i−1 (cid:13)(cid:13)AT
γ i−1AT
(cid:13)(cid:13)2 (cid:107)yt−i (cid:107) ≤ 1−γ t−1
t (cid:107) = (cid:13)(cid:13)(AT
(cid:13)(cid:13)(AT
t At )−1(cid:13)(cid:13)2
l D .
t At )−1AT
l D
and (cid:107)yt − Atx(cid:107) ≤ (cid:107)At(cid:107)2 (cid:107)x(cid:107) + (cid:107)yt(cid:107) ≤ (u/l + 1)D .

t−i yt−i

(cid:17)(cid:13)(cid:13)(cid:13)(cid:13), we

have

i=1

t−i yt−i

i=1

t−i yt−i

(cid:13)(cid:13) ≤

i=1

t−i

1−γ

√

uD . Then we have

(cid:107)θt(cid:107) ≤ √

u

For (cid:107)θ∗
t (cid:107), we have (cid:107)θ∗

t yt

(cid:13)(cid:13) ≤

(cid:13)(cid:13)AT

t

(cid:13)(cid:13)2 (cid:107)yt(cid:107) ≤ √

u

l D . Thus, (cid:107)x(cid:107) ≤ √

u

Corollary 4. Let θt be the result of Eq.(47) and θ∗
[0, 1]}, we have (cid:107)∇ft (x)(cid:107) ≤ √
Proof. For
(cid:107)∇ft (x)(cid:107), we

argmin ft (θ). For x ∈ {v |v = β θt + (1 − β )θ∗
t , β ∈
t =
u(u/l + 1)D .
(cid:13)(cid:13)2 (cid:107)Atx − yt(cid:107)
(cid:107)∇ft (x)(cid:107)
t At (cid:22) uI .

have

=

(cid:13)(cid:13)AT

t Atx − AT

t yt

(cid:13)(cid:13)

≤

(cid:13)(cid:13)AT

t

≤

√

u(u/l + 1)D , where the second inequality is due
to Lemma 7 and the assumption of AT

19

A PR E PR IN T - NOV EMB ER 22 , 2019

Moreover, we need to obtain the relationship between
t and θt − θ∗
t as another necessary step to get
the dynamic regret.
t be the solution to ft (θ) in Eq.(45).
When we use the discounted recursive least-squares up-
date in Eq.(47), the following relationship is obtained:
(cid:17)−1

θt+1 − θ∗

Lemma 8. Let θ∗

= (cid:0)I − γ−1P −1
θt+1 − θ∗
=
I + γ−1P −1

t

t−1AT
t−1AT
t At

t (I + Atγ−1P −1
(θt − θ∗
t )

t−1AT

t )−1At

(cid:1)(θt − θ∗
t )

(cid:16)

Proof. If we set Φt =
t yt , then according to the update of θt+1 in Eq.(46), we

t(cid:80)

i=1

γ i−1AT

t+1−i yt+1−i = γΦt−1 +

AT
have θt+1 = (AT
t At + γPt−1 )−1 (AT
t yt + γΦt−1 ), which

by the use of inverse lemma can be further reformulated
as:

θt+1 =

(cid:16)

γ−1P −1
t−1 − γ−2P −1
t (I +
Atγ−1P −1
t )−1AtP −1
t yt + γΦt−1
t = θt+1 − (AT
t At )−1AT

t−1AT
t−1

t−1AT

(cid:17)(cid:0)AT

(cid:1)

(48)
t yt , we have:

Then for θt+1 − θ∗

= (cid:0)I − γ
θt+1 − θ∗
−1P

t

−1

t−1AT

t (I + At γ

−1P

−1

t−1AT

t )

−1At

(cid:1)
(cid:125)

(cid:124)
(cid:124)

(cid:123)(cid:122)

1

θt + γ

−1P
−1 (cid:1)AT
−1AtP
t−1 − (AT
t At )

−1

t−1AT
t yt

(cid:124)

(cid:123)(cid:122)

(cid:125)
(cid:125)

2.1

− (cid:0)γ

−2P

−1

t−1AT

t (I + At γ

−1P

−1

t−1AT

t )

−1

t yt

(cid:123)(cid:122)

2.2

(49)

We want

to prove

2.1 + 2.2 = 1 (−θ∗

t ) =

1 (−(AT
t At )−1AT
t yt ) = 3 .
Since A(I + BA)−1B = AB (I + AB )−1 = (I +

AB )−1AB , for any compatible matrix A and B , we have:

3

= −(cid:2)I − γ−1P
= −(cid:2)I − (I + γ−1P
= −(cid:2)(AT
t (I + At γ−1P
t )−1At
t At )−1 γ−1P
t At )−1 − (I + γ−1P
t At )−1 γ−1P

−1

t−1AT
t−1AT

−1

t−1AT
t−1AT
t At
t−1

(cid:3)(AT
(cid:3)(AT
t At )−1AT
(cid:3)AT
t At )−1AT

t yt
t yt

−1

−1

−1

t−1AT

−1

t yt

(50)

Also, for any compatible P , we have (I + P )−1 =

I − (I + P )−1P . Then (I + γ−1P −1
= −(cid:2)(AT
t At )−1 =
− (I
+ γ−1P −1
t At )−1γ−1P −1
I
(cid:3)AT
t At .
t At )−1 − γ−1P −1
t−1 + (I +
t At )−1γ−2P −1
t AtP −1
t yt .
(I + γ−1P −1
t At )−1γ−2P −1
t AtP −1
γ−2P −1
t (I + Atγ−1P −1
t )−1AtP −1
t = (cid:0)I − γ−1P −1
(cid:1)(θt − θ∗
t = (cid:0)I + γ−1P −1
Atγ−1P −1
t )−1At
as θt+1 − θ∗
(θt − θ∗
t ).

t−1AT

t−1AT

t−1AT

Then

3

γ−1P −1

t−1AT

t−1AT

t−1

Com-
to prove

pared with

2.1 + 2.2 , we are left

t−1AT

t−1AT
t−1AT

t−1

=

t−1AT

t−1 , which

is always true.
As a result, we have θt+1 − θ∗
(cid:1)−1
t ), which can be simpliﬁed

t−1AT

t (I +

t−1AT

t−1AT
t At

Corollary 5. Let θ∗

t be the solution to ft (θ) in Eq.(45).
When we use the discounted recursive least-squares up-
date in Eq.(47), the following relation is obtained:
(cid:17)−1

(cid:107)θt+1 − θ∗

t (cid:107) ≤ (cid:112) u

l

uγ+l(1−γ ) (cid:107)θt − θ∗
uγ

t (cid:107)

Proof. From Lemma 8 we know that

θt+1 − θ∗
t =

(cid:16)

I + γ−1P −1

t−1AT
t At

(θt − θ∗
t )

which can be reformulated as:

θt+1−θ

∗
(cid:107)θt+1 − θ∗
(cid:13)(cid:13)(cid:13)P 1/2

t = P

−1/2

t−1 (I+γ

−1P

−1/2
t−1 AT

t AtP

−1/2

t−1 )

−1P 1/2

t−1 (θt−θ
t )

∗

which gives us the following inequality:

t (cid:107)

≤ (cid:13)(cid:13)(cid:13)P

−1/2
t−1

(cid:13)(cid:13)(cid:13)2
(cid:13)(cid:13)(cid:13)2

(cid:13)(cid:13)(cid:13)(I + γ−1P

−1/2
t−1 AT

t AtP

−1/2

t−1 )−1(cid:13)(cid:13)(cid:13)2
(cid:107)θt − θ∗
Then we will upper bound the terms on the right-hand side
individually.
t−1(cid:80)
Since lI (cid:22) AT

t−1

t (cid:107)

t−iAt−i (cid:22) uI , 1−γ t−1

1−γ lI (cid:22) Pt−1 =

i=1

γ i−1AT

t−iAt−i (cid:22) 1−γ t−1
−1/2
t−1

1−γ uI .

For
the term
(cid:113) 1−γ
For the term

(cid:13)(cid:13)(cid:13)P
(cid:13)(cid:13)(cid:13)2

(cid:13)(cid:13)(cid:13)2

, we have

(cid:13)(cid:13)(cid:13)P
(cid:13)(cid:13)(cid:13)P

−1/2
t−1
−1/2
t−1
(cid:113) 1−γ t−1
−1/2

(cid:13)(cid:13)(cid:13)2
(cid:13)(cid:13)(cid:13)2

=

1√

σn (Pt−1 )
1−γ t−1 .

. Since σn (Pt−1 ) ≥ 1−γ t−1
1−γ l,
(cid:13)(cid:13)(cid:13)P 1/2
(cid:13)(cid:13)(cid:13)P 1/2
= (cid:112)σ1 (Pt−1 ).
(cid:13)(cid:13)(cid:13)P 1/2
Since σ1 (Pt−1 ) ≤ 1−γ t−1
(cid:13)(cid:13)(cid:13)(I + γ−1P
1−γ u,
u
(cid:13)(cid:13)(cid:13)(I + γ−1P
t AtP
1/σn (I +γ−1P
σn (I +γ−1P
t AtP
t−1 ).
t AtP
t−1 ),
1 + σn (γ−1P
t AtP
t−1 )σn (AT
t At )σn (P
t−1 ).
and σ1 (Pt−1 ) ≤
t−1 ) ≥
σn (AT
t At )
t−1 ) ≥ l
t AtP

≤

1√

l

t−1

, we have

t−1

(cid:13)(cid:13)(cid:13)2

t−1
−1/2
t−1 AT
−1/2
−1/2
t−1 AT
−1/2
−1/2
−1/2
−1/2

(cid:13)(cid:13)(cid:13)2

≤ √

t−1 )−1(cid:13)(cid:13)(cid:13)2
1−γ .
t−1 )−1(cid:13)(cid:13)(cid:13)2
For
the
term
it
is
equal
to
t−1 ), which
is
lower
(cid:113) 1−γ
1−γ u, we have σn (P
1√
t−1 )−1(cid:13)(cid:13)(cid:13)2
Together with
l, we
have
1−γ t−1 , which results in
.
Combining the above three terms’
have (cid:107)θt+1 − θ∗

For

the

term

t AtP

,

we

have

=

−1/2
t−1 AT
−1/2
t−1 AT
−1/2
t−1 AT

bounded by 1 + γ−1σn (P
Since σn (P

−1/2

−1/2

t−1 ) =

1√

σ1 (Pt−1 )
−1/2

1−γ t−1

u

1−γ t−1 .

≥

(cid:13)(cid:13)(cid:13)(I + γ−1P
σn (P
t AtP

−1/2
t−1 AT
−1/2
t−1 AT

−1/2

u
−1/2

1−γ

≤

1
1+γ−1 l

u

1−γ
1−γ t−1

inequalities, we

t (cid:107) ≤ (cid:112) u

l

u(γ−γ t )+l(1−γ ) (cid:107)θt − θ∗
u(γ−γ t )

t (cid:107) ≤

(cid:112) u

l

uγ+l(1−γ ) (cid:107)θt − θ∗
uγ

t (cid:107).

20

A PR E PR IN T - NOV EMB ER 22 , 2019

Now we are ready to present the dynamic regret for the
general recursive least-squares update:
and δ = u/l ≥ 1 be the condition number. When using
t be the solution to ft (θ) in Eq.(45)
the discounted recursive least-squares update in Eq.(47)
with γ <
upper bound the dynamic regret:

Theorem 9. Let θ∗

δ3/2−δ+1 and ρ = (cid:112) u
1
l

uγ

uγ+l(1−γ ) < 1, we can
t−1 (cid:107) (cid:1)
∗

Rd ≤ √

u(u/l + 1)D

1
1 − ρ

(cid:0) (cid:107)θ1 − θ

∗

1 (cid:107)++

T(cid:88)

t=2

(cid:107)θ

∗

t − θ

Proof. The proof follows the similar steps in the proof
of Theorem 2. First, we use the Mean Value Theo-
rem to get ft (θt ) − ft (θ∗
t (cid:107), where x ∈ {v |v = β θt + (1 −
t , β ∈ [0, 1]}. According to Corollary 4, (cid:107)∇ft (x)(cid:107) ≤
u(u/l + 1)D . As a result,
(cid:107)θt − θ∗
t (cid:107).
to
upper
bound
the
(cid:107)θt − θ∗
(cid:107)θ1 − θ∗
t−1 − θ∗
(cid:107)θ1 − θ∗
t − θ∗
(cid:13)(cid:13). According to Corol-
t − θ∗
lary 5, (cid:107)θt+1 − θ∗
t (cid:107) ≤ ρ (cid:107)θt − θ∗
t (cid:107).
(cid:107)θt − θ∗
(cid:13)(cid:13), which
(cid:107)θ1 − θ∗
(cid:107)θt − θ∗
t − θ∗
can be reformulated as
(cid:13)(cid:13)). Then Rd =
(cid:107)θt − θ∗
t − θ∗

t ) = ∇ft (x)T (θt − θ∗
t ) ≤
(cid:0)ft (θt ) − ft (θ∗

(cid:107)∇ft (x)(cid:107) (cid:107)θt − θ∗
β )θ∗

√
√

T(cid:80)

t=1

t )(cid:1) ≤

u(u/l + 1)D

T(cid:80)
T(cid:80)

t=1

Now we
(cid:13)(cid:13)θt − θ∗
(cid:107)θt − θ∗
t (cid:107).
T −1(cid:80)

need

term

T(cid:80)
T(cid:80)
T(cid:80)

t=1

t=1

t (cid:107)

=

1 (cid:107) +
1 (cid:107) +
1 (cid:107) +

t=2

t−1 + θ∗
(cid:107)θt+1 − θ∗
t (cid:107) +
(cid:107)θt+1 − θ∗
t (cid:107) +

t

(cid:13)(cid:13)
T(cid:80)
(cid:13)(cid:13)θ∗
T(cid:80)
(cid:13)(cid:13)θ∗

≤
(cid:13)(cid:13) ≤ (cid:107)θ1 − θ∗

t=1

t=2

t−1

t=1

t=2

t−1

T(cid:80)
(cid:13)(cid:13)θ∗

t=1

t (cid:107) ≤

1 (cid:107) + ρ

T(cid:80)

t=1

t (cid:107) +

T(cid:80)
T(cid:80)

t=2

t−1

T(cid:80)

t=1

t (cid:107) ≤ 1

(cid:0)ft (θt ) − ft (θ∗
1−ρ ((cid:107)θ1 − θ∗
1 (cid:107)+

+

T(cid:80)

t=2

(cid:13)(cid:13)θ∗

t−1

t=1

t )(cid:1) ≤

√

1−ρ ((cid:107)θ1 − θ∗
1 (cid:107) + +
u(u/l + 1)D 1

T(cid:80)

t=2

(cid:13)(cid:13)θ∗

t − θ∗

t−1

(cid:13)(cid:13)).

In the above Theorem 9, the valid range of γ is in
(0, 1/(δ3/2 − δ + 1)). Let us now examine the require-
ment of γ to achieve the sub-linear static regret:
Theorem 10. Let θ∗ be the solution to min
in Eq.(47) with 1 − γ = 1/T α , α ∈ (0, 1), we can upper
When using the discounted recursive least-squares update
bound the static regret:

T(cid:80)

t=1

ft (θ).

Rs ≤ O(T 1−α )

Proof. The proof follows the analysis of the online New-
ton method [13]. From the update in Eq.(47), we have

θt+1 − θ∗ = θt − θ∗ − P −1
t ∇ft (θt ) and Pt (θt+1 − θ∗ ) =
have (θt+1 − θ∗ )T Pt (θt+1 − θ∗ ) = (θt − θ∗ )T Pt (θt −
θ∗ ) − 2∇ft (θt )T (θt − θ∗ ) + ∇ft (θt )T P −1
t ∇ft (θt ).
2 ∇ft (θt )T P −1
t ∇ft (θt ) + 1
2 (θt − θ∗ )T Pt (θt − θ∗ ) −
2 (θt+1 − θ∗ )T Pt (θt+1 − θ∗ ) ≤ 1
2 ∇ft (θt )T P −1
t ∇ft (θt )+
2 (θt − θ∗ )T Pt (θt − θ∗ ) − 1
2 (θt+1 − θ∗ )T γPt (θt+1 − θ∗ ).
∇ft (θt )T (θt − θ∗ ) ≤ T(cid:80)
2 ∇ft (θt )T P −1
t ∇ft (θt ) +
2 (θ1 − θ∗ )T P1 (θ1 − θ∗ ) +
2 (θt − θ∗ )T (Pt −
γPt−1 )(θt − θ∗ ) − 1
2 (θT +1 − θ∗ )T γPT (θT +1 −
2 ∇ft (θt )T P −1
t ∇ft (θt ) + 1
2 (θ1 − θ∗ )T (P1 −
AT
1 A1 )(θ1 − θ∗ ) +
2 (θt − θ∗ )T AT
t At (θt − θ∗ ).
1 A1 and ft (θt ) − ft (θ∗ ) = ∇ft (θt )T (θt −
θ∗ ) − 1
2 (θt − θ∗ )T AT
ft (θt ) − ft (θ∗ )
(cid:16)∇ft (θt )T (θt − θ∗ ) − 1
2 (θt − θ∗ )T AT
t At (θt − θ∗ )
2 ∇ft (θt )T P −1
t ∇ft (θt )
2 (At θt − yt )T AtP −1

Pt (θt − θ∗ ) − ∇ft (θt ). Multiplying the two equalities, we

After the reformulation, we have ∇ft (θt )T (θt − θ∗ ) =

1

1

1

Summing the above inequality from t = 1 to T , we have:

T(cid:80)

t=1

t=1

1

1

T(cid:80)

t=2

1

θ∗ ) ≤ T(cid:80)

t=1

1

T(cid:80)

t=1

1

Since P1 = AT

t At (θt − θ∗ ), we reformulate the

above inequality as:

T(cid:80)
(cid:16)
T(cid:80)
T(cid:80)

t=1

(cid:17)

=

t=1

(cid:17)

≤ T(cid:80)
≤ T(cid:80)

t=1

1

=

t=1

1

t AT
−1/2
t

t (At θt − yt )
) (cid:107)At θt − yt(cid:107)2

t=1

1

2 σ1 (P

−1/2
t

AT
t AtP

(51)

Since σ1 (P

−1/2
σn (Pt ) σ1 (AT
1
t

AT
t AtP

−1/2
t

) ≤ σ1 (P −1

t

)σ1 (AT
t At ) =

t At ). From the proof of Corollary 5 we
know that σn (Pt ) ≥ 1−γ t
t At ) ≤ u. Then
1−γ t . As a result, we have

1−γ l and σ1 (AT
) ≤ u

σ1 (P

−1/2
t

AT
t AtP
ft (θt ) − ft (θ∗ )

−1/2
t

l

1−γ

T(cid:80)

t=1

(cid:16)

(cid:17) ≤ T(cid:80)

t=1

1
2

u
l

1−γ

1−γ t (cid:107)At θt − yt(cid:107)2

≤ T(cid:80)

t=1

1
2

u
l

1−γ

1−γ t (u/l + 1)2D2
≤ O(T 1−α )

(52)
where the second inequality is due to Lemma 7 and the

third inequality is due to the fact that

T(cid:80)

t=1

1/(1 − γ t ) ≤

O(T ) as shown in the proof of Theorem 3.

Recall
that
the valid range of γ in Theorem 9 is
(0, 1/(δ3/2 − δ + 1)), while having sub-linear static re-

21

A PR E PR IN T - NOV EMB ER 22 , 2019

gret requires γ = T α−1
T α . Although for some speciﬁc T ,
there might be some intersection. In general, these two
are contradictory. However, as discussed in the main body
of the paper, more ﬂexible trade-offs between static and
dynamic regret can be achieved via the gradient descent
rule.

22

