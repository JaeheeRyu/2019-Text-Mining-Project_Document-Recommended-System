TREPAN Reloaded: A Knowledge-driven Approach to
Explaining Black-box Models

Roberto Confalonieri1 and Tillman Weyde2 and Tarek R. Besold1 and Ferm´ın Moscoso del Prado Mart´ın1

9
1
0
2

v
o

N

1
2

]
I

A

.

s

c

[

2
v
2
6
3
8
0

.

6
0
9
1

:

v

i

X

r

a

Abstract. Explainability in Artiﬁcial Intelligence has been revived
as a topic of active research by the need of conveying safety and
trust to users in the ‘how’ and ‘why’ of automated decision-making.
Whilst a plethora of approaches have been developed for post-hoc
explainability, only a few focus on how to use domain knowledge,
and how this inﬂuences the understandability of global explanations
from the users’ perspective. In this paper, we show how ontologies
help the understandability of global post-hoc explanations, presented
in the form of symbolic models. In particular, we build on TR E PAN,
an algorithm that explains artiﬁcial neural networks by means of de-
cision trees, and we extend it to include ontologies modeling domain
knowledge in the process of generating explanations. We present the
results of a user study that measures the understandability of deci-
sion trees using a syntactic complexity measure, and through time
and accuracy of responses as well as reported user conﬁdence and
understandability. The user study considers domains where explana-
tions are critical, namely, in ﬁnance and medicine. The results show
that decision trees generated with our algorithm, taking into account
domain knowledge, are more understandable than those generated by
standard TR E PAN without the use of ontologies.

1

INTRODUCTION

In recent years, explainability has been identiﬁed as a key factor
for the adoption of AI systems in a wide range of contexts [6, 16,
12, 21, 29, 23]. The emergence of intelligent systems in self-driving
cars, medical diagnosis, insurance and ﬁnancial services among oth-
ers has shown that when decisions are taken or suggested by auto-
mated systems it is essential for practical, social, and increasingly
legal reasons that an explanation can be provided to users, develop-
ers or regulators. As a case in point, the European Union’s General
Data Protection Regulation (GDPR) stipulates a right to “meaning-
ful information about the logic involved”—commonly interpreted as
a ‘right to an explanation’—for consumers affected by an automatic
decision [24].3
The reasons for equipping intelligent systems with explanation ca-
pabilities are not limited to user rights and acceptance. Explainabil-
ity is also needed for designers and developers to enhance system
robustness and enable diagnostics to prevent bias, unfairness and dis-
crimination [22], as well as to increase trust by all users in why and

1 Telef ´onica Innovaci ´on Alpha, email: {roberto.confalonieri,

tarek.besold, fermin.moscoso}@telefonica.com

2 Dept. of Computer Science, City, University of London, email:

t.e.weyde@city.ac.uk

3 Regulation (EU) 2016/679 on the protection of natural persons with re-
gard to the processing of personal data and on the free movement of such
data, and repealing Directive 95/46/EC (General Data Protection Regula-
tion) [2016] OJ L119/1.

how decisions are made. Against that backdrop, increasing efforts
are directed towards studying and provisioning explainable intelli-
gent systems, both in industry and academia, sparked by initiatives
like the DARPA Explainable Artiﬁcial Intelligence Program (XAI),
and carried by a growing number of scientiﬁc conferences and work-
shops dedicated to explainability.
While interest in XAI had subsided together with that in expert
systems after the mid-1980s [5, 36], recent successes in machine
learning technology have brought explainability back into the focus.
This has led to a plethora of new approaches for local and global post-
hoc explanations of black-box models [14], for both autonomous and
human-in-the-loop systems, aiming to achieve explainability with-
out sacriﬁcing system performance. Only a few of these approaches,
however, focus on how to integrate and use domain knowledge to
drive the explanation process (e.g., [33, 27]) or to measure the un-
derstandability of explanations of black-box models (e.g., [30]). For
that reason an important foundational aspect of explainable AI re-
mains hitherto mostly unexplored: Can the integration of domain
knowledge as, e.g., modeled by means of ontologies, help the un-
derstandability of interpretable machine learning models?
To tackle this research question, we propose a neural-symbolic
learning approach based on TR E PAN [8], an algorithm devised in or-
der to explain trained artiﬁcial neural networks by means of decision
trees, and we extend it to take into account ontologies in the expla-
nation generation process. In particular, we modify the logic of the
algorithm when choosing split nodes, to prefer features associated
with more general concepts in a domain ontology. Having explana-
tions bounded to structured knowledge, in the form of ontologies,
conveys two advantages. First, it enriches explanations (or the el-
ements therein) with semantic information, and facilitates effective
knowledge transmission to users. Second, it supports the customi-
sation of the levels of speciﬁcity and generality of explanations to
speciﬁc user proﬁles [15].
In this paper, we focus on the ﬁrst advantage, and on measuring the
impact of the ontology on the perceived understandabilty of surrogate
decision trees. To evaluate our approach, we designed and conducted
an experiment to measure the understandability of decision trees in
domains where explanations are critical, namely the ﬁnancial and
medical domain. Our study shows that decision trees generated by
our modiﬁed TR E PAN algorithm taking domain knowledge into ac-
count are more understandable than those generated without the use
of domain knowledge. Crucially, this enhanced understandability of
the resulting trees is achieved with little compromise on the accuracy
with which the resulting trees replicate the behaviour of the original
neural network model.
The remainder of the paper is organised as follows. After introduc-
ing TR E PAN, and the notion of ontologies (Section 2), we present our

 
 
 
 
 
 
Algorithm 1 Trepan(Oracle,T raining ,F eatures) .

Priority queue Q ← ∅
Tree T ← ∅
use Oracle to label examples in T raining
enqueue root node into Q

while nr internal nodes < size limit do

pop node n from Q
generate examples for n
use F eatures to build set of candidate splits

use Examples and Oracle to determine B est split

add n to T
for element c ∈ B est split do
add c as child of n
if c is not a leaf according to the Oracle then
enqueue node c into Q with negative
information gain as priority

end if
end for
end while

Return T

revised version of the algorithm that takes into account ontologies in
the decision tree extraction (Section 3). In Section 4, we propose
how to measure understandability of decision trees from a technical
and a user perspective. Section 5 reports and analyses the results of
our experiment. After discussing our approach (Section 6), Section 7
situates our results in the context of related contributions in XAI. Fi-
nally, Section 8 concludes the paper and outlines possible lines of
future work.

2 PRELIMINARIES

In this section, we present the main foundations of our approach,
namely, the TR E PAN algorithm and ontologies.

2.1 The TRE PAN algorithm

TR E PAN is a tree induction algorithm that recursively extracts de-
cision trees from oracles, in particular from feed-forward neural
networks [8]. The original motivation behind the development of
TR E PAN was to approximate a neural network by means of a sym-
bolic structure that is more interpretable than a neural network clas-
siﬁcation model. This was in the context of a wider interest in knowl-
edge extraction from neural networks (see [33, 9] for an overview).
The pseudo-code for TR E PAN is shown in Algorithm 1. TR E PAN
differs from conventional inductive learning algorithms as it uses an
oracle to classify examples during the learning process. It generates
new examples by sampling from distributions over the given exam-
ples and constraints, so that the amount of training data used to select
splitting tests and to label leaves does not decrease with the depth of
the tree. It expands a tree in a best-ﬁrst manner by means of a priority
queue by entropy, that prioritises nodes that have greater potential for
improvement. Further details of the algorithm can be found in [8].
TR E PAN stops the tree extraction process using two criteria: all
nodes do not need to be further expanded because their entropy is
low (they contain almost exclusively instances of a single class), or
a predeﬁned limit of the tree size (the number of nodes) is reached.
Whilst TR E PAN was designed to explain neural networks as the ora-
cle, it is a model-agnostic algorithm and can be used to explain any
other classiﬁcation model.

Entity (cid:118) (cid:62)
AbstractObject (cid:118) Entity
PhysicalObject (cid:118) Entity
Quality (cid:118) Entity
LoanApplicant (cid:118) Person (cid:117) ∃hasApplied.Loan
Domain(hasApplied) = Person

,
,
,
,
,
,

Person (cid:118) PhysicalObject
Loan (cid:118) AbstractObject
Gender (cid:118) Quality
Male (cid:118) Gender
Female (cid:118) Gender
Range(hasApplied) = Loan

Figure 1: An ontology excerpt for the loan domain.

In this paper, our objective is to improve the understandability of
the decision trees extracted by TR E PAN. To this end, we extend the
algorithm to take into account an information content measure, that
is derived using ontologies, and computed using the idea of concept
reﬁnement, as detailed below. In order to evaluate the performance
of both the original and extended TR E PAN algorithms, we measure
the accuracy and the ﬁdelity of the resulting decision trees. Accuracy
is deﬁned as the percentage of test-set examples that are correctly
classiﬁed. In contrast, ﬁdelity is deﬁned as the percentage of test-set
examples on which the classiﬁcation made by a tree agrees with that
provided by its neural-network counterpart. Notice that the crucial
measure for assessing the quality of the reconstructed tree is the ﬁ-
delity, as this is the direct measure of how well the tree’s behaviour
mimics the original neural network.

2.2 Ontologies

An ontology is a set of formulae in an appropriate logical language
with the purpose of describing a particular domain of interest, such as
ﬁnance or medicine. The precise logic used is not crucial for our ap-
proach as the techniques introduced here apply to a variety of logics.
For the sake of clarity we use description logics (DLs) as well-known
ontology languages. We brieﬂy introduce the DL EL⊥ , a DL allow-
ing only conjunctions, existential restrictions, and the empty concept
⊥. For full details, see [2, 1]. EL⊥ is widely used in biomedical on-
tologies for describing large terminologies and it is the base of the
OWL 2 EL proﬁle.
Syntactically, EL⊥ is based on two disjoint sets NC and NR of
concept names and role names, respectively. The set of EL⊥ concepts
is generated by the grammar

C ::= A | C (cid:117) C | ∃R.C ,

where A ∈ NC and R ∈ NR . A TBox is a ﬁnite set of general con-
cept inclusions (GCIs) of the form C (cid:118) D where C and D are con-
cepts. It stores the terminological knowledge regarding the relation-
ships between concepts. An ABox is a ﬁnite set of assertions C (a)
and R(a, b), which express knowledge about objects in the knowl-
edge domain. An ontology is composed by a TBox and an ABox. In
this paper, we focus on the TBox only, thus we will use the terms
ontology and TBox interchangeably.
The semantics of EL⊥ is based on interpretations of the form
I = (∆I , ·I ), where ∆I is a non-empty domain, and ·I is a function
mapping every individual name to an element of ∆I , each concept
name to a subset of the domain, and each role name to a binary rela-
tion on the domain. I satisﬁes C (cid:118) D iff C I ⊆ DI and I satisﬁes
an assertion C (a) (R(a, b)) iff aI ∈ C I ((aI , bI ) ∈ RI ). The in-
terpretation I is a model of the TBox T if it satisﬁes all the GCIs
and all the assertions in T . T is consistent if it has a model. Given
two concepts C and D , C is subsumed by D w.r.t. T (C (cid:118)T D)
if C I ⊆ DI for every model I of T . We write C ≡T D when
C (cid:118)T D and D (cid:118)T C . C is strictly subsumed by D w.r.t. T
(C (cid:64)T D) if C (cid:118)T D and C (cid:54)≡T D .
Figure 1 shows an ontology excerpt modeling concepts and re-
lations relevant to the loan domain. The precise formalisation of the

2

domain is not crucial at this point; different formalisations may exist,
with different levels of granularity. The ontology structures the do-
main knowledge from the most general concept (e.g., Entity) to more
speciﬁc concepts (e.g., LoanApplicant, Female, etc.). The subsump-
tion relation ((cid:118)) induces a partial order among the concepts that can
be built from a TBox T . For instance, the Quality concept is more
general than the Gender concept, and it is more speciﬁc than the
Entity concept.
We will capture the degree of generality (resp. speciﬁcity) of a
concept in terms of an information content measure that is based on
concept reﬁnement. The measure is deﬁned in detail in Section 3
and serves as the basis for the subsequent extension of the TR E PAN
algorithm.

2.3 Concept reﬁnement

The idea behind concept reﬁnement is to make a concept more gen-
eral or more speciﬁc by means of reﬁnement operators. Reﬁnement
operators are well-known in Inductive Logic Programming, where
they are used to learn concepts from examples [35]. Reﬁnement op-
erators for description logics were introduced in [19], and further
developed in [7, 34]. In this setting, two types of reﬁnement op-
erators exist: specialisation reﬁnement operators and generalisation
reﬁnement operators. While the former construct specialisations of
hypotheses, the latter construct generalisations.
In this paper we focus on specialisation operators. A specialisation
operator takes a concept C as input and returns a set of descriptions
that are more speciﬁc than C by taking an ontology into account.
The proposal laid out in this paper can make use of any such oper-
ators (see e.g., [7, 34, 26]). When a speciﬁc reﬁnement operator is
needed, as in the examples and in the experiments, we use the fol-
lowing deﬁnition of specialisation operator based on the downcover
set of a concept C .
Deﬁnition 2.1. Given a Tbox T , and a concept decription C , the
specialisation operator ρT (C ) is deﬁned as follows:

ρT (C ) ⊆ DownCovT (C ).

where DownCovT (C ) is the set of concepts that are more speciﬁc
(or less general) than C :

DownCovT (C ) := {D ∈ sub(T ) | D (cid:118)T C and
(cid:64).D

(cid:48) ∈ sub(T ) with D (cid:64)T D
(cid:48) (cid:64)T C }.
In the above deﬁnition, sub(T ) denotes the union of all the subcon-
cepts in the axioms in T , plus {(cid:62), ⊥}. For any given axiom C (cid:118) D
in T , the set of its subconcepts is sub(C (cid:118) D) = sub(C ) ∪ sub(D);
also, sub(C (a)) = sub(C ). Notice that sub(T ) is a ﬁnite set.
A concept C is specialised by any of its most general specialisa-
tions that belong to sub(T ). Every concept can be specialised into ⊥
in a ﬁnite number of steps.
Deﬁnition 2.2. The unbounded ﬁnite iteration of the reﬁnement op-
erator ρ is deﬁned as:

(cid:91)

i≥0

∗

T (C ) =

ρ

ρiT (C ).

Thus ρ∗
T (C ) is the set of subconcepts of C w.r.t. T . We will de-
note this set by subConcept(C ). Since sub(T ) is a ﬁnite set, the
operator ρ∗
T (C ) is ﬁnite, and it terminates. For a detailed analysis of
properties of reﬁnement operators in DLs we refer to [19, 7].

1.

Example

Let
us
consider
the
concepts Entity,
and
LoanApplicant deﬁned in the ontology in Figure 1. Then:

{Entity, AbstractObject, PhysicalObject,
ρT (Entity)
Quality}; ρ∗
T (Entity) ⊆ sub(T )\{(cid:62)}; ρT (LoanApplicant)
T (LoanApplicant) ⊆ {LoanApplicant, ⊥}.

= ρ∗

⊆

3 TREPAN RELOADED

Our aim is to create decision trees that are more understandable for
humans by determining which features are more understandable for
a user, and assigning priority in the tree generation process according
to increased understandability. Our hypothesis, which we will vali-
date in this paper, is that features are more understandable if they are
associated to more general concepts present in an ontology.
To measure the degree of semantic generality or speciﬁcity of
a concept, we consider its information content [32] as typically
adopted in computational linguistics [28]. There it quantiﬁes the in-
formation provided by a concept when appearing in a context. Clas-
sical information theoretic approaches compute the information con-
tent of a concept as the inverse of its appearance probability in a cor-
pus, so that infrequent terms are considered more informative than
frequent ones.
In ontologies, the information content can be computed either ex-
trinsically from the concept occurrences (e.g., [28]), or intrinsically,
according to the number of subsumed concepts modeled in the on-
tology. Here, we adopt the latter approach. We use this degree of
generality to prioritise features that are more general (thus present-
ing less information content), as our assumption is that the decision
tree becomes more understandable when it uses more general con-
cepts. From a cognitive perspective this appears reasonable, since
more general concepts have been found to be easier to understand
and learn [13], and we test this assumption empirically below.
Deﬁnition 3.1. Given an ontology T , the information content of a
feature Xi is deﬁned as:

1 − log (|subConcepts(Xi )|)
log (|sub(T )|)
0

IC(Xi ) :=

if Xi ∈ sub(T )

otherwise.

where subConcepts(Xi ) is the set of specialisations for Xi , and
sub(T ) is the set of subconcepts that can be built from the axioms in
the TBox T of the ontology (see Section 2.2).
It can readily be seen that the values of IC are smaller for features
associated to more general concepts, and larger for those associated
to more speciﬁc concepts instead.

2.

Example

Let
us
consider
the
concepts Entity,
and
LoanApplicant deﬁned in the ontology in Figure 1 and the
reﬁnements in Example 1. The cardinality of sub(T ) is 13. The car-

where ρiT (C ) is inductively deﬁned as:

ρ0T (C ) = {C },
(C ) = ρjT (C ) ∪ (cid:91)

ρj+1T

C (cid:48)∈ρjT (C )

dinality of subConcepts(Entity) and subConcepts(LoanApplicant)
IC(Entity) = 0.04, and

is 12 and 2 respectively. Then:

IC(LoanApplicant) = 0.73.

Having a way to compute the information content of a feature Xi ,
we now propose to update the information gain used by TR E PAN to
give preference to features with a lower information content.

ρT (C

(cid:48)

), j ≥ 0.

3

Deﬁnition 3.2. The information gain given the information content
IC of a feature Xi is deﬁned as:

(cid:40)

(cid:48)

IG

(Xi , S |IC) :=

(1 − IC(Xi ))IG(Xi , S )
0

if 0 < IC(Xi ) < 1

otherwise.

where IG(Xi , S ) is the information gain as usually deﬁned in the
decision tree literature.
According to the above equation, IG(cid:48) of a feature is decreased by
a certain proportion that varies depending on its information content,
and is set to 0 either when the feature is not present in the ontology
or when its information content is maximal.
Our assumption that using features associated with more general
concepts in the creation of split nodes can enhance the understand-
ability of the tree, is based on users being more familiar with more
general concepts rather than more specialised ones. To validate this
hypothesis we ran a survey-based online study with human partici-
pants. Before proceeding to the details of the study and the results,
as a prerequisite we introduce two measures for the understandabil-
ity of a decision tree—an objective, syntax-based and a subjective,
performance-based one—in the following section.

4 UNDERSTANDABILITY OF DECISION
TREES

Understandability depends not only on the characteristics of the tree
itself, but also on the cognitive load experienced by users in using
the decision model to classify instances, and in understanding the
features in the model itself. However, for practical processing, un-
derstandability of decision trees needs to be approximated by an ob-
jective measure. We compare here two characterisations of the un-
derstandability of decision trees, approaching the topic from these
two different perspectives:
• Understandability based on the syntactic complexity of a decision
tree.
• Understandability based on users’ performances, reﬂecting the
cognitive load in carrying out tasks using a decision tree.

On the one hand, it is desirable to provide a technical characterisation
of understandability that can give a certain control over the process
of generating explanations. For instance, in TR E PAN, experts might
want to stop the extraction of decision trees that do not overcome a
given tree size limit, do have a stable accuracy/ﬁdelity, but have an
increasing syntactic complexity.
Previous work attempting to measure the understandability of
symbolic decision models (e.g., [17]), and decision trees in partic-
ular [25], proposed syntactic complexity measures based on the tree
structure. The syntactic complexity of a decision tree can be mea-
sured, for instance, by counting the number of internal nodes in the
tree or leaves, the number of symbols used in the splits (relevant es-
pecially for m-of-n splits), or the number of branches that decision
nodes have.
For the sake of simplicity, we focus on the combination of two
syntactic measures: the number of leaves n in a decision tree, and
the number of branches b on the paths from the root of the tree to all
the leaves in the decision tree. Based on the results in [25], we deﬁne
the syntactic complexity of a decision tree as:

with α ∈ [0, 1] being a tuning factor that adjusts the weight of n and
b, and k = 5 being the coefﬁcient of the linear regression built using
the results in [25].
On the other hand, the syntactic complexity of decision trees does
not necessarily capture the ease with which actual people can use
the resulting trees. A direct measure of user understandability is how
accurately a user can employ a given decision tree to perform a deci-
sion. An often more precise measure of cognitive difﬁculty in mental
processing is the reaction time (RT) or response latency [11]. RT is
a standard measure used by cognitive psychologists and has even be-
come a staple measure of complexity in the domain of design and
user interfaces [37]. In the following section we describe an experi-
ment measuring the cost of processing in terms of accuracy, and RT
(among other variables) for different types of decision trees.
An additional factor that has to be taken into account is the tree
size. It seems very likely that trees of different sizes, irrespective of
any actual complexity, present more difﬁculties for human under-
standing that are not necessarily linearly related to the increase in
tree size. Therefore, properly understanding the effects on actual un-
derstandability requires explicitly controlling the tree sizes. For our
experiments, we deﬁne three categories of tree sizes based on the
number of internal nodes: small (the number of internal nodes is be-
tween 0 and 10), medium (the number of internal nodes is between
11 and 20), and large (the number of internal nodes is between 21
and 30).

5 EXPERIMENTAL EVALUATION

5.1 Methods

Materials. We used datasets from two different domains to eval-
uate our approach: ﬁnance and medicine. We used the Cleveland
Heart Disease Data Set from the UCI archive4 , and a loan dataset
from Kaggle5 . For each of them, we developed an ontology deﬁn-
ing the main concepts and relevant relations (the heart and loan
ontology contained 29 classes, 66 logical axioms and 28 classes,
65 logical axioms respectively). To extract decision trees using the
TR E PAN and TR E PAN Reloaded algorithm, we trained two artiﬁcial
neural networks implemented in pytorch. The neural networks we
use in our experiments have a single layer of hidden units. The num-
ber of hidden units used for each network is chosen using cross-
validation on the network’s training set, and we use a validation
set to decide when to stop training networks. The accuracy of the
trained neural networks was of 85.98% and 94.65% for the loan
and heart dataset respectively. In total, for each of the neural net-
works, we constructed six decision trees, varying their size (mea-
sured in number of nodes; i.e., small, medium, large), and whether
or not an ontology had been used in generating them. In this manner,
we obtained a total of twelve distinct decision trees (2 domains × 3
sizes × 2 ontology presence values). Figure 2 shows two examples
of distilled decision trees. The (avg.) ﬁdelity of the extracted trees

was of 92.73% (TR E PAN) 92.63% (TR E PAN Reloaded) and 89.23%

(TR E PAN) 88.17% (TR E PAN Reloaded) for the loan and heart dataset
respectively (see also Table 3). Notice that since the trees are post-
hoc explanations of the artiﬁcial neural network, the ﬁdelities of the
distilled trees, rather than their accuracies, are the crucial measure.

U (n, b) := α

n
k

+ (1 − α)

b
k2 .

(1)

5

4 http://archive.ics.uci.edu/ml/datasets/Heart+Disease
https://www.kaggle.com/altruistdelhite04/
loan- prediction- problem- dataset

4

Figure 2: Decision trees of size ‘small’ in the loan domain, extracted without (left) and with (right) a domain ontology. As it can be seen the
features used in the creation of the conditions in the split nodes are different.

(a)

(b)

Procedure. The experiment used two online questionnaires on the
usage of decision trees.The questionnaires contained an introductory
and an experimental phase.
In the introductory phase, subjects were shown a short video about
decision trees, and how they are used for classiﬁcation. In this phase,
participants were asked to provide information on their age, gender,
education, and on their familiarity with decision trees.
The experiment phase was subdivided into two tasks: classiﬁca-
tion, and inspection. Each task starts with an instruction page de-
scribing the task to be performed. In these tasks the participants were
presented with the six trees corresponding to one of the two domains.
In the classiﬁcation task, subjects were asked to use a decision tree
to assign one of two classes to a given case whose features are re-
ported in a table (e.g., Will the bank grant a loan to a male person,
with 2 children, and a yearly income greater than e50.000,00?). In
the inspection task, participants had to decide on the truth value of
a particular statement (e.g., You are a male; your level of education
affects your eligibility for a loan.). The main difference between the
two types of questions used in the two tasks is that the former pro-
vides all details necessary for performing the decision, whereas the
latter only speciﬁes whether a subset of the features inﬂuence the
decision. In these two tasks, for each tree, we recorded:
• Correctness of the response.
• Conﬁdence in the response, as provided on a scale from 1 to 5
(‘Totally not conﬁdent’=1, . . . , ‘Very conﬁdent’=5).
• Response time measured from the moment the tree was presented.
• Perceived tree understandability as provided on a scale from 1 to
5 ( ‘Very difﬁcult to understand’=1, . . . , ‘Very easily understand-
able’=5).

Participants.

63 participants (46 females, 17 males) volunteered
to take part in the experiment via an online survey.6 Of these 34 were
exposed to trees from the ﬁnance domain, and 29 to those in the med-
ical domain. The average age of the participants is 33 (± 12.23) years

6 The participants were recruited among friends and acquaintances of the
authors.

(range: 19 – 67). In terms of educational level their highest level was
a Ph.D. for 28 of them, a Master degree for 9 of them, a Bachelor for
12, and a high school diploma for 14. 47 of the respondents reported
some familiarity with the notion of decision trees, while 16 reported
no such familiarity.

5.2 Results

We ﬁtted a mixed-effects logistic regression model [3] predicting
the correctness of the responses in the classiﬁcation and inspec-
tion tasks. The independent ﬁxed-effect predictors were the syntactic
complexity of the tree, the presence or absence of an ontology in
the tree generation, the task identity (classiﬁcation vs. inspection),
and the domain (ﬁnancial vs. medical), as well as all possible in-
teractions between them, as well as a random effect of the identity
of the participant. A backwards elimination of factors revealed sig-
niﬁcant main effects of the task identity, indicating that responses
were more accurate in the classiﬁcation task than they were in the
inspection (z = −3.00, p = .0027), of the syntactic complex-
ity (z = −3.47, p = .0005), by which more complex tree pro-
duced less accurate responses, and of the presence of the ontology
(z = 3.70, p = .0002), indicating that trees generated using the
ontology indeed produced more accurate responses (Figure 3a). We
did not observe any signiﬁcant interactions or effect of the domain
identity.
We analysed the response times (on the correct responses) us-
ing a linear mixed-effect regression model [3], with the log re-
sponse time as the independent variable. As before, we included
as possible ﬁxed effects the task identity (classiﬁcation vs inspec-
tion), the domain (medical vs ﬁnancial), the syntactic complexity
of the tree, and the presence or absence of ontology in the trees’
generation, as well as all possible interactions between them. In
addition, we also included the identity of the participant as a ran-
dom effect. A step-wise elimination of factors revealed main ef-
fects of task identity (F (1, 593.87) = 20.81, p < .0001), syn-
tactic complexity (F (1, 594.51) = 92.42, p < .0001), ontology
presence (F (1, 594.95) = 51.75, p < .0001), as well as signif-

5

classiﬁcation task.
Finally, we also analysed the user rated understandability rat-
ings using a linear mixed-effect regression model, with the conﬁ-
dence rating as the independent variable. We included as possible
ﬁxed effects the task identity (classiﬁcation vs inspection), the do-
main (medical vs ﬁnancial), the syntactic complexity of the tree,
and the presence or absence of ontology in the trees’ generation,
as well as all possible interactions between then, and an additional
random effect of the identity of the participant as a random effect.
A stepwise elimination of factors revealed signiﬁcant main effects
of task (F (1, 690) = 27.21, p < .0001), syntactic complexity
(F (1, 690) = 104.67, p < .0001), and of the presence of an on-
tology (F (1, 690) = 39.90, p < .0001). These results are in all
relevant aspects almost identical to what was observed in the accu-
racy analysis: the inspection task is harder, more syntactically com-
plex trees are less understandable than less complex ones, and trees
originating from an ontology are perceived as more understandable.

6 DISCUSSION

Our hypothesis was that the use of ontologies to select features for
conditions in split nodes, as described above, leads to decision trees
that are easier to understand. This ease of understanding was mea-
sured theoretically using a syntactic complexity measure, and cog-
nitively through time and accuracy of responses as well as reported
user conﬁdence and understandability.
First of all, the syntactic complexity (Eq. 1) of the trees distilled
with TR E PAN Reloaded is slightly smaller than those generated with
TR E PAN (see Table 1). Such small reduction on syntactic complexity
might or might not reﬂect differences in the actual understandabil-
ity of the distilled trees by people. However, in our experiments, all
online implicit measures (accuracy and response time), and off-line
explicit measures (user conﬁdence and understandability ratings) in-
dicate that trees generated using an ontology are signiﬁcantly more
accurately and easier understood by people than are trees generated
without ontology. The analyses of the four measures are remarkably
consistent in this crucial aspect (see Table 2).

Table 1: Syntactic complexity (Eq. 1) for trees inferred using C4.5,
and distilled using TR E PAN, and TR E PAN Reloaded respectively.

C4.5
5.64
5.9

heart
loan

TR E PAN

TR E PAN Reloaded

3.56
2.89

3.46
2.63

Table 2: Mean values of correct answers, time of response, user con-
ﬁdence, and user understandability for trees distilled using TR E PAN
and TR E PAN Reloaded (standard deviations are reported in paranthe-
sis). The difference in results is statistically signiﬁcant w.r.t. Mann-
Whitney and Wilcoxon tests for all measures.

Task

Class.

Insp.

Measure
%C. Answers
Time (sec)
Conﬁdence
Understd.

%C. Answers
Time (sec)
Conﬁdence
Understd.

TR E PAN

0.87 (0.32)
43.25 (61.16)
4.38 (0.86)
4.06 (0.97)

0.78 (0.41)
35.90 (24.80)
4.10 (0.96)
3.83 (1.01)

TR E PAN Reloaded
0.94 (0.18)
24.29 (15.67)
4.56 (0.80)
4.50 (0.48)

0.90 (0.27)
26.55 (35.74)
4.36 (0.78)
4.20 (0.87)

(a)

(b)

Figure 3: Estimated main effects of ontology presence on accuracies
(top) and time of response (bottom).

icant interactions between task identity and syntactic complexity
(F (1, 594.24) = 4.06, p = .0044), and task identity and domain
(F (2, 107.48) = 5.03, p = .0008). In line with what we observed
in the accuracy analysis, we ﬁnd that those trees that were generated
using an ontology were processed faster than those that were gener-
ated without one (see Figure 3b).
We analysed the user conﬁdence ratings using a linear mixed-
effect regression model, with the conﬁdence rating as the indepen-
dent variable. We included as possible ﬁxed effects the task iden-
tity (classiﬁcation vs inspection), the domain (medical vs ﬁnancial),
the size of the tree, and the presence or absence of ontology in the
trees’ generation, as well as all possible interactions between then.
In addition, we also included the identity of the participant as a ran-
dom effect. A stepwise elimination of factors revealed a main effect
of ontology presence (F (1, 689) = 14.38, p = .0002), as well as
signiﬁcant interactions between task identity and syntactic complex-
ity (F (2, 689) = 46.39, p < .0001), and task identity and domain
(F (2, 110.67) = 3.11, p = .0484). These results are almost identi-
cal to what was observed in the response time analysis: users show
more conﬁdence on judgments performed on trees that involved an
ontology, the effect of syntactic complexity is most marked in the
inspection task, and the difference between domains only affects the

6

ONTOLOGY effect plotOntologyProportion in accuracy response0.820.840.860.880.900.920.940.96absentpresent●●ONTOLOGY effect plotOntologyTime to response (in log scale)2.82.93.03.13.23.33.4absentpresent●●Table 3: Test-set accuracies and ﬁdelities for trees distilled using
TR E PAN and TR E PAN Reloaded.

Accuracy

Fidelity

C4.5
NN
heart 81.97% 94.65% 82.43% 80.87%
loan
80.48% 85.98% 86.03% 82.80%

89.23% 88.17%
92.73% 92.63%

TR E PAN TR E PAN Rld. TR E PAN TR E PAN Rld.

As we anticipated, coercing the outputs of TR E PAN onto a pre-
determined ontology (as in TR E PAN reloaded) impacts the ﬁdelity
(and accuracy) of the resulting trees (see Table 3). Crucially, how-
ever, the very small compromise in the ﬁdelity (on both examples, a
drop of around one percent) of the neural network reconstruction is
more than compensated for by the substantial improvement in the
ease with which actual people can understand the resulting trees.
When the goal is providing model explanations that are actually un-
derstandable by people, such a small compromise in ﬁdelity is well
worth it. Notice that, if we were not willing to compromise on ﬁdelity
at all, it would not make any sense to deviate in any amount from the
original neural network’s performance (i.e., any ﬁdelity below 100%
would be unacceptable). In such case, however, one would retain the
lack of user understandability of the models.
At this point, one might wonder why we should bother to create
surrogate decision trees from black-box models, rather than inferring
them directly from data. As already noticed in the original TR E PAN
work [8], distilling trees from networks can actually result in bet-
ter trees than those one would obtain by building the decision trees
directly. To demonstrate this point, we also trained decision trees di-
rectly from the datasets using the classical C4.5 algorithm. Table 3
shows that the trees inferred by the TR E PAN variants are as accu-
rate –if not more– than those inferred directly. Moreover, the trees
built directly had syntactic complexities that roughly doubled those
of the trees distilled using either TR E PAN variant (see Table 1). This
indicates that constructing trees directly from the data results in trees
substantially more complex than those distilled by TR E PAN variants,
that nevertheless do not outperform them in the task.
There is a similarly small compromise in the accuracy of the de-
cision trees (see Table 3). As we discussed above, in this approach,
the accuracy of the resulting trees (i.e., their ability to replicate the
testing sets) is less relevant than their ﬁdelity (i.e., their ability to
replicate the behaviour of the model we intend to explain). Neverthe-
less, our TR E PAN Reloaded method improves the understandability
of the trees w.r.t. the original TR E PAN, while compromising little on
the accuracy.
Apart from improving the understandability of (distilled) decision
trees, ontologies also pave the way towards the capability of chang-
ing the level of abstraction of explanations to match different user
proﬁles or audiences. For instance, the level of technicality used in
an explanation for a medical doctor should not be the same as that
used for lay users. One wants to adapt explanations without changing
the underlying explanation procedure. Ontologies are amenable to
automated abstractions to improve understandability [18]. The idea
of concept reﬁnement adopted here can be extended to operate on
changing the deﬁnition of concepts and make them more general or
more speciﬁc by means of reﬁnement operators [7, 34, 26]. This is a
line of work that we ﬁnd a natural continuation of the current study.
In its current form, TR E PAN Reloaded requires a predeﬁned ontol-
ogy onto which the features used by our algorithm should be mapped.
In such cases, which are common in many domains (e.g., medi-
cal, pharmaceutical, legal, biological, etc.), one can directly apply
TR E PAN Reloaded to improve the quality of the explanations. Ad-

ditional work –beyond the scope of the current study– would be to
automatically construct the most appropriate ontology to be mapped
onto. Such a process could be achieved by automatically mapping
sets of features into pre-existing general domain ontologies (e.g., MS
Concept Graph [38], DBpedia [20]). The provision of some form of
explicit knowledge, rather than being particular to our method, re-
sides at the core of any attempts at human interpretable explanations.
Whether such knowledge is in the form of a domain-speciﬁc ontol-
ogy (as in this study), or as a domain-general one to be adapted ad-
hoc, will depend on the particulars of speciﬁc applications.

7 RELATED WORKS

Most approaches on interpretable machine learning focus either on
building directly interpretable models, or on reconstructing post-hoc
local explanations. Our approach belongs to the category of post-hoc
global explanation methods.
In this latter category, there are a few approaches that closely relate
to ours. For instance, the work in [27] uses concepts to group features
(either using expert knowledge or correlations), and embed them into
surrogate models in order to constrain their training. In particular,
the authors propose a revised version of TR E PAN that considers only
features belonging to concepts in the extraction of a decision tree.
Whilst their results show that surrogate trees preserve accuracy and
ﬁdelity compared with original versions, the improvement in human-
readability is not explicitly tested with users. The approach in [10]
uses a complex neural network model to improve the accuracy of a
simpler model, e.g., a simpler neural network, or a decision tree. This
approach assumes to have a white-box access to (some of) the layers
of the complex network model, whereas, in our approach we treat
the black-box as an oracle. The authors in [4] describe a method for
extracting decision tree explanations that actively samples new train-
ing points to avoid overﬁtting. Our approach is similar since TR E PAN
also uses new sampled data during the extraction of the decision tree.
Other works focus on building terminological decision trees from
and using ontologies, e.g., [31, 39]). These approaches perform a
classiﬁcation task while building a tree rather than building a deci-
sion tree from a classiﬁcation process computed by a black-box.

8 CONCLUSION AND FUTURE WORKS

In this paper, we proposed an extension of TR E PAN, an algorithm
that extracts global post-hoc explanations of black box-models in the
form of decision trees. Our algorithm TR E PAN Reloaded takes into
account ontologies in the distillation of decision trees.
We showed that the use of ontologies ease the understanding of
the distilled trees by actual users. We measured this ease of under-
standing through a rigorous experimental evaluation: theoretically,
using a syntactic complexity measure, and, cognitively, through time
and accuracy of responses as well as reported user conﬁdence and
understandability. All our measures indicated that trees distilled by
TR E PAN Reloaded are signiﬁcantly more accurately and easier un-
derstood by people than are trees generated by TR E PAN, with only
little compromise of the accuracy and the ﬁdelity (see Section 6).
The results obtained are very promising, and they open several di-
rection of future research. On the one hand, we plan to extend this
work to support the automatic generation of explanations that can
accommodate different user proﬁles. On the other hand, we aim at
investigating to apply our approach to explain CNNs in image classi-
ﬁcation (e.g., [40]). We also believe that this approach can be useful

7

in bias identiﬁcation, to understand, for instance, if any undesirable
discrimination features are affecting a black-box model.

REFERENCES

[1] Franz Baader, Sebastian Brandt, and Carsten Lutz, ‘Pushing the E L
envelope.’, in IJCAI, pp. 364–369, (2005).
[2] The Description Logic Handbook: Theory, Implementation, and Ap-
plications, eds., Franz Baader, Diego Calvanese, Deborah L. McGuin-
ness, Daniele Nardi, and Peter F. Patel-Schneider, Cambridge Univer-
sity Press, New York, NY, USA, 2003.
[3] R.H. Baayen, D.J. Davidson, and D.M. Bates, ‘Mixed-effects modeling
with crossed random effects for subjects and items’, Journal of Memory
and Language, 59(4), 390 – 412, (2008). Special Issue: Emerging Data
Analysis.
[4] Osbert Bastani, Carolyn Kim, and Hamsa Bastani, ‘Interpreting black-
box models via model extraction’, CoRR, abs/1705.08504, (2017).
[5] Bruce G. Buchanan and Edward H. Shortliffe, Rule Based Expert Sys-
tems: The Mycin Experiments of the Stanford Heuristic Programming
Project, Addison-Wesley Longman Publishing Co., Inc., 1984.
[6] Roberto Confalonieri, Tarek R. Besold, Tillman Weyde, Kathleen
Creel, Tania Lombrozo, Shane Mueller, and Patrick Shafto, ‘What
makes a good explanation? Cognitive dimensions of explaining intel-
ligent machines’, in Proc. of the 41th Annual Meeting of the Cognitive
Science Society, CogSci 2019, (2019).
[7] Roberto Confalonieri, Manfred Eppe, Marco Schorlemmer, Oliver
Kutz, Rafael Pe ˜naloza, and Enric Plaza, ‘Upward reﬁnement opera-
tors for conceptual blending in the description logic E L++ ’, Annals
of Maths. and Artiﬁcial Intelligence, 82(1), 69–99, (2018).
[8] Mark W. Craven and Jude W. Shavlik, ‘Extracting tree-structured rep-
resentations of trained networks’, in NIPS 1995, pp. 24–30. MIT Press,
(1995).
[9] A. S. d’Avila Garcez, K. Broda, and D. M. Gabbay, ‘Symbolic knowl-
edge extraction from trained neural networks: A sound approach’, Art.
Intell., 125(1-2), 155–207, (2001).
[10] Amit Dhurandhar, Karthikeyan Shanmugam, Ronny Luss, and Peder A
Olsen, ‘Improving simple models with conﬁdence proﬁles’, in NIPS
2018, 10296–10306, (2018).
[11] Franciscus Cornelis Donders, ‘On the speed of mental processes.’, Acta
Psychologica, 30, 412–31, (1969).
[12] Finale Doshi-Velez and Been Kim, ‘Towards a rigorous science of in-
terpretable machine learning’, CoRR, abs/1702.08608, (2017).
[13] Eleanor, Eleanor Rosch, Bascom Carolyn, Carolyn B Mervis, Wayne D.
Gray, David M. Johnson, Jenifer L. Penny, and Penny Boyes-Braem,
‘Basic objects in natural categories’, Cognitive Psychology, 8, 382–439,
(1976).
[14] Riccardo Guidotti, Anna Monreale, Salvatore Ruggieri, Franco Turini,
Fosca Giannotti, and Dino Pedreschi, ‘A survey of methods for explain-
ing black box models’, ACM Comp. Surv., 51(5), 1–42, (2018).
[15] Michael Hind, ‘Explaining explainable ai’, XRDS, 25(3), 16–19, (April
2019).
[16] Robert R. Hoffman, Shane T. Mueller, Gary Klein, and Jordan Lit-
man, ‘Metrics for explainable AI: challenges and prospects’, CoRR,

abs/1812.04608, (2018).

[17]

Johan Huysmans, Karel Dejaeger, Christophe Mues, Jan Vanthienen,
and Bart Baesens, ‘An empirical evaluation of the comprehensibility of
decision table, tree and rule based predictive models’, Decision Support
Systems, 51(1), 141–154, (2011).
[18] C. Maria Keet, ‘Enhancing Comprehension of Ontologies and Concep-
tual Models Through Abstractions’, in AI*IA 2007: Artiﬁcial Intelli-
gence and Human-Oriented Computing, 10th Congress of the Italian
Association for Artiﬁcial Intelligence, Rome, Italy, September 10-13,
2007, Proceedings, pp. 813–821, (2007).
Jens Lehmann and Pascal Hitzler, ‘Concept learning in description log-
ics using reﬁnement operators’, Machine Learning, 78(1-2), 203–250,
(2010).
Jens Lehmann, Robert Isele, Max Jakob, Anja Jentzsch, Dimitris Kon-
tokostas, Pablo N. Mendes, Sebastian Hellmann, Mohamed Morsey,
Patrick van Kleef, S ¨oren Auer, and Christian Bizer, ‘DBpedia - a large-
scale, multilingual knowledge base extracted from wikipedia’, Seman-
tic Web Journal, 6(2), 167–195, (2015).
[21] Zachary C. Lipton, ‘The mythos of model interpretability’, Queue,
16(3), 30:31–30:57, (June 2018).

[19]

[20]

[22] Ninareh Mehrabi, Fred Morstatter, Nripsuta Saxena, Kristina Lerman,
and Aram Galstyan. A survey on bias and fairness in machine learning,
2019.
[23] Tim Miller, ‘Explanation in artiﬁcial intelligence: Insights from the so-
cial sciences’, CoRR, abs/1706.07269, (2017).
[24] Parliament and Council of the European Union. General Data Protec-
tion Regulation, 2016.
[25] Rok Piltaver, Mitja Luˇstrek, Matja ´z Gams, and Sanda Martin ˇci ´c-Ipˇsi ´c,
‘What makes classiﬁcation trees comprehensible?’, Expert Syst. Appl.,
62(C), 333–346, (November 2016).
[26] Daniele Porello, Nicolas Troquard, Rafael Pe ˜naloza, Roberto Con-
falonieri, Pietro Galliani, and Oliver Kutz, ‘Two Approaches to On-
tology Integration based on Axiom Weakening’, in IJCAI-ECAI 2018,
pp. 1942–1948, (2018).
[27] Xavier Renard, Nicolas Woloszko, Jonathan Aigrain, and Marcin De-
tyniecki, ‘Concept tree: High-level representation of variables for more
interpretable surrogate decision trees’, CoRR, abs/1906.01297, (2019).
[28] Philip Resnik, ‘Using information content to evaluate semantic simi-
larity in a taxonomy’, in IJCAI 1995, pp. 448–453. Morgan Kaufmann
Publishers Inc., (1995).
[29] Marco Tulio Ribeiro, Sameer Singh, and Carlos Guestrin, ‘”Why
Should I Trust You?”: Explaining the Predictions of Any Classiﬁer’, in
Proc. of the 22nd Int. Conf. on Knowledge Discovery and Data Mining,
KDD ’16, pp. 1135–1144. ACM, (2016).
[30] Marco T ´ulio Ribeiro, Sameer Singh, and Carlos Guestrin, ‘Anchors:
High-precision model-agnostic explanations’, in AAAI, pp. 1527–1535.
AAAI Press, (2018).
[31] Giuseppe Rizzo, Claudia dAmato, Nicola Fanizzi, and Floriana Espos-
ito, ‘Tree-based models for inductive classiﬁcation on the web of data’,
Journal of Web Semantics, 45, 1 – 22, (2017).
[32] David S ´anchez, Montserrat Batet, and David Isern, ‘Ontology-based
information content computation’, Knowledge-Based Systems, 24(2),
297 – 303, (2011).
[33] Geoffrey G. Towell and Jude W. Shavlik, ‘Extracting reﬁned rules from
knowledge-based neural networks’, Machine Learning, 13(1), 71–101,
(October 1993).
[34] Nicolas Troquard, Roberto Confalonieri, Pietro Galliani, Rafael
Pe ˜naloza, Daniele Porello, and Oliver Kutz, ‘Repairing Ontologies via
Axiom Weakening’, in AAAI 2018, pp. 1981–1988, (2018).
[35] Patrick R.J. van der Laag and Shan-Hwei Nienhuys-Cheng, ‘Complete-
ness and properness of reﬁnement operators in inductive logic program-
ming’, The Journal of Logic Programming, 34(3), 201 – 225, (1998).
[36] Michael R. Wick and William B. Thompson, ‘Reconstructive expert
system explanation’, Art. Intelligence, 54(1-2), 33–70, (March 1992).
Jill Butler William Lidwell, Kritina Holden, Universal. Principles of
Design., Rockport, 2003.
[38] Wentao Wu, Hongsong Li, Haixun Wang, and Kenny Q. Zhu, ‘Probase:
A Probabilistic Taxonomy for Text Understanding’, in Proceedings of
the 2012 ACM SIGMOD International Conference on Management of
Data, SIGMOD ’12, pp. 481–492, New York, NY, USA, (2012). ACM.
Jun Zhang, Adrian Silvescu, and Vasant G. Honavar, ‘Ontology-driven
induction of decision trees at multiple levels of abstraction’, in Proc.
of the 5th Int. Symposium on Abstraction, Reformulation and Approx-
imation (SARA) 2002, volume 2371 of LNCS, pp. 316–323. Springer,
(2002).
[40] Quanshi Zhang, Yu Yang, Haotian Ma, and Ying Nian Wu, ‘Interpreting
cnns via decision trees’, in The IEEE Conference on Computer Vision
and Pattern Recognition (CVPR), (June 2019).

[37]

[39]

8

