A Continuous Teleoperation Subspace with Empirical and
Algorithmic Mapping Algorithms for Non-Anthropomorphic Hands

Cassie Meeker1 , Maximilian Haas-Heger1 , and Matei Ciocarlie1

9
1
0
2

v
o

N

1
2

]

O

R

.

s

c

[

1
v
5
6
5
9
0

.

1
1
9
1

:

v

i

X

r

a

Abstract—Teleoperation is a valuable tool for robotic manip-
ulators in highly unstructured environments. However, ﬁnding
an intuitive mapping between a human hand and a non-
anthropomorphic robot hand can be difﬁcult, due to the hands’
dissimilar kinematics. In this paper, we seek to create a map-
ping between the human hand and a fully actuated, non-
anthropomorphic robot hand that is intuitive enough to enable
effective real-time teleoperation, even for novice users. To accom-
plish this, we propose a low-dimensional teleoperation subspace
which can be used as an intermediary for mapping between
hand pose spaces. We present two different methods to deﬁne the
teleoperation subspace: an empirical deﬁnition, which requires a
person to deﬁne hand motions in an intuitive, hand-speciﬁc way,
and an algorithmic deﬁnition, which is kinematically indepen-
dent, and uses objects to deﬁne the subspace. We use each of these
deﬁnitions to create a teleoperation mapping for different hands.
We validate both the empirical and algorithmic mappings with
teleoperation experiments controlled by novices and performed
on two kinematically distinct hands. The experiments show that
the proposed subspace is relevant to teleoperation,
intuitive
enough to enable control by novices, and can generalize to non-
anthropomorphic hands with different kinematic conﬁgurations.
Index Terms—Telerobotics and Teleoperation, Grasping, Hu-
man Factors and Human-in-the-Loop

I . IN TRODUC T ION

Teleoperation is a valuable tool for robotic manipulators
in highly unstructured environments, where a wide array of
scenarios and objects can be encountered. In such condi-
tions, the robot can rely on human cognition to deal with
corner cases faster and more easily than fully autonomous
manipulation planners. An important research direction for
robot teleoperation aims to make the controls available to the
operator as intuitive as possible: intuitive controls minimize
the training time required for human teleoperators and can
make teleoperation more accessible to novices. They also
ensure a safe and effective workﬂow.
For manipulation, teleoperation controls which harvest the
user’s hand motions, rather than using a joystick or a point-
and-click interface, can provide an intuitive and user friendly
interface [1], because they harness motions which are already
natural to the teleoperator. An example of this workﬂow is
shown in Figure 1a.
Teleoperating a robot hand using a human hand as input
requires a teleoperation mapping, which tells the robot hand
how to move in response to movements of the human hand.
Robot hand designs that are fully-actuated and highly anthro-
pomorphic allow for a direct joint mapping to the human hand

* This work was supported in part by the ONR Young Investigator Program
award N00014-16-1-2026.
1Department of Mechanical Engineering, Columbia University, New York,
NY 10027, USA.
{c.meeker, m.haas, matei.ciocarlie}@columbia.edu

(a)

(b)
Fig. 1.
(a) Teleoperated manipulation where the operator’s hand movements
are recorded and used to drive a non-anthropomorphic robotic hand. To
achieve this, a mapping between the human and robot hand kinematics is
required. (b) A teleoperation subspace used as an intermediary between the
pose spaces of different hands. Here we show how the motions that are
associated with the subspace basis vectors can be intuitively deﬁned by a
user, based on the hand’s kinematics.

and thus are intuitive for a human to teleoperate; however,
the hardware tends to be fragile and expensive. In contrast,
non-anthropomorphic hands have proven to be robust and
versatile in unstructured environments. However, ﬁnding an
easy or intuitive mapping between the human hand and a
non-anthropomorphic robot hand can be difﬁcult, due to the
different joint conﬁgurations, different axes, different numbers
of ﬁngers, or any number of dissimilarities between the hands.
In this paper, we seek to create a mapping between the
human hand and a fully actuated but non-anthropomorphic
robot hand that is intuitive enough to enable effective real-
time teleoperation, even for novice users.
The method we propose uses a subspace relevant to tele-
operation as an intermediary between the pose spaces of
two different hands. Our method enables teleoperation by

 
 
 
 
 
 
projecting the pose of the master hand into the deﬁned
teleoperation subspace, which it shares with the slave hand,
and then projecting from the teleoperation subspace into the
pose space of the slave hand.
At a conceptual level, each of the basis vectors that deﬁne
the subspace corresponds to a hand motion: hand opening,
ﬁnger curl, and ﬁnger spread (Figure 1b). While these concepts
are natural for the human hand, we need to also deﬁne them
in the context of non-anthropomorphic robot hands. We show
that this process can be done empirically: in this formulation,
the person creating the teleoperation mapping deﬁnes what
the motions of ‘open’, ‘curl’, and ‘spread’ mean for a speciﬁc
robot hand. In this way, the mapping is tied to hand kinematics,
since the hand motions mean different things for different
hands. We show that this empirical mapping can be created
following a series of simple steps (Section IV) and leads to
effective teleoperation for novices.
One shortcoming of using an empirical mapping is its
reliance on human intuition: effective teleoperation could be
attributed to either the structure of the subspace, or simply to
the hand-speciﬁc intuition provided by the person creating the
mapping. We strive to show that the teleoperation subspace
which we propose can also be deﬁned without such hand-
speciﬁc intuition.
We therefore propose a second, algorithmic, method where
we formalize the notion of the hand motions used to deﬁne the
subspace. Rather than considering, for example ‘hand opening’
as an intuitive concept to be deﬁned by the person creating the
mapping, this paradigm considers hand opening as the hand
grasping a series of incrementally larger objects. In this way,
we can use a set of objects to provide the same understanding
of hand motions as the user provided in the empirical method.
This deﬁnition of the subspace is done exclusively through
an object set, and is hand-independent. However, it lends itself
to the algorithmic creation of a teleoperation mapping for
any hand. We introduce a method which uses this algorithmic
deﬁnition to generate subspace mappings for hands in a fully
automated fashion (Section V). We aim to show that this
algorithmic mapping also enables effective teleoperation for
novices, implying that the value of the teleoperation subspace
does not derive exclusively from hand-speciﬁc human intuition
used to create it.
We
create
teleoperation mappings
for
two
non-
anthropomorphic robotic hands using both the empirical
mapping and the algorithmic mapping.
In teleoperation
experiments with novices, we show that the mappings created
with both these paradigms can enable teleoperation as fast as
or faster than state-of-the-art teleoperation methods. Overall,
the main contributions of this paper are:
• We introduce a continuous, low-dimensional teleopera-
tion subspace as an intuitive way to map human to robot
hand poses for teleoperation. We posit that this method
allows for intuitive teleoperation as long as both the
master and the slave hand poses can be projected into
this subspace.
• We provide an empirical method to deﬁne the subspace
and to create a projection into the subspace. This method
requires the user to deﬁne the hand motions for the

subspace using their intuition and based on the hand’s
kinematics.
• We provide an algorithmic method for deﬁning the sub-
space as well. This deﬁnition is independent of hand
kinematics and lends itself to an automated algorithm
which can create a mapping for hands of various kine-
matics. We are the ﬁrst to show that an automated method
for generating a teleoperation mapping can enable online
teleoperation which is intuitive for novices.
• We show experimentally that the subspace is relevant
to teleoperation for two different non-anthropomorphic
hands and for two different manipulation tasks. These
experiments demonstrate that our mappings created em-
pirically and algorithmically allow novice teleoperators
to pick and place objects and perform in-hand manip-
ulation as fast as or faster than state-of-the-art teleop-
eration mapping methods using robot hands with non-
anthropomorphic kinematics. As we discuss in the next
section, in most literature on hand teleoperation, map-
pings are usually only validated on one or two expert
users and on one robotic system. We validate the teleop-
eration subspace mappings on nine novice users and two
different robotic hands.
In an earlier version of this study [2], we have introduced
the concept of a teleoperation subspace deﬁned exclusively
via empirical mapping, and validated it with teleoperation
experiments on a single robotic hand for a single task. Here we
show that the subspace can be deﬁned in a hand-independent
fashion by considering variations in the grasped object shape,
and introduce a fully automated process for creating mappings
into this subspace. We also validate both mapping methods
with multiple manipulation tasks on two kinematically distinct
robotic hands.

I I . R ELAT ED WORK

The most common ways to create teleoperation mappings
are: joint mapping [3], ﬁngertip mapping [4], and pose map-
ping [5].
Joint mapping (also called joint-to-joint mapping) is used
when the slave hand has similar kinematics to the human [6].
If the human and robot joints have a clear correspondence,
the human joint angles can be imposed directly onto the robot
joints with little or no transformation [3]. This mapping is
most useful for power grasps [7], and is limited if the robot
hand is non-anthropomorphic.
Fingertip mapping (also called point-to-point mapping) is
the most common teleoperation mapping method. Forward
kinematics transform human joint angles into Cartesian ﬁn-
gertip positions. These undergo scaling to ﬁnd the desired
robot Cartesian ﬁngertip positions and then inverse kinematics
determine robot
joint angles. This mapping is useful for
precision grasps [7].
For ﬁngertip and joint mapping, how to reconcile kinematic
differences between the human and robotic workspaces is
an open question. Humans use only 3.6% of the potential
workspace for the thumb during grasping [8]. So, if a robot
ﬁnger maps to the human thumb, that ﬁnger will not move

signiﬁcantly during grasping tasks, unless the teleoperator
adapts their grasping in unintuitive ways. To solve this prob-
lem, researchers combine different types of mappings [9],
use virtual object mapping with special considerations for
the workspace differences [10], optimize distances in task
space [6], use error compensation [4], and alter the robotic
hand frame for individual grasp postures to minimize the
workspace differences [11]. All of this is additional work and
computation for the human to create and test.
Pose mapping attempts to replicate the pose of the human
hand with a robot hand, which is appealing because, unlike
ﬁngertip and joint mapping, it attempts to interpret the function
of the human grasp rather than replicate hand position. Pao
and Speeter deﬁne transformation matrices relating human and
robot poses, using least squared error compensation when this
transformation is not exact [5]. Others use neural networks to
identify the human pose and map the pose to a robot either
through another neural network [12] or pre-programmed joint-
to-joint mapping [13]. Recently, an end-to-end solution was
proposed, trained on a traditional pose mapping dataset but
which tries to directly predict joint angles of the robot hand
without ﬁrst classifying the pose of the human hand [14].
Outside of a discrete set of known poses, pose mapping
can lead to unpredictable hand motions. Furthermore,
the
mappings that use neural networks require classiﬁcation of
the human hand pose before it is mapped to the robotic hand.
If this classiﬁer misidentiﬁes the human pose, the robot hand
will move in undesirable ways. Our method also attempts to
replicate hand shape, rather than ﬁngertip or joint positions,
making it most similar to pose mapping, but we do not require
discrete classiﬁcation of human pose before mapping.
This paper introduces a low-dimensional mapping. Other
methods that deﬁne grasping in a low dimensional space
include postural synergies, which are low dimensional and
continuous [15]. Just as synergies move the description of
human hand position from discrete, static poses [16] into a
continuous space, we seek to allow pose mapping between
the human and robotic hand to be continuous instead of
interpolating between discrete poses.
Some works ﬁnd synergies of robot hands by ﬁnding robot
poses that resemble grasping poses for human hands, and then
performing PCA on those poses. The poses are either found
through joint mapping [17], pose mapping [18], or human in-
tuition [19] [20]. Others use postural synergies to underactuate
anthropomorphic hands, like the Pisa/IIT Softhand 2 [21], but
user intuition is required to build synergies into the mechanical
design of the robot.
Other works use low dimensional latent variables which are
not based on synergies to approximate human poses in non-
anthropomorphic models. These latent variables have enabled
both the animation of non-anthropomorphic creatures [22] and
teleoperation. Gaussian process latent variable models (GP-
LVM) can enable teleoperation of humanoid robots. In some
formulations, the latent space changes with every different
master-slave pairing [23]. In other formulations, multiple
robots and a human share the same latent space [24].
Training data driven mappings,
like some pose map-
pings [19], or GP-LVMs, requires the user to create many

corresponding poses between the human and robotic hands.
Creating these corresponding poses is often tedious and time
consuming. We are inspired by a similar desire to ﬁnd shared
subspaces between robotic and human hands, but our algorith-
mic and empirical methods for generating teleoperation map-
pings reduce the burden placed on the user by eliminating the
requirement to create tens or even hundreds of corresponding
poses between the human and robot.
There are works which, like our algorithmic mapping, try to
create teleoperation mappings without requiring that the user
provide intuition about hand kinematics.
Kheddar et al. proposed high level abstraction teleoperation,
where the operator manipulates a virtual environment and a bi-
lateral transform translates changes in the virtual environment
into commands for the robot, such that the slave mimics the
change in the real environment [25]. The gripper control is
object based, i.e. the robot must manipulate and transport an
object in the real world in the same way it is being manipulated
in the virtual environment [26]. Although they describe several
possible ways to transform between the human and robotic
hands, their ultimate solution is autonomous, and does not
include a teleoperation mapping.
Kang et al. also introduced an object based approach to
identifying human grasps using the contact web [27]. Once
the human grasp has been identiﬁed, the robot hand is shaped
based on virtual ﬁngers and the human grasp. However, the
user still provides an understanding of how the robot functions
- for each new robot, they assign the ﬁngers as being a primary
ﬁnger, a secondary ﬁnger, or a palm.
Finally, Gioioso et al. deﬁned an object based approach
for mapping between hands with dissimilar kinematics using
virtual objects [28], [29], [30], [31]. This work replicates the
deformation of the virtual object in the human hand with
the virtual object in the robot hand. This is the ﬁrst time a
(virtual) object set was used to deﬁne a teleoperation mapping.
However, the authors have reported varying performance for
the same hand with different number of virtual points and
different numbers of synergies, meaning that creating the
mapping for each hand requires the user to tune control
parameters, including how many contact points to use, where
to place these contacts, and which synergies to use.
All of the above work requires the user to give some input
which provides an understanding about the hand’s kinematics
or function. Some groups have moved towards teleoperation
mappings which reduce the burden on the user, but our algo-
rithmic mapping is the ﬁrst to create a teleoperation mapping
completely automatically, using a subspace deﬁnition which is
independent of hand kinematics.
In the literature on hand teleoperation, most works validate
their proposed teleoperation methods on one or two expert
users or perform their experiments in simulation (e.g. [3], [4],
[5], [7], [9], [10], [12], [13], [28], [29], [30]). We were only
able to ﬁnd two works that validate a proposed teleoperation
method with novice users on a physical robot [14], [31].
Both of these works validated their method with ﬁve novices
users teleoperating a single robotic hand. In this paper, we
validate our work over two different tasks with a total of nine
novice users, using two robotic hands with different kinematic

1) Origin o: To project between joint space and T , we
require a hand-speciﬁc, “neutral” origin pose o ∈ (cid:60)N .

o = [o1 , o2 , ..., oN ]

(1)

This represents a hand position which will standardize the
data as we project between joint space and T . The origin
pose of the master is arbitrary; however, it is crucial that the
origin pose of the slave corresponds to the master’s origin.
The two hands should assume approximately the same shape
while positioned at their respective origins.
2) Projection Matrix A: The projection matrix A ∈
(cid:60)N ×3 is hand speciﬁc and consists of three basis vectors
αH , σH , H ∈ (cid:60)N . Whereas α, σ , and  represent the general
concept of a hand motion, αH , σH , and H are the projection
of that motion into the pose space of a speciﬁc hand H .

A = [αH , σH , H ]
αH = [αH 1 , αH 2 , ..., αHN ](cid:62)
σH = [σH 1 , σH 2 , ..., σHN ](cid:62)
H = [H 1 , H 2 , ..., HN ](cid:62)

(2)
(3)
(4)
(5)

3) Scaling Factor δ : We wish to normalize such that any
conﬁguration in pose space will project to a pose in T whose
value is less than or equal to 1 along each of the basis vectors.
We therefore require a scaling factor δ ∈ (cid:60)3 to normalize the
projection:
(6)

δ = [δα , δσ , δ ]

To calculate δ , we evaluate poses which illustrate the
extrema of the hand’s kinematic limits along the basis vectors.
For example, the maximum and minimum values along σ are
illustrated by projecting poses where the hand is holding the
largest object possible and the smallest object possible from
pose space into T . Once we select the illustrative poses for
the hand, we project these poses from pose space into T using
ψ = (q − o) · A, where ψ ∈ T . From this set of poses in T ,
we ﬁnd the minimum and maximum values along each axis.
Along α, the minimum and maximum are referred to as αmin
and αmax , respectively. From these values, we calculate δα
as:

(cid:40)

αrange = abs(αmax ) + abs(αmin )
if αrange = 0

0

δα =

1/αrange

otherwise

(7)

(8)

Finding δσ and δ uses the same calculation.
δ normalizes the projection from pose space to T ; however,
to project from T back to pose space, we require an inverse
∗ :
scaling factor δ

(cid:40)

δ∗

α =

∗

δ

0
1/δα

= [δ∗
α , δ∗
σ , δ∗
 ]
if δα = 0

otherwise

(9)

(10)

where we ﬁnd δ∗
σ and δ∗
 with similar calculations.

Fig. 2. Steps to enable real time teleoperation using teleoperation subspace

conﬁgurations. These experiments show that
the subspace
mappings we propose are intuitive and encode information
relevant to teleoperation for hands with different kinematics.

I I I . T E LEO P ERAT ION SUB S PAC E

As a general concept, we posit that, for many hands, there
exists a three dimensional space T isomorphic to (cid:60)3 that can
encapsulate the range of movement needed for teleoperation.
The three dimensions of T correlate to certain hand motions:
opening and closing the hand, spreading the ﬁngers, and
curling the ﬁngers. We will refer to these as the size σ , spread
α, and curl  basis vectors, respectively.
We chose these bases on intuition, guided by Santello’s
research of postural synergies [15]. Since Santello et al.
used principle component analysis (PCA), a linear dimension
reduction method, to ﬁnd postural synergies, we also assume
that mapping between pose space and teleoperation subspace
is linear.
We assume that many hands will be able to project their
pose spaces into T . If this projection is possible, T is
embedded as a subspace in the pose space of the hand. T
is thus a subspace “shared” by all hands that can project
their pose space into T . If the user can construct a projection
matrix which projects pose space to teleoperation subspace
in a meaningful way, our method will enable teleoperation.
Experimentally, we will show that this is the case for at least
the human hand, the Schunk SDH, and a two-ﬁngered gripper.
We theorize that T is also relevant to teleoperation for other
hands.
To teleoperate using T , there are two steps:
1) Given joint values of the master hand, ﬁnd the equivalent
pose ψ in teleoperation subspace T .
2) Given ψ computed above, ﬁnd the joint values of the
slave hand, and move the slave hand to these values.
These steps are illustrated in Figure 2. In order to enact the
teleoperation steps, we must ﬁrst deﬁne the mapping between
T and the relevant pose spaces.

A. Teleoperation Subspace Mapping
For a given hand with N joints, projecting from joint space
q ∈ (cid:60)N (we use pose space and joint space interchangeably)
into teleoperation subspace T requires an origin pose o ∈ (cid:60)N ,
a projection matrix A ∈ (cid:60)N ×3 , and a scaling factor δ ∈ (cid:60)3 .

Jointspaceofmasterhandqm∈<NTeleoperationsubspaceT∈<3Jointspaceofslavehandqs∈<KOrigin pose of the human hand. Origin pose of the Schunk SDH.
Fig. 3. Origin poses of two example hands.

4) A Complete Projection Algorithm: To project between
teleoperation subspace T and joint space q , we use the hand-
speciﬁc matrix A, the origin o, and the scaling factor δ :

ψ = ((q − o) · A) (cid:12) δ
q = ((ψ (cid:12) δ
) · A(cid:62) ) + o

∗

(11)
(12)

where (cid:12) represents element-wise multiplication.
To use T for teleoperation, Eq. 11 projects from the master
hand’s pose space into the shared teleoperation subspace and
then Eq. 12 projects from the shared teleoperation subspace
into the slave hand’s pose space (Figure 2).
So, given the joint angles of the master hand, we are able
to calculate the joint angles of the slave hand using:
∗

qs = (((qm − om ) · Am ) (cid:12) δm (cid:12) δ

s ) · A(cid:62)
s + os

(13)

Now that we have formalized the subspace and what is
required to map between pose space and the subspace, we
propose two different methods to deﬁne the mapping. The
ﬁrst, empirical method, is dependent on hand kinematics and
relies on the intuition of the person creating the mapping. The
second, algorithmic mapping, is created automatically, using
a deﬁnition of the subspace which is independent of hand
kinematics.

IV. EM P IR ICA LLY D E FIN ING THE SUB S PAC E MA P P ING

The teleoperation subspace mapping can be created empir-
ically through a relatively simple process. For each hand, the
user must:
• Select an origin pose. Figure 3 shows the pose we chose
for the human hand and the Schunk SDH robot in our
experiments.
• Determine poses which illustrate the extrema of the
hand’s kinematic limits along the basis vectors. It is up to
the user to determine poses which illustrate the full range
of values for each basis vector. Figure 4 shows the poses
which demonstrate these ranges for the human hand.
• Deﬁne what the hand motions (ﬁnger spread, ﬁnger curl,
and hand opening) mean in the context of the hand’s
kinematics, then identify which joints contribute to that
motion. This is a winner-take-all approach, so a joint may
only contribute to a single motion. We set joints which
adduct the ﬁngers to 1 in αH , joints which open the hand
to 1 in σH , and joints which curl the ﬁngers to 1 in H .
We then normalize the vectors to create A. Table I shows
this process for the Schunk SDH hand.

Pose for
maximum along

σhuman ,

minimum along

αhuman .

Pose for
maximum along

αhuman .

Pose for
minimum along

human , and

minimum along

σhuman .

Pose for
maximum along

human .

Fig. 4. Poses which demonstrate the human hand’s kinematics limits along
the basis vectors of T .

Creating the the empirical mapping is a simple, winner-
take-all, three step approach. Despite this simplicity, we show
experimentally that these calculations are sufﬁcient to mean-
ingfully project pose space into T in a way that enables
teleoperation for novice users.

V. A LGOR I THM ICA LLY D E FIN ING THE SUB S PAC E
MA P P ING

In the previous section, we rely on a human to look at the
hand’s kinematics, deﬁne each of the motions associated with
the different subspace basis vectors, and then determine which
joints contribute to that motion.
We would like to demonstrate that we can deﬁne the
subspace in a way which is independent of hand kinematics.
We also hypothesize that this subspace deﬁnition allows us to
create a teleoperation subspace mapping for a hand automati-
cally (i.e. an algorithmic mapping). If the algorithmic mapping
can enable teleoperation for novices, this would demonstrate
that the value of the teleoperation subspace does not derive
exclusively from the human intuition used to create it.
To create a subspace mapping algorithmically, we must
formalize the notion of a hand motion in a way that does not
depend on the hand’s kinematics. We do this using objects.
Hand opening can be thought of as the hand grasping a series
of objects that grow incrementally larger. Spreading the ﬁngers
results from the hand grasping a series of objects whose
curvature increases incrementally. Finger curl is binary, and
can be deﬁned as the difference between a precision grasp
and a power grasp for the same object.
Based on this formalized notion of hand movements, we can
use object characteristics to predict the location of a grasp in
T . We posit that when a hand, regardless of kinematics, is
holding an object, we can predict where the resulting grasp
will lie in the teleoperation subspace, based on the object’s
size, shape, and the type of grasp used. This is illustrated in
Figure 5.
If we can use the object’s characteristics to predict where a
grasp will lie on the subspace, we can create a set of objects
which we predict will result in grasps along the basis vectors
of T . Regardless of a hand’s kinematics, when the hand holds
any of the objects in this set, the resulting grasp will lie along
one of the basis vectors of T . The object set we design consists
of 8 objects and is described in more detail in Section V-A.

PROC E S S TO EM P IR ICA LLY D E FIN E TH E PRO J ECT ION MATR IX FOR TH E TE LEO P ERAT ION SUB S PACE MA P P ING

TABLE I

Basis vector

αschunk = [1, 0, 0, 0, 0, 0, 0]

Motion Deﬁned by Hand-Speciﬁc
Kinematics

Joints which affect
the motion

Hand Motion

Finger Spread

Hand Opening

σschunk = [0, 0.577, 0, 0.577, 0, 0.577, 0]

Finger Curl

schunk = [0, 0, 0.577, 0, 0.577, 0, 0.577]

If a hand of any kinematic conﬁguration grasps all the
objects in our set, the result will be a set of grasps in the pose
space of that hand, but that we predict can be used to ﬁnd the
basis vectors of T . So, given a hand with a speciﬁc kinematic
conﬁguration, for each of the objects in our object set, we
can generate a set of grasps Gobj ect . Each of the grasps g in
Gobj ect shows one possible way for a hand to grasp that object
in a stable conﬁguration. Each grasp g is an N dimensional
vector, where N is the number of degrees of freedom for the
hand. Once we have generated grasps for each of the objects,
we can combine these individual sets into one grasp set G ,
which encompasses all the objects:

G = Gobj ect1 ∪ Gobj ect2 , ..., Gobj ect8
Gobj ect1 = {g1
obj ect1 , g2

obj ect1 , ...}, g ∈ RN

Since G is a set of grasps in pose space which spans T ,
we can ﬁnd a model of T by ﬁtting a subspace to G . The
model for T provides us with the subspace mapping needed
to teleoperate the hand. The model of the subspace includes the
origin and the directions of the basis vectors, which translate
to o and A in the teleoperation mapping. We can then ﬁnd δ
with a simple iterative method.
Once the object set has been designed, algorithmically
creating a subspace teleoperation mapping requires three steps.
For both the master and the slave hand, we need to:

Fig. 5. We can formalize the hand motions that deﬁne the subspace with
objects, then use this deﬁnition to predict where a grasp for that object will
lie in the subspace, regardless of the hand’s kinematics. This ﬁgure shows the
human hand and the Schunk hand grasping the same set of objects. When an
object is held by either hand, the resulting grasp will lie at the same location
in the subspace T .

• Generate a set of grasps G where the hand is grasping
each of the objects in the object set.
• Fit a subspace to the grasps. The subspace model provides
us with the projection between T and joint space.
• Use an iterative approach to ﬁnd δ .

Once the mapping has been generated for a given hand, it
does not have to be generated again for a new master slave
pairing. For example, once we generate the human mapping,

it will work with slave hand mappings that we generate in the
same way.
We discuss the design of the object set, and the steps needed
to implement teleoperation in the sections below.

A. Object Set
We hypothesize that we can design a set of objects to
elicit grasps which lie along the basis vectors of T . Table II,
and Figure 6a show the objects in our set, and where in the
subspace we predict hands grasping those objects will lie.
The object set consists of eight objects. We use disks and
boxes as our shape primitives. We specify the type of grasp
(power or precision) which must be used with each object,
in order to guarantee the grasp’s location along the curl basis
vector of T . In the set, there are objects that have the same
dimensions, but are grasped with a different grasp type.
The approach direction of the hand is along the z axis, and
we orient the objects in the same way relative to the hand.

B. Grasp Generation
Once we deﬁne our object set, we generate a set of grasps G ,
which demonstrate how a hand of a speciﬁc kinematic conﬁg-
uration can hold the objects in our set in stable conﬁgurations.
For robotic hands, we generate G using a grasp planner, and
for human hands, we use human subjects.
We acknowledge that there are many ways to grasp an
object. To compensate, we generate multiple grasps for each
object, and use a subspace ﬁtting method which is robust
to outliers. In this way, we assume that we have sufﬁciently
sampled grasps for the object set which would fall along the
basis vectors of the subspace.
1) Robot Datasets: To generate the robot grasps, we use
a grasp planner provided by the GraspIt! simulator. Given a
hand and an object, the planner returns grasp conﬁgurations in
which the hand stably grasps that object, ranked by the epsilon
quality metric [32]. This quality metric is a geometric method
that determines the total space of possible wrenches, within
certain friction constraints, for a given grasp.
For grasp planning we apply a random search: we randomly
sample an object pose (3 dimensions, we do not consider
object rotation) that lies within the workspace of the hand.
We also sample pre-grasp pose joint angles (N dimensions,
where N is the number of degrees of freedom of the hand)
that lie within the joint limits. We then close the ﬁngers until
they make contact with the object and evaluate the resulting
grasp. In order to ensure robustness of the resulting grasps,
particularly with respect to small deviations in object and pre-
grasp pose, we also evaluate the grasps that arise when small
perturbations are applied. Speciﬁcally we apply both positive
and negative disturbances along each coordinate axis of the
search space individually. Thus, for the 3+N dimensions from
which candidate object and pre-grasp poses are sampled, we
evaluate a total of 3(3 + N ) grasps. We choose the minimum
quality encountered across these trials to represent the sampled
grasp overall. This process is repeated until an iteration limit
is reached and the sampled grasps are stored in a database.

Identiﬁer
1
2
3
4
5
6
7
8

Object
Primitive
Disk
Disk
Box
Box
Box
Disk
Box
Box

TABLE II

OB J EC T S ET

Dimensions (in mm)
x
y
z
70
70
10
110
110
10
45
300
10
70
300
10
100
300
10
70
70
10
45
300
10
70
300
10

Grasp
Type
Precision
Precision
Precision
Precision
Precision
Power
Power
Power

Predicted location
of grasp in T

ψ = [1, 0.5, 0]
ψ = [1, 1, 0]
ψ = [0, 0, 0]
ψ = [0, 0.5, 0]
ψ = [0, 1, 0]
ψ = [1, 0.5, 1]
ψ = [0, 0, 1]
ψ = [0, 0.5, 1]

Given a hand and an object, the planner returns up to 1,000
stable grasp conﬁgurations for that object. We parse the dataset
by removing grasps which are closer than a parsing threshold
ξ in Euclidean distance to a higher ranking grasp. ξ starts at
0.0 and is increased in intervals of 0.1. Each time ξ increases,
the dataset is re-parsed. This is repeated until each object has
fewer than 20 grasps remaining. We note that this means that
the ﬁnal parsed grasp set may have a different number of
grasps for each object.
The object set we present is sized to the human hand. How-
ever, some robot hands are larger than the human hand. We
therefore scale the objects based on hand size. For example,
the Schunk SDH hand is approximately 1.5 times the size of
the human hand. We therefore multiply the dimensions of the
objects by 1.5 when we plan grasps for the Schunk SDH.
2) Human Dataset: Our grasp planner does not have a
robust model of the human hand, so we instead generate a
dataset for the human hand using grasps generated by test
subjects.
Subjects were asked to don a instrumented dataglove (a
Cyberglove III) and grasp objects in the object set. After the
subjects grasp a given object stably, their joint angles are
collected from the Cyberglove. We collected grasps from ﬁve
subjects. The human dataset is not parsed because there are
no metrics available which would tell us how well each of the
subjects grasped the objects.

C. Fitting a Subspace to a Grasp Dataset
We hypothesized that grasps created by holding the objects
in our set would exist in the pose space of the hand, but lie
along the basis vectors of T . If this is true, then we can ﬁnd
a model of T by ﬁtting a subspace to the grasps in G . We
wish for this model of T to explain enough of G to enable
teleoperation.
A model of T would provide us with the information
necessary to create a teleoperation mapping for the hand. The
model of the subspace consists of an origin and three N -
dimensional orthogonal vectors, which describe the bases of
the subspace. For a given hand, the origin pose of the subspace
provides us with an origin pose o for T , and the basis vectors
provide us with a projection matrix A.
To ﬁnd the model of T , we ﬁt a subspace to the set of
grasps G using RANSAC [33], [34]. RANSAC is a consensus
based algorithm used to ﬁnd the model underlying data with

(a)

(b)
Fig. 6.
(a) Four example objects from our set, held by the Schunk SDH
hand. From left to right: Object 2, Object 6, Object 5, and Object 8 and (b) a
visualization of where we predict grasps will lie in the teleoperation subspace
when a hand is holding objects in the object set. This image can be used to
interpret the last column of Table II, where ψ = [α, σ , ].

a large number of outliers. The basic algorithm of RANSAC
is as follows:
• Generate a model hypothesis using random samples from
the dataset. The number of samples selected should be the
minimum number needed to deﬁne your model.
• Looking at all the points in the dataset, determine how
well the hypothesis model explains/supports the data. If
it is better than the best hypothesis to date, update the
best model to your current hypothesis.
This process is repeated M times, where M is a number
high enough to ensure that the probability of ﬁnding a model
that is better than the current best model is sufﬁciently low.
For our algorithm, we use M = 2, 000, 000. When RANSAC
is parallelized, its runtime is 187 minutes on a computer with
24 CPUs.
Since our subspace is three dimensional, our model hypoth-
esis consists of an origin grasp and three basis vectors. We also
keep track of which of the three basis vectors corresponds to
size, spread, and curl.
To generate a model hypothesis, we select random samples
from the dataset. We ﬁrst select an origin grasp. We specify
that the origin must come from the set of grasps where the
hand is holding Object 8 (G8 ). Preliminary tests with several
hands showed that the performance for this origin was the
highest. We hypothesize that this is because the constraints
of the enveloping grasps are greater than the constraints of
ﬁngertip grasps. This gives the grasp planner (and the human)
fewer options in how to grasp the objects, so the grasps have

less variability.
Next, we select three additional grasps. We specify that
each additional grasp must be selected from an object whose
position in the subspace is identical to the origin object, except
along a single basis vector. Since we have speciﬁed the origin,
we randomly select one grasp from the set where the hand is
holding Object 7 (G7 ), another grasp from G4 , and the last
grasp from G6 . These objects correspond to the size, curl and
spread directions, respectively.
After we choose four random samples, we generate our
model hypothesis. We subtract the three non-origin grasps
from the origin and normalize the result to ﬁnd the three
basis vectors of the subspace. We randomize the order of
the three basis vectors, then orthogonalize these three vectors
using Gram-Schmidt orthogonalization [35].
We determine the quality of our model hypothesis by how
many objects from the object set can be grasped using the
hypothesized subspace. To determine how well the hypothesis
model explains the grasp data, we ﬁnd the inliers in G by cal-
culating the distance from each grasp to the subspace deﬁned
by the hypothesis model. The distance d from each grasp g
to the hypothesis subspace model is found by projecting the
grasp onto the subspace gproj and then ﬁnding the distance
between the true grasp and the projected grasp :

P = ω(cid:62)
1 · ω1 + ω(cid:62)
2 · ω2 + ω(cid:62)
3 · ω3
gproj = P · (g − o) + o
d = (cid:107)g − gproj (cid:107)

(14)
(15)
(16)

where ω1 , ω2 , and ω3 are the basis vectors of the hypothesis
model, and o is the origin of the hypothesis model. Grasps
which are closer than ξ (the ﬁnal threshold used when we
parsed the datasets) in Euclidean distance to the subspace are
considered inliers:

(cid:40)

g =

inlier
not an inlier

if |d| < ξ
otherwise

(17)

Since we did not parse the human grasps, we simply set ξ
for the human dataset as 0.1.
Many RANSAC algorithms use the total number of inliers
to estimate how well the model explains the data; however, we
wish all parts of our subspace to ﬁt equally well. If the grasps
for a few objects contain all the inliers and grasps for all other
objects are far from the subspace, we do not consider this to
be a sufﬁciently good model, even if it has the highest total
number of inliers. We want our model to be able to grasp all
the objects in our dataset. So, we use a tiered metric which
considers the quality of ﬁt in all parts of the subspace.
Our tiered metric has 4 components, ranked by importance:
1) Minimum number of inliers per object, over all the
objects in our set. For example, if each object has at
least one inlying grasp, then we consider that model to
be better than a model where one or more of the objects
have no inliers, because we can grasp all the objects in
our object set.
2) Number of objects which have the minimum number of
inliers. If the minimum number of inliers is one, if only
one object has one inlier and all other objects have more

than one inlier, this is preferable to all of the objects only
having one inlier.
3) Total number of inliers across all grasps. The higher the
number of inliers, the better the model.
4) Sum of the distances (error) between all the grasps and
the subspace. The model with the lower error is better.
When two models tie in one or more of the metrics, the
subsequent tier is used as a tiebreaker to determine the best
model between two hypotheses.
Once we have generated and tested a sufﬁcient number of
hypotheses, the hypothesis model which explained the data the
best, as deﬁned by our metric, is considered to be the model
of our subspace.
We perform one more processing step to ﬁnd our ﬁnal
model. The same preliminary testing which indicated the best
origin for the subspace model was Object 8 also showed that
this was not the best origin when we combined the mappings
for two hands into a complete teleoperation pipeline. For the
ﬁnal processing step, we choose a grasp from a different object
to serve as the origin; empirically, we have found Object 1
to serve best in this role. For the robot hand, we move the
origin to the grasp from Gobj ect1 that is closest to the original
subspace. For the human hand, we ask the teloperator to grasp
a model of Object 1, and use the resulting pose as the subspace
origin. Performing this additional step for every teleoperator
also calibrates the mapping to the dimensions of their hand.

D. A Complete Mapping
1) Projection Matrix: We use the three basis vectors of
the subspace model found by RANSAC as the vectors which
make up the projection matrix A. During RANSAC, we keep
track of which of the three vectors corresponds to size, spread,
and curl. We use this information to determine which vector
is σH , αH , and H , respectively.
2) Origin Pose: For a robot hand, the origin of the sub-
space model found by RANSAC becomes o, the origin of
the teleoperation mapping. For a human hand, we ﬁnd the
origin by asking the user to perform a calibration pose at the
beginning of teleoperation. A standardized pose will not work
for humans because user hand size varies.
3) Scaling Factor: To determine the scaling factors for our
mapping, we require poses for the hand at the extremes of the
subspace. We could select grasps from the dataset to determine
these ranges, but it is faster to use a simple assumption and
an iterative solution to ﬁnd them.
For a robot, we assume that
the hand will achieve its
minimum and maximum value along each basis vector when
the joints relevant to that basis are at some combination of their
maximum and minimum values. We are given the maximum
and minimum values for each joint from our robot model and
the projection matrix tells us which joints are relevant to each
subspace basis (if they are non-zero, they are relevant). For
each subspace basis, we iterate through all the combinations
of the relevant joints at their maximum and minimum values.
These become the poses which show the hand’s kinematic
extrema.
For the human hand, we require the human to perform four
calibration poses which will give us the ranges along each

basis vector (see Figure 4). We require these poses because
the differences in user hand size mean ranges which work for
one person may not work for another.
For both human and robot hands, we project all the poses
into the subspace, using the projection matrix and the origin
of our subspace model. We use the largest and the smallest
value for each of the dimensions to calculate the range of that
∗ .
basis, and use Eq. 8 and Eq. 10 to ﬁnd δ and δ
4) Using the Mapping to Teleoperate: Once we have A,
∗ for both hands, we can use Equation 13 to
o, δ , and δ
teleoperate the slave hand.

V I . EX PER IM EN T S

To validate that both the algorithmic and empirical map-
pings project to a subspace which is relevant to teleoperation,
we asked nine novice users to complete manipulation tasks
using both our mappings, and two state-of-the-art mappings
as baselines. Four of the novices performed pick and place
experiments with a Schunk SDH hand, and ﬁve of them
performed in-hand manipulation tasks with a two ﬁngered
gripper.
For both the pick and place and the in-hand manipulation
experiments, subjects were presented with the objects in the
same order, and completed objects with one control before
moving on to another control. We randomized the order in
which the subjects used the controls. We also did not tell
subjects how each of the control methods worked, but gave
them two minutes to play with the hand when they were
introduced to a new control method. The subjects gave their
informed consent and the study was approved by the Columbia
University IRB.
Below, we describe the mappings used in both experiments
and the experiments themselves.

A. Subspace Teleoperation Mappings
We generated teleoperation mappings for the human hand,
the Schunk SDH and a two ﬁngered gripper. For each hand,
we created teleoperation mappings empirically and algorith-
mically, using the procedures outlined in Section IV and
Section V, respectively. Figure 7 shows the resulting mappings
for all three hands.

B. State-of-the-Art Comparisons
We selected two state-of-the-art
teleoperation mappings
with which to compare our subspace mappings:
1) Fingertip Mapping: We use ﬁngertip mapping as a state-
of-the-art comparison because it is one of the most common
mapping methods and it is applicable to precision grasps,
particularly with smaller objects [4]. The ﬁngertip mapping
was designed as follows: ﬁrst, we found the cartesian positions
of the thumb, index, and ring ﬁngers of the human hand using
the joint values from the Cyberglove and forward kinematics.
The kinematic model we used for the human hand is described
elsewhere [36]. We multiplied these positions by a scaling
factor of 1.5, the ratio between an average human ﬁnger and
the robot ﬁngers. This ratio is 1.5 for both the Schunk SDH

Human Hand

Schunk SDH

Two Finger Gripper

Empirical
Mapping

Algorithmic
Mapping

Fig. 7. Teleoperation mappings generated for the human hand, Schunk SDH, and two ﬁnger gripper, both empirically and algorithmically. Each of the spokes
represents a degree of freedom for the hand, and the blue (spread), red (size) and green (curl) values along those spokes indicate the values in the αH , σH ,
and H , respectively, at that degree of freedom.

JO IN T MA P P ING FROM THE CYB ERGLOVE TO TH E SCHUNK SDH

TABLE III

Cyberglove Sensor

Name
Index/Middle adduction
Thumb adduction
Thumb distal ﬂexion
Index proximal ﬂexion
Index medial ﬂexion
Middle proximal ﬂexion
Middle medial ﬂexion

Joint
Label
e
a
b
c
d
f
g

Robotic Hand Joints

Name
Finger 1 adduction
Thumb proximal ﬂexion
Thumb distal ﬂexion
Finger 1 proximal ﬂexion
Finger 1 distal ﬂexion
Finger 2 proximal ﬂexion
Finger 2 distal ﬂexion

Joint
Label
0
1
2
3
4
5
6

and the two ﬁngered gripper. We assign each human ﬁnger a
corresponding robot ﬁnger (for the two ﬁnger gripper, only the
thumb and the index ﬁngers are used). We translated the coor-
dinates from the hand frame into the ﬁnger frame to ﬁnd the
desired robotic ﬁngertip positions. Finally, inverse kinematics
determined the joint angles which placed the robot ﬁngertips
at these positions. This process is documented elsewhere for
the Schunk SDH [11], and we use the same approach for the
two ﬁngered gripper.
2) Joint Mapping: We chose joint mapping as the second
state-of-the-art comparison because of its common use in
the ﬁeld, its applicability to power grasps, and because we
predicted that explicit control over individual joints of the
robotic ﬁngers would be intuitive for novice users [3]. To
implement
joint mapping, we assigned each of the robot
joints to a corresponding human hand joint. This mapping
can be found in Table III for the Schunk SDH and Table IV
for the two ﬁngered gripper. Once we received joint angles
from the Cyberglove, we set the corresponding joints of the
robot hand to the same values. Preliminary tests showed
teleoperation is difﬁcult if the robot thumb’s proximal joint
maps to the human thumb’s metacarpophalangeal (MCP) joint.
We therefore mapped the Schunk thumb’s proximal joint and
the left proximal joint of the two ﬁnger gripper to the human
thumb’s adductor.

C. Pick and Place Experiments
We asked four novice users to complete pick and place
experiments with our mappings and with state-of-the-art map-
pings.
We asked our novice users to pick and place the ten
objects shown in Figure 8 using a Schunk SDH mounted

on a Sawyer arm. The Sawyer’s end effector position and
orientation are controlled with a simple cartesian controller
(completely separate from the hand control) using a magnetic
tracker (Ascension 3D Guidance trakSTARTM ) placed on the
back of the user’s hand. Figure 8 shows the experimental setup.
Subjects were asked to pick up one object at a time and move
the object across a line based on visual feedback.

JO INT MA P P ING FROM THE CYB ERG LOVE TO TH E TWO FINGER GR I P P ER

TABLE IV

Cyberglove Sensor

Name
Thumb adduction
Index distal ﬂexion
Middle proximal ﬂexion
Middle medial ﬂexion

Joint
Label
a
b
c
d

Robotic Hand Joints

Name
Finger 1 proximal ﬂexion
Finger 1 distal ﬂexion
Finger 2 proximal ﬂexion
Finger 2 distal ﬂexion

Joint
Label
0
1
2
3

Fig. 9.
(Left) Top view of experimental set-up with object in a precision
grasp, (Middle) top view of experimental set-up with object in a power grasp,
and (right) object set for our in-hand manipulation experiments.

TABLE V

P ICK AND P LAC E EX PER IM EN T R E SU LT S

Objects
All
Small
Large
Irregular

Fingertip
75.5 ± 6.8
92.4 ± 12.0
54.2 ± 10.4
86.9 ± 10.9

Joint
63.8 ± 7.1
97.9 ± 11.9
44.1 ± 8.7
55.9 ± 12.3

Empirical

25.4 ± 3.0
35.2 ± 8.4
19.4 ± 2.4
23.5 ± 4.0

Algorithmic
44.2 ± 5.2
55.4 ± 12.5
34.1 ± 5.5
46.4 ± 9.5

(a) AV ERAGE T IM E TO P ICK AND P LAC E ( S ECOND S )

Objects
All
Small
Large
Irregular

Fingertip
2.1 ± 0.2
2.0 ± 0.4
1.8 ± 0.3
2.8 ± 0.4

Joint
1.9 ± 0.2
2.4 ± 0.4
1.6 ± 0.3
1.8 ± 0.3

Empirical

1.2 ± 0.1
1.3 ± 0.2
1.0 ± 0.0
1.2 ± 0.1

Algorithmic
1.7 ± 0.2
2.3 ± 0.5
1.4 ± 0.2
1.5 ± 0.3

(b) AV ERAGE TR I E S TO P ICK AND P LAC E

Objects
All
Small
Large
Irregular

Fingertip
6.0 ± 0.8
1.0 ± 0.4
3.3 ± 0.5
1.8 ± 0.6

Joint
6.8 ± 0.8
0.8 ± 0.3
3.8 ± 0.3
2.3 ± 0.5

Empirical

10.0 ± 0.0
3.0 ± 0.0
4.0 ± 0.0
3.0 ± 0.0

Algorithmic
9.0 ± 0.7
2.3 ± 0.5
2.8 ± 0.3

4.0 ± 0.0

(c) AV ERAGE NUMB ER O F OB J ECT S P ICK ED

Fig. 8.
(Left) Experimental set-up, and (right) object set for our pick and
place experiments.

D. In-Hand Manipulation Experiments
We asked ﬁve novice users to perform in-hand manipulation
tasks with a two ﬁngered gripper. The gripper is stationary and
placed on a table. An object was placed on the table between
the distal links of the ﬁngers in a precision grasp. We then
asked the subjects to transition the object to a power grasp by
moving the object closer to the palm and enveloping it with
the robot ﬁngers. For a transition to be counted as successful,
the subject had to move an object so that it was in contact with
both the proximal and distal links on one ﬁnger and at least one
link on the other ﬁnger. Figure 9 shows the experimental setup
and the objects used for the in-hand manipulation experiments.

V I I . R E SU LT S

For both experiments, we use three metrics to determine
performance: time to complete the task, how many tries the
subject needed to complete the task and how many objects
for which the task was completed. We describe these metrics
below.

Our ﬁrst metric was time to completion: we timed how long
it took for the user to perform the task with each object. If
the user did not complete the task in the given time limit (two
minutes for pick and place experiments and one minute for
in-hand manipulation experiments), they were considered to
be unable to pick up the object and their ﬁnal time was set to
the respective time limit. We elected to shorten the time limit
for the in-hand manipulation because there is no arm to move,
and therefore the task should be completed faster.
Our second metric is the number of tries needed to complete
a task. We deﬁne a try as a completed task, an attempt where
the user drops the object, an attempt where the user knocks
over the object, or an attempt where the user knocks an object
out of the range of the robot hand. In the last two scenarios, the
object is reset by the experimenter. If the subject was unable
to pick the object, we report the number of tries the user took
before the time elapsed.
Our ﬁnal metric is how many objects for which the task
was completed: for each mapping we counted the number of
objects with which the user was able to successfully complete
the assigned task.

A. Pick and Place Results
We report our results as the average across all subjects. We
report averages for all objects, for large objects (the box, ball,
wire spool, and water bottle), for small objects (the peg, valve,
and marbles), and for irregular objects (the drill, screwdriver,
and lego stack). We classify the drill, screwdriver, and legos
as irregular objects because their width to length ratios and
their irregular shapes allow users to pick up the objects with
a wide variety of grasps. Users tended to pick up the large
objects and the small objects with consistent grasps.
We report the average time to pick and place across all
subjects in Table Va. Across all subjects and all objects,
novices using the ﬁngertip mapping took 3 times longer than
when using the empirical subspace mapping, and 1.7 times
longer than when using the algorithmic subspace mapping.
Similarly,
joint mapping took 2.5 times longer than the
empirical subspace mapping and 1.4 times longer than the
algorithmic subspace mapping.
For the four combinations of objects we look at (all objects,
small, large, and irregular), the empirical subspace mapping
took the least amount of time, with the algorithmic subspace
mapping coming in second in every case. The algorithmic
subspace mapping was at least 1.6 times slower than the em-
pirical subspace mapping for all of these object combinations.
However, in turn, the state-of-the-art mappings were at least
1.3 times slower than the algorithmic subspace mapping.
We report the average number of tries in Table Vb. In all
object combinations, users were able to pick and place objects
with the fewest amount of tries using the empirical subspace
mapping. The algorithmic subspace mapping came in second
in all cases except for with the small objects, where ﬁngertip
mapping came in second.
Finally, we report the average number of objects the users
were able to pick up with each of the mappings in Table Vc.
For all the objects, the maximum number of objects that can be
picked is 10, for the small and irregular objects, the maximum
is three, and for the large objects, the maximum is four.
The empirical subspace mapping allowed every novice to
pick up every object. The algorithmic subspace mapping
allowed the novices to pick up most objects, and the state-
of-the-art mappings allowed novices to pick up the majority
of objects, but still fewer than either of the subspace mapping
methods.

B. In-Hand Manipulation Results
We report our results as the average across all subjects. We
report averages for all objects, for circular objects (the bottle,
peanut butter container, and goblet), and for irregularly shaped
objects (the wheels, legos, lettuce, and mustard).
We report the average time to perform the in-hand ma-
nipulation task across all subjects in Table VIa. Across all
subjects and all objects, novices using the ﬁngertip mapping
took 2 times longer than when using the empirical subspace
mapping, and 1.3 times longer than when using the algorithmic
subspace mapping. Joint mapping performed about the same
as the empirical subspace mapping and was 1.5 times faster
than the algorithmic subspace mapping.

IN -HAND MAN I PU LAT ION EX PER IM EN T R E SU LT S

TABLE VI

Objects
All
Circular
Irregular

Fingertip
16.6 ± 2.3
15.4 ± 2.6
17.5 ± 3.6

Joint
8.8 ± 1.7
7.5 ± 1.5

9.7 ± 2.8

Empirical

8.5 ± 1.7
5.6 ± 0.9

10.7 ± 2.9

Algorithmic
13.1 ± 2.0
13.6 ± 2.6
12.8 ± 2.9

(a) AV ERAGE T IM E TO TRAN S I T ION FROM A PREC I S ION GRA S P TO A POW ER
GRA S P ( S ECOND S )

Objects
All
Circular
Irregular

Fingertip
1.6 ± 0.2
1.1 ± 0.1
2.1 ± 0.3

Joint
1.2 ± 0.1
1.1 ± 0.1
1.3 ± 0.2

Empirical

1.1 ± 0.0
1.0 ± 0.0
1.1 ± 0.1

Algorithmic
1.3 ± 0.1
1.1 ± 0.1
1.5 ± 0.2

(b) AV ERAGE TR I E S TO TRAN S IT ION

Objects
All
Circular
Irregular

Fingertip
6.6 ± 0.4
3.0 ± 0.0
3.6 ± 0.4

Joint

7.0 ± 0.0
4.0 ± 0.0
3.0 ± 0.0

Empirical
6.8 ± 0.2
3.0 ± 0.0
3.8 ± 0.2

Algorithmic

7.0 ± 0.0
4.0 ± 0.0
3.0 ± 0.0

(c) AV ERAGE NUMB ER O F OB J ECT S MAN I PU LATED

For the three combinations of objects we look at (all objects,
circular, and irregular), manipulation with the empirical sub-
space mapping took the least amount of time for all objects and
the circular objects, with joint mapping taking the least amount
of time for the irregular objects. In all cases, the algorithmic
subspace mapping was third and ﬁngertip mapping took the
longest.
We report the average number of tries subjects took to ma-
nipulate the objects in Table VIb. In all object combinations,
users were able to transition the objects with the fewest amount
of tries using the empirical subspace mapping.
Finally, we report the average number of objects the users
were able to manipulate with each of the mappings in Ta-
ble VIc. For all objects, the maximum number of objects that
can be manipulated is 7, for the circular objects, the maximum
is three, and for the irregular objects, the maximum is four.
The joint and algorithmic subspace mappings allowed every
novice to manipulate every object. For the empirical subspace
mapping, one subject was not able to transition one object,
and for the ﬁngertip mapping, one subject was not able to
manipulate two objects.

V I I I . D I SCU S S ION

We begin by discussing the teleoperation mappings gen-
erated algorithmically and empirically. In both cases, novice
users were able to complete two different types of manip-
ulation tasks using two different non-anthropomorphic robot
hands. This shows that the mappings created with both meth-
ods rely on a subspace which is relevant to teleoperation
and which can encompass the range of motion necessary to
manipulate a variety of objects in different ways. Similarly,
these experiments show that the subspace is relevant for more
than one hand.
The algorithmic subspace mapping, in particular, not only
shows that the subspace we propose is relevant to teleopera-
tion, but that the beneﬁt of using such a subspace does not

derive exclusively from the human intuition used to create the
mapping. Since this mapping is created automatically, without
hand-speciﬁc intuition from the mapping creator, and can still
enable teleoperation, we conclude that T is a concept that has
value even when there is no human intelligence ‘built into’
the mapping. That being said, we do note that the empirical
subspace mapping, which is deﬁned with the beneﬁt of human
intuition, outperforms the algorithmic subspace mapping. The
use of human intuition to deﬁne the basis vectors, while not
exclusively deﬁning the value of the subspace, can make it a
more powerful and intuitive control.
Both experiments showed that the empirical and algorithmic
subspace mappings were as intuitive as or more intuitive
than the state-of-the-art mappings we selected as baseline
comparisons. We measure intuitiveness as the combination of
our three metrics: we hypothesize that controls which allow
the user to manipulate more objects in less time, with fewer
tries are more intuitive. We note that the measure of which
method is preferable is a trade off between intuitiveness for the
teleoperator and intuitiveness for the person who must generate
the teleoperation mapping. For our real-time experiments, the
three metrics we have selected only measure intuitiveness for
the teleoperator.
For the pick and place experiments, the empirical subspace
mapping was the most intuitive control for novices, and the
algorithmic subspace mapping was the second most intuitive
control. In all metrics, the empirical and algorithmic subspace
mappings outperformed the state-of-the-art. The empirical
subspace mapping provides the greatest advantage for small
objects, but still has a signiﬁcant edge for all other object com-
binations. It is worth noting that the standard error we report
for all the metrics is also lowest for the empirical subspace
mapping. We hypothesize that this means the novices were
able to use the empirical subspace mapping more consistently
than the other controls.
For the in-hand manipulation experiments, our empirical
subspace mapping proved the most effective in terms of time
to perform the experiments, followed by joint mapping and the
algorithmic subspace mapping. A similar result was observed
for the average number of tries required to succeed; for total
objects manipulated, all three of these mappings showed simi-
lar performance, with joint mapping and algorithmic subspace
mapping having a very slight advantage. This ranking is thus
less deﬁnitive than for the pick and place experiments because
different mappings performed better for different metrics.
We note that the in-hand manipulation experiments lend
themselves particularly well to joint mapping, which allows
users to individuate the robot digits, an advantage when
performing in-hand manipulation, and something which our
subspace mappings do not allow. This individuation provides
a particular advantage for irregular objects and is likely why
joint mapping outperformed the empirical subspace mapping
in time to completion for that particular object category.
These experiments show that our two subspace mapping
methods can generalize across different hands and different
manipulation tasks.

IX . CONC LU S ION S AND FU TUR E WORK

In this paper, we propose an intuitive, low dimensional
mapping between the pose spaces of the human hand and
non-anthropomorphic robot hands. We present an empirical
algorithm to generate this mapping that leverages the user’s
intuition to deﬁne hand motions for a speciﬁc kinematic
conﬁguration. We also propose an algorithmic method to
generate the mapping, which is completely automatic. This
automated process is made possible by deﬁning the subspace
independently of hand kinematics, using objects to deﬁne hand
motions that span the desired subspace.
We validate both the empirical and algorithmic subspace
mappings with real-time teleoperation experiments with novice
users on two kinematically different robotic hands. We found
that, for pick and place experiments, our empirical subspace
mapping was most intuitive for users, with the algorithmic
subspace mapping still performing better than state-of-the-
art alternatives. For the in-hand manipulation experiments,
we found that our empirical subspace mapping performed as
well as joint mapping, one of the state-of-the-art methods, and
better than ﬁngertip mapping, the other baseline we employed.
For the in-hand manipulation experiments, the algorithmic
subspace mapping was generally less intuitive for novices than
joint mapping, but more intuitive than ﬁngertip mapping.
The real-time teleoperation experiments show that the sub-
space we propose is relevant to teleoperation for multiple
hands with distinct kinematics and for different manipula-
tion tasks. This is the ﬁrst time, to our knowledge, that a
teleoperation mapping generated without requiring a user’s
understanding of hand-speciﬁc kinematics has been shown to
be intuitive for novices for real-time teleoperation. The fact
that the algorithmic mapping can enable teleoperation shows
that the subspace encodes useful information for teleoperation
that does not rely exclusively on human intuition.
The future of this work could take a number of directions.
Our experiments in this paper show that
the subspace is
relevant for at least three different hands, and we would like
to continue to show that is relevant for other hands with
different kinematic conﬁgurations. We have already shown that
the teleoperation subspace is suitable for lower dimensional
controls, like electromyography (EMG) [37] and would like
to validate this control with more kinematic conﬁgurations as
well. Finally, we would like to show that the subspace is useful
for more complex tasks, like assembling and disassembling
machinery.

ACKNOW L EDGM ENT

We would like to thank Rami J. Hamati for his help setting
up the hardware for our experiments.

R E F ER ENC E S

[1] M. Ferre, R. Aracil, C. Balaguer, M. Buss, and C. Melchiorri, Advances
in telerobotics. Springer, 2007, vol. 31.
[2] C. Meeker, T. Rasmussen, and M. Ciocarlie, “Intuitive hand teleoper-
ation by novice operators using a continuous teleoperation subspace,”
in Robotics and Automation, 2018. Proceedings. ICRA’18. 2018 IEEE
International Conference on, 2018.

[3] I. Cerulo, F. Ficuciello, V. Lippiello, and B. Siciliano, “Teleoperation
of the schunk s5fh under-actuated anthropomorphic hand using human
hand motion tracking,” Robotics and Autonomous Systems, vol. 89, pp.
75–84, 2017.
[4] R. N. Rohling, J. M. Hollerbach, and S. C. Jacobsen, “Optimized
ﬁngertip mapping: a general algorithm for robotic hand teleoperation,”
Presence: Teleoperators & Virtual Environments, vol. 2, no. 3, pp. 203–
220, 1993.
[5] L. Pao and T. H. Speeter, “Transformation of human hand positions for
robotic hand control,” in Robotics and Automation, 1989. Proceedings.,
1989 IEEE International Conference on.
IEEE, 1989, pp. 1758–1763.
[6] M. V. Liarokapis, P. Artemiadis, C. Bechlioulis, and K. Kyriakopoulos,
“Directions, methods and metrics for mapping human to robot motion
with functional anthropomorphism: A review,” School of Mechanical
Engineering, National Technical University of Athens, 2013.
[7] R. Chattaraj, B. Bepari, and S. Bhaumik, “Grasp mapping for dexterous
robot hand: A hybrid approach,” in Contemporary Computing (IC3),
International Conference on.
IEEE, 2014, pp. 242–247.
[8] D. G. Kamper, E. G. Cruz, and M. P. Siegel, “Stereotypical ﬁngertip
trajectories during grasp,” Journal of neurophysiology, vol. 90, no. 6,
pp. 3702–3710, 2003.
[9] L. Colasanto, R. Su ´arez, and J. Rosell, “Hybrid mapping for the assis-
tance of teleoperated grasping tasks,” IEEE Transactions on Systems,
Man, and Cybernetics: Systems, vol. 43, no. 2, pp. 390–401, 2013.
[10] W. B. Grifﬁn, R. P. Findley, M. L. Turner, and M. R. Cutkosky, “Cali-
bration and mapping of a human hand for dexterous telemanipulation,”
in ASME IMECE 2000 Symposium on Haptic Interfaces for Virtual
Environments and Teleoperator Systems, 2000, pp. 1–8.
[11] T. Geng, M. Lee, and M. H ¨ulse, “Transferring human grasping synergies
to a robot,” Mechatronics, vol. 21, no. 1, pp. 272–284, 2011.
[12] S. Ekvall and D. Kragic, “Interactive grasp learning based on human
demonstration,” in Robotics and Automation. Proceedings. IEEE Inter-
national Conference on, vol. 4.
IEEE, 2004, pp. 3519–3524.
[13] T. Wojtara and K. Nonami, “Hand posture detection by neural network
and grasp mapping for a master slave hand system,” in Intelligent
Robots and Systems, 2004.(IROS 2004). Proceedings. 2004 IEEE/RSJ
International Conference on, vol. 1.
IEEE, 2004, pp. 866–871.
[14] S. Li, X. Ma, H. Liang, M. G ¨orner, P. Ruppel, B. Fang, F. Sun, and
J. Zhang, “Vision-based teleoperation of shadow dexterous hand using
end-to-end deep neural network,” in 2019 International Conference on
Robotics and Automation (ICRA).
IEEE, 2019, pp. 416–422.
[15] M. Santello, M. Flanders, and J. F. Soechting, “Postural hand synergies
for tool use,” Journal of Neuroscience, vol. 18, no. 23, pp. 10 105–
10 115, 1998.
[16] T. Feix, R. Pawlik, H.-B. Schmiedmayer, J. Romero, and D. Kragic,
“A comprehensive grasp taxonomy,” in Robotics, science and systems:
workshop on understanding the human hand for advancing robotic
manipulation, 2009, pp. 2–3.
[17] F. Ficuciello, G. Palli, C. Melchiorri, and B. Siciliano, “Postural syner-
gies and neural network for autonomous grasping: a tool for dextrous
prosthetic and robotic hands,” in Converging Clinical and Engineering
Research on Neurorehabilitation. Springer, 2013, pp. 467–480.
[18] S. Kim, M. Kim, J. Lee, and J. Park, “Robot hand synergy mapping
using multi-factor model and emg signal,” in Experimental robotics.
Springer, 2016, pp. 671–683.
[19] F. Ficuciello, G. Palli, C. Melchiorri, and B. Siciliano, “Planning and
control during reach to grasp using the three predominant ub hand iv
postural synergies,” in Robotics and Automation (ICRA), 2012 IEEE
International Conference on.
IEEE, 2012, pp. 2255–2260.
[20] M. T. Ciocarlie and P. K. Allen, “Hand posture subspaces for dexterous

robotic grasping,” The International Journal of Robotics Research,
vol. 28, no. 7, pp. 851–867, 2009.
[21] C. Della Santina, C. Piazza, G. Grioli, M. G. Catalano, and A. Bicchi,
“Toward dexterous manipulation with augmented adaptive synergies:
The pisa/iit softhand 2,” IEEE Transactions on Robotics, no. 99, pp.
1–16, 2018.
[22] K. Yamane, Y. Ariki, and J. Hodgins, “Animating non-humanoid char-
acters with human motion data,” in Proceedings of the 2010 ACM
SIGGRAPH/Eurographics Symposium on Computer Animation.
Eu-
rographics Association, 2010, pp. 169–178.
[23] A. Shon, K. Grochow, A. Hertzmann, and R. P. Rao, “Learning shared
latent structure for image synthesis and robotic imitation,” in Advances
in neural information processing systems, 2006, pp. 1233–1240.
[24] B. Delhaisse, D. Esteban, L. Rozo, and D. Caldwell, “Transfer learning
of shared latent spaces between robots with similar kinematic structure,”
in Neural Networks (IJCNN), 2017 International Joint Conference on.
IEEE, 2017, pp. 4142–4149.
[25] A. Kheddar, C. Tzafestas, and P. Coiffet, “The hidden robot concept-high
level abstraction teleoperation,” in Intelligent Robots and Systems, 1997.
IROS’97., Proceedings of the 1997 IEEE/RSJ International Conference
on, vol. 3.
IEEE, 1997, pp. 1818–1825.
[26] A. Kheddar, “Teleoperation based on the hidden robot concept,” IEEE
Transactions on Systems, Man, and Cybernetics-Part A: Systems and
Humans, vol. 31, no. 1, pp. 1–13, 2001.
[27] S. B. Kang and K. Ikeuchi, “Toward automatic robot instruction from
perception-mapping human grasps to manipulator grasps,” IEEE trans-
actions on robotics and automation, vol. 13, no. 1, pp. 81–95, 1997.
[28] G. Gioioso, G. Salvietti, M. Malvezzi, and D. Prattichizzo, “An object-
based approach to map human hand synergies onto robotic hands with
dissimilar kinematics,” Robotics: Science and Systems VIII, pp. 97–104,
2013.
[29] ——, “Mapping synergies from human to robotic hands with dissimilar
kinematics: an approach in the object domain,” IEEE Transactions on
Robotics, vol. 29, no. 4, pp. 825–837, 2013.
[30] G. Salvietti, M. Malvezzi, G. Gioioso, and D. Prattichizzo, “On the use
of homogeneous transformations to map human hand movements onto
robotic hands,” in 2014 IEEE International Conference on Robotics and
Automation (ICRA).
IEEE, 2014, pp. 5352–5357.
[31] G. Salvietti, L. Meli, G. Gioioso, M. Malvezzi, and D. Prattichizzo,
“Object-based bilateral telemanipulation between dissimilar kinematic
structures,” in 2013 IEEE/RSJ International Conference on Intelligent
Robots and Systems.
IEEE, 2013, pp. 5451–5456.
[32] C. Ferrari and J. Canny, “Planning optimal grasps,” in Robotics and
Automation, 1992. Proceedings., 1992 IEEE International Conference
on.
IEEE, 1992, pp. 2290–2295.
[33] K. G. Derpanis, “Overview of the ransac algorithm,” Image Rochester
NY, vol. 4, no. 1, pp. 2–3, 2010.
[34] R. Raguram, J.-M. Frahm, and M. Pollefeys, “A comparative analysis
of ransac techniques leading to adaptive real-time random sample
consensus,” in European Conference on Computer Vision.
Springer,
2008, pp. 500–513.
˚A. Bj ¨orck, “Numerics of gram-schmidt orthogonalization,” Linear Alge-
bra and Its Applications, vol. 197, pp. 297–316, 1994.
[36] S. Cobos, M. Ferre, M. S. Uran, J. Ortego, and C. Pena, “Efﬁcient
human hand kinematics for manipulation tasks,” in 2008 IEEE/RSJ
International Conference on Intelligent Robots and Systems.
IEEE,
2008, pp. 2246–2251.
[37] C. Meeker and M. Ciocarlie, “Emg-controlled hand teleoperation using
a continuous teleoperation subspace,” 2019.

[35]

