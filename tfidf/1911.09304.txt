9
1
0
2

v
o

N

1
2

]

L

C

.

s

c

[

1
v
4
0
3
9
0

.

1
1
9
1

:

v

i

X

r

a

Automatic Text-based Personality Recognition on Monologues and Multiparty
Dialogues Using Attentive Networks and Contextual Embeddings

Hang Jiang, 1 Xianzhe Zhang,2 Jinho D. Choi3

1Symbolic Systems Program, Stanford University, CA 94305
2Department of Electrical Engineering, Stanford University, CA 94305
3Department of Computer Science, Emory University, Atlanta, GA 30322
hjian42@stanford.edu, xianzhez@stanford.edu, jinho.choi@emory.edu

Abstract

Previous works related to automatic personality recognition
focus on using traditional classiﬁcation models with linguis-
tic features. However, attentive neural networks with contex-
tual embeddings, which have achieved huge success in text
classiﬁcation, are rarely explored for this task. In this project,
we have two major contributions. First, we create the ﬁrst
dialogue-based personality dataset, FriendsPersona , by
annotating 5 personality traits of speakers from Friends TV
Show through crowdsourcing. Second, we present a novel ap-
proach to automatic personality recognition using pre-trained
contextual embeddings (BERT and RoBERTa) and attentive
neural networks. Our models largely improve the state-of-art
results on the monologue Essays dataset by 2.49%, and estab-
lish a solid benchmark on our FriendsPersona. By com-
paring results in two datasets, we demonstrate the challenges
of modeling personality in multi-party dialogue.

Introduction

Automatic text-based personality recognition, as an im-
portant topic in computational psycho-linguistics, focuses
on determining one’s personality traits from text. The Big
Five Hypothesis is usually used for measuring one’s per-
sonality in ﬁve binary traits: agreeableness (AGR), consci-
entiousness (CON), extraversion (EXT), openness (OPN),
neuroticism (NEU). Recently, Majumder et al. (2017) use
Convolutional Neural Networks (CNN) with static word
embeddings and outperform the previous feature-based
systems (Mairesse et al. 2007; Tighe et al. 2016) on Es-
says dataset (Pennebaker and King 1999). However, previ-
ous works have neither explored dialogue data nor use at-
tentive networks and contextual embeddings such as BERT
(Devlin et al. 2018) and RoBERTa (Liu et al. 2019) for the
task.
To address
these issues, we create the ﬁrst dia-
logue dataset FriendsPersona for automatic personal-
ity recognition with a novel and scalable dialogue extraction
algorithm, MainSpeakerFinder. Besides, we introduce both
attentive networks and contextual embeddings (BERT and
roBERTa) to the task. We not only outperform the previous

models on the benchmark Essays dataset, but also achieve
strong baseline results on our FriendsPersona dataset.

Dataset

We focus on Essays and FriendsPersona datasets. Es-
says Dataset (Pennebaker and King 1999) is the benchmark
dataset for text-based personality recognition with 2,468
self-report essays. Our new FriendsPersona1 dataset
is developed upon the public Friends TV Show Dataset
(Chen and Choi 2016) and contains 711 extracted conversa-
tions. Each essay or conversation in the two datasets is an-
notated by the binary Big Five personality traits.

MSF Extraction Algorithm To build our own dataset, we
develop a novel dialogue extraction algorithm, MainSpeak-
erFinder (MSF), to extract sub-scenes from full scenes and
mark each sub-scene with a main speaker for three anno-
tators to annotate. First of all, we slide a window of size
5 across the full dialogue to track the utterance count per
speaker at each step. This allows us to obtain a smoothed ut-
terance count curve per speaker. Then, we ﬁnd peaks in each
speaker’s utterance count curve. At last, we extract conver-
sations around peaks as sub-scenes, in which the speaker of
the curve is always the main speaker. This extraction step is
necessary for two reasons. First, it allows annotators to fo-
cus on a short dialogue text. Moreover, the algorithm reuses
full scenes to generate many short sub-scenes, which is ben-
eﬁcial to building a comparatively large dataset for training.

Annotation Processing Due to limited funding, we anno-
tated 711 sub-scenes from the ﬁrst 4 seasons of the Friends
TV Show on Amazon Mechanical Turk. Each sub-scene is
annotated by 3 annotators for Big Five personality traits with
-1, 0, and 1. We sum scores from 3 annotators and convert
them to binary class with the median split.

Inter-Annotator Agreement
In terms of inter-annotator
agreement, we achieve an average pair-wise kappa of
54.92% between 2 annotators and Fleiss’ kappa of 20.54%
among 3 annotators across ﬁve personality traits. The low
Fleiss’ kappa is rather expected because text-based person-
ality recognition is highly subjective so that annotators often

Copyright c(cid:13) 2020, Association for the Advancement of Artiﬁcial
Intelligence (www.aaai.org). All rights reserved.

1 https://github.com/emorynlp/personality- detection

 
 
 
 
 
 
Models
Majority
LIWC (2016)
HCNN (2017)
ABCNN
ABLSTM
HAN
BERT
RoBERTa

AGR CON EXT OPN NEU
53.08
50.81
51.74
51.54
50.04
57.50
56.00
55.70
61.95
58.30
56.71
57.30
58.09
62.68
59.38
57.82
60.13
58.75
63.65
58.51
58.85
59.55
59.32
63.99
59.56
57.62
59.32
59.77
63.61
58.75
58.10
57.69
59.12
61.17
59.20
59.72
58.55
60.62
65.86
61.07

Table 1: The performance of models in accuracy on Essays.

Trait

AGR

CON

EXT

OPN

NEU

Format Majority ABCNN ABLSTM HAN BERT RoBERTa
65.58
S
56.96
63.86
64.56
64.00
62.02
S+C
56.96
59.64
60.76
61.60
59.77
57.77
F
56.96
59.21
62.01
61.88
62.77
64.49
S
53.59
56.40
57.38
58.66
55.21
56.78
S+C
53.59
54.71
57.53
57.53
57.77
55.92
F
53.59
54.99
56.67
57.81
57.07
57.35
S
56.12
59.78
59.50
60.35
61.77
64.21
S+C
56.12
59.64
62.03
57.25
60.34
59.05
F
56.12
58.93
61.60
58.37
63.62
60.05
S
64.98
65.40
66.52
67.23
67.19
68.47
S+C
64.98
66.95
66.10
66.67
67.61
66.90
F
64.98
66.39
66.52
66.39
66.33
67.19
S
53.31
56.54
57.52
58.23
59.06
60.06
S+C
53.31
57.12
59.22
59.21
60.20
58.76
F
53.31
57.82
58.66
58.36
56.49
59.33

judge different personality traits that are all acceptable for
the same utterance. This may also be attributed to the limi-
tation of our data; a higher agreement could be achieved if a
multimodal dataset (e.g. text, image, audio) is provided.

Experiments

Data Preparation

To
be
consistent
with
previous
works
(Majumder et al. 2017), Essays and FriendsPersona
datasets use accuracy and 10-fold cross validation with a
constant seed for sampling. In FriendsPersona, we
replaced speaker names with marks like ’speaker0’ and
’speaker1’. Both datasets have binary class labels for each
personality trait.

Experiment on Essays Dataset

First, we experiment baseline models on Essays dataset
with FastText embeddings. In Table 1, Majority repre-
sents the percentage of the dominant class. LIWC (2016)
represents the best LIWC-based model’s performance
(Tighe et al. 2016). HCNN is Hierarchical CNN model
(Majumder et al. 2017). ABCNN and ABLSTM represents
CNN and Bidirectional LSTM models with attention mech-
anism. HAN is Hierarchical Attention Network. Besides,
we ﬁne-tuned pre-trained base BERT and RoBERTa embed-
dings for the task. Overall, ABCNN achieves the best score
on CON, whereas RoBERTa gets the best on the other four
traits. We improve 2.49% for 5 traits on average (AGR by
2.22%, CON by 2.83%, EXT by 2.53%, OPN by 3.18%,
NEU by 1.69%).

Adaptation to FriendsPersona Dataset

We also experiment these models on FriendsPersona.
We experiment with three ways of feeding dialogue text to
the classiﬁers: 1. single (S): use only the concatenation of the
single target speaker’s utterances; 2. single + context (S+C):
use S + the concatenation of other speakers’ utterances; 3.
full (F): use the full dialogue text in the natural order.
First, the models perform the best on S out of 3 formats
for 5 traits (Table 2). It makes sense because our mod-
els are originally designed to classify simple monologue
text instead of multi-party dialogue text and S converts dia-
logue to the target speaker’s monologue. Second, BERT and
RoBERTa together achieve the most best results (10 out of
15 cases). But BERT and RoBERTa do not beat other mod-
els for CON on both datasets. At last, HAN achieves 3 best

Table 2: The performance on FriendsPersona with 3 formats.

results out of 15 cases on FriendsPersona, better than
its performance (0 case) on Essays dataset. This is because
HAN encodes dialogue on both utterance and token levels,
which allows HAN to attend the main speaker’s utterances.
In the future, we need a customized model to leverage dia-
logue information between speakers.

Conclusions

In this paper, we have two major contributions. We create
a new dialogue corpus FriendsPersona for automatic
personality recognition with a novel and scalable MSF di-
alogue extraction algorithm. Besides, we introduce both at-
tentive neural networks and contextual embeddings to the
task. We signiﬁcantly outperform the state-of-art results on
the monologue Essays dataset, and establish a solid bench-
mark on FriendsPersona. In the future, we want to de-
sign a BERT-based attention network to model utterances in
dialogue and improve the performance on our dataset. Be-
sides, we plan to assign more annotators to improve annota-
tion quality and to expand the corpus size.

References

[Chen and Choi 2016] Chen, Y.-H., and Choi, J. D. 2016.
Character identiﬁcation on multiparty conversation: Identi-
fying mentions of characters in tv shows. In SIGDIAL Con-
ference, 90–100.

[Devlin et al. 2018] Devlin, J.; Chang, M.-W.; Lee, K.; and
Toutanova, K. 2018. Bert: Pre-training of deep bidirectional
transformers for language understanding. arXiv preprint
arXiv:1810.04805.

[Liu et al. 2019] Liu, Y.; Ott, M.; Goyal, N.; Du, J.; Joshi,
M.; Chen, D.; Levy, O.; Lewis, M.; Zettlemoyer, L.; and
Stoyanov, V. 2019. Roberta: A robustly optimized bert pre-
training approach. arXiv preprint arXiv:1907.11692.

[Mairesse et al. 2007] Mairesse, F.; Walker, M. A.; Mehl,
M. R.; and Moore, R. K. 2007. Using linguistic cues for
the automatic recognition of personality in conversation and
text. Journal of artiﬁcial intelligence research 30:457–500.

[Majumder et al. 2017] Majumder, N.; Poria, S.; Gelbukh,
A.; and Cambria, E. 2017. Deep learning-based document
modeling for personality detection from text. IEEE Intelli-
gent Systems 32(2):74.

[Pennebaker and King 1999] Pennebaker, J. W., and King,
L. A. 1999. Linguistic styles: language use as an individ-
ual difference. Journal of personality and social psychology
77(6):1296.

[Tighe et al. 2016] Tighe, E. P.; Ureta, J. C.; Pollo, B. A. L.;
Cheng, C. K.; and de Dios Bulos, R. 2016. Personality
trait classiﬁcation of essays with the application of feature
reduction. In SAAIP@ IJCAI, 22–28.

