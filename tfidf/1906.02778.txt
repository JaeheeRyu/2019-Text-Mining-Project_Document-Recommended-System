ACCEPTED TO IEEE TRANSACTIONS ON COMMUNICATIONS

1

Active Deep Decoding of Linear Codes

Ishay Be’ery, Nir Raviv, Tomer Raviv, and Yair Be’ery, Senior Member, IEEE

9
1
0
2

v
o

N

1
2

]

T

I

.

s

c

[

2
v
8
7
7
2
0

.

6
0
9
1

:

v

i

X

r

a

Abstract—High quality data is essential in deep learning to
train a robust model. While in other ﬁelds data is sparse and
costly to collect, in error decoding it is free to query and label
thus allowing potential data exploitation. Utilizing this fact and
inspired by active learning, two novel methods are introduced
to improve Weighted Belief Propagation (WBP) decoding. These
methods incorporate machine-learning concepts with error de-
coding measures. For BCH(63,36), (63,45) and (127,64) codes,
with cycle-reduced parity-check matrices, improvement of up to
0.4dB at the waterfall region, and of up to 1.5dB at the error-
ﬂoor region in FER, over the original WBP, is demonstrated
by smartly sampling the data, without
increasing inference
(decoding) complexity. The proposed methods constitutes an
example guidelines for model enhancement by incorporation
of domain knowledge from error-correcting ﬁeld into a deep
learning model. These guidelines can be adapted to any other
deep learning based communication block.
Index Terms—Deep Learning, Error Correcting Codes, Ma-
chine Learning, Active Learning, Belief Propagation

I . IN TRODUC T ION

Decoding of error-correcting codes has changed over the last
few years. The rise of machine-learning methods, primarily of
the deep learning subset, changed the ﬁeld signiﬁcantly and
comprehensively.
Nachmani et al. [1], [2], proposed a model-based approach,
placing learnable weights on Tanner graph’s edges of the
Belief Propagation (BP) algorithm [3] for linear codes. This
approach is acknowledged as the original WBP, since had been
the ﬁrst in the ﬁeld to learn a parameterized BP algorithm
employing Stochastic Gradient Descent (SGD). The intuition
offered was that the weights compensated for the short cycles
in the Tanner graph. This addition improved the decoding
performance. Lian et al. [4] validated these results and further
explored spatial and temporal weights sharing. Xu et al. [5],
[6] generalized the method for both Tanner and factor graphs
of polar codes. Considering model-free approaches, Gruber
et al. [7] proposed a fully connected (FC) neural networks
(NN) approach, composed of linear and ReLU [8] layers. This
model achieved Maximum a-posteriori (MAP) performance
on very short polar codes. Bennatan et al. [9] presented a
combination of model-based and model-free approaches in
which a NN was trained by the syndrome of the received
message. Utilizing concurrent NN designs in addition with
learning the code properties, via the composed syndrome,
achieved performance improvement. Further contributions to
the ﬁeld lie in [10] and [11], where neural decoders for

This work was presented in part in the Future of Wireless Technology
Workshop, Stockholm, Sweden, June 2019.
I. Be’ery, N. Raviv, T. Raviv and Y. Be’ery are with the School of Electrical
Engineering, Tel-Aviv University, Tel-Aviv 6997801,
Israel
(e-mails:
ishaybe@gmail.com,
nirraviv89@gmail.com,
tomerraviv95@gmail.com,
ybeery@eng.tau.ac.il).

convolutional codes were proposed together with beﬁtting
training methodologies. However, these methodologies impose
substantial increase of complexity, at both training and decod-
ing (inference). Speciﬁcally, [10] explores a recurrent neural
networks (RNN) architecture for decoding while [11] focuses
on an unconstrained novel structure requiring no knowledge
of the BCJR (Bahl-Cocke-Jelinek-Raviv) algorithm.
As evident from the above recap, many researches paid great
attention to the decoder’s architecture, revealing a typical trend
in the ﬁeld. Yet another aspect of the mentioned decoding
problem is the training data. In [1], [2], training over varying
SNR (signal to noise ratio) ranges was explored. This leaded
to different decoding performances over the same validation
set. Regarding choice of a single optimal training point, Kim
et al. [10] provides guidelines for choosing the best training
SNR value. Gruber et al. [7] showed that the choice of a
training SNR value for generalization purposes is essential.
A grid search is applied to locate the optimal single training
SNR. This empirical result was followed by an analytical
study in [12]. In the study, an entropy based analysis was
performed, deriving a bound on the increase of the maximal
error probability due to mismatched training and validation
sets. The main conclusion is that no optimal training SNR for
all validation sets exists, but rather depends on the speciﬁc
validation data. One realization of this result is presented in
[13], where the WBP parameters are assumed to be SNR
dependent. Multiple NN are used to infer the value for each
parameter in the WBP algorithm at
the validation phase,
conditioned on the SNR.
Data is a vital part of deep learning methods, yet we see
that it is not fully comprehended. Many researchers focus
on preliminary choice of training data, followed by passive
generation of examples during training. We rather search for
an adaptive scheme which actively samples the training data to
feed the neural decoder. Regarding complexity, [13] empha-
sizes that distribution-speciﬁc data requires unique analysis,
but the additional NN cause extra complexity. In this paper we
narrowed the view for schemes with no additional modules.
Our main contributions are:
1) Active learning inspired approach is ﬁrst applied, to our
best knowledge, in the error-correcting codes ﬁeld.
2) Performance improvement with no decoding complexity
penalty.
3) Directing the effort of the machine-learning decoding
community to data-tailored solutions.
We call our approach active deep decoding.
The paper is organized as follows. Section II covers no-
tation and deﬁnitions. Section III explores different decoding
parameters and sets the ground for the novel methods. Section
IV introduces a detailed explanation of the methods. Section

 
 
 
 
 
 
2

ACCEPTED TO IEEE TRANSACTIONS ON COMMUNICATIONS

V presents experiments and results and section VI concludes
the paper.

I I . PR EL IM INAR I E S

edge exists between a variable v and a check node h iff
variable v participates (has coefﬁcient 1) in the condition
deﬁned by the hth row in the parity check matrix.
The initialization of the variable nodes:

A. Notation
We denote scalars in italics letters and vectors in bold.
Capital and lowercase letters stand for a random vector and
it’s realization, respectively. For example, C and c stand for
the codeword random vector and it’s realization vector. X and
Y are the transmitted and received channel words. ˆX denotes
the decoded modulated-word, while ˆC denotes the decoded
codeword. The ith element of a vector v will be denoted with
a subscript vi .
We will only deal with the AWGN channel in this work, de-
noting the SNR by ρ for convenience. We also denote the code
by C, with minimum Hamming distance dmin and code length
N . Let u, x and y denote the message word, the transmitted
word (after encoding and BPSK modulation) and the received
word (with Gaussian noise n ∼ N (0, σ2
n I)) respectively, see
Figure 1. Note that one always decodes the received LLR
word z, not y. Let dist(c1 , c2 ) denote the Hamming distance
between two codewords c1 and c2 . Speciﬁcally, we denote by
dH the Hamming distance between the encoded codeword c
and the decoded word ˆc. The received word will always be
decoded correctly by a hard-decision decoder if the Hamming
distance between c and y demodulated by hard-decision is
(cid:99) at most. Let T be a latent binary variable
[14], which denotes successful decoding of the N N decoder,
with a value of 1 if c = ˆc (similarly dH = 0).
At
last we denote I (X ; Y ) as the mutual
between the two random variables, X and Y .

tH = (cid:98) dmin−1
2

information

B. Training by Different Parameters
Let Γθ (S ) be a distribution over received words Y , param-
eterized by hyperparameters θ ∈ Θ set with values S . For
example, let θ be ρ and S = 1dB. Then, a training sample
is drawn (assuming the all-zero codeword is transmitted)
according to PY (y; ρ = 1). For a batch of i.i.d. training
samples, the entire sampling procedure is repeated n times,
where n is the required batch size and both θ and S may vary
in the same batch. We denote a batch sampled according to Γ
by yγ .

C. Weighted BP Decoding
The Belief Propagation (BP) is an inference algorithm used
to calculate the marginal probabilities of nodes in a graph
efﬁciently. Pearl [3] also advocated the utilization of this
algorithm for graphs with loops, along with a remark that it is
an approximation only. This version is called the loopy belief
propagation. A full derivation from the general case to linear
codes can be found in [15]. We provide main details next.
The Tanner graph is an undirected graphical model, con-
structed of nodes and edges. The nodes are of two types
- variables and checks nodes. A variable node corresponds
to a single bit of the received codeword. Each check node
corresponds to a row in the code’s parity check matrix. An

zv = log

P (cv = 0|yv )
P (cv = 1|yv )

=

2yv
σ2

n

The subscript v indicates a variable node and z stands for a
LLR (log-likelihood ratio) value. The last equality is true for
AWGN channels with common BPSK mapping to {±1}.
The message passing algorithm proceeds by iteratively
passing messages over edges from variables nodes to check
nodes and vice versa. The BP message from node a to node b
at iteration i will be denoted by mi,(a,b) with the convention
that m0,(a,b) = 0 for all a,b combinations. Variable-to-check
messages are updated in odd iterations according to the rule:

mi,(v ,h) = zv +

mi−1,(h(cid:48) ,v)

While the check-to-variable messages are updated in even
iterations by:

(cid:16) mi−1,(v (cid:48) ,h)

2

(cid:17)

(cid:48) (cid:54)=h

(cid:48)

(h

,v),h

(cid:88)
 (cid:89)
(cid:88)

(v (cid:48) ,h),v (cid:48) (cid:54)=v

,v),h

(cid:48) (cid:54)=h

mi,(h,v) = 2 arctanh

tanh

Finally, the output variable node value is calculated by:

m2τ ,(h(cid:48) ,v)

Where τ is the number of BP iterations and all values
considered are LLR values. In [1], [2], learnable weights are
assigned to the variable-check message passing rule:

mi,(v ,h) = tanh

wi,(h(cid:48) ,v ,h)mi−1,(h(cid:48) ,v)




 (2)

(1)

(cid:48)

(h

 1

ˆxv = zv +
wi,v zv +
2
w2τ +1,v zv +

(cid:88)

,v)

(cid:48)
(cid:48) (cid:54)=h

(h
h

−

(cid:88)

,v)

(cid:48)
(cid:48) (cid:54)=h

(h
h

And to the output marginalization:

ˆxv = σ

w2τ +1,(h(cid:48) ,v)m2τ ,(h(cid:48) ,v)

where σ is the sigmoid function. We denote by w =
{wi,v , wi,(h(cid:48) ,v ,h) , wi,(v ,h(cid:48) ) } the set of weights. Note that no
weights are assigned to the check-variable rule, which now
takes the form:

 (cid:89)

(v (cid:48) ,h),v (cid:48) (cid:54)=v



mi,(h,v) = 2 arctanh

mi−1,(v (cid:48) ,h)

(3)

This decision is explained by expected numerical instabili-
ties due to the arctanh domain.
This formulation unfolds the loopy algorithm into a NN.
One can see that the hyperbolic tangent function was moved
from check-variable rule to scale the message to a reasonable
output range. A sigmoid function is used to scale the LLR
values into the range [0, 1]. An output value in the range

ACTIVE DEEP DECODING OF LINEAR CODES

3

Fig. 1: System Diagram

(0.5, 1] is considered a ’1’ bit, otherwise a ’0’ bit (value of
0.5 was attributed to the ’0’ bit randomly). Training is done
with the Binary Cross Entropy (BCE) multiloss:

τ(cid:88)

N(cid:88)

t=1

v=1

L(c, ˆx) = −

1
N

[cv log ˆxv ,t + (1 − cv ) log (1 − ˆxv ,t )]

For a comprehensive explanation of the subject, please refer
to [1], [2].

I I I . DATA EX PLORAT ION

We start exploring the data with a question in mind - do all
words contribute equally to the neural training?

A. The SNR Parameter - A Motivation
We inspect how possessing the knowledge of ρ can affect
the training data and model choices.
Regarding training data, Gruber et al. [7] trains multiple
neural decoders, each decoder trained with data drawn from
Γρ (i) where −4 ≤ i ≤ 8, i ∈ Z. The N V E (ρt , ρv ) measure
is suggested in [7] to compare between the trained models.
One can notice that the model diverges when trained over
only correct or noisy words, drawn from high or low SNR,
respectively. In [10] guidelines for choosing ρt are provided.
The value is chosen so that the neural decoder’s training set
is comprised from y near the decision boundary.
Regarding model choices, a hidden assumption of [13] is
that yγ which are drawn from Γρ (S1 ) and Γρ (S2 ) (S1 (cid:54)= S2 )
require different decoder weights, w1 , w2 . One may observe
that knowledge possession of ρv is also mandatory for all
LLR-based decoders (as an estimate is required to compute
LLRs). It is quite straightforward to show that the next mutual
information inequality holds:

(a)

(b)

I (Y , ρv ; T )

= I (Y ; T ) + I (ρv ; T |Y)

≥ I (Y ; T )

where (a) follows from the mutual information chain rule,
and (b) follows from the non-negativity of mutual information.
Thus, the additional information of ρv can only aid decoding.
This information of the channel and the decoder distributions,
conditioned on the received word, may be non-zero for sub-
optimal decoders. In [13], inference does not only require ρv
knowledge, but is also ρv dependent. In other words - the
model is data dependent.

B. Objective Formulation
Motivated by the above discussion, our main goal is to ﬁnd
parameters other than the SNR, which deﬁne a new Γ, Γnew .
We want that training the WBP over Γnew will achieve as high
decoding performance as possible.
Let κ denote the contribution of a word, in the training
phase, to the validation decoding performance. We associate
higher contribution words with higher κ value. Our goal is to
ﬁnd parameters θ ∈ Θ and corresponding values S deﬁning
words distribution Γθ (S ) such that the κ value integrated over
the distribution is maximized:

(cid:90)

arg max

κ(y)

θ,S

y∈Γθ (S )

κ(y).

y∈Γθ (S )

The solution to this equation is intractable due to the inﬁnite
number of such parameters and values, thus we seek heuristic-
based solution. We choose the parameters based on the vast
decoding knowledge while using the above insights. In partic-
ular, yγ should be neither too noisy nor absolutely correct
and should lie close to the decision boundary. Recall that
throughout the paper we use the AWGN channel. Therefore,
we search for parameters θ (cid:48) which limit the feasible yγ of
denote Kθ (S ) = (cid:82)
the channel distribution Γρ (S ), associated with Kρ (S ), to
Γρ,θ(cid:48) (S, A), associated with higher Kρ,θ(cid:48) (S, A), where we
C. Distance Parameter
Some received words are undecodable due to the locality
of the decoding algorithm, the Tanner graph structure induced
by the parity-check matrix or a high Hamming distance. By
sampling from speciﬁc Γρ,dH (S, A) one can easily control
the number of erroneous bits in y. Choosing such words
with a reasonable Hamming distance between them and the
transmitted words decreases the amount of undecodable words
in Γ.
To justify the above claims we trained a WBP decoder
without any correct received words, dH =0, and without high
noise words, dH > tH . The training setup is similar to the
one used in section V. The results show an improvement of
up to 0.5dB by sampling according to this simple scheme,
conﬁrming our intuitions. By drawing data according to a
distribution, and not according to the SNR, we have further
control on training words properties. We elaborate more on
this subject in IV-A.

EncoderBPSKmodulator+n×2σ2nDecoderHard-DecisionucxyzˆxˆcTransmitterReceiver4

ACCEPTED TO IEEE TRANSACTIONS ON COMMUNICATIONS

With this short experiment we manage to answer the ques-
tion we set to ask - do all words contribute equally to the
training? A deﬁnitive answer is no.

D. Reliability Parameters
Soft in soft out (SISO) decoding compose the received sig-
nal to n LLR values, {z1 , . . . , zn}. In general zv ∈ (−∞, ∞)
but in practice we limit their value by choosing appropriate
threshold. The closer the zv to 0, the less reliable it is. We
consider mapping the LLR values to bits in two steps. First
mapping LLR values to probabilities:

The next rule maps probability into corresponding bit:

ΠLLR→P r (zi ) = σ(−zi )
ΠP r→bit ((cid:101)zi ) =
if (cid:101)zi > 0.5

(cid:40)

1,

0, otherwise
The process of direct quantization from LLR to bits is called
hard decision (HD) decoding:

ΠHD (zi ) = ΠP r→bit (ΠLLR→P r (zi ))

(4)

Obviously there is information loss in the process:

ΠHD (z1 ) = ΠHD (z2 ) (cid:59) z1 = z2

We seek numeric parameters which quantify reliability of a
given z. Two parameters that we inspected and found ﬁtting
to the task are deﬁned below:
Average Bit Probability - the deviation of the channel
output probabilities from the corresponding transmitted bits:

ηABP (ci , zi ) =

1
N

|ci − ΠLLR→P r (zi )|

(5)

Mean Bit Cross Entropy - this parameter quantiﬁes how
close are the two probability distributions at the transmitter
and at the receiver (before decoding):

N(cid:88)

i=1

N(cid:88)

(cid:96)M BCE (ci , zi ) =

1
|ci · log(ΠLLR→P r (zi ))+
N
+(1 − ci ) · log(1 − ΠLLR→P r (zi ))|.

i=1

(6)

By limiting the distribution to Γρ,ηABP ,(cid:96)M BCE (S, A1 , A2 ),
we have a better control of the distribution of y, and con-
sequently of z, such that yγ has higher κ on average. The
intuition guiding us, again, is that higher κ words lie close to
the decision boundaries. Referring to III-B, we need to choose

A1 , A2 such that Kρ,ηABP ,(cid:96)M BCE (S, A1 , A2 ) is maximized.

E. Correlation with SNR
Figures 2 and 3 show the correlation of the above parameters
to ρ and T . In both ﬁgures 100,000 codewords were simulated
per ρ on code with length of 63 bits. Regarding Figure 2, one
can see that each ρ deﬁnes a different probability distribution
of dH values. This ﬁgure is unique for each code length and
simulated ρ. The higher the SNR - the lower the dH center of
this probability distribution. High ρ includes high amount of
no errors frames, while low ρ value induces lots of high noise

Fig. 2: Distribution of dH by ρ

Fig. 3: Distribution of reliability by ρ

received words with dH higher than tH . Both tH values for
the two codes BCH(63,36) and BCH(63,45) are also plotted
on this ﬁgure. Figure 3 represents similar notion in regard
to reliability. Each ρ deﬁnes a probability distribution over
the two parameters so that the higher the ρ is, the closer the
distribution is to the origin. Here we do not have a deﬁned
threshold for correct and highly incorrect words, y, as before,
thus we must sample from this probability distribution much
more carefully.
One thing we ignored so far is the evolution of the decoder
during training. Obviously, as the decoder trains its’ decision
regions are altered - changing the optimal θ , S to sample by.
In order to train the decoder with y close to the decision
boundaries at every stage, the distribution Γθ (S ) we draw from
must change actively during the training. A known method in
machine-learning ﬁeld for doing so is called - active learning.

ACTIVE DEEP DECODING OF LINEAR CODES

5

Algorithm 1: Stream-based sampling by Hamming dis-
tance

Initialization: DEC as in [1]
Input

: current decoder DEC
S = {s1 , . . . ., sn} set of SNR values
c encoded word
: improved model DEC

A = {1, . . . ., dmax } set of dH values

Output
1 SampleByDistance (DEC, S, A, c)

while error decreases do
sample batch Q from Γρ,dH (S, A);

for y in Q do

din ← dist(ΠHD (y), c);
dout ← dist(ˆc, c);
if dout = 0 or dout ≥ din then

end

Q ← Q \ y;
DEC ← update model based on Q;

end
return DEC ;

Algorithm 2: Stream-based sampling by reliability param-
eters

Initialization: DEC as in [1]
Input

: current decoder DEC
S = {s1 , . . . ., sn} set of SNR values
b desired batch size
c encoded word
: improved model DEC

Output
1 SampleByReliability (DEC, S, m, c)

µ, Σ ← ChoosePrior(S ,c);
while error decreases do
sample batch Q from Γρ (S );
ηABP ← calculate by equation (5) per word;
(cid:96)M BCE ← calculate by equation (6) per word;

θ ← [ηABP , (cid:96)M BCE ];
w ← f (θ |µ, Σ);

˜w ← w/||w||1 ;
˜Q ← random sampling b words from Q w.p ˜w ;
DEC ← update the model based on ˜Q;

end
return DEC ;

IV. AC T IV E L EARN ING

Active learning is a supervised learning method, which deals
with an oracle that actively chooses the samples from a large
pool of unlabelled data to feed the model. The oracle can
be human annotator or a machine based one. Two important
questions regarding this process are ”why is active learning
used” and ”how is a batch queried”. The solution to the former
question is straightforward - the reason for using this method
is the queried batch is assumed to beneﬁt the training of the
model more than using a random training batch, on average.
The solution to the second question is achieved by introducing
a metric which shows informativeness. At each training step,
the batch with the highest metric value is considered the most
informative thus it is queried. It is widely used in medical
systems and in situations when annotating data is expensive,
thus training data must be chosen punctiliously. For additional
information on active learning see [16], [17].
In a stream based approach batches are generated one by
one. A selective sampling approach is one in which the data
to be queried is selected based on some metric. An underlying
assumption in active learning stream-based selective sampling
approach is that data is free to obtain. In our error-correcting
codes domain data is unlimited when the channel model is
known (as AWGN) or can be fairly easily collected when
channel model is unknown, and we do not need to annotate it
by hand. This is a huge advantage and a strong claim in favor
of using this method in decoding. Traditionally, this method
is used on unlabeled stream or pool of data. In this case one
would want to choose which samples are worth labeling and
training. In our case all samples are labeled. Therefore, our
goal is to perform the training procedure with the highest κ
for the received words.
We hereby present the main two active learning approaches
taken.

A. Stream-Based Selective Sampling by Hamming Distance
The ﬁrst approach is presented in Algorithm 1, where at
each time step, the current neural model (line 6) determines
the next queried batch (line 8) for the model update (line
10). This algorithm is based on intuitions from Subsection
III-C, remove successfully decoded y in addition to very noisy
y from training (lines 7-8). These received words are far
from the decision boundary thus harm training. Why these
y can harm the training can also be explained from the
learning signal perspective. On one hand, the real signal is
nearly impossible to be recovered from a very noisy y, thus
the learning signal towards a minima is very low. On the
other hand, for very reliable y, the learning signal is low,
since for every direction of decision the model takes these
reliable words will be decoded successfully. Thus, they are
not informative for learning.

B. Stream-Based Selective Sampling by Reliability Parameters
The second approach we present exploits the reliability
of a given y, see Algorithm 2. Inspired by the common
uncertainty sampling query framework, we ﬁrst calculate

2
3
4
5
6
7
8
9
10
11
12

2
3
4
5
6
7
8
9
10
11
12
13

Γρ,ηABP ,(cid:96)M BCE (S, A1 , A2 ) for several untrained BP decoders
with different number of iterations τset = {τ1 , . . . , τr } em-
pirically. We chose to query each batch by setting a prior on
ηABP , (cid:96)M BCE . We elaborate on the prior and batch selections.
Firstly, the prior was chosen as a Normal distribution with
expectation, µ, and covariance matrix, Σ, over y that are
decodable by adding iterations to the standard BP decoder.
The prior selection is summarised in Algorithm 3. These y
are assumed to be close to the decision boundaries, since BP
decoders with additional iterations are able to decode them. We
want the WBP to compensate for these additional iterations by
training. Secondly, in Algorithm 2, the batch was queried by

6

ACCEPTED TO IEEE TRANSACTIONS ON COMMUNICATIONS

Algorithm 3: Choose prior
Input

Data

: S = {s1 , . . . ., sn} set of SNR values
c encoded word
: τset = {τ1 , . . . , τr } set of iterations
Output: prior distribution parameters µ,Σ

set = {1 , . . . , r+1} set of colors

1 ChoosePrior (S, c)

sample batch Q from Γρ (S );
ηABP ← calculate by eq. (5);
(cid:96)M BCE ← calculate by eq. (6);
for τi in τset do
DEC ← construct BP with τi iterations;

for y in Q do

d ← dist(ˆc, c);

if d = 0 then

else

2
3
4
5
6
7
8
9
10
11
12
13
14
15
16

Plot point (ηABP ,(cid:96)M BCE ) in color i ;

Plot point (ηABP ,(cid:96)M BCE ) in color i+1 ;

end
end

µ, Σ ← set empirically on decodable words;

return µ, Σ;

performing a few trivial steps (lines 2-9). The last step (line
10) includes random sampling of a given size batch by the
normalized weights as the probabilities, without replacement.
One important note is that the uncertainty sampling method
is usually performed over the neural model output signal,
while here we use it over the input signal. That is because
the multiple BP decoders are the baseline for improvement,
not the weighted decoder.

V. EX PER IM EN T S AND R E SU LT S

We present the results of training and applying the ap-
proaches mentioned in IV for three different
linear codes
BCH(63,45), BCH(63,36), BCH(127,64) with tH = 3, tH = 5
and tH = 10, respectively. We use the cycle-reduced (CR)
parity-check matrices as appear in [18], thus evaluating our
method when the number of short cycles is already small and
improvement by altering weights is harder to achieve. The
number of iterations is chosen as 5 as in [1], [2], [4]–[6], [9],
who set a benchmark in the ﬁeld. The zero codeword is used
for training, due to symmetry, as in [1], [2]. It also serves
as the codeword in Algorithms 1 and 2. All other training
relevant hyperparameters are summarised in Table I. All WBP
decoders are trained until convergence. We apply two methods
- Hamming distance based and reliability based. Regarding the
active learning hyperparameters, for the distance approach, and
in order to stay consistent, we chose the same dmax for the
two short codes. All hyperparameters are summarised in Table
II. As a follow-up to section III-C, we also apply dH ﬁltering
to the reliability method. This is referred as the reliability &
dH ﬁltering in Table II.
We simulate the WBP over a validation set of 1dB to
10dB until at
least 1000 errors are accumulated at each
given point. In addition, we adopt the syndrome based early

TABLE I: Training Hyperparameters

Hyperparameters

Values

Architecture
Initialization
Loss Function
Optimizer
ρt range
Learning Rate
Batch Size
Messages Range

Feed Forward
as in [1] (*)
BCE with Multiloss
RMSPROP
4dB to 7dB
0.01
1250 / 300 words per SNR (**)

(−10, 10)

(*) wi,v in eqs. (1) and (2) set to constant 1 since no additional
improvement was observed.
(**) for the 63 / 127 code length, respectively.

termination, as we saw that some correctly decoded codewords
were misclassiﬁed again by the following layers. This can also
beneﬁt complexity since the average number of iterations is
less than or equal to 5 when using this rule.
Results for the simulation are presented in Figure 4. One can
see that both distance-based and reliability-based approaches
outperform the original BP-FF model with hyperparameters
as in [1], [2]. We separate the contribution of our methods to
two different regions. At the waterfall region the improvement
varies from 0.25dB to 0.4dB in FER and 0.2dB to 0.3dB in
BER for the different codes. At the error-ﬂoor region, the gain
is increased by 0.75dB to 1.5dB in FER and by 0.75 to 1dB
in BER for all codes. The best decoding gains per code are
summarized in Table III. The measured error value, where
the gain is observed, is speciﬁed in parentheses. Comparing
to [13] in the BER graphs, a gain of 0.25dB is achieved in
the CR-BCH(63,36) code, while in CR-BCH(127,64) one can
observe similar performance. Furthermore, the difference in
gains between the reliability curve and the reliability & dH
ﬁltering curve indicates that the two methods indeed train on
different distributions of words.
The FER metric is observed to gain the most from all
approaches, with the reliability & dH ﬁltering approach having
the best performance. One conjecture is that all these methods
are optimized to improve FER directly. For the Hamming
distance approach, lowering the number of errors in a single
codeword reﬂects the FER directly. The reliability parameters
are taken as a mean over the received words, thus adding
more information on each y rather than on each single bit,
yi . One can see that all methods achieve better performance
while keeping the same decoding complexity as before in [1],
[2]. This is achieved solely by smartly sampling the data to
train the neural decoder.

V I . CONCLU S ION S

In this paper we proposed two novel sampling methods,
incorporating error decoding measures with methodologies
from the vast machine-learning ﬁeld. Increases in performance
of up to 0.4dB at the waterfall region, and of up to 1.5dB
at the error-ﬂoor region, compared to the original WBP, are
possible with no decoding complexity penalty, only by smartly
sampling the training set. Furthermore, note that an aggregated

ACTIVE DEEP DECODING OF LINEAR CODES

7

TABLE II: Active Learning Hyperparameters

Method

Hyperparameters CR-BCH N=63 CR-BCH N=127

4

5

2

3

(cid:21)

(cid:20)6.25 · 10−4
{5, 7, 10, 15}
(0.025, 0.1)
(0.03, 0.1)
0
5.625 · 10−3
0
(cid:20)6.25 · 10−4
{5, 7, 10, 15}
(0.025, 0.1)
(0.03, 0.1)
0
5.625 · 10−3
0

(cid:21)

Hamming distance

Reliability

Reliability & dH ﬁltering

dmax
τset

µ

Σ

dmax
τset

µ

Σ

(a) CR-BCH(63,36)

(b) CR-BCH(63,45)

8

ACCEPTED TO IEEE TRANSACTIONS ON COMMUNICATIONS

Fig. 4: BER and FER comparison: (•) Regular BP, ((cid:4)) BP-FF [1], (×) Hamming distance approach, ((cid:7)) Reliability approach,
((cid:78)) Reliability approach with ﬁlter by dH

(c) CR-BCH(127,64)

TABLE III: Best Decoding Gains (*)(**)

Region

Code

CR-BCH(63,36)
CR-BCH(63,45)
CR-BCH(127,64)

Waterfall

Error-ﬂoor

BER[dB]

0.2 (10−5 )
0.2 (10−5 )
0.3 (10−4 )

FER[dB]

0.25 (10−3 )
0.25 (10−4 )
0.4 (10−3 )

BER[dB]

1 (4 · 10−7 )
0.75 (2 · 10−7 )
0.75 (10−6 )

FER[dB]

1.5 (10−5 )
0.75 (3 · 10−6 )
1.25 (10−4 )

(*) all gains are WRT the original WBP [1],[2].
(**) The measured error value, where the gain is observed, is speciﬁed in parentheses.

increase in gain of about 2dB in high SNR, compared to
BP, is achieved. We provided general guidelines for choosing
training data in communications, starting in data exploration,
validating assumptions by experiments and ﬁnally developing
active learning based algorithms. We highlighted that SNR
does not reveal the whole story. By introducing other key
parameters one can have more control over the training data.
Our conjecture is that sampling close to the decision boundary
is crucial. At
last, we urge the readers to seek sampling
schemes in their communication application.
As for the next step, one may aim to ﬁnd new ways of in-
corporating important parameters in training and validation for
improved results. Likewise, one may explore a reinforcement
learning algorithm which ﬁnds the optimal parameters during
training with no conjectures whatsoever. Another direction
is applying the proposed methods into the mRRD decoder
[19], [20] for approaching maximum-likelihood performance
with further complexity reduction. Lastly, further analysis of
dropped training samples could enhance explainability and
provide insights about the proposed methods.

ACKNOW L EDGM ENT

We would like to thank Eran Asa for the insightful discus-
sions. We also thank the reviewers and the editor for their
beneﬁcial comments.

R E F ER ENC E S
[1] E. Nachmani, Y. Be’ery, and D. Burshtein, “Learning to decode linear
codes using deep learning,” in IEEE Annual Allerton Conference on
Communication, Control, and Computing (Allerton), Monticello, IL.,
USA, 2016, pp. 341–346.
[2] E. Nachmani, E. Marciano, L. Lugosch, W. J. Gross, D. Burshtein, and
Y. Be’ery, “Deep learning methods for improved decoding of linear
codes,” IEEE Journal of Selected Topics in Signal Processing, vol. 12,
no. 1, pp. 119–131, Feb. 2018.
[3] J. Pearl, Probabilistic reasoning in intelligent systems: networks of
plausible inference. Elsevier, 2014.
[4] M. Lian, C. H ¨ager, and H. D. Pﬁster, “What can machine learning teach
us about communications?” in IEEE Information Theory Workshop (ITW),
Guangzhou, China, 2018, pp. 410–414.
[5] W. Xu, Z. Wu, Y.-L. Ueng, X. You, and C. Zhang, “Improved polar
decoder based on deep learning,” in IEEE International Workshop on
Signal Processing Systems (SiPS), Lorient, France, Oct. 2017, pp. 1–6.
[6] W. Xu, X. You, C. Zhang, and Y. Be’ery, “Polar decoding on sparse
graphs with deep learning,” in IEEE Asilomar Conference on Signals,
Systems, and Computers, Paciﬁc Grove, CA, USA, 2018, pp. 599–603.

ACTIVE DEEP DECODING OF LINEAR CODES

9

[7] T. Gruber, S. Cammerer, J. Hoydis, and S. ten Brink, “On deep learning-
based channel decoding,” in IEEE Annual Conference on Information
Sciences and Systems (CISS), Baltimore, MD, USA, 2017, pp. 1–6.
[8] V. Nair and G. E. Hinton, “Rectiﬁed linear units improve restricted boltz-
mann machines,” in 27th international conference on machine learning
(ICML), Haifa, Israel, 2010, pp. 807–814.
[9] A. Bennatan, Y. Choukroun, and P. Kisilev, “Deep learning for decoding
of linear codes-a syndrome-based approach,” in IEEE International
Symposium on Information Theory (ISIT), Vail, CO, USA, 2018, pp.
1595–1599.
[10] H. Kim, Y. Jiang, R. Rana, S. Kannan, S. Oh, and P. Viswanath,
“Communication
algorithms
via
deep
learning,”
arXiv
preprint
arXiv:1805.09317, 2018.
[11] Y. Jiang, H. Kim, H. Asnani, S. Kannan, S. Oh, and P. Viswanath,
“Deepturbo: Deep turbo decoder,” arXiv preprint arXiv:1903.02295,
2019.
[12] M. Benammar and P. Piantanida, “Optimal training channel statistics
for neural-based decoders,” in IEEE Asilomar Conference on Signals,
Systems, and Computers, Paciﬁc Grove, CA, USA, 2018, pp. 2157–2161.
[13] M. Lian, F. Carpi, C. H ¨ager, and H. D. Pﬁster, “Learned belief-
propagation decoding with simple scaling and snr adaptation,” arXiv
preprint arXiv:1901.08621, 2019.
[14] I. Goodfellow, Y. Bengio, and A. Courville, Deep learning. MIT press,
2016.
[15] T. Richardson and R. Urbanke, Modern coding theory.
university press, 2008.
[16] B. Settles, “Active learning literature survey,” University of Wisconsin-
Madison Department of Computer Sciences, Tech. Rep., 2009.
[17] Y. Fu, X. Zhu, and B. Li, “A survey on instance selection for active
learning,” Knowledge and information systems, vol. 35, no. 2, pp. 249–
283, 2013.
[18] M. Helmling, S. Scholl, F. Gensheimer, T. Dietz, K. Kraft, S. Ruzika,
and N. Wehn, “Database of channel codes and ml simulation results,”
www.uni- kl.de/channel- codes, 2019.
[19] E. Nachmani, Y. Bachar, E. Marciano, D. Burshtein, and Y. Be’ery,
“Near maximum likelihood decoding with deep learning,” in Interna-
tional Zurich Seminar on Information and Communication (IZS), Zurich,
Switzerland, 2018, pp. 40–44.
[20] I. Dimnik and Y. Be’ery, “Improved random redundant iterative hdpc
decoding,” IEEE Transactions on Communications, vol. 57, no. 7, pp.
1982–1985, 2009.

Cambridge

